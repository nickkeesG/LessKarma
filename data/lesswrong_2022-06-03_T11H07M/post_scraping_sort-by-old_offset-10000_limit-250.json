{"results": [{"createdAt": null, "postedAt": "2013-12-02T16:53:35.360Z", "modifiedAt": null, "url": null, "title": "A critique of effective altruism", "slug": "a-critique-of-effective-altruism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:08.300Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "benkuhn", "user": {"username": "benkuhn", "createdAt": "2013-05-28T21:13:11.124Z", "isAdmin": false, "displayName": "benkuhn"}, "userId": "cdndD4NnRf6ud2hL5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E3beR7bQ723kkNHpA/a-critique-of-effective-altruism", "pageUrlRelative": "/posts/E3beR7bQ723kkNHpA/a-critique-of-effective-altruism", "linkUrl": "https://www.lesswrong.com/posts/E3beR7bQ723kkNHpA/a-critique-of-effective-altruism", "postedAtFormatted": "Monday, December 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20critique%20of%20effective%20altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20critique%20of%20effective%20altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE3beR7bQ723kkNHpA%2Fa-critique-of-effective-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20critique%20of%20effective%20altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE3beR7bQ723kkNHpA%2Fa-critique-of-effective-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE3beR7bQ723kkNHpA%2Fa-critique-of-effective-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3568, "htmlBody": "<p>I recently ran across Nick Bostrom&rsquo;s idea of subjecting your strongest beliefs to a <a href=\"http://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html\">hypothetical apostasy</a> in which you try to muster the strongest arguments you can against them. As you might have figured out, I believe strongly in <a href=\"http://en.wikipedia.org/wiki/Effective_altruism\">effective altruism</a>&mdash;the idea of applying evidence and reason to finding the best ways to improve the world. As such, I thought it would be productive to write a hypothetical apostasy on the effective altruism movement.</p>\n<p>(EDIT: As per the comments of <a href=\"/lw/j8n/a_critique_of_effective_altruism/a4z1\">Vaniver</a>, <a href=\"/lw/j8n/a_critique_of_effective_altruism/a4zb\">Carl Shulman</a>, and others, this didn't quite come out as a hypothetical apostasy. I originally wrote it with that in mind, but decided that a focus on more plausible, more moderate criticisms would be more productive.)</p>\n<h2 id=\"how-to-read-this-post\">How to read this post</h2>\n<p>(EDIT: the following two paragraphs were written before I softened the tone of the piece. They're less relevant to the more moderate version that I actually published.)</p>\n<p>Hopefully this is clear, but as a disclaimer: this piece is written in a fairly critical tone. This was part of an attempt to get &ldquo;in character&rdquo;. <em>This tone does not indicate my current mental state with regard to the effective altruism movement.</em> I agree, to varying extents, with some of the critiques I present here, but I&rsquo;m not about to give up on effective altruism or stop cooperating with the EA movement. The apostasy is purely hypothetical.</p>\n<p>Also, because of the nature of a hypothetical apostasy, I&rsquo;d guess that for effective altruist readers, the critical tone of this piece may be especially likely to trigger defensive rationalization. Please read through with this in mind. (A good way to counteract this effect might be, for instance, to imagine that you&rsquo;re not an effective altruist, but your friend is, and it&rsquo;s them reading through it: how should they update their beliefs?)</p>\n<p>(End less relevant paragraphs.)</p>\n<p>Finally, if you&rsquo;ve never heard of effective altruism before, I don&rsquo;t recommend making this piece your first impression of it! You&rsquo;re going to get a very skewed view because I don&rsquo;t bother to mention all the things that are awesome about the EA movement.</p>\n<h2 id=\"abstract\">Abstract</h2>\n<p>Effective altruism is, to my knowledge, the first time that a substantially useful set of ethics and frameworks to analyze one&rsquo;s effect on the world has gained a broad enough appeal to resemble a social movement. (I&rsquo;d say these principles are something like <em>altruism</em>, <em>maximization</em>, <em>egalitarianism</em>, and <em>consequentialism</em>; together they imply many improvements over the social default for trying to do good in the world&mdash;earning to give as opposed to doing direct charity work, working in the developing world rather than locally, using evidence and feedback to analyze effectiveness, etc.) Unfortunately, as a movement effective altruism is failing to use these principles to acquire correct nontrivial beliefs about how to improve the world.</p>\n<p>By way of clarification, consider a distinction between two senses of the word &ldquo;trying&rdquo; I used above. Let&rsquo;s call them &ldquo;actually trying&rdquo; and &ldquo;pretending to try&rdquo;. Pretending to try to improve the world is something like responding to social pressure to improve the world by querying your brain for a thing which improves the world, taking the first search result and rolling with it. For example, for a while I thought that I would try to improve the world by developing computerized methods of checking informally-written proofs, thus allowing more scalable teaching of higher math, democratizing education, etc. <em>Coincidentally</em>, computer programming and higher math happened to be the two things that I was best at. This is pretending to try. Actually trying is looking at the things that improve the world, figuring out which one maximizes utility, and then doing that thing. For instance, I now run an effective altruist student organization at Harvard because I realized that even though I&rsquo;m a comparatively bad leader and don&rsquo;t enjoy it very much, it&rsquo;s still very high-impact if I work hard enough at it. This isn&rsquo;t to say that I&rsquo;m actually trying yet, but I&rsquo;ve gotten closer.</p>\n<p>Using this distinction between pretending and actually trying, I would summarize a lot of effective altruism as &ldquo;pretending to actually try&rdquo;. As a social group, effective altruists have successfully noticed the pretending/actually-trying distinction. But they seem to have stopped there, assuming that knowing the difference between fake trying and actually trying translates into ability to actually try. Empirically, it most certainly doesn&rsquo;t. A lot of effective altruists still end up satisficing&mdash;finding actions that are on their face acceptable under core EA standards and then picking those which seem appealing because of other essentially random factors. This is more likely to converge on good actions than what society does by default, because the principles are better than society&rsquo;s default principles. Nevertheless, it fails to make much progress over what is directly obvious from the core EA principles. As a result, although &ldquo;doing effective altruism&rdquo; feels like truth-seeking, it often ends up being just a more credible way to pretend to try.<a id=\"more\"></a></p>\n<p>Below I introduce various ways in which effective altruists have failed to go beyond the social-satisficing algorithm of establishing some credibly acceptable alternatives and then picking among them based on essentially random preferences. I exhibit other areas where the norms of effective altruism fail to guard against motivated cognition. Both of these phenomena add what I call &ldquo;epistemic inertia&rdquo; to the effective-altruist consensus: effective altruists become more subject to pressures on their beliefs other than those from a truth-seeking process, meaning that the EA consensus becomes less able to update on new evidence or arguments and preventing the movement from moving forward. I argue that this stems from effective altruists&rsquo; reluctance to think through issues of the form &ldquo;being a successful social movement&rdquo; rather than &ldquo;correctly applying utilitarianism individually&rdquo;. This could potentially be solved by introducing an additional principle of effective altruism&mdash;e.g. &ldquo;group self-awareness&rdquo;&mdash;but it may be too late to add new things to effective altruism&rsquo;s DNA.</p>\n<h2 id=\"philosophical-difficulties\">Philosophical difficulties</h2>\n<p>There is <a href=\"http://www.overcomingbias.com/2008/01/protecting-acro.html\">currently</a> <a href=\"http://www.amirrorclear.net/academic/ideas/negative-utilitarianism/\">wide</a> <a href=\"http://www.utilitarian-essays.com/hedonistic-vs-preference.html\">disagreement</a> <a href=\"http://www.utilitarian-essays.com/suffering-nature.html\">among</a> <a href=\"/lw/91/average_utilitarianism_must_be_correct/\">effective</a> <a href=\"http://rationalaltruist.com/2013/06/03/my-outlook/comment-page-1/#comment-445\">altruists</a> on the correct framework for population ethics. This is crucially important for determining the best way to improve the world: different population ethics can lead to drastically different choices (or at least so we would expect <em>a priori</em>), and if the EA movement can&rsquo;t converge on at least their instrumental goals, it will quickly fragment and lose its power. Yet there has been little progress towards discovering the correct population ethics (or, from a moral anti-realist standpoint, constructing arguments that will lead to convergence on a particular population ethics), or even determining which ethics lead to which interventions being better.</p>\n<h2 id=\"poor-cause-choices\">Poor cause choices</h2>\n<p>Many effective altruists donate to <a href=\"http://www.givewell.org/\">GiveWell&rsquo;s</a> top charities. All three of these charities work in global health. Is that because GiveWell knows that global health is the highest-leverage cause? No. It&rsquo;s because it was the only one with enough data to say anything very useful about. There&rsquo;s little reason to suppose that this correlates with being particularly high-leverage&mdash;on the contrary, heuristic but less rigorous arguments for causes like <a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\">existential risk prevention</a>, <a href=\"http://www.everydayutilitarian.com/essays/how-much-does-it-cost-to-buy-a-vegetarian/\">vegetarian advocacy</a> and <a href=\"http://www.facebook.com/l.php?u=http%3A%2F%2Fopenborders.info%2F&amp;h=aAQGxloz1\">open borders</a> suggest that these could be even more efficient.</p>\n<p>Furthermore, the our current &ldquo;best known intervention&rdquo; is likely to change (in a more cost-effective direction) in the future. There are two competing effects here: we might discover better interventions to donate to than the ones we currently think are best, but we also might run out of opportunities for the current best known intervention, and have to switch to the second. So far we seem to be in a regime where the first effect dominates, and there&rsquo;s no evidence that we&rsquo;ll reach a tipping point very soon, especially given how new the field of effective charity research is.</p>\n<p>Given these considerations, it&rsquo;s quite surprising that effective altruists are donating to global health causes now. Even for those looking to use their donations to set an example, a donor-advised fund would have many of the benefits and none of the downsides. And anyway, donating when you believe it&rsquo;s not (except for example-setting) the best possible course of action, in order to make a point about figuring out the best possible course of action and then doing that thing, seems perverse.</p>\n<h2 id=\"non-obviousness\">Non-obviousness</h2>\n<p>Effective altruists often express surprise that the idea of effective altruism only came about so recently. For instance, my student group recently hosted Elie Hassenfeld for a talk in which he made remarks to that effect, and I&rsquo;ve heard other people working for EA organizations express the same sentiment. But no one seems to be actually worried about this&mdash;just smug that they&rsquo;ve figured out something that no one else had.</p>\n<p>The &ldquo;market&rdquo; for ideas is at least somewhat efficient: most simple, obvious and correct things get thought of fairly quickly after it&rsquo;s possible to think them. If a meme as simple as effective altruism hasn&rsquo;t taken root yet, we should at least try to understand why before throwing our weight behind it. The absence of such attempts&mdash;in other words, the fact that non-obviousness doesn&rsquo;t make effective altruists worried that they&rsquo;re missing something&mdash;is a strong indicator against the &ldquo;effective altruists are actually trying&rdquo; hypothesis.</p>\n<h2 id=\"efficient-markets-for-giving\">Efficient markets for giving</h2>\n<p>It&rsquo;s often claimed that &ldquo;nonprofits are not a market for doing good; they&rsquo;re a market for warm fuzzies&rdquo;. This is used as justification for why it&rsquo;s possible to do immense amounts of good by donating. However, while it&rsquo;s certainly true that most donors aren&rsquo;t explicitly trying to purchase utililty, there&rsquo;s still a lot of money that is.</p>\n<p>The <a href=\"http://www.gatesfoundation.org/\">Gates Foundation</a> is an example of such an organization. They&rsquo;re effectiveness-minded and with $60 billion behind them. 80,000 Hours has already <a href=\"http://80000hours.org/blog/239-show-me-the-harm\">noted</a> that they&rsquo;ve probably saved over 6 million lives with their vaccine programs alone&mdash;given that they&rsquo;ve spent a relatively small part of their endowment, they must be getting a much better exchange rate than our current best guesses.</p>\n<p>So why not just donate to the Gates Foundation? Effective altruists need a better account of the &ldquo;market inefficiencies&rdquo; that they&rsquo;re exploiting that Gates isn&rsquo;t. Why didn&rsquo;t the Gates Foundation fund the <a href=\"https://www.againstmalaria.com/\">Against Malaria Foundation</a>, GiveWell&rsquo;s top charity, when it&rsquo;s in one of their main research areas? It seems implausible that the answer is simple incompetence or the like.</p>\n<p>A general rule of markets is that if you don&rsquo;t know what your edge is, you&rsquo;re the sucker. Many effective altruists, when asked what their edge is, give some answer along the lines of &ldquo;actually being strategic/thinking about utility/caring about results&rdquo;, and stop thinking there. This isn&rsquo;t a compelling case: as mentioned before, it&rsquo;s not clear why no one else is doing these things.</p>\n<h2 id=\"inconsistent-attitude-towards-rigor\">Inconsistent attitude towards rigor</h2>\n<p>Effective altruists insist on extraordinary rigor in their charity recommendations&mdash;cf. for instance GiveWell&rsquo;s work. Yet for many ancillary problems&mdash;donating now vs. later, choosing a career, and deciding how &ldquo;meta&rdquo; to go (between direct work, earning to give, doing advocacy, and donating to advocacy), to name a few&mdash;they seem happy to choose between the not-obviously-wrong alternatives based on intuition and gut feelings.</p>\n<h2 id=\"poor-psychological-understanding\">Poor psychological understanding</h2>\n<p>John Sturm suggests, and I agree, that many of these issues are psychological in nature:</p>\n<blockquote>\n<p>I think a lot of these problems take root a commitment level issue:</p>\n<p>I, for instance, am thrilled about changing my mentality towards charity, not my mentality towards having kids. My first guess is that - from an EA and overall ethical perspective - it would be a big mistake for me to have kids (even after taking into account the normal EA excuses about doing things for myself). At least right now, though, I just don&rsquo;t care that I&rsquo;m ignoring my ethics and EA; I want to have kids and that&rsquo;s that.</p>\n<p>This is a case in which I&rsquo;m not &ldquo;being lazy&rdquo; so much as just not trying at all. But when someone asks me about it, it&rsquo;s easier for me to give some EA excuse (like that having kids will make me happier and more productive) that I don&rsquo;t think is true - and then I look like I&rsquo;m being a lazy or careless altruist rather than not being one at all.</p>\n<p>The model I&rsquo;m building is this: there are many different areas in life where I could apply EA. In some of them, I&rsquo;m wholeheartedly willing. In some of them, I&rsquo;m not willing at all. Then there are two kinds of areas where it looks like I&rsquo;m being a lazy EA: those where I&rsquo;m willing and want to be a better EA&hellip; and those where I&rsquo;m not willing but I&rsquo;m just pretending (to myself or others or both).</p>\n<p>The point of this: when we ask someone to be a less lazy EA, we are (1) helping them do a better job at something they want to do, and (2) trying to make them either do more than they want to or admit they are &ldquo;bad&rdquo;.</p>\n</blockquote>\n<p>In general, most effective altruists respond to deep conflicts between effective altruism and other goals in one of the following ways:</p>\n<ol>\n<li>Unconsciously resolve the cognitive dissonance with motivated reasoning: &ldquo;it&rsquo;s clearly my comparative advantage to spread effective altruism through poetry!&rdquo;</li>\n<li>Deliberately and knowingly use motivated reasoning: &ldquo;dear Facebook group, what are the best utilitarian arguments in favor of becoming an EA poet?&rdquo;</li>\n<li>Take the easiest &ldquo;honest&rdquo; way out: &ldquo;I wouldn&rsquo;t be psychologically able to do effective altruism if it forced me to go into finance instead of writing poetry, so I&rsquo;ll become an effective altruist poet instead&rdquo;.</li>\n</ol>\n<p>The third is debatably defensible&mdash;though, for a community that purports to put stock in rationality and self-improvement, effective altruists have shown surprisingly little interest in self-modification to have more altruistic intentions. This seems obviously worthy of further work.</p>\n<p>Furthermore, EA norms do not proscribe even the first two, leading to a group norm that doesn&rsquo;t cause people to notice when they&rsquo;re engaging in a certain amount of motivated cognition. This is quite toxic to the movement&rsquo;s ability to converge on the truth. (As before, effective altruists are still better than the general population at this; the core EA principles are strong enough to make people notice the most obvious motivated cognition that obviously runs afoul of them. But that&rsquo;s not nearly good enough.)</p>\n<h2 id=\"historical-analogues\">Historical analogues</h2>\n<p>With the partial exception of GiveWell&rsquo;s <a href=\"http://www.givewell.org/history-of-philanthropy\">history of philanthropy project</a>, there&rsquo;s been no research into good historical outside views. Although there are no direct precursors of effective altruism (worrying in its own right; see above), there is one notably similar movement: communism, where the idea of &ldquo;from each according to his ability, to each according to his needs&rdquo; originated. Communism is also notable for its various abject failures. Effective altruists need to be more worried about how they will avoid failures of a similar class&mdash;and in general they need to be more aware of the pitfalls, as well as the benefits, of being an increasingly large social movement.</p>\n<p>Aaron Tucker elaborates better than I could:</p>\n<blockquote>\n<p>In particular, Communism/Socialism was a movement that was started by philosophers, then continued by technocrats, where they thought reason and planning could make the world much better, and that if they coordinated to take action to fix everything, they could eliminate poverty, disease, etc.</p>\n<p>Marx totally got the &ldquo;actually trying vs. pretending to try&rdquo; distinction AFAICT (&ldquo;Philosophers have only explained the world, but the real problem is to change it&rdquo; is a quote of his), and he really strongly rails against people who unreflectively try to fix things in ways that make sense to the culture they&rsquo;re starting from&mdash;the problem isn&rsquo;t that the bourgeoisie aren&rsquo;t trying to help people, it&rsquo;s that the only conception of help that the bourgeoisie have is one that&rsquo;s mostly epiphenomenal to actually improving the lives of the proletariat&mdash;giving them nice boureoisie things like education and voting rights, but not doing anything to improve the material condition of their life, or fix the problems of why they don&rsquo;t have those in the first place, and don&rsquo;t just make them themselves.</p>\n<p>So if Marx got the pretend/actually try distinction, and his followers took over countries, and they had a ton of awesome technocrats, it seems like it&rsquo;s the perfect EA thing, and it totally didn&rsquo;t work.</p>\n</blockquote>\n<h2 id=\"monoculture\">Monoculture</h2>\n<p>Effective altruists are not very diverse. The vast majority are white, &ldquo;upper-middle-class&rdquo;, intellectually and philosophically inclined, from a developed country, etc. (and I think it skews significantly male as well, though I&rsquo;m less sure of this). And as much as the multiple-perspectives argument for diversity is hackneyed by this point, it seems quite germane, especially when considering e.g. global health interventions, whose beneficiaries are culturally very foreign to us.</p>\n<p>Effective altruists are not very humanistically aware either. EA came out of analytic philosophy and spread from there to math and computer science. As such, they are too hasty to dismiss many arguments as moral-relativist postmodernist fluff, e.g. that effective altruists are promoting cultural imperialism by forcing a Westernized conception of &ldquo;the good&rdquo; onto people they&rsquo;re trying to help. Even if EAs are quite confident that the utilitarian/reductionist/rationalist worldview is correct, the outside view is that really engaging with a greater diversity of opinions is very helpful.</p>\n<h2 id=\"community-problems\">Community problems</h2>\n<p>The discourse around effective altruism in e.g. the <a href=\"https://www.facebook.com/groups/effective.altruists/\">Facebook group</a> used to be of fairly high quality. But as the movement grows, the traditional venues of discussion are getting inundated with new people who haven&rsquo;t absorbed the norms of discussion or standards of proof yet. If this is not rectified quickly, the EA community will cease to be useful at all: there will be no venue in which a group truth-seeking process can operate. Yet nobody seems to be aware of the magnitude of this problem. There have been some half-hearted attempts to fix it, but nothing much has come of them.</p>\n<h2 id=\"movement-building-issues\">Movement building issues</h2>\n<p>The whole point of having an effective altruism &ldquo;movement&rdquo; is that it&rsquo;ll be bigger than the sum of its parts. Being organized as a movement should turn effective altruism into the kind of large, semi-monolithic actor that can actually get big stuff done, not just make marginal contributions.</p>\n<p>But in practice, large movements and truth-seeking hardly ever go together. As movements grow, they get more &ldquo;epistemic inertia&rdquo;: it becomes much harder for them to update on evidence. This is because they have to rely on social methods to propagate their memes rather than truth-seeking behavior. But people who have been drawn to EA by social pressure rather than truth-seeking take much longer to change their beliefs, so once the movement reaches a critical mass of them, it will become difficult for it to update on new evidence. As described above, this is already happening to effective altruism with the ever-less-useful Facebook group.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>I&rsquo;ve presented several areas in which the effective altruism movement fails to converge on truth through a combination of the following effects:</p>\n<ol>\n<li>Effective altruists &ldquo;stop thinking&rdquo; too early and satisfice for &ldquo;doesn&rsquo;t obviously conflict with EA principles&rdquo; rather than optimizing for &ldquo;increases utility&rdquo;. (For instance, they choose donations poorly due to this effect.)</li>\n<li>Effective altruism puts strong demands on its practitioners, and EA group norms do not appropriately guard against motivated cognition to avoid them. (For example, this often causes people to choose bad careers.)</li>\n<li>Effective altruists don&rsquo;t notice important areas to look into, specifically issues related to &ldquo;being a successful movement&rdquo; rather than &ldquo;correctly implementing utilitarianism&rdquo;. (For instance, they ignore issues around group epistemology, historical precedents for the movement, movement diversity, etc.)</li>\n</ol>\n<p>These problems are worrying on their own, but the lack of awareness of them is the real problem. The monoculture is worrying, but the lackadaisical attitude towards it is worse. The lack of rigor is unfortunate, but the fact that people haven&rsquo;t noticed it is the real problem.</p>\n<p>Either effective altruists don&rsquo;t yet realize that they&rsquo;re subject to the failure modes of any large movement, or they don&rsquo;t feel motivation to do the boring legwork of e.g. engaging with viewpoints that your inside view says are annoying but that the outside view says are useful on expectation. Either way, this bespeaks worrying things about the movement&rsquo;s staying power.</p>\n<p>More importantly, it also indicates an epistemic failure on the part of effective altruists. The fact that no one else within EA has done a substantial critique yet is a huge red flag. If effective altruists aren&rsquo;t aware of strong critiques of the EA movement, why aren&rsquo;t they looking for them? This suggests that, contrary to the emphasis on rationality within the movement, many effective altruists&rsquo; beliefs are based on social, rather than truth-seeking, behavior.</p>\n<p>If it doesn&rsquo;t solve these problems, effective-altruism-the-movement won&rsquo;t help me achieve any more good than I could individually. All it will do is add epistemic inertia, as it takes more effort to shift the EA consensus than to update my individual beliefs.</p>\n<h2 id=\"are-these-problems-solvable\">Are these problems solvable?</h2>\n<p>It seems to me that the third issue above (lack of self-awareness as a social movement) subsumes the other two: if effective altruism as a movement were sufficiently introspective, it could probably notice and solve the other two problems, as well as future ones that will undoubtedly crop up.</p>\n<p>Hence, I propose an additional principle of effective altruism. In addition to being <em>altruistic</em>, <em>maximizing</em>, <em>egalitarian</em>, and <em>consequentialist</em> we should be <em>self-aware</em>: we should think carefully about the issues associated with being a successful movement, in order to make sure that we can move beyond the obvious applications of EA principles and come up with non-trivially better ways to improve the world.</p>\n<h2 id=\"acknowledgments\">Acknowledgments</h2>\n<p>Thanks to <a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a> for coining the idea of a hypothetical apostasy, and to <a href=\"http://becomingeden.com/\">Will Eden</a> for <a href=\"http://becomingeden.com/hypothetical-apostasy-on-nutrition/\">mentioning</a> it recently.</p>\n<p>Thanks to Michael Vassar, <a href=\"http://philogenic.wordpress.com/\">Aaron Tucker</a> and Andrew Rettek for inspiring various of these points.</p>\n<p>Thanks to Aaron Tucker and John Sturm for reading an advance draft of this post and giving valuable feedback.</p>\n<p><em>Cross-posted from&nbsp;</em><a href=\"http://www.benkuhn.net/ea-critique\">http://www.benkuhn.net/ea-critique</a><em>&nbsp;since I want outside perspectives, and also LW's comments are nicer than mine.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JsJPrdgRGRqnci8cZ": 2, "qAvbtzdG2A2RBn7in": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E3beR7bQ723kkNHpA", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}], "voteCount": 80, "baseScore": 95, "extendedScore": null, "score": 0.000245, "legacy": true, "legacyId": "24935", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 99, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I recently ran across Nick Bostrom\u2019s idea of subjecting your strongest beliefs to a <a href=\"http://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html\">hypothetical apostasy</a> in which you try to muster the strongest arguments you can against them. As you might have figured out, I believe strongly in <a href=\"http://en.wikipedia.org/wiki/Effective_altruism\">effective altruism</a>\u2014the idea of applying evidence and reason to finding the best ways to improve the world. As such, I thought it would be productive to write a hypothetical apostasy on the effective altruism movement.</p>\n<p>(EDIT: As per the comments of <a href=\"/lw/j8n/a_critique_of_effective_altruism/a4z1\">Vaniver</a>, <a href=\"/lw/j8n/a_critique_of_effective_altruism/a4zb\">Carl Shulman</a>, and others, this didn't quite come out as a hypothetical apostasy. I originally wrote it with that in mind, but decided that a focus on more plausible, more moderate criticisms would be more productive.)</p>\n<h2 id=\"How_to_read_this_post\">How to read this post</h2>\n<p>(EDIT: the following two paragraphs were written before I softened the tone of the piece. They're less relevant to the more moderate version that I actually published.)</p>\n<p>Hopefully this is clear, but as a disclaimer: this piece is written in a fairly critical tone. This was part of an attempt to get \u201cin character\u201d. <em>This tone does not indicate my current mental state with regard to the effective altruism movement.</em> I agree, to varying extents, with some of the critiques I present here, but I\u2019m not about to give up on effective altruism or stop cooperating with the EA movement. The apostasy is purely hypothetical.</p>\n<p>Also, because of the nature of a hypothetical apostasy, I\u2019d guess that for effective altruist readers, the critical tone of this piece may be especially likely to trigger defensive rationalization. Please read through with this in mind. (A good way to counteract this effect might be, for instance, to imagine that you\u2019re not an effective altruist, but your friend is, and it\u2019s them reading through it: how should they update their beliefs?)</p>\n<p>(End less relevant paragraphs.)</p>\n<p>Finally, if you\u2019ve never heard of effective altruism before, I don\u2019t recommend making this piece your first impression of it! You\u2019re going to get a very skewed view because I don\u2019t bother to mention all the things that are awesome about the EA movement.</p>\n<h2 id=\"Abstract\">Abstract</h2>\n<p>Effective altruism is, to my knowledge, the first time that a substantially useful set of ethics and frameworks to analyze one\u2019s effect on the world has gained a broad enough appeal to resemble a social movement. (I\u2019d say these principles are something like <em>altruism</em>, <em>maximization</em>, <em>egalitarianism</em>, and <em>consequentialism</em>; together they imply many improvements over the social default for trying to do good in the world\u2014earning to give as opposed to doing direct charity work, working in the developing world rather than locally, using evidence and feedback to analyze effectiveness, etc.) Unfortunately, as a movement effective altruism is failing to use these principles to acquire correct nontrivial beliefs about how to improve the world.</p>\n<p>By way of clarification, consider a distinction between two senses of the word \u201ctrying\u201d I used above. Let\u2019s call them \u201cactually trying\u201d and \u201cpretending to try\u201d. Pretending to try to improve the world is something like responding to social pressure to improve the world by querying your brain for a thing which improves the world, taking the first search result and rolling with it. For example, for a while I thought that I would try to improve the world by developing computerized methods of checking informally-written proofs, thus allowing more scalable teaching of higher math, democratizing education, etc. <em>Coincidentally</em>, computer programming and higher math happened to be the two things that I was best at. This is pretending to try. Actually trying is looking at the things that improve the world, figuring out which one maximizes utility, and then doing that thing. For instance, I now run an effective altruist student organization at Harvard because I realized that even though I\u2019m a comparatively bad leader and don\u2019t enjoy it very much, it\u2019s still very high-impact if I work hard enough at it. This isn\u2019t to say that I\u2019m actually trying yet, but I\u2019ve gotten closer.</p>\n<p>Using this distinction between pretending and actually trying, I would summarize a lot of effective altruism as \u201cpretending to actually try\u201d. As a social group, effective altruists have successfully noticed the pretending/actually-trying distinction. But they seem to have stopped there, assuming that knowing the difference between fake trying and actually trying translates into ability to actually try. Empirically, it most certainly doesn\u2019t. A lot of effective altruists still end up satisficing\u2014finding actions that are on their face acceptable under core EA standards and then picking those which seem appealing because of other essentially random factors. This is more likely to converge on good actions than what society does by default, because the principles are better than society\u2019s default principles. Nevertheless, it fails to make much progress over what is directly obvious from the core EA principles. As a result, although \u201cdoing effective altruism\u201d feels like truth-seeking, it often ends up being just a more credible way to pretend to try.<a id=\"more\"></a></p>\n<p>Below I introduce various ways in which effective altruists have failed to go beyond the social-satisficing algorithm of establishing some credibly acceptable alternatives and then picking among them based on essentially random preferences. I exhibit other areas where the norms of effective altruism fail to guard against motivated cognition. Both of these phenomena add what I call \u201cepistemic inertia\u201d to the effective-altruist consensus: effective altruists become more subject to pressures on their beliefs other than those from a truth-seeking process, meaning that the EA consensus becomes less able to update on new evidence or arguments and preventing the movement from moving forward. I argue that this stems from effective altruists\u2019 reluctance to think through issues of the form \u201cbeing a successful social movement\u201d rather than \u201ccorrectly applying utilitarianism individually\u201d. This could potentially be solved by introducing an additional principle of effective altruism\u2014e.g. \u201cgroup self-awareness\u201d\u2014but it may be too late to add new things to effective altruism\u2019s DNA.</p>\n<h2 id=\"Philosophical_difficulties\">Philosophical difficulties</h2>\n<p>There is <a href=\"http://www.overcomingbias.com/2008/01/protecting-acro.html\">currently</a> <a href=\"http://www.amirrorclear.net/academic/ideas/negative-utilitarianism/\">wide</a> <a href=\"http://www.utilitarian-essays.com/hedonistic-vs-preference.html\">disagreement</a> <a href=\"http://www.utilitarian-essays.com/suffering-nature.html\">among</a> <a href=\"/lw/91/average_utilitarianism_must_be_correct/\">effective</a> <a href=\"http://rationalaltruist.com/2013/06/03/my-outlook/comment-page-1/#comment-445\">altruists</a> on the correct framework for population ethics. This is crucially important for determining the best way to improve the world: different population ethics can lead to drastically different choices (or at least so we would expect <em>a priori</em>), and if the EA movement can\u2019t converge on at least their instrumental goals, it will quickly fragment and lose its power. Yet there has been little progress towards discovering the correct population ethics (or, from a moral anti-realist standpoint, constructing arguments that will lead to convergence on a particular population ethics), or even determining which ethics lead to which interventions being better.</p>\n<h2 id=\"Poor_cause_choices\">Poor cause choices</h2>\n<p>Many effective altruists donate to <a href=\"http://www.givewell.org/\">GiveWell\u2019s</a> top charities. All three of these charities work in global health. Is that because GiveWell knows that global health is the highest-leverage cause? No. It\u2019s because it was the only one with enough data to say anything very useful about. There\u2019s little reason to suppose that this correlates with being particularly high-leverage\u2014on the contrary, heuristic but less rigorous arguments for causes like <a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\">existential risk prevention</a>, <a href=\"http://www.everydayutilitarian.com/essays/how-much-does-it-cost-to-buy-a-vegetarian/\">vegetarian advocacy</a> and <a href=\"http://www.facebook.com/l.php?u=http%3A%2F%2Fopenborders.info%2F&amp;h=aAQGxloz1\">open borders</a> suggest that these could be even more efficient.</p>\n<p>Furthermore, the our current \u201cbest known intervention\u201d is likely to change (in a more cost-effective direction) in the future. There are two competing effects here: we might discover better interventions to donate to than the ones we currently think are best, but we also might run out of opportunities for the current best known intervention, and have to switch to the second. So far we seem to be in a regime where the first effect dominates, and there\u2019s no evidence that we\u2019ll reach a tipping point very soon, especially given how new the field of effective charity research is.</p>\n<p>Given these considerations, it\u2019s quite surprising that effective altruists are donating to global health causes now. Even for those looking to use their donations to set an example, a donor-advised fund would have many of the benefits and none of the downsides. And anyway, donating when you believe it\u2019s not (except for example-setting) the best possible course of action, in order to make a point about figuring out the best possible course of action and then doing that thing, seems perverse.</p>\n<h2 id=\"Non_obviousness\">Non-obviousness</h2>\n<p>Effective altruists often express surprise that the idea of effective altruism only came about so recently. For instance, my student group recently hosted Elie Hassenfeld for a talk in which he made remarks to that effect, and I\u2019ve heard other people working for EA organizations express the same sentiment. But no one seems to be actually worried about this\u2014just smug that they\u2019ve figured out something that no one else had.</p>\n<p>The \u201cmarket\u201d for ideas is at least somewhat efficient: most simple, obvious and correct things get thought of fairly quickly after it\u2019s possible to think them. If a meme as simple as effective altruism hasn\u2019t taken root yet, we should at least try to understand why before throwing our weight behind it. The absence of such attempts\u2014in other words, the fact that non-obviousness doesn\u2019t make effective altruists worried that they\u2019re missing something\u2014is a strong indicator against the \u201ceffective altruists are actually trying\u201d hypothesis.</p>\n<h2 id=\"Efficient_markets_for_giving\">Efficient markets for giving</h2>\n<p>It\u2019s often claimed that \u201cnonprofits are not a market for doing good; they\u2019re a market for warm fuzzies\u201d. This is used as justification for why it\u2019s possible to do immense amounts of good by donating. However, while it\u2019s certainly true that most donors aren\u2019t explicitly trying to purchase utililty, there\u2019s still a lot of money that is.</p>\n<p>The <a href=\"http://www.gatesfoundation.org/\">Gates Foundation</a> is an example of such an organization. They\u2019re effectiveness-minded and with $60 billion behind them. 80,000 Hours has already <a href=\"http://80000hours.org/blog/239-show-me-the-harm\">noted</a> that they\u2019ve probably saved over 6 million lives with their vaccine programs alone\u2014given that they\u2019ve spent a relatively small part of their endowment, they must be getting a much better exchange rate than our current best guesses.</p>\n<p>So why not just donate to the Gates Foundation? Effective altruists need a better account of the \u201cmarket inefficiencies\u201d that they\u2019re exploiting that Gates isn\u2019t. Why didn\u2019t the Gates Foundation fund the <a href=\"https://www.againstmalaria.com/\">Against Malaria Foundation</a>, GiveWell\u2019s top charity, when it\u2019s in one of their main research areas? It seems implausible that the answer is simple incompetence or the like.</p>\n<p>A general rule of markets is that if you don\u2019t know what your edge is, you\u2019re the sucker. Many effective altruists, when asked what their edge is, give some answer along the lines of \u201cactually being strategic/thinking about utility/caring about results\u201d, and stop thinking there. This isn\u2019t a compelling case: as mentioned before, it\u2019s not clear why no one else is doing these things.</p>\n<h2 id=\"Inconsistent_attitude_towards_rigor\">Inconsistent attitude towards rigor</h2>\n<p>Effective altruists insist on extraordinary rigor in their charity recommendations\u2014cf. for instance GiveWell\u2019s work. Yet for many ancillary problems\u2014donating now vs. later, choosing a career, and deciding how \u201cmeta\u201d to go (between direct work, earning to give, doing advocacy, and donating to advocacy), to name a few\u2014they seem happy to choose between the not-obviously-wrong alternatives based on intuition and gut feelings.</p>\n<h2 id=\"Poor_psychological_understanding\">Poor psychological understanding</h2>\n<p>John Sturm suggests, and I agree, that many of these issues are psychological in nature:</p>\n<blockquote>\n<p>I think a lot of these problems take root a commitment level issue:</p>\n<p>I, for instance, am thrilled about changing my mentality towards charity, not my mentality towards having kids. My first guess is that - from an EA and overall ethical perspective - it would be a big mistake for me to have kids (even after taking into account the normal EA excuses about doing things for myself). At least right now, though, I just don\u2019t care that I\u2019m ignoring my ethics and EA; I want to have kids and that\u2019s that.</p>\n<p>This is a case in which I\u2019m not \u201cbeing lazy\u201d so much as just not trying at all. But when someone asks me about it, it\u2019s easier for me to give some EA excuse (like that having kids will make me happier and more productive) that I don\u2019t think is true - and then I look like I\u2019m being a lazy or careless altruist rather than not being one at all.</p>\n<p>The model I\u2019m building is this: there are many different areas in life where I could apply EA. In some of them, I\u2019m wholeheartedly willing. In some of them, I\u2019m not willing at all. Then there are two kinds of areas where it looks like I\u2019m being a lazy EA: those where I\u2019m willing and want to be a better EA\u2026 and those where I\u2019m not willing but I\u2019m just pretending (to myself or others or both).</p>\n<p>The point of this: when we ask someone to be a less lazy EA, we are (1) helping them do a better job at something they want to do, and (2) trying to make them either do more than they want to or admit they are \u201cbad\u201d.</p>\n</blockquote>\n<p>In general, most effective altruists respond to deep conflicts between effective altruism and other goals in one of the following ways:</p>\n<ol>\n<li>Unconsciously resolve the cognitive dissonance with motivated reasoning: \u201cit\u2019s clearly my comparative advantage to spread effective altruism through poetry!\u201d</li>\n<li>Deliberately and knowingly use motivated reasoning: \u201cdear Facebook group, what are the best utilitarian arguments in favor of becoming an EA poet?\u201d</li>\n<li>Take the easiest \u201chonest\u201d way out: \u201cI wouldn\u2019t be psychologically able to do effective altruism if it forced me to go into finance instead of writing poetry, so I\u2019ll become an effective altruist poet instead\u201d.</li>\n</ol>\n<p>The third is debatably defensible\u2014though, for a community that purports to put stock in rationality and self-improvement, effective altruists have shown surprisingly little interest in self-modification to have more altruistic intentions. This seems obviously worthy of further work.</p>\n<p>Furthermore, EA norms do not proscribe even the first two, leading to a group norm that doesn\u2019t cause people to notice when they\u2019re engaging in a certain amount of motivated cognition. This is quite toxic to the movement\u2019s ability to converge on the truth. (As before, effective altruists are still better than the general population at this; the core EA principles are strong enough to make people notice the most obvious motivated cognition that obviously runs afoul of them. But that\u2019s not nearly good enough.)</p>\n<h2 id=\"Historical_analogues\">Historical analogues</h2>\n<p>With the partial exception of GiveWell\u2019s <a href=\"http://www.givewell.org/history-of-philanthropy\">history of philanthropy project</a>, there\u2019s been no research into good historical outside views. Although there are no direct precursors of effective altruism (worrying in its own right; see above), there is one notably similar movement: communism, where the idea of \u201cfrom each according to his ability, to each according to his needs\u201d originated. Communism is also notable for its various abject failures. Effective altruists need to be more worried about how they will avoid failures of a similar class\u2014and in general they need to be more aware of the pitfalls, as well as the benefits, of being an increasingly large social movement.</p>\n<p>Aaron Tucker elaborates better than I could:</p>\n<blockquote>\n<p>In particular, Communism/Socialism was a movement that was started by philosophers, then continued by technocrats, where they thought reason and planning could make the world much better, and that if they coordinated to take action to fix everything, they could eliminate poverty, disease, etc.</p>\n<p>Marx totally got the \u201cactually trying vs. pretending to try\u201d distinction AFAICT (\u201cPhilosophers have only explained the world, but the real problem is to change it\u201d is a quote of his), and he really strongly rails against people who unreflectively try to fix things in ways that make sense to the culture they\u2019re starting from\u2014the problem isn\u2019t that the bourgeoisie aren\u2019t trying to help people, it\u2019s that the only conception of help that the bourgeoisie have is one that\u2019s mostly epiphenomenal to actually improving the lives of the proletariat\u2014giving them nice boureoisie things like education and voting rights, but not doing anything to improve the material condition of their life, or fix the problems of why they don\u2019t have those in the first place, and don\u2019t just make them themselves.</p>\n<p>So if Marx got the pretend/actually try distinction, and his followers took over countries, and they had a ton of awesome technocrats, it seems like it\u2019s the perfect EA thing, and it totally didn\u2019t work.</p>\n</blockquote>\n<h2 id=\"Monoculture\">Monoculture</h2>\n<p>Effective altruists are not very diverse. The vast majority are white, \u201cupper-middle-class\u201d, intellectually and philosophically inclined, from a developed country, etc. (and I think it skews significantly male as well, though I\u2019m less sure of this). And as much as the multiple-perspectives argument for diversity is hackneyed by this point, it seems quite germane, especially when considering e.g. global health interventions, whose beneficiaries are culturally very foreign to us.</p>\n<p>Effective altruists are not very humanistically aware either. EA came out of analytic philosophy and spread from there to math and computer science. As such, they are too hasty to dismiss many arguments as moral-relativist postmodernist fluff, e.g. that effective altruists are promoting cultural imperialism by forcing a Westernized conception of \u201cthe good\u201d onto people they\u2019re trying to help. Even if EAs are quite confident that the utilitarian/reductionist/rationalist worldview is correct, the outside view is that really engaging with a greater diversity of opinions is very helpful.</p>\n<h2 id=\"Community_problems\">Community problems</h2>\n<p>The discourse around effective altruism in e.g. the <a href=\"https://www.facebook.com/groups/effective.altruists/\">Facebook group</a> used to be of fairly high quality. But as the movement grows, the traditional venues of discussion are getting inundated with new people who haven\u2019t absorbed the norms of discussion or standards of proof yet. If this is not rectified quickly, the EA community will cease to be useful at all: there will be no venue in which a group truth-seeking process can operate. Yet nobody seems to be aware of the magnitude of this problem. There have been some half-hearted attempts to fix it, but nothing much has come of them.</p>\n<h2 id=\"Movement_building_issues\">Movement building issues</h2>\n<p>The whole point of having an effective altruism \u201cmovement\u201d is that it\u2019ll be bigger than the sum of its parts. Being organized as a movement should turn effective altruism into the kind of large, semi-monolithic actor that can actually get big stuff done, not just make marginal contributions.</p>\n<p>But in practice, large movements and truth-seeking hardly ever go together. As movements grow, they get more \u201cepistemic inertia\u201d: it becomes much harder for them to update on evidence. This is because they have to rely on social methods to propagate their memes rather than truth-seeking behavior. But people who have been drawn to EA by social pressure rather than truth-seeking take much longer to change their beliefs, so once the movement reaches a critical mass of them, it will become difficult for it to update on new evidence. As described above, this is already happening to effective altruism with the ever-less-useful Facebook group.</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>I\u2019ve presented several areas in which the effective altruism movement fails to converge on truth through a combination of the following effects:</p>\n<ol>\n<li>Effective altruists \u201cstop thinking\u201d too early and satisfice for \u201cdoesn\u2019t obviously conflict with EA principles\u201d rather than optimizing for \u201cincreases utility\u201d. (For instance, they choose donations poorly due to this effect.)</li>\n<li>Effective altruism puts strong demands on its practitioners, and EA group norms do not appropriately guard against motivated cognition to avoid them. (For example, this often causes people to choose bad careers.)</li>\n<li>Effective altruists don\u2019t notice important areas to look into, specifically issues related to \u201cbeing a successful movement\u201d rather than \u201ccorrectly implementing utilitarianism\u201d. (For instance, they ignore issues around group epistemology, historical precedents for the movement, movement diversity, etc.)</li>\n</ol>\n<p>These problems are worrying on their own, but the lack of awareness of them is the real problem. The monoculture is worrying, but the lackadaisical attitude towards it is worse. The lack of rigor is unfortunate, but the fact that people haven\u2019t noticed it is the real problem.</p>\n<p>Either effective altruists don\u2019t yet realize that they\u2019re subject to the failure modes of any large movement, or they don\u2019t feel motivation to do the boring legwork of e.g. engaging with viewpoints that your inside view says are annoying but that the outside view says are useful on expectation. Either way, this bespeaks worrying things about the movement\u2019s staying power.</p>\n<p>More importantly, it also indicates an epistemic failure on the part of effective altruists. The fact that no one else within EA has done a substantial critique yet is a huge red flag. If effective altruists aren\u2019t aware of strong critiques of the EA movement, why aren\u2019t they looking for them? This suggests that, contrary to the emphasis on rationality within the movement, many effective altruists\u2019 beliefs are based on social, rather than truth-seeking, behavior.</p>\n<p>If it doesn\u2019t solve these problems, effective-altruism-the-movement won\u2019t help me achieve any more good than I could individually. All it will do is add epistemic inertia, as it takes more effort to shift the EA consensus than to update my individual beliefs.</p>\n<h2 id=\"Are_these_problems_solvable_\">Are these problems solvable?</h2>\n<p>It seems to me that the third issue above (lack of self-awareness as a social movement) subsumes the other two: if effective altruism as a movement were sufficiently introspective, it could probably notice and solve the other two problems, as well as future ones that will undoubtedly crop up.</p>\n<p>Hence, I propose an additional principle of effective altruism. In addition to being <em>altruistic</em>, <em>maximizing</em>, <em>egalitarian</em>, and <em>consequentialist</em> we should be <em>self-aware</em>: we should think carefully about the issues associated with being a successful movement, in order to make sure that we can move beyond the obvious applications of EA principles and come up with non-trivially better ways to improve the world.</p>\n<h2 id=\"Acknowledgments\">Acknowledgments</h2>\n<p>Thanks to <a href=\"http://www.nickbostrom.com/\">Nick Bostrom</a> for coining the idea of a hypothetical apostasy, and to <a href=\"http://becomingeden.com/\">Will Eden</a> for <a href=\"http://becomingeden.com/hypothetical-apostasy-on-nutrition/\">mentioning</a> it recently.</p>\n<p>Thanks to Michael Vassar, <a href=\"http://philogenic.wordpress.com/\">Aaron Tucker</a> and Andrew Rettek for inspiring various of these points.</p>\n<p>Thanks to Aaron Tucker and John Sturm for reading an advance draft of this post and giving valuable feedback.</p>\n<p><em>Cross-posted from&nbsp;</em><a href=\"http://www.benkuhn.net/ea-critique\">http://www.benkuhn.net/ea-critique</a><em>&nbsp;since I want outside perspectives, and also LW's comments are nicer than mine.</em></p>", "sections": [{"title": "How to read this post", "anchor": "How_to_read_this_post", "level": 1}, {"title": "Abstract", "anchor": "Abstract", "level": 1}, {"title": "Philosophical difficulties", "anchor": "Philosophical_difficulties", "level": 1}, {"title": "Poor cause choices", "anchor": "Poor_cause_choices", "level": 1}, {"title": "Non-obviousness", "anchor": "Non_obviousness", "level": 1}, {"title": "Efficient markets for giving", "anchor": "Efficient_markets_for_giving", "level": 1}, {"title": "Inconsistent attitude towards rigor", "anchor": "Inconsistent_attitude_towards_rigor", "level": 1}, {"title": "Poor psychological understanding", "anchor": "Poor_psychological_understanding", "level": 1}, {"title": "Historical analogues", "anchor": "Historical_analogues", "level": 1}, {"title": "Monoculture", "anchor": "Monoculture", "level": 1}, {"title": "Community problems", "anchor": "Community_problems", "level": 1}, {"title": "Movement building issues", "anchor": "Movement_building_issues", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"title": "Are these problems solvable?", "anchor": "Are_these_problems_solvable_", "level": 1}, {"title": "Acknowledgments", "anchor": "Acknowledgments", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "154 comments"}], "headingsCount": 17}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 154, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["px4nYEy3rDqeegJw3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-02T18:15:11.724Z", "modifiedAt": null, "url": null, "title": "Failing to update", "slug": "failing-to-update", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:03.708Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ialdabaoth", "createdAt": "2012-10-11T00:04:32.345Z", "isAdmin": false, "displayName": "ialdabaoth"}, "userId": "335oJYQyzcd85RAoe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GDt75Y8NpAqXypT9F/failing-to-update", "pageUrlRelative": "/posts/GDt75Y8NpAqXypT9F/failing-to-update", "linkUrl": "https://www.lesswrong.com/posts/GDt75Y8NpAqXypT9F/failing-to-update", "postedAtFormatted": "Monday, December 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Failing%20to%20update&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFailing%20to%20update%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDt75Y8NpAqXypT9F%2Ffailing-to-update%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Failing%20to%20update%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDt75Y8NpAqXypT9F%2Ffailing-to-update", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDt75Y8NpAqXypT9F%2Ffailing-to-update", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 252, "htmlBody": "<p>You understand <a href=\"http://yudkowsky.net/rational/bayes\">Bayes' Theorem</a>. You enter into a situation with an intuitive <a href=\"/lw/iao/common_sense_as_a_prior/\">\"common sense\" prior</a>. You observe the situation, and then you <a href=\"http://wiki.lesswrong.com/wiki/Shut_up_and_multiply\">shut up and multiply</a>.</p>\n<p>And then you go to update, you compute the desired behavior to maximize utility... and some cognitive module buried in your brain says \"no\".&nbsp;</p>\n<p>Example:</p>\n<p>I realize I need a physical examination. I have no rational reason to fear going to the doctor. I am, in fact, acutely aware that my fear of going to the doctor is based on a fear that they will find something wrong that I can't afford to fix, but <a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Gendlin\">the truth is already so</a>. So I bite the bullet, make an appointment, and then at the scheduled time I get in the car and drive to the doctor's office.</p>\n<p>And then I just keep driving past the doctor's office, turn around and go home.</p>\n<p>I tell myself that if I'm not going, I should call the doctor to avoid a $100 no-show fee, but I don't.</p>\n<p>And then I get home, and I tell myself that that was dumb, and that I need to update my behavior - and that physically punishing myself for not going to the doctor is not an efficient use of my energy.</p>\n<p>So I punch my hand through a mirror.</p>\n<p>NOW, finally, I have an excuse to go to the doctor - so I wrap my hand in bandages and go back to bed, instead.</p>\n<p>&nbsp;</p>\n<p>What do you do when your computed probabilities and utility function have NO EFFECT WHATSOEVER on your actual behavior?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GDt75Y8NpAqXypT9F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "24952", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wgdfxQJ2DQuju73zC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-02T18:24:59.444Z", "modifiedAt": null, "url": null, "title": "December 2013 Media Thread", "slug": "december-2013-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:02.057Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tixs5HvwKxrdMhrzv/december-2013-media-thread", "pageUrlRelative": "/posts/tixs5HvwKxrdMhrzv/december-2013-media-thread", "linkUrl": "https://www.lesswrong.com/posts/tixs5HvwKxrdMhrzv/december-2013-media-thread", "postedAtFormatted": "Monday, December 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20December%202013%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecember%202013%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftixs5HvwKxrdMhrzv%2Fdecember-2013-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=December%202013%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftixs5HvwKxrdMhrzv%2Fdecember-2013-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftixs5HvwKxrdMhrzv%2Fdecember-2013-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tixs5HvwKxrdMhrzv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.4484753671683555e-06, "legacy": true, "legacyId": "24953", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-02T19:51:21.982Z", "modifiedAt": null, "url": null, "title": "[Link] Good Judgment Project, Season Three", "slug": "link-good-judgment-project-season-three", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:00.389Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m9ZwtT5cP8D2yeoL8/link-good-judgment-project-season-three", "pageUrlRelative": "/posts/m9ZwtT5cP8D2yeoL8/link-good-judgment-project-season-three", "linkUrl": "https://www.lesswrong.com/posts/m9ZwtT5cP8D2yeoL8/link-good-judgment-project-season-three", "postedAtFormatted": "Monday, December 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Good%20Judgment%20Project%2C%20Season%20Three&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Good%20Judgment%20Project%2C%20Season%20Three%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm9ZwtT5cP8D2yeoL8%2Flink-good-judgment-project-season-three%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Good%20Judgment%20Project%2C%20Season%20Three%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm9ZwtT5cP8D2yeoL8%2Flink-good-judgment-project-season-three", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm9ZwtT5cP8D2yeoL8%2Flink-good-judgment-project-season-three", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 304, "htmlBody": "<p><em>Previously: <a href=\"/lw/c8a/test_your_forecasting_ability_contribute_to_the/\">\"Test Your Forecasting Ability, Contribute to the Science of Human Judgment\"</a> (May 2012), <a href=\"/lw/6ya/link_get_paid_to_train_your_rationality/\">\"Get Paid to Train Your Rationality\"</a> (August 2011)</em></p>\n<p>-</p>\n<p>Think you have what it takes to make good predictions? &nbsp;Since 2011, the <a href=\"http://www.goodjudgmentproject.com/\">Good Judgment Project</a>&nbsp;(GJP) has been making predictions on issues of international relations and foreign affairs, recently winning the IARPA (Intelligence Advanced Research Projects Activity) prediction contest. &nbsp;Predictions from the GJP have been startlingly accurate, outperforming prediction markets, and exceeding even optimistic expectations. &nbsp;It's run by Phillip Tetlock, the famous predictor of <a href=\"http://www.amazon.com/Expert-Political-Judgment-Good-Know/dp/0691128715\">\"foxes and hedgehogs\" fame</a>.</p>\n<p>From <a href=\"http://www.washingtonpost.com/blogs/monkey-cage/wp/2013/11/26/good-judgment-in-forecasting-international-affairs-and-an-invitation-for-season-3/\">the Monkey Cage article</a>:</p>\n<blockquote>\n<p>How does the Good Judgment Project achieve such strikingly accurate results? The Project uses modern social-science methods ranging from harnessing the wisdom of crowds to prediction markets to putting together teams of forecasters. The GJP research team attributes its success to a blend of getting the right people (i.e., the &ldquo;right&rdquo; individual forecasters) on the bus, offering basic tutorials on inferential traps to avoid and best practices to embrace, concentrating the most talented forecasters into super teams, and constantly fine-tuning the aggregation algorithms it uses to combine individual forecasts into a collective prediction on each forecasting question. The Project&rsquo;s best forecasters are typically talented and highly motivated amateurs, rather than subject matter experts.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>But the good news is that you now have a chance to get involved with GJP Season 3 if you think you're a great predictor:</p>\n<blockquote>\n<p>If you enjoy world politics and appreciate a good challenge, consider joining the Good Judgment Project, which has openings right now for Season 3 forecasters. The Project will give you the opportunity to receive training, to get regular feedback on your forecasting accuracy, and to test your forecasting skills against those of some of the most accurate forecasters around. Interested? To find out more and to register, go to <a href=\"http://www.goodjudgmentproject.com\">www.goodjudgmentproject.com</a>.</p>\n</blockquote>\n<p>-</p>\n<p><em>Also <a href=\"http://www.everydayutilitarian.com/essays/good-judgment-project-season-three/\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m9ZwtT5cP8D2yeoL8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.4485627670253692e-06, "legacy": true, "legacyId": "24954", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["37YSQe3oCBvqWpCNR", "PyuRcbHvxXNdCHoG3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-02T21:46:04.779Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, December 1-15", "slug": "group-rationality-diary-december-1-15-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:05.423Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YnxSxHnDvj3neYFzy/group-rationality-diary-december-1-15-0", "pageUrlRelative": "/posts/YnxSxHnDvj3neYFzy/group-rationality-diary-december-1-15-0", "linkUrl": "https://www.lesswrong.com/posts/YnxSxHnDvj3neYFzy/group-rationality-diary-december-1-15-0", "postedAtFormatted": "Monday, December 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20December%201-15&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20December%201-15%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnxSxHnDvj3neYFzy%2Fgroup-rationality-diary-december-1-15-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20December%201-15%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnxSxHnDvj3neYFzy%2Fgroup-rationality-diary-december-1-15-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYnxSxHnDvj3neYFzy%2Fgroup-rationality-diary-december-1-15-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 209, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">This is the public group instrumental rationality diary for December 1-15.&nbsp;</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Immediate past diary: &nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/j2l/group_rationality_diary_november_1630/\">November 16-30</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YnxSxHnDvj3neYFzy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "24955", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["T5kWv7rENdFuatqov"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-02T22:48:14.923Z", "modifiedAt": null, "url": null, "title": "Meetup : Lesswrong Boulder CO", "slug": "meetup-lesswrong-boulder-co", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.572Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "yakurbe0112", "createdAt": "2012-07-01T23:40:26.855Z", "isAdmin": false, "displayName": "yakurbe0112"}, "userId": "xXr6Jngw57uMXveQ9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8zEKPEuitNnNmNpNr/meetup-lesswrong-boulder-co", "pageUrlRelative": "/posts/8zEKPEuitNnNmNpNr/meetup-lesswrong-boulder-co", "linkUrl": "https://www.lesswrong.com/posts/8zEKPEuitNnNmNpNr/meetup-lesswrong-boulder-co", "postedAtFormatted": "Monday, December 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Lesswrong%20Boulder%20CO&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Lesswrong%20Boulder%20CO%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8zEKPEuitNnNmNpNr%2Fmeetup-lesswrong-boulder-co%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Lesswrong%20Boulder%20CO%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8zEKPEuitNnNmNpNr%2Fmeetup-lesswrong-boulder-co", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8zEKPEuitNnNmNpNr%2Fmeetup-lesswrong-boulder-co", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/u9'>Lesswrong Boulder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 December 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2690 Baseline Rd, Boulder, CO 80305</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>No topic in particular. Just come and hang out with us and have some pizza. (its really good pizza)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/u9'>Lesswrong Boulder CO</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8zEKPEuitNnNmNpNr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4487417759121927e-06, "legacy": true, "legacyId": "24956", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Lesswrong_Boulder_CO\">Discussion article for the meetup : <a href=\"/meetups/u9\">Lesswrong Boulder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 December 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2690 Baseline Rd, Boulder, CO 80305</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>No topic in particular. Just come and hang out with us and have some pizza. (its really good pizza)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Lesswrong_Boulder_CO1\">Discussion article for the meetup : <a href=\"/meetups/u9\">Lesswrong Boulder CO</a></h2>", "sections": [{"title": "Discussion article for the meetup : Lesswrong Boulder CO", "anchor": "Discussion_article_for_the_meetup___Lesswrong_Boulder_CO", "level": 1}, {"title": "Discussion article for the meetup : Lesswrong Boulder CO", "anchor": "Discussion_article_for_the_meetup___Lesswrong_Boulder_CO1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-03T02:19:57.212Z", "modifiedAt": null, "url": null, "title": "Meetup : Bay Area Solstice", "slug": "meetup-bay-area-solstice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:00.128Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ben_LandauTaylor", "createdAt": "2013-07-25T17:40:48.283Z", "isAdmin": false, "displayName": "Ben_LandauTaylor"}, "userId": "ZvoQwr4zZjPuC2oNJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mMpT25d7RFTsFX89W/meetup-bay-area-solstice", "pageUrlRelative": "/posts/mMpT25d7RFTsFX89W/meetup-bay-area-solstice", "linkUrl": "https://www.lesswrong.com/posts/mMpT25d7RFTsFX89W/meetup-bay-area-solstice", "postedAtFormatted": "Tuesday, December 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bay%20Area%20Solstice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bay%20Area%20Solstice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmMpT25d7RFTsFX89W%2Fmeetup-bay-area-solstice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bay%20Area%20Solstice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmMpT25d7RFTsFX89W%2Fmeetup-bay-area-solstice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmMpT25d7RFTsFX89W%2Fmeetup-bay-area-solstice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 114, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/ua\">Bay Area Solstice</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">07 December 2013 06:00:00PM (-0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">San Francisco</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The Bay Area community is holding a Solstice celebration, and you&rsquo;re invited! Join us for a night of group singing, ritual, light, warmth, and companionship, plus the first-ever performance of the rationalist choir, as we celebrate human progress and potential at the darkest time of the year.</p>\n<p>The Bay Area Solstice will be held on Saturday, December 7, from 6:00 PM until 10:00 PM. We&rsquo;ll provide a shuttle to and from the Civic Center BART station. Space is limited, so please fill out the <a rel=\"nofollow\" href=\"https://docs.google.com/forms/d/1wQgq6KFezba9kskq24BpZehWmnug-bE9LYWyLbwSBzg/viewform\">RSVP form</a>. I hope to see you there!</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/ua\">Bay Area Solstice</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mMpT25d7RFTsFX89W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "24958", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bay_Area_Solstice\">Discussion article for the meetup : <a href=\"/meetups/ua\">Bay Area Solstice</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">07 December 2013 06:00:00PM (-0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">San Francisco</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The Bay Area community is holding a Solstice celebration, and you\u2019re invited! Join us for a night of group singing, ritual, light, warmth, and companionship, plus the first-ever performance of the rationalist choir, as we celebrate human progress and potential at the darkest time of the year.</p>\n<p>The Bay Area Solstice will be held on Saturday, December 7, from 6:00 PM until 10:00 PM. We\u2019ll provide a shuttle to and from the Civic Center BART station. Space is limited, so please fill out the <a rel=\"nofollow\" href=\"https://docs.google.com/forms/d/1wQgq6KFezba9kskq24BpZehWmnug-bE9LYWyLbwSBzg/viewform\">RSVP form</a>. I hope to see you there!</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Bay_Area_Solstice1\">Discussion article for the meetup : <a href=\"/meetups/ua\">Bay Area Solstice</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bay Area Solstice", "anchor": "Discussion_article_for_the_meetup___Bay_Area_Solstice", "level": 1}, {"title": "Discussion article for the meetup : Bay Area Solstice", "anchor": "Discussion_article_for_the_meetup___Bay_Area_Solstice1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-03T05:10:53.909Z", "modifiedAt": null, "url": null, "title": "Open Thread, December 2-8, 2013", "slug": "open-thread-december-2-8-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:55.283Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5hxcnGJKaKXpAqttP/open-thread-december-2-8-2013", "pageUrlRelative": "/posts/5hxcnGJKaKXpAqttP/open-thread-december-2-8-2013", "linkUrl": "https://www.lesswrong.com/posts/5hxcnGJKaKXpAqttP/open-thread-december-2-8-2013", "postedAtFormatted": "Tuesday, December 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20December%202-8%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20December%202-8%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5hxcnGJKaKXpAqttP%2Fopen-thread-december-2-8-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20December%202-8%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5hxcnGJKaKXpAqttP%2Fopen-thread-december-2-8-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5hxcnGJKaKXpAqttP%2Fopen-thread-december-2-8-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5hxcnGJKaKXpAqttP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "24962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 185, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-03T14:46:23.986Z", "modifiedAt": "2021-08-18T16:46:46.146Z", "url": null, "title": "December Monthly Bragging Thread", "slug": "december-monthly-bragging-thread", "viewCount": null, "lastCommentedAt": "2013-12-31T15:25:40.783Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eBMkABrytT3CzT2rf/december-monthly-bragging-thread", "pageUrlRelative": "/posts/eBMkABrytT3CzT2rf/december-monthly-bragging-thread", "linkUrl": "https://www.lesswrong.com/posts/eBMkABrytT3CzT2rf/december-monthly-bragging-thread", "postedAtFormatted": "Tuesday, December 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20December%20Monthly%20Bragging%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecember%20Monthly%20Bragging%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeBMkABrytT3CzT2rf%2Fdecember-monthly-bragging-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=December%20Monthly%20Bragging%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeBMkABrytT3CzT2rf%2Fdecember-monthly-bragging-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeBMkABrytT3CzT2rf%2Fdecember-monthly-bragging-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p>As in Joshua Blaine's original description (below), but may be used to brag about things you've accomplished either this month (December) or the previous one (November), assuming that you haven't brought it up in any earlier Monthly Bragging Thread.</p>\n<blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 12.666666984558105px; text-align: justify;\">In an attempt to encourage more people to&nbsp;<em>actually do awesome things&nbsp;</em>(a la instrumental rationality), I am proposing a new monthly thread (can be changed to bi-weekly, should that be demanded). Your job, should you choose to accept it, is to comment on this thread explaining&nbsp;<strong>the most awesome thing you've done this month</strong>. You may be as blatantly proud of you self as you feel. You may unabashedly consider yourself&nbsp;<em>the coolest freaking person ever</em>&nbsp;because of that awesome thing you're dying to tell everyone about. This is the place to do just that.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 12.666666984558105px; text-align: justify;\">Remember, however, that this&nbsp;<strong>isn't</strong>&nbsp;any kind of progress thread. Nor is it any kind of proposal thread.<em>This thread is solely for people to talk about the awesomest thing they've done all month. not will do. not are working on</em>.<strong>have already done.&nbsp;</strong>This is to cultivate an environment of object level productivity rather than meta-productivity methods.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 12.666666984558105px; text-align: justify;\">So, what's the coolest thing you've done this month?</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eBMkABrytT3CzT2rf", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 17, "extendedScore": null, "score": 1.4497121267356325e-06, "legacy": true, "legacyId": "24967", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 119, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-12-03T14:46:23.986Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-03T20:25:27.846Z", "modifiedAt": null, "url": null, "title": "Meetup : Frankfurt meetup:", "slug": "meetup-frankfurt-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kendra", "createdAt": "2012-02-29T23:10:44.583Z", "isAdmin": false, "displayName": "Kendra"}, "userId": "BPB6kHkfZwFLrhcbG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x5TBs423JFwJgcxLh/meetup-frankfurt-meetup-0", "pageUrlRelative": "/posts/x5TBs423JFwJgcxLh/meetup-frankfurt-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/x5TBs423JFwJgcxLh/meetup-frankfurt-meetup-0", "postedAtFormatted": "Tuesday, December 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Frankfurt%20meetup%3A&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Frankfurt%20meetup%3A%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx5TBs423JFwJgcxLh%2Fmeetup-frankfurt-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Frankfurt%20meetup%3A%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx5TBs423JFwJgcxLh%2Fmeetup-frankfurt-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx5TBs423JFwJgcxLh%2Fmeetup-frankfurt-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ub'>Frankfurt meetup:</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 December 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Frankfurt, Ginnheimer Landstra\u00dfe</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Same location as usual. If you are new, contact me here or write me to my phone: 0176<em>34</em>095*760\n(Sorry, the date was wrong originally. The meetup will be on Sunday.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ub'>Frankfurt meetup:</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x5TBs423JFwJgcxLh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4500557838081346e-06, "legacy": true, "legacyId": "24968", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Frankfurt_meetup_\">Discussion article for the meetup : <a href=\"/meetups/ub\">Frankfurt meetup:</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 December 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Frankfurt, Ginnheimer Landstra\u00dfe</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Same location as usual. If you are new, contact me here or write me to my phone: 0176<em>34</em>095*760\n(Sorry, the date was wrong originally. The meetup will be on Sunday.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Frankfurt_meetup_1\">Discussion article for the meetup : <a href=\"/meetups/ub\">Frankfurt meetup:</a></h2>", "sections": [{"title": "Discussion article for the meetup : Frankfurt meetup:", "anchor": "Discussion_article_for_the_meetup___Frankfurt_meetup_", "level": 1}, {"title": "Discussion article for the meetup : Frankfurt meetup:", "anchor": "Discussion_article_for_the_meetup___Frankfurt_meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-04T03:06:38.970Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston/Cambridge - The Attention Economy", "slug": "meetup-boston-cambridge-the-attention-economy", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xxMnh9d6tTjpStT6p/meetup-boston-cambridge-the-attention-economy", "pageUrlRelative": "/posts/xxMnh9d6tTjpStT6p/meetup-boston-cambridge-the-attention-economy", "linkUrl": "https://www.lesswrong.com/posts/xxMnh9d6tTjpStT6p/meetup-boston-cambridge-the-attention-economy", "postedAtFormatted": "Wednesday, December 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%2FCambridge%20-%20The%20Attention%20Economy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%2FCambridge%20-%20The%20Attention%20Economy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxxMnh9d6tTjpStT6p%2Fmeetup-boston-cambridge-the-attention-economy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%2FCambridge%20-%20The%20Attention%20Economy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxxMnh9d6tTjpStT6p%2Fmeetup-boston-cambridge-the-attention-economy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxxMnh9d6tTjpStT6p%2Fmeetup-boston-cambridge-the-attention-economy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/uc'>Boston/Cambridge - The Attention Economy</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 December 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Citadel, 98 Elm St, apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Robin Gane-McCalla will be presenting on \"The Attention Economy: how our focus determines the future\".</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square).</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 3pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/uc'>Boston/Cambridge - The Attention Economy</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xxMnh9d6tTjpStT6p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4504625895693144e-06, "legacy": true, "legacyId": "24970", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston_Cambridge___The_Attention_Economy\">Discussion article for the meetup : <a href=\"/meetups/uc\">Boston/Cambridge - The Attention Economy</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 December 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Citadel, 98 Elm St, apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Robin Gane-McCalla will be presenting on \"The Attention Economy: how our focus determines the future\".</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square).</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 3pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston_Cambridge___The_Attention_Economy1\">Discussion article for the meetup : <a href=\"/meetups/uc\">Boston/Cambridge - The Attention Economy</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston/Cambridge - The Attention Economy", "anchor": "Discussion_article_for_the_meetup___Boston_Cambridge___The_Attention_Economy", "level": 1}, {"title": "Discussion article for the meetup : Boston/Cambridge - The Attention Economy", "anchor": "Discussion_article_for_the_meetup___Boston_Cambridge___The_Attention_Economy1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-04T17:50:58.246Z", "modifiedAt": null, "url": null, "title": "Questions and comments about Eliezer's Dec. 2 2013 Oxford speech", "slug": "questions-and-comments-about-eliezer-s-dec-2-2013-oxford", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:02.281Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c7dXoXmBEwH4Gztqn/questions-and-comments-about-eliezer-s-dec-2-2013-oxford", "pageUrlRelative": "/posts/c7dXoXmBEwH4Gztqn/questions-and-comments-about-eliezer-s-dec-2-2013-oxford", "linkUrl": "https://www.lesswrong.com/posts/c7dXoXmBEwH4Gztqn/questions-and-comments-about-eliezer-s-dec-2-2013-oxford", "postedAtFormatted": "Wednesday, December 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Questions%20and%20comments%20about%20Eliezer's%20Dec.%202%202013%20Oxford%20speech&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestions%20and%20comments%20about%20Eliezer's%20Dec.%202%202013%20Oxford%20speech%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc7dXoXmBEwH4Gztqn%2Fquestions-and-comments-about-eliezer-s-dec-2-2013-oxford%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Questions%20and%20comments%20about%20Eliezer's%20Dec.%202%202013%20Oxford%20speech%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc7dXoXmBEwH4Gztqn%2Fquestions-and-comments-about-eliezer-s-dec-2-2013-oxford", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc7dXoXmBEwH4Gztqn%2Fquestions-and-comments-about-eliezer-s-dec-2-2013-oxford", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 308, "htmlBody": "<p>Notes I took while listening to the speech:</p>\n<p><a href=\"https://www.youtube.com/watch?v=7c1WSwqwMOE\">Eliezer Yudkowsky on Friendly AI</a></p>\n<p>If the human race is down to 1000 people, what are the odds that it will continue and do well? I realize this is a nitpick-- the argument would be the same if the human race were reduced to a million or ten million.</p>\n<p>Suppose that a blind person in a first world country wants help paying for a guide dog and/or wants guide dogs for other blind people in first world countries, but has heard of effective altruism. What honest arguments could the blind person use?</p>\n<p>If I were designing an intelligence, I'm not sure how much control I would give it over its own brain. People are already able to damage themselves pretty badly, even with the crude tools they've got. I would experiment with intelligent species to see how they'd behave with more control over their brains. What would you do?</p>\n<p>Sidenote: Birds show some possibilities of making brains more efficient per weight.</p>\n<p><a href=\"http://www.ted.com/talks/suzana_herculano_houzel_what_is_so_special_about_the_human_brain.html\">TED talk about neurons and brains</a>. This is not a great TED talk, but it's got somewhat about comparisons between brains in different species, in particular that neuron size and density varies between species. Comparisons of brain size tells you less than people assume.</p>\n<p>Brains and competition aren't just about sexual selection: Females (especially) compete for resources to feed and care for themselves and their children. In some species, males also compete for resources for their children. Reproductive selection isn't just about mating selection. See <a href=\"http://www.amazon.com/Mother-Nature-Maternal-Instincts-Species/dp/0345408934\">Mother Nature</a> by Sarah Hrdy. <a href=\"http://blogs.scientificamerican.com/primate-diaries/2012/03/16/raising-darwins-consciousness-an-interview-with-sarah-blaffer-hrdy-on-mother-nature/\">Interview about humans as cooperative breeders</a></p>\n<p>Do we need to think about hardware, software, and firmware (at least) for brains, rather than just hardware and software?</p>\n<p>[Sound cuts off at 38:00. comes back at 39:10]</p>\n<p>How much of organisms consist of traits which aren't being selected for?</p>\n<p>The sound quality deteriorates enough at about an hour that I'm giving up.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c7dXoXmBEwH4Gztqn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 6, "extendedScore": null, "score": 1.451360013085605e-06, "legacy": true, "legacyId": "24976", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-04T19:19:37.048Z", "modifiedAt": null, "url": null, "title": "Ethics of Brain Emulation", "slug": "ethics-of-brain-emulation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:02.491Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "summerstay", "createdAt": "2011-06-15T22:18:28.917Z", "isAdmin": false, "displayName": "summerstay"}, "userId": "qkcFeNZSPyEibsWwK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MkbPjtwTSe9GRxTLW/ethics-of-brain-emulation", "pageUrlRelative": "/posts/MkbPjtwTSe9GRxTLW/ethics-of-brain-emulation", "linkUrl": "https://www.lesswrong.com/posts/MkbPjtwTSe9GRxTLW/ethics-of-brain-emulation", "postedAtFormatted": "Wednesday, December 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ethics%20of%20Brain%20Emulation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEthics%20of%20Brain%20Emulation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMkbPjtwTSe9GRxTLW%2Fethics-of-brain-emulation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ethics%20of%20Brain%20Emulation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMkbPjtwTSe9GRxTLW%2Fethics-of-brain-emulation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMkbPjtwTSe9GRxTLW%2Fethics-of-brain-emulation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<p>I felt like this draft paper by Anders Sandberg was a well-thought-out essay on the morality of experiments on brain emulations. Is there anything you disagree with here, or think he should handle differently?</p>\n<p><a href=\"http://www.aleph.se/papers/Ethics%20of%20brain%20emulations%20draft.pdf\">http://www.aleph.se/papers/Ethics%20of%20brain%20emulations%20draft.pdf</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2b1": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MkbPjtwTSe9GRxTLW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 2, "extendedScore": null, "score": 1.4514500273428265e-06, "legacy": true, "legacyId": "24977", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-05T01:09:33.431Z", "modifiedAt": null, "url": null, "title": "International cooperation vs. AI arms race", "slug": "international-cooperation-vs-ai-arms-race", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.044Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Brian_Tomasik", "createdAt": "2013-06-08T20:34:53.353Z", "isAdmin": false, "displayName": "Brian_Tomasik"}, "userId": "6XzBgrHtBL4M7CpKL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vw9QAviBxcGodMHfN/international-cooperation-vs-ai-arms-race", "pageUrlRelative": "/posts/vw9QAviBxcGodMHfN/international-cooperation-vs-ai-arms-race", "linkUrl": "https://www.lesswrong.com/posts/vw9QAviBxcGodMHfN/international-cooperation-vs-ai-arms-race", "postedAtFormatted": "Thursday, December 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20International%20cooperation%20vs.%20AI%20arms%20race&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInternational%20cooperation%20vs.%20AI%20arms%20race%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvw9QAviBxcGodMHfN%2Finternational-cooperation-vs-ai-arms-race%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=International%20cooperation%20vs.%20AI%20arms%20race%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvw9QAviBxcGodMHfN%2Finternational-cooperation-vs-ai-arms-race", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvw9QAviBxcGodMHfN%2Finternational-cooperation-vs-ai-arms-race", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1055, "htmlBody": "<p><strong>Summary</strong></p>\n<p>I think there's a decent chance that governments will be the first to build artificial general intelligence (AI). International hostility, especially an <a href=\"http://wiki.lesswrong.com/wiki/AI_arms_race\">AI arms race</a>, could exacerbate risk-taking, hostile motivations, and errors of judgment when creating AI. If so, then international cooperation could be an important factor to consider when evaluating the <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a> of charities. That said, we may not want to popularize the arms-race consideration too openly lest we accelerate the race.</p>\n<p><strong>Will governments build AI first?</strong></p>\n<p>AI poses a national-security threat, and unless the militaries of powerful countries are very naive, it seems to me unlikely they'd allow AI research to proceed in private indefinitely. At some point the US military would confiscate the project from Google or Goldman Sachs, if the US military isn't already ahead of them in secret by that point. (DARPA already funds a lot of public AI research.)</p>\n<p>There are <em>some </em>scenarios in which private AI research wouldn't be nationalized:</p>\n<ul>\n<li>An unexpected AI foom before anyone realizes what was coming.</li>\n<li>The private developers stay underground for long enough not to be caught. This becomes less likely the more government surveillance improves (see \"<a href=\"http://intelligence.org/files/ArmsControl.pdf\">Arms Control and Intelligence Explosions</a>\").</li>\n<li>AI developers move to a \"safe haven\" country where they can't be taken over. (It seems like the international community might prevent this, however, in the same way it now seeks to suppress terrorism in other countries.)</li>\n</ul>\n<div>Each of these scenarios could happen, but it seems most likely to me that governments would ultimately control AI development.</div>\n<div><br /></div>\n<div><strong>AI arms races</strong></div>\n<div><br /></div>\n<div>Government AI development could go wrong in several ways. Probably most on LW feel the prevailing scenario is that governments would botch the process by not realizing the risks at hand. It's also possible that governments would use the AI for malevolent, totalitarian purposes.</div>\n<p>It seems that both of these bad scenarios would be exacerbated by international conflict. Greater hostility means countries are more inclined to use AI as a weapon. Indeed, whoever builds the first AI can take over the world, which makes building AI the ultimate arms race. A <a href=\"/lw/hoz/do_earths_with_slower_economic_growth_have_a/9590\">USA-China race</a> is one reasonable possibility.</p>\n<p>Arms races encourage risk-taking -- being willing to skimp on safety measures to improve your odds of winning (\"<a href=\"http://intelligence.org/2013/11/27/new-paper-racing-to-the-precipice/\">Racing to the Precipice</a>\"). In addition, the weaponization of AI could lead to worse expected outcomes in general. <a href=\"http://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition\">CEV</a> seems to have less hope of success in a Cold War scenario. (\"What? You want to include the evil <em>Chinese</em> in your CEV??\") (ETA: With a pure CEV, presumably it would eventually count Chinese values even if it started with just Americans, because people would become more enlightened during the process. However, when we imagine more crude democratic decision outcomes, this becomes less likely.)</p>\n<p><strong>Ways to avoid an arms race</strong></p>\n<p>Averting an AI arms race seems to be an important topic for research. It could be partly informed by the Cold War and other nuclear arms races, as well as by <a href=\"http://cns.miis.edu/\">other efforts</a> at nonproliferation of chemical and biological weapons.</p>\n<p>Apart from more robust arms control, other factors might help:</p>\n<ul>\n<li>Improved international institutions like the UN, allowing for better enforcement against defection by one state.</li>\n<li>In the long run, a scenario of <a href=\"https://en.wikipedia.org/wiki/Global_governance\">global governance</a>&nbsp;(i.e., a <a href=\"https://en.wikipedia.org/wiki/Leviathan_(book)\">Leviathan</a> or <a href=\"http://www.nickbostrom.com/fut/singleton.html\">singleton</a>) would likely be ideal for strengthening international cooperation, just like nation states <a href=\"https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature\">reduce intra-state violence</a>.</li>\n<li>Better construction and enforcement of nonproliferation treaties.</li>\n<li>Improved game theory and international-relations scholarship on the causes of arms races and how to avert them. (For instance, arms races have sometimes been modeled as iterated prisoner's dilemmas with imperfect information.)</li>\n<li>How to improve verification, which has historically been a weak point for nuclear arms control. (The concern is that if you haven't verified well enough, the other side might be arming while you're not.)</li>\n<li>Moral tolerance and multicultural perspective, aiming to reduce people's sense of nationalism. (In the limit where neither Americans nor Chinese cared which government won the race, there would be no point in having the race.)</li>\n<li>Improved trade, democracy, and other forces that historically have reduced the likelihood of war.</li>\n</ul>\n<p><strong>Are these efforts cost-effective?</strong></p>\n<p>World peace is hardly a goal unique to effective altruists (EAs), so we shouldn't necessarily expect low-hanging fruit. On the other hand, projects like nuclear nonproliferation seem relatively underfunded even compared with anti-poverty charities.</p>\n<p>I suspect more direct MIRI-type research has higher expected value, but among EAs who don't want to fund MIRI specifically, encouraging donations toward international cooperation could be valuable, since it's certainly a more mainstream cause. I wonder if GiveWell would consider studying global cooperation specifically beyond its <a href=\"http://utilitarian-essays.com/catastrophic-risks-and-compromise.html\">indirect relationship</a> with catastrophic risks.</p>\n<p><strong>Should we publicize AI arms races?</strong></p>\n<p>When I mentioned this topic to a friend, he pointed out that we might not want the idea of AI arms races too widely known, because then governments might take the concern more seriously and therefore start the race earlier -- giving us less time to prepare and less time to work on FAI in the meanwhile. From David Chalmers, \"<a href=\"http://consc.net/papers/singularity.pdf\">The Singularity: A Philosophical Analysis</a>\" (footnote 14):</p>\n<p style=\"padding-left: 30px;\">When I discussed these issues with cadets and staff at the West Point Military Academy, the question arose as to whether the US military or other branches of the government might attempt to prevent the creation of AI or AI+, due to the risks of an intelligence explosion. The consensus was that they would not, as such prevention would only increase the chances that AI or AI+ would first be created by a foreign power. One might even expect an AI arms race at some point, once the potential consequences of an intelligence explosion are registered. According to this reasoning, although AI+ would have risks from the standpoint of the US government, the risks of Chinese AI+ (say) would be far greater.</p>\n<div>We should take this information-hazard concern seriously and remember the <a href=\"http://www.nickbostrom.com/papers/unilateralist.pdf\">unilateralist's curse</a>. If it proves to be fatal for explicitly discussing AI arms races, we might instead encourage international cooperation without explaining <em>why</em>. Fortunately, it wouldn't be hard to encourage international cooperation on grounds other than AI arms races if we wanted to do so.</div>\n<div><br /></div>\n<div>ETA: Also note that a government-level arms race might be preferable to a Wild West race among a dozen private AI developers where coordination and compromise would be not just difficult but potentially impossible.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qHDus5MuMNqQxJbjD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vw9QAviBxcGodMHfN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 23, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "24978", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Summary\">Summary</strong></p>\n<p>I think there's a decent chance that governments will be the first to build artificial general intelligence (AI). International hostility, especially an <a href=\"http://wiki.lesswrong.com/wiki/AI_arms_race\">AI arms race</a>, could exacerbate risk-taking, hostile motivations, and errors of judgment when creating AI. If so, then international cooperation could be an important factor to consider when evaluating the <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a> of charities. That said, we may not want to popularize the arms-race consideration too openly lest we accelerate the race.</p>\n<p><strong id=\"Will_governments_build_AI_first_\">Will governments build AI first?</strong></p>\n<p>AI poses a national-security threat, and unless the militaries of powerful countries are very naive, it seems to me unlikely they'd allow AI research to proceed in private indefinitely. At some point the US military would confiscate the project from Google or Goldman Sachs, if the US military isn't already ahead of them in secret by that point. (DARPA already funds a lot of public AI research.)</p>\n<p>There are <em>some </em>scenarios in which private AI research wouldn't be nationalized:</p>\n<ul>\n<li>An unexpected AI foom before anyone realizes what was coming.</li>\n<li>The private developers stay underground for long enough not to be caught. This becomes less likely the more government surveillance improves (see \"<a href=\"http://intelligence.org/files/ArmsControl.pdf\">Arms Control and Intelligence Explosions</a>\").</li>\n<li>AI developers move to a \"safe haven\" country where they can't be taken over. (It seems like the international community might prevent this, however, in the same way it now seeks to suppress terrorism in other countries.)</li>\n</ul>\n<div>Each of these scenarios could happen, but it seems most likely to me that governments would ultimately control AI development.</div>\n<div><br></div>\n<div><strong>AI arms races</strong></div>\n<div><br></div>\n<div>Government AI development could go wrong in several ways. Probably most on LW feel the prevailing scenario is that governments would botch the process by not realizing the risks at hand. It's also possible that governments would use the AI for malevolent, totalitarian purposes.</div>\n<p>It seems that both of these bad scenarios would be exacerbated by international conflict. Greater hostility means countries are more inclined to use AI as a weapon. Indeed, whoever builds the first AI can take over the world, which makes building AI the ultimate arms race. A <a href=\"/lw/hoz/do_earths_with_slower_economic_growth_have_a/9590\">USA-China race</a> is one reasonable possibility.</p>\n<p>Arms races encourage risk-taking -- being willing to skimp on safety measures to improve your odds of winning (\"<a href=\"http://intelligence.org/2013/11/27/new-paper-racing-to-the-precipice/\">Racing to the Precipice</a>\"). In addition, the weaponization of AI could lead to worse expected outcomes in general. <a href=\"http://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition\">CEV</a> seems to have less hope of success in a Cold War scenario. (\"What? You want to include the evil <em>Chinese</em> in your CEV??\") (ETA: With a pure CEV, presumably it would eventually count Chinese values even if it started with just Americans, because people would become more enlightened during the process. However, when we imagine more crude democratic decision outcomes, this becomes less likely.)</p>\n<p><strong id=\"Ways_to_avoid_an_arms_race\">Ways to avoid an arms race</strong></p>\n<p>Averting an AI arms race seems to be an important topic for research. It could be partly informed by the Cold War and other nuclear arms races, as well as by <a href=\"http://cns.miis.edu/\">other efforts</a> at nonproliferation of chemical and biological weapons.</p>\n<p>Apart from more robust arms control, other factors might help:</p>\n<ul>\n<li>Improved international institutions like the UN, allowing for better enforcement against defection by one state.</li>\n<li>In the long run, a scenario of <a href=\"https://en.wikipedia.org/wiki/Global_governance\">global governance</a>&nbsp;(i.e., a <a href=\"https://en.wikipedia.org/wiki/Leviathan_(book)\">Leviathan</a> or <a href=\"http://www.nickbostrom.com/fut/singleton.html\">singleton</a>) would likely be ideal for strengthening international cooperation, just like nation states <a href=\"https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature\">reduce intra-state violence</a>.</li>\n<li>Better construction and enforcement of nonproliferation treaties.</li>\n<li>Improved game theory and international-relations scholarship on the causes of arms races and how to avert them. (For instance, arms races have sometimes been modeled as iterated prisoner's dilemmas with imperfect information.)</li>\n<li>How to improve verification, which has historically been a weak point for nuclear arms control. (The concern is that if you haven't verified well enough, the other side might be arming while you're not.)</li>\n<li>Moral tolerance and multicultural perspective, aiming to reduce people's sense of nationalism. (In the limit where neither Americans nor Chinese cared which government won the race, there would be no point in having the race.)</li>\n<li>Improved trade, democracy, and other forces that historically have reduced the likelihood of war.</li>\n</ul>\n<p><strong id=\"Are_these_efforts_cost_effective_\">Are these efforts cost-effective?</strong></p>\n<p>World peace is hardly a goal unique to effective altruists (EAs), so we shouldn't necessarily expect low-hanging fruit. On the other hand, projects like nuclear nonproliferation seem relatively underfunded even compared with anti-poverty charities.</p>\n<p>I suspect more direct MIRI-type research has higher expected value, but among EAs who don't want to fund MIRI specifically, encouraging donations toward international cooperation could be valuable, since it's certainly a more mainstream cause. I wonder if GiveWell would consider studying global cooperation specifically beyond its <a href=\"http://utilitarian-essays.com/catastrophic-risks-and-compromise.html\">indirect relationship</a> with catastrophic risks.</p>\n<p><strong id=\"Should_we_publicize_AI_arms_races_\">Should we publicize AI arms races?</strong></p>\n<p>When I mentioned this topic to a friend, he pointed out that we might not want the idea of AI arms races too widely known, because then governments might take the concern more seriously and therefore start the race earlier -- giving us less time to prepare and less time to work on FAI in the meanwhile. From David Chalmers, \"<a href=\"http://consc.net/papers/singularity.pdf\">The Singularity: A Philosophical Analysis</a>\" (footnote 14):</p>\n<p style=\"padding-left: 30px;\">When I discussed these issues with cadets and staff at the West Point Military Academy, the question arose as to whether the US military or other branches of the government might attempt to prevent the creation of AI or AI+, due to the risks of an intelligence explosion. The consensus was that they would not, as such prevention would only increase the chances that AI or AI+ would first be created by a foreign power. One might even expect an AI arms race at some point, once the potential consequences of an intelligence explosion are registered. According to this reasoning, although AI+ would have risks from the standpoint of the US government, the risks of Chinese AI+ (say) would be far greater.</p>\n<div>We should take this information-hazard concern seriously and remember the <a href=\"http://www.nickbostrom.com/papers/unilateralist.pdf\">unilateralist's curse</a>. If it proves to be fatal for explicitly discussing AI arms races, we might instead encourage international cooperation without explaining <em>why</em>. Fortunately, it wouldn't be hard to encourage international cooperation on grounds other than AI arms races if we wanted to do so.</div>\n<div><br></div>\n<div>ETA: Also note that a government-level arms race might be preferable to a Wild West race among a dozen private AI developers where coordination and compromise would be not just difficult but potentially impossible.</div>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Will governments build AI first?", "anchor": "Will_governments_build_AI_first_", "level": 1}, {"title": "Ways to avoid an arms race", "anchor": "Ways_to_avoid_an_arms_race", "level": 1}, {"title": "Are these efforts cost-effective?", "anchor": "Are_these_efforts_cost_effective_", "level": 1}, {"title": "Should we publicize AI arms races?", "anchor": "Should_we_publicize_AI_arms_races_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "144 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 144, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-05T01:54:07.000Z", "modifiedAt": null, "url": null, "title": "The Logician And The God-Emperor", "slug": "the-logician-and-the-god-emperor", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2gWs8SScqeDFidqyv/the-logician-and-the-god-emperor", "pageUrlRelative": "/posts/2gWs8SScqeDFidqyv/the-logician-and-the-god-emperor", "linkUrl": "https://www.lesswrong.com/posts/2gWs8SScqeDFidqyv/the-logician-and-the-god-emperor", "postedAtFormatted": "Thursday, December 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Logician%20And%20The%20God-Emperor&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Logician%20And%20The%20God-Emperor%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2gWs8SScqeDFidqyv%2Fthe-logician-and-the-god-emperor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Logician%20And%20The%20God-Emperor%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2gWs8SScqeDFidqyv%2Fthe-logician-and-the-god-emperor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2gWs8SScqeDFidqyv%2Fthe-logician-and-the-god-emperor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 283, "htmlBody": "<p>Once upon a time a logician accomplished a great deed, and the God-Emperor offered him a choice of rewards. &#8220;You may,&#8221; said the God-Emperor &#8220;have the hand of my eldest daughter, who is the heir to the throne, yet plain to look upon. Or you may take my youngest daughter, who is beautiful beyond words, but without inheritance.&#8221;</p>\n<p>The next day, the God-Emperor caught the logician in bed with <i>both</i> his daughters. Enraged, he hurled threats and abuse at the scholar, who responded with a grin: &#8220;Guess someone never learned the difference between &#8216;or&#8217; and &#8216;xor&#8217;.&#8221;</p>\n<p>The God-Emperor ordered the logician brought to the throne room in chains, and told him &#8220;You have offended me and betrayed my generosity, so you will be subjected to trial by ordeal. I have placed in front of you seven chests. Six of the chests contain skulls. One of the chests contains the key to your chains. I have asked the most devious minds in my kingdom to prepare a logic puzzle giving hints as to which chest is which. You may open a single chest. If you do not find the chest with the key on your first try, you will be slathered in barbecue sauce and thrown to the wolves.&#8221;</p>\n<p>The logician approached the chests, and upon each was written a clue in complicated logical notation. He examined all seven, and then stood a while, deep in thought. Finally, he opened the third chest. Inside was a golden key.</p>\n<p>&#8220;Very impressive!&#8221; said the God-Emperor. Then he yelled &#8220;Guards! Slather this man in barbecue sauce and throw him to the wolves!&#8221;</p>\n<p>&#8220;But&#8230;but!&#8221; babbled the terrified logician &#8220;&#8230;but you said&#8230;!&#8221;</p>\n<p>The God-Emperor grinned. &#8220;Guess someone never learned the difference between &#8216;if&#8217; and &#8216;iff&#8217;.&#8221;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"F2XfCTxXLQBGjbm8P": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2gWs8SScqeDFidqyv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 31, "extendedScore": null, "score": 8.1e-05, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "TQW9brvXJ5Fajorr4", "canonicalCollectionSlug": "codex", "canonicalBookId": "jF58hKP9ZLzgy22Jr", "canonicalNextPostSlug": "reverse-psychology", "canonicalPrevPostSlug": "confidence-levels-inside-and-outside-an-argument", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-05T10:42:07.430Z", "modifiedAt": null, "url": null, "title": "Meetup : Saint Petersburg, Russia. On discussions and some social skills", "slug": "meetup-saint-petersburg-russia-on-discussions-and-some", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "efim", "createdAt": "2013-04-14T00:57:28.743Z", "isAdmin": false, "displayName": "efim"}, "userId": "Y8azdhZD6fvWdGwaB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4soHJiiiSAwuHkiWs/meetup-saint-petersburg-russia-on-discussions-and-some", "pageUrlRelative": "/posts/4soHJiiiSAwuHkiWs/meetup-saint-petersburg-russia-on-discussions-and-some", "linkUrl": "https://www.lesswrong.com/posts/4soHJiiiSAwuHkiWs/meetup-saint-petersburg-russia-on-discussions-and-some", "postedAtFormatted": "Thursday, December 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Saint%20Petersburg%2C%20Russia.%20On%20discussions%20and%20some%20social%20skills&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Saint%20Petersburg%2C%20Russia.%20On%20discussions%20and%20some%20social%20skills%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4soHJiiiSAwuHkiWs%2Fmeetup-saint-petersburg-russia-on-discussions-and-some%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Saint%20Petersburg%2C%20Russia.%20On%20discussions%20and%20some%20social%20skills%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4soHJiiiSAwuHkiWs%2Fmeetup-saint-petersburg-russia-on-discussions-and-some", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4soHJiiiSAwuHkiWs%2Fmeetup-saint-petersburg-russia-on-discussions-and-some", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ud'>Saint Petersburg, Russia. On discussions and some social skills</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 December 2013 04:00:19PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043c. \u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u043d\u0441\u0442\u0438\u0442\u0443\u0442, \u0443\u043b.1-\u044f \u041a\u0440\u0430\u0441\u043d\u043e\u0430\u0440\u043c\u0435\u0439\u0441\u043a\u0430\u044f, \u0434\u043e\u043c 15</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please look for event details in russian in our newsletter or vk group:\n<a href=\"https://groups.google.com/forum/#!topic/less-wrong-saint-petersburg/mT-HqQFfFb0\" rel=\"nofollow\">newsletter</a> or \n<a href=\"http://vk.com/lw_spb\" rel=\"nofollow\">http://vk.com/lw_spb</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ud'>Saint Petersburg, Russia. On discussions and some social skills</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4soHJiiiSAwuHkiWs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4523873520038527e-06, "legacy": true, "legacyId": "24985", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Saint_Petersburg__Russia__On_discussions_and_some_social_skills\">Discussion article for the meetup : <a href=\"/meetups/ud\">Saint Petersburg, Russia. On discussions and some social skills</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 December 2013 04:00:19PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043c. \u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u043d\u0441\u0442\u0438\u0442\u0443\u0442, \u0443\u043b.1-\u044f \u041a\u0440\u0430\u0441\u043d\u043e\u0430\u0440\u043c\u0435\u0439\u0441\u043a\u0430\u044f, \u0434\u043e\u043c 15</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please look for event details in russian in our newsletter or vk group:\n<a href=\"https://groups.google.com/forum/#!topic/less-wrong-saint-petersburg/mT-HqQFfFb0\" rel=\"nofollow\">newsletter</a> or \n<a href=\"http://vk.com/lw_spb\" rel=\"nofollow\">http://vk.com/lw_spb</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Saint_Petersburg__Russia__On_discussions_and_some_social_skills1\">Discussion article for the meetup : <a href=\"/meetups/ud\">Saint Petersburg, Russia. On discussions and some social skills</a></h2>", "sections": [{"title": "Discussion article for the meetup : Saint Petersburg, Russia. On discussions and some social skills", "anchor": "Discussion_article_for_the_meetup___Saint_Petersburg__Russia__On_discussions_and_some_social_skills", "level": 1}, {"title": "Discussion article for the meetup : Saint Petersburg, Russia. On discussions and some social skills", "anchor": "Discussion_article_for_the_meetup___Saint_Petersburg__Russia__On_discussions_and_some_social_skills1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-05T14:42:12.946Z", "modifiedAt": null, "url": null, "title": "Snowdenizing UFAI", "slug": "snowdenizing-ufai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:28.004Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PSAyQmLWyRW72vc93/snowdenizing-ufai", "pageUrlRelative": "/posts/PSAyQmLWyRW72vc93/snowdenizing-ufai", "linkUrl": "https://www.lesswrong.com/posts/PSAyQmLWyRW72vc93/snowdenizing-ufai", "postedAtFormatted": "Thursday, December 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Snowdenizing%20UFAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASnowdenizing%20UFAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPSAyQmLWyRW72vc93%2Fsnowdenizing-ufai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Snowdenizing%20UFAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPSAyQmLWyRW72vc93%2Fsnowdenizing-ufai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPSAyQmLWyRW72vc93%2Fsnowdenizing-ufai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 708, "htmlBody": "<p>Here is a suggestion for slowing down future secretive and unsafe UFAI projects.</p>\n<p>Take the American defense and intelligence community as a case in point. They are a top candidate for the creation of Artificial General Intelligence (AGI): They can get the massive funding, and they can get some top (or near-top) brains on the job. The AGI will be unfriendly, unless friendliness is a primary goal from the start.</p>\n<p>The American defense and intelligence community created the Manhattan Project, which is the canonical example for a giant, secret, leading-edge science-technology project with existential-risk implications.</p>\n<p><a href=\"http://consc.net/papers/singularity.pdf\">David Chalmers (2010)</a>:&nbsp;\"When I discussed [AI existential risk] with cadets and sta\u000bff at the West Point Military Academy, the question arose as to whether the US military or other branches of the government might attempt to prevent the creation of AI or AI+, due to the risks of an intelligence explosion. The consensus was that they would not, as such prevention would only increase the chances that AI or AI+ would \ufb01rst be created by a foreign power.\"</p>\n<p>Edward Snowden broke the intelligence community's norms by reporting what he saw to be tremendous ethical and legal violations. This requires an exceptionally well-developed personal sense of ethics (even if you disagree with those ethics). His actions have drawn a lot of support by those who share his values. Many who condemn him a traitor are still criticizing government intrusions in the basis of his revelations.</p>\n<p>When the government AGI project starts rolling, will it have Snowdens who can warn internally about Unfriendly AI (UFAI) risks? They will probably be ignored and suppressed--that's how it goes in hierarchical bureaucratic organizations. Will these future Snowdens have the courage to keep fighting internally, and eventually to report the risks to the public or to their allies in the Friendly AI (FAI) research community</p>\n<p>Naturally, the Snowden scenario is not limited to the US government. We can seek ethical dissidents, truthtellers, and whistleblowers in any large and powerful organization that does unsafe research, whether a government or a corporation.</p>\n<p>Should we start preparing budding AGI researchers to think this way? We can do this by encouraging people to take consequentialist ethics seriously, which by itself can lead to Snowden-like results. and LessWrong is certainly working on that. But another approach is to start talking more directly about the \"UFAI Whistleblower Pledge.\"</p>\n<p style=\"padding-left: 30px;\">I hereby promise to fight unsafe AGI development in whatever way I can, through internal channels in my organization, by working with outside allies, or even by revealing the risks to the public.</p>\n<p>If this concept becomes widespread, and all the more so if people sign on, the threat of ethical whistleblowing will hover over every unsafe AGI project. Even with all the oaths and threats they use to make new employees keep&nbsp; secrets, the notion that speaking out on UFAI is deep in the consensus of serious AGI developers will cast a shadow on every project.</p>\n<p>To be clear, the beneficial effect I am talking about here is not the leaks--it is the atmosphere of potential leaks, the lack of trust by management that researchers are completely committed to keeping any&nbsp; secret. &nbsp;For example, post Snowden, the intelligence agencies are requiring that sensitive files only be accessed by two people working together and they are probably tightening their approval guidelines and so rejecting otherwise suitable candidates. These changes make everything more cumbersome.</p>\n<p>In creating the OpenCog project, Ben Goertzel advocated total openness as a way of accelerating the progress of those who are willing to expose any dangerous work they might be doing--even if this means that the safer researchers are giving their ideas to the unsafe, secretive ones.</p>\n<p>On the other hand, Eliezer Yudkowsky has suggested that MIRI keep its AGI implementation ideas secret, to avoid handing them to an unsafe project. (See \"<a href=\"/lw/g93/evaluating_the_feasibility_of_sis_plan/\">Evaluating the Feasibility of SI's Plans</a>,\" and, if you can stomach some argument from fictional evidence, \"<a href=\"http://robinhanson.typepad.com/files/three-worlds-collide.pdf\">Three Worlds Collide</a>.\") Encouraging openness and leaks could endanger Eliezer's strategy. But if we follow Eliezer's position, a truly ethical consequentialist would understand that exposing unsafe projects is good, while exposing safer projects is bad.</p>\n<p>So, what do you think? Should we start signing as many current and upcoming AGI researchers as possible to the UFAI Whistleblower Pledge, or work to make this an ethical norm in the community?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PSAyQmLWyRW72vc93", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 8, "extendedScore": null, "score": 1.452631476240208e-06, "legacy": true, "legacyId": "24983", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5evRqMmGxTKf98pvT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-05T17:08:34.812Z", "modifiedAt": null, "url": null, "title": "Meetup : The Return of the Rationalists!", "slug": "meetup-the-return-of-the-rationalists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.349Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ratcourse", "createdAt": "2012-11-03T10:26:05.641Z", "isAdmin": false, "displayName": "Ratcourse"}, "userId": "qwnfbBpAcbxLT4JBr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KbGttcz4DxwTa8syB/meetup-the-return-of-the-rationalists", "pageUrlRelative": "/posts/KbGttcz4DxwTa8syB/meetup-the-return-of-the-rationalists", "linkUrl": "https://www.lesswrong.com/posts/KbGttcz4DxwTa8syB/meetup-the-return-of-the-rationalists", "postedAtFormatted": "Thursday, December 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20The%20Return%20of%20the%20Rationalists!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20The%20Return%20of%20the%20Rationalists!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKbGttcz4DxwTa8syB%2Fmeetup-the-return-of-the-rationalists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20The%20Return%20of%20the%20Rationalists!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKbGttcz4DxwTa8syB%2Fmeetup-the-return-of-the-rationalists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKbGttcz4DxwTa8syB%2Fmeetup-the-return-of-the-rationalists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ue'>The Return of the Rationalists!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 December 2013 03:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Reichsratsstra\u00dfe 17, 1010 Wien</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup at the Cafe Votiv!\nAn introduction to Bayesian Reasoning by Viliam!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ue'>The Return of the Rationalists!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KbGttcz4DxwTa8syB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 1.4527803344263544e-06, "legacy": true, "legacyId": "24987", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___The_Return_of_the_Rationalists_\">Discussion article for the meetup : <a href=\"/meetups/ue\">The Return of the Rationalists!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 December 2013 03:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Reichsratsstra\u00dfe 17, 1010 Wien</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup at the Cafe Votiv!\nAn introduction to Bayesian Reasoning by Viliam!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___The_Return_of_the_Rationalists_1\">Discussion article for the meetup : <a href=\"/meetups/ue\">The Return of the Rationalists!</a></h2>", "sections": [{"title": "Discussion article for the meetup : The Return of the Rationalists!", "anchor": "Discussion_article_for_the_meetup___The_Return_of_the_Rationalists_", "level": 1}, {"title": "Discussion article for the meetup : The Return of the Rationalists!", "anchor": "Discussion_article_for_the_meetup___The_Return_of_the_Rationalists_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-05T17:09:53.604Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, The First Winter One", "slug": "meetup-moscow-the-first-winter-one", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5XRMMRtENYhS5oCSw/meetup-moscow-the-first-winter-one", "pageUrlRelative": "/posts/5XRMMRtENYhS5oCSw/meetup-moscow-the-first-winter-one", "linkUrl": "https://www.lesswrong.com/posts/5XRMMRtENYhS5oCSw/meetup-moscow-the-first-winter-one", "postedAtFormatted": "Thursday, December 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20The%20First%20Winter%20One&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20The%20First%20Winter%20One%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5XRMMRtENYhS5oCSw%2Fmeetup-moscow-the-first-winter-one%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20The%20First%20Winter%20One%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5XRMMRtENYhS5oCSw%2Fmeetup-moscow-the-first-winter-one", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5XRMMRtENYhS5oCSw%2Fmeetup-moscow-the-first-winter-one", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/uf'>Moscow, The First Winter One</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 December 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will have a report about psychedelic drugs and a discussion session.</p>\n\n<p>We will also have <a href=\"http://lesswrong.ru/forum/index.php/topic,103.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_prediction_market+20131208_meet_up&amp;utm_content=20131208_meet_up&amp;utm_campaign=moscow_meetups\">Prediction Market</a> and <a href=\"http://lesswrong.ru/forum/index.php/topic,223.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_honor_board+20131208_meet_up&amp;utm_content=20131208_meet_up&amp;utm_campaign=moscow_meetups\">Scavenger Hunt</a>. You can follow the corresponding links to the discussions on the Russian forum.</p>\n\n<p>We may move to another location from Yandex office, so please do not be late.</p>\n\n<p><strong>If you are going for the first time:</strong>\nWe gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/uf'>Moscow, The First Winter One</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5XRMMRtENYhS5oCSw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4527816700891725e-06, "legacy": true, "legacyId": "24988", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__The_First_Winter_One\">Discussion article for the meetup : <a href=\"/meetups/uf\">Moscow, The First Winter One</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 December 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will have a report about psychedelic drugs and a discussion session.</p>\n\n<p>We will also have <a href=\"http://lesswrong.ru/forum/index.php/topic,103.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_prediction_market+20131208_meet_up&amp;utm_content=20131208_meet_up&amp;utm_campaign=moscow_meetups\">Prediction Market</a> and <a href=\"http://lesswrong.ru/forum/index.php/topic,223.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_honor_board+20131208_meet_up&amp;utm_content=20131208_meet_up&amp;utm_campaign=moscow_meetups\">Scavenger Hunt</a>. You can follow the corresponding links to the discussions on the Russian forum.</p>\n\n<p>We may move to another location from Yandex office, so please do not be late.</p>\n\n<p><strong>If you are going for the first time:</strong>\nWe gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__The_First_Winter_One1\">Discussion article for the meetup : <a href=\"/meetups/uf\">Moscow, The First Winter One</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, The First Winter One", "anchor": "Discussion_article_for_the_meetup___Moscow__The_First_Winter_One", "level": 1}, {"title": "Discussion article for the meetup : Moscow, The First Winter One", "anchor": "Discussion_article_for_the_meetup___Moscow__The_First_Winter_One1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-06T01:46:51.570Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign fun and games", "slug": "meetup-urbana-champaign-fun-and-games-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xDDDoijKm3z62pFaY/meetup-urbana-champaign-fun-and-games-1", "pageUrlRelative": "/posts/xDDDoijKm3z62pFaY/meetup-urbana-champaign-fun-and-games-1", "linkUrl": "https://www.lesswrong.com/posts/xDDDoijKm3z62pFaY/meetup-urbana-champaign-fun-and-games-1", "postedAtFormatted": "Friday, December 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%20fun%20and%20games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%20fun%20and%20games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDDDoijKm3z62pFaY%2Fmeetup-urbana-champaign-fun-and-games-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%20fun%20and%20games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDDDoijKm3z62pFaY%2Fmeetup-urbana-champaign-fun-and-games-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDDDoijKm3z62pFaY%2Fmeetup-urbana-champaign-fun-and-games-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ug'>Urbana-Champaign fun and games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 December 2013 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">412 W. Elm St., Urbana, IL, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come drop by my house for board games, food, and word-talky-stuff.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ug'>Urbana-Champaign fun and games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xDDDoijKm3z62pFaY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.4533076612199847e-06, "legacy": true, "legacyId": "24991", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign_fun_and_games\">Discussion article for the meetup : <a href=\"/meetups/ug\">Urbana-Champaign fun and games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 December 2013 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">412 W. Elm St., Urbana, IL, 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come drop by my house for board games, food, and word-talky-stuff.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign_fun_and_games1\">Discussion article for the meetup : <a href=\"/meetups/ug\">Urbana-Champaign fun and games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign fun and games", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign_fun_and_games", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign fun and games", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign_fun_and_games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-06T02:12:01.708Z", "modifiedAt": null, "url": null, "title": "Personal examples of semantic stopsigns", "slug": "personal-examples-of-semantic-stopsigns", "viewCount": null, "lastCommentedAt": "2021-06-12T15:55:14.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexei", "createdAt": "2010-08-02T15:14:11.411Z", "isAdmin": false, "displayName": "Alexei"}, "userId": "CD3DC5D7GHtgBmxz5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tymen39BXdebHPeJG/personal-examples-of-semantic-stopsigns", "pageUrlRelative": "/posts/tymen39BXdebHPeJG/personal-examples-of-semantic-stopsigns", "linkUrl": "https://www.lesswrong.com/posts/tymen39BXdebHPeJG/personal-examples-of-semantic-stopsigns", "postedAtFormatted": "Friday, December 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Personal%20examples%20of%20semantic%20stopsigns&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APersonal%20examples%20of%20semantic%20stopsigns%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftymen39BXdebHPeJG%2Fpersonal-examples-of-semantic-stopsigns%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Personal%20examples%20of%20semantic%20stopsigns%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftymen39BXdebHPeJG%2Fpersonal-examples-of-semantic-stopsigns", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftymen39BXdebHPeJG%2Fpersonal-examples-of-semantic-stopsigns", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<p>I think most of us are familiar with the common&nbsp;<a href=\"/lw/it/semantic_stopsigns/\">semantic stopsigns</a> like \"God\", \"just because\", and \"it's a tradition.\" However, I've recently been noticing more interesting ones that I haven't really seen discussed on LW. (Or it's also likely that I missed those discussion.)</p>\n<p>The first one is \"humans are stupid.\" I notice this one very often, in particular in LW and other rationalist communities. The obvious problem here is that humans are not <em>that</em> stupid. Often what might seem like sheer stupidity was caused by a rather reasonable chain of actions and events. And even if a person or a group of people <em>is</em> being stupid, it's very interesting to chase down the cause. That's how you end up discovering biases from scratch or finding a great opportunity.</p>\n<p>The second semantic stopsign is \"should.\" Hat tip to Michael Vassar for bringing this one up. If you and I have a discussing about how I eat too much chocolate, and I say, \"You are right, I should eat less chocolate,\" the conversation will basically end there. But 99 times out of a 100 nothing will actually come out of it. I try to taboo the word \"should\" from my vocabulary, so instead I will say something like, \"You are right, I will not purchase any chocolate this month.\" This is a concrete <em>actionable</em> statement.</p>\n<p>What other semantic stopsigns have you noticed in yourself and others?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LnEEs8xGooYmQ8iLA": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tymen39BXdebHPeJG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 69, "extendedScore": null, "score": 0.0006461284158107938, "legacy": true, "legacyId": "24993", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FWMfQKG3RpZx6irjm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-06T09:54:33.170Z", "modifiedAt": null, "url": null, "title": "Kurzban et al. on opportunity cost models of mental fatigue and resource-based models of willpower", "slug": "kurzban-et-al-on-opportunity-cost-models-of-mental-fatigue", "viewCount": null, "lastCommentedAt": "2022-02-17T06:20:38.119Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HWvqNAc7aeeeWngEh/kurzban-et-al-on-opportunity-cost-models-of-mental-fatigue", "pageUrlRelative": "/posts/HWvqNAc7aeeeWngEh/kurzban-et-al-on-opportunity-cost-models-of-mental-fatigue", "linkUrl": "https://www.lesswrong.com/posts/HWvqNAc7aeeeWngEh/kurzban-et-al-on-opportunity-cost-models-of-mental-fatigue", "postedAtFormatted": "Friday, December 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Kurzban%20et%20al.%20on%20opportunity%20cost%20models%20of%20mental%20fatigue%20and%20resource-based%20models%20of%20willpower&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKurzban%20et%20al.%20on%20opportunity%20cost%20models%20of%20mental%20fatigue%20and%20resource-based%20models%20of%20willpower%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHWvqNAc7aeeeWngEh%2Fkurzban-et-al-on-opportunity-cost-models-of-mental-fatigue%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Kurzban%20et%20al.%20on%20opportunity%20cost%20models%20of%20mental%20fatigue%20and%20resource-based%20models%20of%20willpower%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHWvqNAc7aeeeWngEh%2Fkurzban-et-al-on-opportunity-cost-models-of-mental-fatigue", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHWvqNAc7aeeeWngEh%2Fkurzban-et-al-on-opportunity-cost-models-of-mental-fatigue", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1396, "htmlBody": "<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/12/Kurzban-et-al-An-opportunity-cost-model-of-subjective-effort-and-task-performance-plus-responses.pdf\">An opportunity cost model of subjective effort and task performance</a>&nbsp;(h/t lukeprog) is a very interesting paper on why we accumulate mental fatigue: Kurzban et al. suggest an opportunity cost model, where intense focus on a single task means that we become less capable of using our mental resources for anything else, and accumulating mental fatigue is part of a cost-benefit calculation that encourages us to shift our attention instead of monomaniacally concentrating on just one task which may not be the most rewarding possible. Correspondingly, the amount of boredom or mental fatigue we experience with a task should correspond with the perceived rewards from other tasks available at the moment. A task will feel more boring/effortful if there's something more rewarding that you could be doing instead (i.e. if the opportunity costs for pursuing your current task are higher), and if it requires exclusive use of cognitive resources that could also be used for something else.</p>\n<p>This seems to make an amount of intuitive/introspective sense - I had a much easier time doing stuff without getting bored as a kid, when there simply wasn't much else that I could be doing instead. And it does roughly feel like I would get more quickly bored with things in situations where more engaging pursuits were available. I'm also reminded of the thing I noticed as a kid where, if I borrowed a single book from the library, I would likely get quickly engrossed in it, whereas if I had several alternatives it would be more likely that I'd end up looking at each for a bit but never really get around reading any of them.</p>\n<p>An opportunity cost model also makes more sense than resource models of willpower which, as Kurzban <a href=\"/lw/hzd/stupid_questions_thread/9d66\">quite persuasively argued</a> in his earlier book, don't really fit together with the fact that the brain is an information-processing system. My computer doesn't need to use any more electricity in situations where it \"decides\" to do something as opposed to not doing something, but resource models of willpower have tried to postulate that we would need more of e.g. glucose in order to maintain willpower. (Rather, it makes more sense to presume that a low level of blood sugar would shift the cost-benefit calculations in a way that led to e.g. conservation of resources.)</p>\n<p>This isn't just Kurzban et al's opinion - the paper was published in <em>Behavioral and Brain Sciences</em>, which invites diverse comments to all the papers that they publish. In this particular case, it was surprising how muted the defenses of the resource model were. As Kurzban et al point out in their response to responses:</p>\n<blockquote>\n<p>As context for our expectations, consider the impact of one of the central ideas with which we were taking issue, the claim that &ldquo;willpower&rdquo; is a resource that is consumed when self-control is exerted. To give a sense of the reach of this idea, in the same month that our target article was accepted for publication Michael Lewis reported in Vanity Fair that no less a figure than President Barack Obama was aware of, endorsed, and based his decision- making process on the general idea that &ldquo;the simple act of making decisions degrades one&rsquo;s ability to make further decisions,&rdquo; with Obama explaining: &ldquo;I&rsquo;m trying to pare down decisions. I don&rsquo;t want to make decisions about what I&rsquo;m eating or wearing. Because I have too many other decisions to make &rdquo; (Lewis 2012 ).</p>\n<p>Add to this the fact that a book based on this idea became a New York Times bestseller (Baumeister &amp; Tierney 2011 ), the fact that a central paper articulating the idea (Baumeister et al. 1998 ) has been cited more than 1,400 times, and, more broadly, the vast number of research programs using this idea as a foundation, and we can be forgiven for thinking that we would have kicked up something of a hornet&rsquo;s nest in suggesting that the willpower-as-resource model was wrong. So we anticipated no small amount of stings from the large number of scholars involved in this research enterprise. These were our expectations before receiving the commentaries.</p>\n<p>Our expectations were not met. Take, for example, the reaction to our claim that the glucose version of the resource argument is false (Kurzban 2010a ). Inzlicht &amp; Schmeichel, scholars who have published widely in the willpower-as-resource literature, more or less casually bury the model with the remark in their commentary that the &ldquo;mounting evidence points to the conclusion that blood glucose is not the proximate mechanism of depletion.&rdquo; ( Malecek &amp; Poldrack express a similar view.) Not a single voice has been raised to defend the glucose model, and, given the evidence that we advanced to support our view that this model is unlikely to be correct, we hope that researchers will take the fact that none of the impressive array of scholars submitting comments defended the view to be a good indication that perhaps the model is, in fact, indefensible. Even if the opportunity cost account of effort turns out not to be correct, we are pleased that the evidence from the commentaries &ndash; or the absence of evidence &ndash; will stand as an indication to audiences that it might be time to move to more profitable explanations of subjective effort.</p>\n<p>While the silence on the glucose model is perhaps most obvious, we are similarly surprised by the remarkably light defense of the resource view more generally. As Kool &amp; Botvinick put it, quite correctly in our perception: &ldquo;Research on the dynamics of cognitive effort have been <em>dominated</em>, over recent decades, by accounts centering on the notion of a limited and depletable &lsquo;resource&rsquo;&rdquo; (italics ours). It would seem to be quite surprising, then, that in the context of our critique of the dominant view, arguably the strongest pertinent remarks come from Carter &amp; McCullough, who imply that the strength of the key phenomenon that underlies the resource model &ndash; two-task &ldquo;ego-depletion&rdquo; studies &ndash; might be considerably less than previously thought or perhaps even nonexistent. Despite the confidence voiced by Inzlicht &amp; Schmeichel about the two-task findings, the strongest voices surrounding the model, then, are raised against it, rather than for it. (See also Monterosso &amp; Luo , who are similarly skeptical of the resource account.)</p>\n<p>Indeed, what defenses there are of the resource account are not nearly as adamant as we had expected. Hagger wonders if there is &ldquo;still room for a &lsquo;resource&rsquo; account,&rdquo; given the evidence that cuts against it, conceding that &ldquo;[t]he ego-depletion literature is problematic.&rdquo; Further, he relies largely on the argument that the opportunity cost model we offer might be incomplete, thus &ldquo;leaving room&rdquo; for other ideas.</p>\n</blockquote>\n<p>(I'm leaving out discussion of some commentaries which do attempt to defend resource models.)</p>\n<p>Though the model still seems to be missing pieces - as one of the commentaries points out, it doesn't really address the fact that some tasks are more inherently boring than others. Some of it might be explained by the argument given in <a href=\"http://simulacrumbs.com/2013/09/shouts-whispers-and-the-myth-of-willpower-a-recursive-guide-to-efficacy/\">Shouts, Whispers, and the Myth of Willpower: A Recursive Guide to Efficacy</a> (I quote the most relevant bit <a href=\"/lw/ita/how_habits_work_and_how_you_may_control_them/9xbl\">here</a>), where the author suggests that \"self-discipline\" in some domain is really about sensitivity for feedback in that domain: a novice in some task doesn't really manage to notice the small nuances that have become so significant for an expert, so they receive little feedback for their actions and it ends up being a boring vigilance task. Whereas an expert will instantly notice the effects that their actions have on the system and get feedback of their progress, which in the opportunity cost model could be interpreted as raising the worthwhileness of the task they're working on. If we go with Kurzban et al.'s notion of us acquiring further information about the expected utility of the task we're working on as we continue working on it, then getting feedback from the task could possibly be read as a sign of the task being one in which we can expect to succeed in.</p>\n<p>Another missing piece with the model is that it doesn't really seem to explain the way that one can come home after a long day at work and then feel too exhausted to do <em>anything at all</em> - it can't really be about opportunity costs if you end up so tired that you can't come up with ~any activity that you'd want to do.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ksdiAMKfgSyEeKMo6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HWvqNAc7aeeeWngEh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 34, "extendedScore": null, "score": 9e-05, "legacy": true, "legacyId": "25007", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-06T15:27:57.730Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Fermi Estimates Meetup", "slug": "meetup-washington-dc-fermi-estimates-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:02.407Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TFcCtTjYjx6HmQowM/meetup-washington-dc-fermi-estimates-meetup", "pageUrlRelative": "/posts/TFcCtTjYjx6HmQowM/meetup-washington-dc-fermi-estimates-meetup", "linkUrl": "https://www.lesswrong.com/posts/TFcCtTjYjx6HmQowM/meetup-washington-dc-fermi-estimates-meetup", "postedAtFormatted": "Friday, December 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Fermi%20Estimates%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Fermi%20Estimates%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTFcCtTjYjx6HmQowM%2Fmeetup-washington-dc-fermi-estimates-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Fermi%20Estimates%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTFcCtTjYjx6HmQowM%2Fmeetup-washington-dc-fermi-estimates-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTFcCtTjYjx6HmQowM%2Fmeetup-washington-dc-fermi-estimates-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/uh'>Washington DC Fermi Estimates Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 December 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>By the power of the meetup topics list, we'll be meeting to do fermi estimates. I'll bring some numbers for us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/uh'>Washington DC Fermi Estimates Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TFcCtTjYjx6HmQowM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.4541437910582148e-06, "legacy": true, "legacyId": "25009", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Fermi_Estimates_Meetup\">Discussion article for the meetup : <a href=\"/meetups/uh\">Washington DC Fermi Estimates Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 December 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>By the power of the meetup topics list, we'll be meeting to do fermi estimates. I'll bring some numbers for us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Fermi_Estimates_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/uh\">Washington DC Fermi Estimates Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Fermi Estimates Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Fermi_Estimates_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Fermi Estimates Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Fermi_Estimates_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-06T17:33:00.969Z", "modifiedAt": null, "url": null, "title": "Meetup : December Meetup ", "slug": "meetup-december-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:03.283Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jzCT3ri8EgbFKgEmr/meetup-december-meetup", "pageUrlRelative": "/posts/jzCT3ri8EgbFKgEmr/meetup-december-meetup", "linkUrl": "https://www.lesswrong.com/posts/jzCT3ri8EgbFKgEmr/meetup-december-meetup", "postedAtFormatted": "Friday, December 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20December%20Meetup%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20December%20Meetup%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzCT3ri8EgbFKgEmr%2Fmeetup-december-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20December%20Meetup%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzCT3ri8EgbFKgEmr%2Fmeetup-december-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzCT3ri8EgbFKgEmr%2Fmeetup-december-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ui'>December Meetup </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 December 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">491 Lindbergh Place NE Apt 618 Atlanta, GA 30324</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join us! We'll be celebrating the Solstice while doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times! Check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/Atlanta.Lesswrong/ where you can connect with Atlanta Lesswrongers, suggest a topics for discussion at this meetup, and join our book club or study group!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ui'>December Meetup </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jzCT3ri8EgbFKgEmr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 0, "extendedScore": null, "score": 1.4542712083698099e-06, "legacy": true, "legacyId": "25010", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___December_Meetup_\">Discussion article for the meetup : <a href=\"/meetups/ui\">December Meetup </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 December 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">491 Lindbergh Place NE Apt 618 Atlanta, GA 30324</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join us! We'll be celebrating the Solstice while doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times! Check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/Atlanta.Lesswrong/ where you can connect with Atlanta Lesswrongers, suggest a topics for discussion at this meetup, and join our book club or study group!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___December_Meetup_1\">Discussion article for the meetup : <a href=\"/meetups/ui\">December Meetup </a></h2>", "sections": [{"title": "Discussion article for the meetup : December Meetup ", "anchor": "Discussion_article_for_the_meetup___December_Meetup_", "level": 1}, {"title": "Discussion article for the meetup : December Meetup ", "anchor": "Discussion_article_for_the_meetup___December_Meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-06T17:41:46.365Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Newcastle-upon-Tyne", "slug": "new-lw-meetup-newcastle-upon-tyne", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.860Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yQyFJCK9Ttub9HrAt/new-lw-meetup-newcastle-upon-tyne", "pageUrlRelative": "/posts/yQyFJCK9Ttub9HrAt/new-lw-meetup-newcastle-upon-tyne", "linkUrl": "https://www.lesswrong.com/posts/yQyFJCK9Ttub9HrAt/new-lw-meetup-newcastle-upon-tyne", "postedAtFormatted": "Friday, December 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Newcastle-upon-Tyne&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Newcastle-upon-Tyne%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyQyFJCK9Ttub9HrAt%2Fnew-lw-meetup-newcastle-upon-tyne%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Newcastle-upon-Tyne%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyQyFJCK9Ttub9HrAt%2Fnew-lw-meetup-newcastle-upon-tyne", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyQyFJCK9Ttub9HrAt%2Fnew-lw-meetup-newcastle-upon-tyne", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 530, "htmlBody": "<p><strong>This summary was posted to LW main on November 29th. The following week's summary is <a href=\"/lw/jar/new_lw_meetups_leipzig_utrecht/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/tv\"></a><a href=\"/meetups/tv\">Mumbai Meetup:&nbsp;<span class=\"date\">15 December 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/tz\">Newcastle-upon-Tyne meetup, December:&nbsp;<span class=\"date\">07 December 2013 01:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/tx\">Boston / Cambridge - The future of life: a cosmic perspective (Max Tegmark), Dec 1:&nbsp;<span class=\"date\">01 December 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/tf\">Berlin:&nbsp;<span class=\"date\">01 January 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/ty\">Helsinki Meetup:&nbsp;<span class=\"date\">15 December 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/tx\"></a><a href=\"/meetups/u1\">Munich Meetup:&nbsp;<span class=\"date\">07 December 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/u3\">San Francisco / App Academy meetup:&nbsp;<span class=\"date\">07 December 2014 07:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/ti\">Brussels monthly meetup: time!:&nbsp;<span class=\"date\">14 December 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/u0\">London Social Meetup, 01/12/2013:&nbsp;<span class=\"date\">01 December 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/u2\">West LA&mdash;A Conversation About Conversations:&nbsp;<span class=\"date\">04 December 2013 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yQyFJCK9Ttub9HrAt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.4542801311657097e-06, "legacy": true, "legacyId": "24912", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["u97qcQK7DBFy3AHXH", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-06T17:42:51.971Z", "modifiedAt": null, "url": null, "title": "Philadelphia Meetup: Sunday 12/6, Postrel's _The Power of Glamour_--  Winfrey/Moore video link added", "slug": "philadelphia-meetup-sunday-12-6-postrel-s-_the-power-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:03.654Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CfTEgygPnYtup9Luo/philadelphia-meetup-sunday-12-6-postrel-s-_the-power-of", "pageUrlRelative": "/posts/CfTEgygPnYtup9Luo/philadelphia-meetup-sunday-12-6-postrel-s-_the-power-of", "linkUrl": "https://www.lesswrong.com/posts/CfTEgygPnYtup9Luo/philadelphia-meetup-sunday-12-6-postrel-s-_the-power-of", "postedAtFormatted": "Friday, December 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Philadelphia%20Meetup%3A%20Sunday%2012%2F6%2C%20Postrel's%20_The%20Power%20of%20Glamour_--%20%20Winfrey%2FMoore%20video%20link%20added&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhiladelphia%20Meetup%3A%20Sunday%2012%2F6%2C%20Postrel's%20_The%20Power%20of%20Glamour_--%20%20Winfrey%2FMoore%20video%20link%20added%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCfTEgygPnYtup9Luo%2Fphiladelphia-meetup-sunday-12-6-postrel-s-_the-power-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Philadelphia%20Meetup%3A%20Sunday%2012%2F6%2C%20Postrel's%20_The%20Power%20of%20Glamour_--%20%20Winfrey%2FMoore%20video%20link%20added%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCfTEgygPnYtup9Luo%2Fphiladelphia-meetup-sunday-12-6-postrel-s-_the-power-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCfTEgygPnYtup9Luo%2Fphiladelphia-meetup-sunday-12-6-postrel-s-_the-power-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<p>1:00 PM, December 6, Nam Phuong (Vietnamese restaurant),&nbsp;<span style=\"font-size: 13px; color: #222222; font-family: arial, sans-serif; line-height: 16px;\">&nbsp;</span><span style=\"font-size: 13px; color: #222222; font-family: arial, sans-serif; line-height: 16px;\">1100 Washington Ave, Philadelphia, PA 19147 (west side of street, just south of Washington)</span></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; line-height: 16px;\">http://www.thedailybeast.com/articles/2013/11/09/the-persistence-of-glamour.html&nbsp;</span></p>\n<p>The topic is Postrel's&nbsp; <a href=\"http://www.amazon.com/The-Power-Glamour-Longing-Persuasion/dp/1416561110\">The Power of Glamour: Longing and the Art of Visual Persuasion</a>-- a category of visual images which sometimes cause people to change their lives.</p>\n<p><a href=\"https://www.youtube.com/watch?v=oPd77roTra4\">https://www.youtube.com/watch?v=oPd77roTra4</a></p>\n<p>This is a video about Oprah having been inspired by Mary Tyler Moore. Oprah was inspired by the Mary Tyler Moore show, and eventually had a video made of herself in the opening bit for the Mary Tyler Moore Show and had Mary Tyler Moore on her own show.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CfTEgygPnYtup9Luo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.4542812453459228e-06, "legacy": true, "legacyId": "25012", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-06T19:37:27.685Z", "modifiedAt": null, "url": null, "title": "Consciousness affecting the world", "slug": "consciousness-affecting-the-world", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:07.162Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DavidPlumpton", "createdAt": "2011-07-08T09:36:12.175Z", "isAdmin": false, "displayName": "DavidPlumpton"}, "userId": "JyR3HoGsDEckYxEGE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kFFRypgw22ixqRXH3/consciousness-affecting-the-world", "pageUrlRelative": "/posts/kFFRypgw22ixqRXH3/consciousness-affecting-the-world", "linkUrl": "https://www.lesswrong.com/posts/kFFRypgw22ixqRXH3/consciousness-affecting-the-world", "postedAtFormatted": "Friday, December 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Consciousness%20affecting%20the%20world&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConsciousness%20affecting%20the%20world%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFFRypgw22ixqRXH3%2Fconsciousness-affecting-the-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Consciousness%20affecting%20the%20world%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFFRypgw22ixqRXH3%2Fconsciousness-affecting-the-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFFRypgw22ixqRXH3%2Fconsciousness-affecting-the-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<p>In Zombies! Zombies? Eliezer mentions that one aspect of consciousness is that it can causally affect the real world, e.g. cause you to say \"I feel conscious right now\", or result in me typing out these words.</p>\n<p>Even if a generally accepted mechanism of consciousness has not been found yet are there any tentative explanations for this \"can change world\" property? Googling around I was unable to find anything (although Zombies are certainly popular).</p>\n<p>I had an idea of how this might work, but just wanted to see if it was worth the effort of writing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kFFRypgw22ixqRXH3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -5, "extendedScore": null, "score": 1.4543980250370746e-06, "legacy": true, "legacyId": "25013", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-07T00:20:30.981Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Weekly meetup", "slug": "meetup-vancouver-weekly-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kytael", "createdAt": "2010-09-06T10:08:43.209Z", "isAdmin": false, "displayName": "Kytael"}, "userId": "PTeFKC8ezhTDEi2qL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NPCKi4LfWoH8Tfgey/meetup-vancouver-weekly-meetup", "pageUrlRelative": "/posts/NPCKi4LfWoH8Tfgey/meetup-vancouver-weekly-meetup", "linkUrl": "https://www.lesswrong.com/posts/NPCKi4LfWoH8Tfgey/meetup-vancouver-weekly-meetup", "postedAtFormatted": "Saturday, December 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Weekly%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Weekly%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNPCKi4LfWoH8Tfgey%2Fmeetup-vancouver-weekly-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Weekly%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNPCKi4LfWoH8Tfgey%2Fmeetup-vancouver-weekly-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNPCKi4LfWoH8Tfgey%2Fmeetup-vancouver-weekly-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/uj'>Vancouver Weekly meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 December 2013 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Benny's Bagels 2505 W Broadway, Vancouver, British Columbia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>bring your rationality problems! we'll provide computing power to help. unofficial topic if nothing else: we'll continue our discussion of the value of rationality/what rationality looks like to outsiders</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/uj'>Vancouver Weekly meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NPCKi4LfWoH8Tfgey", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.454686546851013e-06, "legacy": true, "legacyId": "25014", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Weekly_meetup\">Discussion article for the meetup : <a href=\"/meetups/uj\">Vancouver Weekly meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 December 2013 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Benny's Bagels 2505 W Broadway, Vancouver, British Columbia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>bring your rationality problems! we'll provide computing power to help. unofficial topic if nothing else: we'll continue our discussion of the value of rationality/what rationality looks like to outsiders</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Weekly_meetup1\">Discussion article for the meetup : <a href=\"/meetups/uj\">Vancouver Weekly meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Weekly meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Weekly_meetup", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Weekly meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Weekly_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-07T08:23:47.600Z", "modifiedAt": null, "url": null, "title": "The Limits of Intelligence and Me: Domain Expertise", "slug": "the-limits-of-intelligence-and-me-domain-expertise", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:20.377Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d693kbwauFNnq96WQ/the-limits-of-intelligence-and-me-domain-expertise", "pageUrlRelative": "/posts/d693kbwauFNnq96WQ/the-limits-of-intelligence-and-me-domain-expertise", "linkUrl": "https://www.lesswrong.com/posts/d693kbwauFNnq96WQ/the-limits-of-intelligence-and-me-domain-expertise", "postedAtFormatted": "Saturday, December 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Limits%20of%20Intelligence%20and%20Me%3A%20Domain%20Expertise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Limits%20of%20Intelligence%20and%20Me%3A%20Domain%20Expertise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd693kbwauFNnq96WQ%2Fthe-limits-of-intelligence-and-me-domain-expertise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Limits%20of%20Intelligence%20and%20Me%3A%20Domain%20Expertise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd693kbwauFNnq96WQ%2Fthe-limits-of-intelligence-and-me-domain-expertise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd693kbwauFNnq96WQ%2Fthe-limits-of-intelligence-and-me-domain-expertise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1583, "htmlBody": "<p>Related to: <a href=\"/lw/iu0/trusting_expert_consensus/\">Trusting Expert Consensus</a></p>\n<p>In <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">the sequences</a>, Eliezer&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age\">tells the story</a>&nbsp;of how in childhood he fell into an affective death spiral around intelligence. In his story, his mistakes were failing to understand until he was much older that intelligence does not guarantee morality, and that very intelligent people can still end up believing crazy things because of human irrationality.</p>\n<p style=\"text-align: justify;\">I have my own story about learning the limits of intelligence, but I ended up learning a very different lesson than the one Eliezer learned. It also started somewhat differently. It involved no dramatic death spiral, just being extremely smart and knowing it from the time I was in kindergaarden. To the point that I grew up with the expectation that, when it came to doing anything mental, sheer smarts would be enough to make me crushingly superior to all the other students around me and many of the adults.</p>\n<p style=\"text-align: justify;\">In <em>Harry Potter and the Methods of Rationality,</em> Harry complains of having once had a math teacher who didn't know what a logarithm is. I wonder if this is autobiographical on Eliezer's part. I have an even better story, though: in second grade, I had a teacher who <em>insisted there was no such thing as negative numbers. </em>The experience of knowing I was right about this, when the adult authority figure was so very wrong, was probably not good for my humility.</p>\n<p style=\"text-align: justify;\">But such brushes with stupid teachers probably weren't the main thing that drove my early self-image. It was enough to be smarter than the other kids around me, and know it. Looking back, there's little that seems worth bragging about. I learned calculus at age 15, not <a href=\"http://en.wikipedia.org/wiki/John_von_Neumann#Early_life_and_education\">age 8</a>. But that was still younger than any of the other kids I knew took calculus (if they took it at all). And knowing I didn't know any other kids as smart as me did funny things to my view of the world.</p>\n<p style=\"text-align: justify;\">I'm honestly not sure I realized there were any kids in the whole <em>world</em> smarter than me until sophomore year, when I qualified to go to a national-level math competition. That was something that no one else at my high school managed to do, not even the seniors... but at the competition itself, I didn't do particularly well. It was one of the things that made me realize that I wasn't, in fact, going to be the next Einstein. But <em>all</em> I took from the math competition was that there were people smarter than me in the world. It didn't, say, occur to me that maybe some of the other competitors had spent more time practicing really hard math problems.</p>\n<p style=\"text-align: justify;\">Eliezer <a href=\"/lw/c3/the_sin_of_underconfidence/\">once said</a>,&nbsp;\"I think I should be able to handle damn near anything on the fly.\" That's a pretty good description of how I felt at this point in my life. At least as long as we were talking about mental challenges and not sports, and assuming I wasn't going up against someone smarter than myself.</p>\n<p style=\"text-align: justify;\">I think my first memory of getting some inkling that maybe sufficient intelligence <em>wouldn't </em>lead to automatically being the best at everything comes from... *drum roll* ...<em>playing Starcraft. </em>I think it was probably junior or senior that I got into the game, and at first I just did the standard campaign playing against the computer, but then I got into online play, and promptly got crushed. And not just by one genius player I encountered on a fluke, but in virtually every match.</p>\n<p style=\"text-align: justify;\">This was a shock. I mean, I had friends who could beat me at Super Smash Bros, but Starcraft was a <em>strategy </em>game, which meant it should be like chess, and I'd never had any trouble beating my friends at chess. Sure, when I'd gone to local chess tournaments back in grade school, I'd gotten soundly beat by many of the older players then, but it's not like I'd ever expected <em>all </em>older people to be as stupid as my second grade teacher. But by the time I'd gotten into Starcraft, I was <em>almost </em>an adult, so what was going on?</p>\n<p style=\"text-align: justify;\">The answer of course was that most of the other people playing online had played a hell of a lot more Starcraft than me. Also, I'd thought I'd figured the game designer's game-design philosophy (I hadn't), which had let me to make all kinds of incorrect assumptions about the game, assumptions which I could have found out were false if I'd tested them, or (probably) if I'd just looked for an online guide that reported the results of other people's tests.</p>\n<p style=\"text-align: justify;\">It all sounds very silly in retrospect, and it didn't change my worldview overnight. But it was (among?) the first of a series of events that made me realize that trying to master something just by thinking about it tends to go badly wrong. That when untrained brilliance goes up against domain expertise, domain expertise will generally win.</p>\n<p style=\"text-align: justify;\">A whole bunch of caveats here. I'm not denying that being smart is pretty awesome. As a smart person, I highly recommend it. And acquiring domain expertise requires a certain minimum level of intelligence, which varies from field to field. It's only once you get beyond that minimum that more intelligence doesn't help as much as expertise. Finally, I'm talking about human scale intelligence here, <a href=\"/lw/ql/my_childhood_role_model/\">the gap between the village idiot and Einstein is tiny compared to the gap between Einstein and possible superintelligences</a>, so maybe a superintelligence could school any human expert in anything without acquiring any particular domain expertise.</p>\n<p style=\"text-align: justify;\">Still, when I hear Eliezer say he thinks he should be able to handle anything on the fly, it strikes me as incredibly foolish. And I worry when I see fellow smart people who seem to think that being very smart and rational gives them grounds to dismiss other people's domain expertise. As Robin Hanson <a href=\"http://intelligence.org/2013/11/01/robin-hanson/\">has said</a>:</p>\n<blockquote>\n<p style=\"text-align: justify;\">I was a physics student and then a physics grad student. In that process, I think I assimilated what was the standard worldview of physicists, at least as projected on the students. That worldview was that physicists were great, of course, and physicists could, if they chose to, go out to all those other fields, that all those other people keep mucking up and not making progress on, and they could make a lot faster progress, if progress was possible, but they don&rsquo;t really want to, because that stuff isn&rsquo;t nearly as interesting as physics is, so they are staying in physics and making progress there...</p>\n<p style=\"text-align: justify;\">Surely you can look at some little patterns but because you can&rsquo;t experiment on people, or because it&rsquo;ll be complicated, or whatever it is, it&rsquo;s just not possible. Partly, that&rsquo;s because they probably tried for an hour, to see what they could do, and couldn&rsquo;t get very far. It&rsquo;s just way too easy to have learned a set of methods, see some hard problem, try it for an hour, or even a day or a week, not get very far, and decide it&rsquo;s impossible, especially if you can make it clear that your methods definitely won&rsquo;t work there. You don&rsquo;t, often, know that there are any other methods to do anything with because you&rsquo;ve learned only certain methods...</p>\n<p style=\"text-align: justify;\">As one of the rare people who have spent a lot of time learning a lot of different methods, I can tell you there are a lot out there. Furthermore, I&rsquo;ll stick my neck out and say most fields know a lot. Almost all academic fields where there&rsquo;s lots of articles and stuff published, they know a lot.</p>\n</blockquote>\n<p style=\"text-align: justify;\">(For those who don't know: Robin spent time doing physics, philosophy, and AI before landing in his current field of economics. When he says he's spent a lot of time learning a lot of different methods, it isn't an idle boast.)</p>\n<p style=\"text-align: justify;\">Finally, what about the original story that Eliezer says set off his original childhood death spiral around intelligence?:</p>\n<blockquote>\n<p style=\"text-align: justify;\">My parents always used to downplay the value of intelligence. And play up the value of&mdash;effort, as recommended by the latest research? No, not effort. Experience. A nicely unattainable hammer with which to smack down a bright young child, to be sure. That was what my parents told me when I questioned the Jewish religion, for example. I tried laying out an argument, and I was told something along the lines of: \"Logic has limits, you'll understand when you're older that experience is the important thing, and then you'll see the truth of Judaism.\" I didn't try again. I made one attempt to question Judaism in school, got slapped down, didn't try again. I've never been a slow learner.</p>\n</blockquote>\n<p style=\"text-align: justify;\">I think concluding experience isn't all that great is the wrong response here. Experience is important. The right response is to ask whether all older, more experienced people see the truth of Judaism. The answer of course is that they don't; a depressing number stick with whatever religion they grew up with (which usually isn't Judaism), a significant number end up non-believers, and a few convert to a new religion. But when almost everyone with a high level relevant experience agrees on something, beware thinking you know better than them based on your superior intelligence and supposed rationality.</p>\n<p style=\"text-align: justify;\"><strong>Edit: </strong>One thing I meant to include when I posted this but forgot: one effect of my experiences is that I tend to see domain expertise where other people see intelligence. See e.g. this <a href=\"/lw/ub/competent_elites/njl\">old comment</a> by Robin Hanson: are hedge fundies really that smart, or have they simply spent a lot of time learning to <em>seem </em>smart in conversation?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 2, "5f5c37ee1b5cdee568cfb118": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d693kbwauFNnq96WQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 45, "extendedScore": null, "score": 0.00015, "legacy": true, "legacyId": "24928", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["R8YpYTq8LoD3k948L", "pkFazhcTErMw7TFtT", "3Jpchgy53D2gB5qdk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-07T19:11:26.622Z", "modifiedAt": null, "url": null, "title": "CFAR Winter 2013 Fundraiser", "slug": "cfar-winter-2013-fundraiser", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:12.799Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "W55i9XXdye9ETAyu5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fGfRimec5oTB3fCM4/cfar-winter-2013-fundraiser", "pageUrlRelative": "/posts/fGfRimec5oTB3fCM4/cfar-winter-2013-fundraiser", "linkUrl": "https://www.lesswrong.com/posts/fGfRimec5oTB3fCM4/cfar-winter-2013-fundraiser", "postedAtFormatted": "Saturday, December 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CFAR%20Winter%202013%20Fundraiser&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACFAR%20Winter%202013%20Fundraiser%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfGfRimec5oTB3fCM4%2Fcfar-winter-2013-fundraiser%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CFAR%20Winter%202013%20Fundraiser%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfGfRimec5oTB3fCM4%2Fcfar-winter-2013-fundraiser", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfGfRimec5oTB3fCM4%2Fcfar-winter-2013-fundraiser", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 21, "htmlBody": "<p>For those who haven't heard, CFAR is holding a matching fundraiser from now until Jan 31, 2014, up to $150,000 USD.</p>\n<p>Link:&nbsp;<a href=\"http://rationality.org/fundraiser2013/\">http://rationality.org/fundraiser2013/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fGfRimec5oTB3fCM4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 1.4558403276533652e-06, "legacy": true, "legacyId": "25027", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-07T19:17:53.389Z", "modifiedAt": "2021-08-18T16:48:11.069Z", "url": null, "title": "How to not be a fatalist? Need help from people who care about true beliefs.", "slug": "how-to-not-be-a-fatalist-need-help-from-people-who-care", "viewCount": null, "lastCommentedAt": "2014-01-14T12:04:42.287Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Laoch", "createdAt": "2010-08-03T08:22:01.285Z", "isAdmin": false, "displayName": "Laoch"}, "userId": "jRKuWSW4uPnBJG4xS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oGSozsEsPmzntHTZ5/how-to-not-be-a-fatalist-need-help-from-people-who-care", "pageUrlRelative": "/posts/oGSozsEsPmzntHTZ5/how-to-not-be-a-fatalist-need-help-from-people-who-care", "linkUrl": "https://www.lesswrong.com/posts/oGSozsEsPmzntHTZ5/how-to-not-be-a-fatalist-need-help-from-people-who-care", "postedAtFormatted": "Saturday, December 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20not%20be%20a%20fatalist%3F%20Need%20help%20from%20people%20who%20care%20about%20true%20beliefs.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20not%20be%20a%20fatalist%3F%20Need%20help%20from%20people%20who%20care%20about%20true%20beliefs.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoGSozsEsPmzntHTZ5%2Fhow-to-not-be-a-fatalist-need-help-from-people-who-care%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20not%20be%20a%20fatalist%3F%20Need%20help%20from%20people%20who%20care%20about%20true%20beliefs.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoGSozsEsPmzntHTZ5%2Fhow-to-not-be-a-fatalist-need-help-from-people-who-care", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoGSozsEsPmzntHTZ5%2Fhow-to-not-be-a-fatalist-need-help-from-people-who-care", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 243, "htmlBody": "<p>Dear LessWrongers</p>\n<p>I've been struggling a bit with the idea of fatalism or at least I keep find myself slipping that direction.To be clear the only reason I use the word fatalism is because of it's dictionary definition. I have not allegiance to the concept. Are there powerful arguments to counter fatalism? I've read the sequence about dissolving the question about free will for example, i.e. I understand how the question itself(have I free will?) is incoherent. I.e. free from what?</p>\n<p>I also accept that I am a physics and that my cognition and subjective experience are more than adequately accounted for by non mysterious understandings of the evolution of life. However I can't seem to figure out a way of reconciling my current understanding of those ideas with the idea that I'm in control of my future. Maybe I already have the answer and haven't got the corresponding affective/emotional state which is not an unprecedented problem for me.</p>\n<p>My biggest fear is that for me to believe that my future is not set that I'm going to take on some irrational silly belief? Can anybody give some useful algo's for thinking about this in a coherent, detached from desire way?</p>\n<p>One thing that I've heard is that physics is non-deterministic but I always thought that had to do with the observer. Surely particles were going to do what they were going to do anyway regardless of whether I can determine the reason or not?</p>\n<p>Thanks for reading,</p>\n<p>Laoch&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oGSozsEsPmzntHTZ5", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 1.4558469086568239e-06, "legacy": true, "legacyId": "25028", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-12-07T19:17:53.389Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-09T03:44:42.065Z", "modifiedAt": null, "url": null, "title": "Walkthrough of \"Definability of Truth in Probabilistic Logic\"", "slug": "walkthrough-of-definability-of-truth-in-probabilistic-logic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:06.188Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kFDikC8kbukAhSnbe/walkthrough-of-definability-of-truth-in-probabilistic-logic", "pageUrlRelative": "/posts/kFDikC8kbukAhSnbe/walkthrough-of-definability-of-truth-in-probabilistic-logic", "linkUrl": "https://www.lesswrong.com/posts/kFDikC8kbukAhSnbe/walkthrough-of-definability-of-truth-in-probabilistic-logic", "postedAtFormatted": "Monday, December 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Walkthrough%20of%20%22Definability%20of%20Truth%20in%20Probabilistic%20Logic%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWalkthrough%20of%20%22Definability%20of%20Truth%20in%20Probabilistic%20Logic%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFDikC8kbukAhSnbe%2Fwalkthrough-of-definability-of-truth-in-probabilistic-logic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Walkthrough%20of%20%22Definability%20of%20Truth%20in%20Probabilistic%20Logic%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFDikC8kbukAhSnbe%2Fwalkthrough-of-definability-of-truth-in-probabilistic-logic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFDikC8kbukAhSnbe%2Fwalkthrough-of-definability-of-truth-in-probabilistic-logic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3171, "htmlBody": "<p>I recently went through the paper&nbsp;<a href=\"http://intelligence.org/wp-content/uploads/2013/03/Christiano-et-al-Naturalistic-reflection-early-draft.pdf\">Definability of Truth in Probabilistic Logic</a>&nbsp;for a second time. Explaining things to others often helps me solidify my own knowledge, so I'm doing a walkthrough of the paper.</p>\n<p>Within, I explain things that I found confusing and expand on sections where the paper is somewhat brief. This is designed to be something that I could send to a copy of myself that has not read the paper, along with the paper, to help my clone learn the concepts as fast as possible. Your mileage may vary: its quite likely that your sticking points differ from my own.</p>\n<p>I'll assume competence in the subject area, including knowledge of <a href=\"http://en.wikipedia.org/wiki/Tarski's_undefinability_theorem\">Tarski's theorem</a> and the impossibility of adding a truth predicate to a sufficiently expressive language.</p>\n<p>Because this paper is an early draft, I've included a few suggestions for edits that would have helped me out. You can find them throughout, or collected at the bottom of the post.</p>\n<p><a id=\"more\"></a></p>\n<h2>Motivation</h2>\n<p>Section 1 of the paper is well put together. For posterity, I'll summarize it here:</p>\n<ul>\n<li>Given a sufficiently expressive language of first-order logic, you cannot embed that language's truth predicate into the language itself. For if a language has access to its own truth predicate, it can express the liar's paradox: <code>G &hArr; True('&not;G')</code>.</li>\n<li>Responses to this include: \n<ul>\n<li>Working with a tower of meta-languages, each containing the previous system's truth predicate</li>\n<li>Allowing some sentences must take on a third \"undefined\" value, instead of true or false</li>\n</ul>\n</li>\n</ul>\n<p>This paper explores a third alternative: assign probabilities to sentences instead of binary (or trinary) truth-values. This is potentially far stronger than a tri-valued logic: it's all well and good to say that some sentences are \"undefined\", but it turns out that many sentences of interest &mdash; not just pathological paradoxes &mdash; become undefined. A probability function, by contrast, allows you to retain much more information about sentences which cannot be assigned values \"true\" or \"false\".</p>\n<h3>Preliminaries</h3>\n<p>Throughout the paper, we'll fix a language L that we're working with. It doesn't matter what language L is, so long as it's strong enough to perform a G&ouml;del numbering, and that it has terms corresponding to the rational numbers.</p>\n<p>We also consider a particular theory T (for example, ZFC) and assume that the rationals have their usual properties in T. Furthermore:</p>\n<ul>\n<li><code>'&phi;'</code>&nbsp;(&phi;, in quotes) will denote the G&ouml;del encoding of the sentence &phi;.</li>\n<li><code>Q</code> will denote the set of all rational numbers. I point this out because I lack a script font.</li>\n</ul>\n<p>The paper is a little lax when throwing around variable names, especially in second section. Here's a quick run-down:</p>\n<ul>\n<li>P refers to both a function and a symbol. I'll try to be explicit about which is which. In general, the <em>function</em> P operates on sentences, whereas the <em>symbol</em> P is contained in a sentence, and operates on quoted sentences. So P(&phi;) refers to the output of the function on the sentence &phi;, and P('&phi;') refers to the symbol P applied to a G&ouml;del encoding of the sentence &phi;. This is usually not ambiguous.</li>\n<li>&mu; is used for probability measures, and does not tend to be fixed. Remember that a probability measure is defined over a set of objects. It assigns a measure to each object such that the measure of the whole set is 1, while obeying the laws of probability.</li>\n<li>T generally refers to the theory under consideration (eg ZFC), but there are times where it doesn't. I'll point them out.</li>\n</ul>\n<h3>Probability Predicates</h3>\n<p>We want a probability function defined on sentences of L. What does that actually mean?</p>\n<p>We begin by considering all functions P that map sentences of L onto real numbers. Note that P could be <em>any</em> function from L onto reals. It could be that P takes \"x\"&nbsp;to 100 and \"x or x\"&nbsp;to -3. Don't let the suggestive name 'P' fool you: we haven't yet narrowed down the behavior of functions under consideration.</p>\n<p>Clearly, such functions cannot in general be viewed as measuring the \"probability\" of a sentence. What, precisely, do we mean when we say we want a probability function for our logical language?</p>\n<p>Intuitively, we <em>want</em> a P that treats sentences in a \"consistent\" manner. More formally, we want there to be some underlying probability measure &mu; over models such that P is \"talking about\" &mu;.</p>\n<p>Note that we do <em>not</em> require P be a mere probability measure over sentences of L! That would be too weak a claim: such a P could assign 1 to the sentence <code>x &and; &not;x</code> while being a perfectly good probability measure over sentences, though clearly such a P (which assigns 1 to a contradiction!) is not what we had in mind when we went hunting for a probability function.</p>\n<p>Claiming that P obeys a probability measure &mu; over <em>models</em> of L is a much stronger claim. It states that there is some way &mu; of assigning probabilities to models such that P(&phi;) is the probability of the sentence &phi; <em>according to how &mu; weights models</em>. For example, such a P is forced to assign 0 to every contradiction, because there are no models that model a contradiction.</p>\n<p>Let's make that explicit: We want P(&phi;) to be the proportion of all models (weighted by &mu;) that model &phi;. In other words, <code>P(&phi;) = &mu;({M : M\u22a7&phi;})</code>.</p>\n<p>We call such a P \"coherent\". Coherent P are quite well-behaved: They map all contradictions to 0 (as no model models a contradiction) and all tautologies to 1 (as every model models a tautology). Coherent P also act like probability functions: we know that P(&phi;)=P(&phi;&and;&psi;)+P(&phi;&and;&not;&psi;) for any &phi; and &psi;, because clearly the models that model &phi; consist of the models that model both &phi; and &psi;, and the models that model &phi; but don't model &psi;.</p>\n<p>These three properties are necessary for coherent P. Turns out, they're also sufficient for a coherent P. This may be rather surprising: remember that P is otherwise unrestricted. So the above claim is equivalent to saying that these three axioms are sufficient to make a coherent P:</p>\n<ol>\n<li>For all &phi; and &psi;, P(&phi;)=P(&phi;&and;&psi;)+P(&phi;&and;&not;&psi;)</li>\n<li>For all tautologies &phi;, P(&phi;)=1</li>\n<li>For all contradictions &phi;, P(&phi;)=0</li>\n</ol>\n<div>The paper then seems to make the assumption that these axioms constrain the range of P to [0, 1] (or, equivalently, that these axioms are sufficient to guarantee that P maps logically equivalent sentences to the same value). This is not obvious to me, and may in fact be incorrect. (See discussion, below.)</div>\n<div><br /></div>\n<div>But *assuming* that the range of P is [0, 1], the rest of the proof is fairly simple. For formal details, see the paper. I will only sketch the proof.</div>\n<p>We're going to give a method for constructing theories. <em>In this section, T refers to the constructed theory, NOT the T that we're holding fixed.&nbsp;</em>Also, we're going to be assessing the probability of the theory under construction, using syntax like <code>P(Ti)</code>. This is just the probability of the sentence constructed by conjuncting all sentences in the theory <code>Ti</code>. We can always assess this because each <code>Ti</code> will be finite.</p>\n<p>Fix an enumeration of all sentences &phi;i. Set T0 to be the empty set.</p>\n<p><em>Note: The paper claims that \"By axiom 3, P(T0)=1\". I believe this is a typo: first of all, it should read \"By axiom 2\". Second of all, it's not obvious to me that P must assign a probability to the empty sentence, nor that it should be 1. This is, however, easily worked around by defining P(&phi;|Ti) to be P(&phi;) if i=0. For all other Ti, P(&phi;|Ti) is defined in the usual way.</em></p>\n<p>Now iterate sentences, considering only sentences independent of the theory Ti built so far. For each independent sentence &phi;j, set <code>Tj=Ti&cup;{&phi;j}</code> with probability <code>P(&phi;j|Ti)</code> and <code>Tj=Ti&cup;{&not;&phi;j}</code> with probability <code>P(&not;&phi;j|Ti)</code>.</p>\n<p>Define <code>T=&cup;Ti</code>, this is a complete consistent theory. At each step <code>i</code> along the way, the probability that the sequence derives &phi; is <code>P(&phi;|Ti)</code>. Thus, the sequence of Ti is a martingale. Because T is complete and consistent, P(&phi;|Ti) stabilizes at either 0 or 1. Thus, the probability that any given T generated by this procedure derives &phi; is P(&phi;|T0), which is just P(&phi;).</p>\n<p><em>Note: Briefly, a martingale is a sequence where the expectation of the next value is equal to the present observed value. I'm not entirely clear on why you also need the fact that P(&phi;|Ti) stabalizes at either 0 or 1 before concluding that T\u22a2&phi; with probability P(&phi;), due to inexperience with martingales. Regardless, the point is that given a probability function P over sentences, you can generate theories in such a way that the resulting theory models &phi; with probability P(&phi;), and this is not too surprising.</em></p>\n<p>We have just given a process for constructing theories at random. This process defines a probability measure over complete consistent theories: every complete consistent theory has a particular probability of being generated by this process.</p>\n<p>The chance that the T coming out of this process derives &phi; is P(&phi;). In other words, we have defined a &mu; such that <code>&mu;({T : T\u22a2&phi;})=P(&phi;)</code>. By the completeness theorem, each complete consistent theory has a model, so this process also defines a measure &mu; over models such that <code>P(&phi;)=&mu;({M:M\u22a7&phi;})</code>. This is exactly what we wanted.</p>\n<p>We have now showed that <em>any</em> P following the above axioms is coherent in that it acts as if there is an underlying probability measure over models of L such that P(&phi;) measures the probability of selecting a model (weighted by &mu;) that models &phi;.</p>\n<h3>Reflective Probability</h3>\n<p>Let's go back to considering our fixed theory T. Our goal is to allow T to \"talk about\" the \"subjective probability\" of its sentences: in other words, we want to extend the language L to include a coherent probability function P.</p>\n<p>We can't include just any P, obviously. Specifically, we want to embed a probability function that talks about the probability <em>of sentences of T</em>: In other words, it must map all sentences derived by T to probability 1, and then assign probability values to sentences not derived by T. (Otherwise, it clearly isn't talking about T.) It's easy to see that probability functions that assign probability 1 to all sentences of T correspond to probability functions derived from some probability measure over models of T.</p>\n<p>Pick a coherent probability function P that assigns probability 1 to all sentences of T. We now want to extend the language L to include a <em>symbol</em> P which is intended to represent the function P. Call the extended language L'.</p>\n<p>It's not enough to just shove P into L and call it a day. We know that P assigns \"sane\" probabilities to sentences of L, but we have no idea how P reacts to sentences of L': the fact that P was sane when it was talking about L does not mean that P will remain sane when it can start referring to itself.</p>\n<p>For example, imagine a probability function P such that <code>P(&phi;)=1</code>, but <code>P(P('&phi;')=1)=0</code>. This is legal, so long as P is coherent and <code>P(&phi;)=1</code> is consistent with T. However, it's clearly not the probability function we're looking for, for it lies about itself: we would like the probability function to interpret the symbol P as the function P itself.</p>\n<p>We could simply require this property, and consider only P where</p>\n<pre><code>&forall;&phi;&isin;L'. &forall;a,b&isin;Q. a&lt;P(&phi;)&lt;b &hArr; P(a&lt;P('&phi;')&lt;b)=1\n</code></pre>\n<p>This translates to \"whenever the function P says that the probability of &phi; is between rational numbers a and b, the function P also says that the sentence <code>a &lt; P('&phi;') &lt; b</code> has probability 1\". In other words, this narrows our search down to functions P that treat the symbol P exactly as the function P itself acts.</p>\n<p>Unfortunately, no such P exists. For imagine that there is such a P, and consider the sentence <code>G &hArr; P('G')&lt;1</code>. Then</p>\n<pre><code>P(G)&lt;1 &hArr; P(P('G')&lt;1)=1 &hArr; P(G)=1\n</code></pre>\n<p>In other words, <code>P(G)&lt;1 &hArr; P(G)=1</code>. Because P is coherent and has range [0, 1], this is a contradiction: no such P exists.</p>\n<p>I'll call this the \"strong reflective consistency\" requirement, and as shown above it is unsatisfiable for exactly the same reason that we can't define a predicate <code>True</code> such that <code>&forall;&phi;&isin;L'. True(&phi;) &hArr; True(True('&phi;'))</code>.</p>\n<p>So we must weaken our requirements. Fortunately, it turns out we don't have to weaken them very much. Instead of requiring that P can talk about itself <em>exactly</em>, we allow P to talk about itself with arbitrarily small (infinitesimal) error. In other words, we require</p>\n<pre><code>&forall;&phi;&isin;L'. &forall;a,b&isin;Q. a&lt;P(&phi;)&lt;b &rArr; P(a&lt;P('&phi;')&lt;b)=1\n</code></pre>\n<p>If there is such a P, we're in business: While sentences of the language L' cannot precisely assert the probability of a sentence, they can do so with arbitrarily small error. Before we discuss whether such a P exists, let's make a few notes and see an example.</p>\n<p>First, the above (weakened) \"reflection principle\" above implies the following:</p>\n<pre><code>&forall;&phi;&isin;L'. &forall;a,b&isin;Q. &not;(a&le;P('&phi;')&le;b) &rArr; P(a&le;P('&phi;')&le;b)=0\n</code></pre>\n<p>Because if P('&phi;') is not in [a, b] then either <code>P(P('&phi;')&lt;a)=1</code> or <code>P(P('&phi;')&gt;b)=1</code>. This can be rephrased as</p>\n<pre><code>&forall;&phi;&isin;L'. &forall;a,b&isin;Q. P(a&le;P('&phi;')&le;b)&gt;0 &rArr; a&le;P('&phi;')&le;b\n</code></pre>\n<p>which is the \"other direction\" of our weak reflection principle.</p>\n<p>Second, an example: Consider the sentence <code>G&hArr;P('G')&lt;p</code> for some p&isin;(0,1]. A reflectively consistent P must assign <code>P(G)=p</code>: If it assigns <code>P(G)&gt;p</code> then <code>P(P('G')&gt;p)=1</code> so <code>P(G)=0</code> (contradiction) and if it assigns <code>P(G)&lt;p</code> then <code>P(P('G')&lt;p)=1</code> so <code>P(G)=1</code> (contradiction). If we required the \"strong\" reflection principle, then P could not exist, because <code>P(G)=p</code> would imply <code>P(P('G')&lt;p)=0</code> and thus <code>P(G)=0</code>, a contradiction. But with the weak reflection principle, no such contradiction can follow, because <code>P(G)=p</code> does <em>not</em> force <code>P(P('G')&lt;p)=0</code>: the <em>symbol</em> P is \"uncertain\" about the true value of the function P, and cannot prove that P('G') is precisely p.</p>\n<p>Models of L' with a reflectively consistent P will be able to show that P(G)&isin;[p-&epsilon;,p+&epsilon;] for arbitrarily small &epsilon;, but <em>cannot</em> prove that <code>P(G)=p</code>. This is the mechanism by which consistency is maintained.</p>\n<h3>Finding P</h3>\n<p>The question is, are there (weakly) reflectively consistent P? We know that there are <em>not</em> strongly reflectively consistent P, just as there are not consistent truth predicates. Before we can get excited about reflectively consistent P, we have to prove that there are some.</p>\n<p>I lack the knowledge to fully understand this proof, as it uses theorems from topology that I do not yet understand. I can, however, give you a sketch:</p>\n<p>Consider the set A of coherent probability distributions over L' that assign probability 1 to T. View it as a subset of the space [0,1]^L'. A is convex, closed, and (by Tychonoff's theorem) compact. A is also non-empty, because there is at least one model of T: we can make a probability distribution that assigns 1 to M and 0 to any other model.</p>\n<p>Given any P, we can construct the set of axioms that describe that P. For every &phi;, we add every sentence <code>a&lt;P('&phi;')&lt;b</code> for every rational interval (a, b) which does indeed contain P(&phi;). For each P, call this set Rp. We say that a probability function \"obeys\" Rp if it assigns probability 1 to every sentence in Rp. We need to show that there is some P with obeys its own reflexivity axioms.</p>\n<p>We can define a function f : A &rarr; Powerset(A) such that f(P') = {P&isin;A : P(Rp')=1}. In other words, f takes probability functions P' to the set of probability functions that obey the reflectivity axioms of P'.</p>\n<p>Each f(P) is convex and non-empty by the same arguments as above. We show that f has a closed graph and apply Kukatani's fixed point theorem to show that f has a fixed point. This guarantees the existence of some P such that P&isin;f(P), which means P obeys its own reflexivity axioms.</p>\n<p>Thus, there exists a reflectively consistent P.</p>\n<p><em>Note: I don't understand Kukatani's fixed point theroem. I need to brush up on my topology. In the meantime, the point is that we can construct a function from probability functions P onto the set of probability functions obeying the reflectivity axioms of P, and that this function has a fixed point.</em></p>\n<p>Since f in general has a fixed point, we can say in general that reflectively consistent P do exist.</p>\n<h3>Knowing your limits</h3>\n<p>Section 3.3 is fairly clear and worth a read. For posterity, I'll summarize it below.</p>\n<p>Note that a reflectively consistent P does not necessarily \"know\" it is reflectively consistent. Such a P <em>is</em> reflectively consistent, but it may not <em>claim</em> to be. In fact, if a reflectively consistent P assigns probability 1 to the general statement of its own reflective consistency, a contradiction can be derived.</p>\n<p>We can weaken the reflection principle further, but it is not yet known whether there is a version that both captures the interesting behavior of reflection and which is both true <em>of</em> P and asserted <em>in</em> P.</p>\n<p>I'll close with the closing quote of the paper:</p>\n<blockquote>\n<p>[this] work shows that the obstructions presented by the liar's paradox can be overcome by tolerating an infinitesimal error, and that Tarski's result on the undefinability of truth is in some sense an artifact of the infinite precision demanded by reasoning about complete certainty.</p>\n</blockquote>\n<h2>Discussion</h2>\n<p>I don't have much to add at this time. I clearly need to read up on topology, and am doing so. The result is very interesting, and I'm looking forward to playing around with other reflection principles.</p>\n<p>Next up is a walkthrough of&nbsp;<a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the L&ouml;bian Obstacle</a>.</p>\n<h2>Compiled Notes</h2>\n<ol>\n<li>\n<p>p3. I, like John Baez <a href=\"http://johncarlosbaez.wordpress.com/2013/03/31/probability-theory-and-the-undefinability-of-truth/\">here</a>, originally thought that axiom 3 was unnecessary. With a little help from Daniel Dewey, I realized that P is not restricted to [0, 1], nor even restricted to map logically equivalent sentences to the same probability. Now I'm not convinced that axioms 1, 2, and 3 are sufficient to force P to act like a probability function. If they are, I think the paper should either show that any P following the three axioms has range [0,1] or that it maps logically equivalent sentences to the same value. (There's some discussion about this in the comments.)</p>\n</li>\n<li>\n<p>p3. It's weird to \"fix a theory T\" at the beginning of section 2.1, and then \"Define T=&cup;Ti\" on p3. It's not particularly confusing in context, but it irks the programmer in me.</p>\n</li>\n<li>\n<p>p3. I'm pretty sure that \"By axiom 3, P(T0)=1\" is a typo. Furthermore, it's not clear to me that we should treat the empty set like a tautological sentence. I would have appreciated a sentence after \"Let T0=&empty;\" to the effect of \"P(Ti) is the value of P on the conjunction of all Ti, which we can do because each Ti is finite\", with special treatment given to the case where Ti is empty.</p>\n</li>\n<li>\n<p>p4. I misparsed \"which contradicts P(G)&isin;[0,1]\" on the first pass. I took Range(P)=[0,1] as background knowledge and thought the claim should have been \"so P(G)&isin;[0,1) and P(G)=1, a contradiction\". I concluded there was a typo. This may have been a personal fluke. The meaning was clear, regardless.</p>\n</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kFDikC8kbukAhSnbe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 21, "extendedScore": null, "score": 5.5e-05, "legacy": true, "legacyId": "25034", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I recently went through the paper&nbsp;<a href=\"http://intelligence.org/wp-content/uploads/2013/03/Christiano-et-al-Naturalistic-reflection-early-draft.pdf\">Definability of Truth in Probabilistic Logic</a>&nbsp;for a second time. Explaining things to others often helps me solidify my own knowledge, so I'm doing a walkthrough of the paper.</p>\n<p>Within, I explain things that I found confusing and expand on sections where the paper is somewhat brief. This is designed to be something that I could send to a copy of myself that has not read the paper, along with the paper, to help my clone learn the concepts as fast as possible. Your mileage may vary: its quite likely that your sticking points differ from my own.</p>\n<p>I'll assume competence in the subject area, including knowledge of <a href=\"http://en.wikipedia.org/wiki/Tarski's_undefinability_theorem\">Tarski's theorem</a> and the impossibility of adding a truth predicate to a sufficiently expressive language.</p>\n<p>Because this paper is an early draft, I've included a few suggestions for edits that would have helped me out. You can find them throughout, or collected at the bottom of the post.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Motivation\">Motivation</h2>\n<p>Section 1 of the paper is well put together. For posterity, I'll summarize it here:</p>\n<ul>\n<li>Given a sufficiently expressive language of first-order logic, you cannot embed that language's truth predicate into the language itself. For if a language has access to its own truth predicate, it can express the liar's paradox: <code>G \u21d4 True('\u00acG')</code>.</li>\n<li>Responses to this include: \n<ul>\n<li>Working with a tower of meta-languages, each containing the previous system's truth predicate</li>\n<li>Allowing some sentences must take on a third \"undefined\" value, instead of true or false</li>\n</ul>\n</li>\n</ul>\n<p>This paper explores a third alternative: assign probabilities to sentences instead of binary (or trinary) truth-values. This is potentially far stronger than a tri-valued logic: it's all well and good to say that some sentences are \"undefined\", but it turns out that many sentences of interest \u2014 not just pathological paradoxes \u2014 become undefined. A probability function, by contrast, allows you to retain much more information about sentences which cannot be assigned values \"true\" or \"false\".</p>\n<h3 id=\"Preliminaries\">Preliminaries</h3>\n<p>Throughout the paper, we'll fix a language L that we're working with. It doesn't matter what language L is, so long as it's strong enough to perform a G\u00f6del numbering, and that it has terms corresponding to the rational numbers.</p>\n<p>We also consider a particular theory T (for example, ZFC) and assume that the rationals have their usual properties in T. Furthermore:</p>\n<ul>\n<li><code>'\u03c6'</code>&nbsp;(\u03c6, in quotes) will denote the G\u00f6del encoding of the sentence \u03c6.</li>\n<li><code>Q</code> will denote the set of all rational numbers. I point this out because I lack a script font.</li>\n</ul>\n<p>The paper is a little lax when throwing around variable names, especially in second section. Here's a quick run-down:</p>\n<ul>\n<li>P refers to both a function and a symbol. I'll try to be explicit about which is which. In general, the <em>function</em> P operates on sentences, whereas the <em>symbol</em> P is contained in a sentence, and operates on quoted sentences. So P(\u03c6) refers to the output of the function on the sentence \u03c6, and P('\u03c6') refers to the symbol P applied to a G\u00f6del encoding of the sentence \u03c6. This is usually not ambiguous.</li>\n<li>\u03bc is used for probability measures, and does not tend to be fixed. Remember that a probability measure is defined over a set of objects. It assigns a measure to each object such that the measure of the whole set is 1, while obeying the laws of probability.</li>\n<li>T generally refers to the theory under consideration (eg ZFC), but there are times where it doesn't. I'll point them out.</li>\n</ul>\n<h3 id=\"Probability_Predicates\">Probability Predicates</h3>\n<p>We want a probability function defined on sentences of L. What does that actually mean?</p>\n<p>We begin by considering all functions P that map sentences of L onto real numbers. Note that P could be <em>any</em> function from L onto reals. It could be that P takes \"x\"&nbsp;to 100 and \"x or x\"&nbsp;to -3. Don't let the suggestive name 'P' fool you: we haven't yet narrowed down the behavior of functions under consideration.</p>\n<p>Clearly, such functions cannot in general be viewed as measuring the \"probability\" of a sentence. What, precisely, do we mean when we say we want a probability function for our logical language?</p>\n<p>Intuitively, we <em>want</em> a P that treats sentences in a \"consistent\" manner. More formally, we want there to be some underlying probability measure \u03bc over models such that P is \"talking about\" \u03bc.</p>\n<p>Note that we do <em>not</em> require P be a mere probability measure over sentences of L! That would be too weak a claim: such a P could assign 1 to the sentence <code>x \u2227 \u00acx</code> while being a perfectly good probability measure over sentences, though clearly such a P (which assigns 1 to a contradiction!) is not what we had in mind when we went hunting for a probability function.</p>\n<p>Claiming that P obeys a probability measure \u03bc over <em>models</em> of L is a much stronger claim. It states that there is some way \u03bc of assigning probabilities to models such that P(\u03c6) is the probability of the sentence \u03c6 <em>according to how \u03bc weights models</em>. For example, such a P is forced to assign 0 to every contradiction, because there are no models that model a contradiction.</p>\n<p>Let's make that explicit: We want P(\u03c6) to be the proportion of all models (weighted by \u03bc) that model \u03c6. In other words, <code>P(\u03c6) = \u03bc({M : M\u22a7\u03c6})</code>.</p>\n<p>We call such a P \"coherent\". Coherent P are quite well-behaved: They map all contradictions to 0 (as no model models a contradiction) and all tautologies to 1 (as every model models a tautology). Coherent P also act like probability functions: we know that P(\u03c6)=P(\u03c6\u2227\u03c8)+P(\u03c6\u2227\u00ac\u03c8) for any \u03c6 and \u03c8, because clearly the models that model \u03c6 consist of the models that model both \u03c6 and \u03c8, and the models that model \u03c6 but don't model \u03c8.</p>\n<p>These three properties are necessary for coherent P. Turns out, they're also sufficient for a coherent P. This may be rather surprising: remember that P is otherwise unrestricted. So the above claim is equivalent to saying that these three axioms are sufficient to make a coherent P:</p>\n<ol>\n<li>For all \u03c6 and \u03c8, P(\u03c6)=P(\u03c6\u2227\u03c8)+P(\u03c6\u2227\u00ac\u03c8)</li>\n<li>For all tautologies \u03c6, P(\u03c6)=1</li>\n<li>For all contradictions \u03c6, P(\u03c6)=0</li>\n</ol>\n<div>The paper then seems to make the assumption that these axioms constrain the range of P to [0, 1] (or, equivalently, that these axioms are sufficient to guarantee that P maps logically equivalent sentences to the same value). This is not obvious to me, and may in fact be incorrect. (See discussion, below.)</div>\n<div><br></div>\n<div>But *assuming* that the range of P is [0, 1], the rest of the proof is fairly simple. For formal details, see the paper. I will only sketch the proof.</div>\n<p>We're going to give a method for constructing theories. <em>In this section, T refers to the constructed theory, NOT the T that we're holding fixed.&nbsp;</em>Also, we're going to be assessing the probability of the theory under construction, using syntax like <code>P(Ti)</code>. This is just the probability of the sentence constructed by conjuncting all sentences in the theory <code>Ti</code>. We can always assess this because each <code>Ti</code> will be finite.</p>\n<p>Fix an enumeration of all sentences \u03c6i. Set T0 to be the empty set.</p>\n<p><em>Note: The paper claims that \"By axiom 3, P(T0)=1\". I believe this is a typo: first of all, it should read \"By axiom 2\". Second of all, it's not obvious to me that P must assign a probability to the empty sentence, nor that it should be 1. This is, however, easily worked around by defining P(\u03c6|Ti) to be P(\u03c6) if i=0. For all other Ti, P(\u03c6|Ti) is defined in the usual way.</em></p>\n<p>Now iterate sentences, considering only sentences independent of the theory Ti built so far. For each independent sentence \u03c6j, set <code>Tj=Ti\u222a{\u03c6j}</code> with probability <code>P(\u03c6j|Ti)</code> and <code>Tj=Ti\u222a{\u00ac\u03c6j}</code> with probability <code>P(\u00ac\u03c6j|Ti)</code>.</p>\n<p>Define <code>T=\u222aTi</code>, this is a complete consistent theory. At each step <code>i</code> along the way, the probability that the sequence derives \u03c6 is <code>P(\u03c6|Ti)</code>. Thus, the sequence of Ti is a martingale. Because T is complete and consistent, P(\u03c6|Ti) stabilizes at either 0 or 1. Thus, the probability that any given T generated by this procedure derives \u03c6 is P(\u03c6|T0), which is just P(\u03c6).</p>\n<p><em>Note: Briefly, a martingale is a sequence where the expectation of the next value is equal to the present observed value. I'm not entirely clear on why you also need the fact that P(\u03c6|Ti) stabalizes at either 0 or 1 before concluding that T\u22a2\u03c6 with probability P(\u03c6), due to inexperience with martingales. Regardless, the point is that given a probability function P over sentences, you can generate theories in such a way that the resulting theory models \u03c6 with probability P(\u03c6), and this is not too surprising.</em></p>\n<p>We have just given a process for constructing theories at random. This process defines a probability measure over complete consistent theories: every complete consistent theory has a particular probability of being generated by this process.</p>\n<p>The chance that the T coming out of this process derives \u03c6 is P(\u03c6). In other words, we have defined a \u03bc such that <code>\u03bc({T : T\u22a2\u03c6})=P(\u03c6)</code>. By the completeness theorem, each complete consistent theory has a model, so this process also defines a measure \u03bc over models such that <code>P(\u03c6)=\u03bc({M:M\u22a7\u03c6})</code>. This is exactly what we wanted.</p>\n<p>We have now showed that <em>any</em> P following the above axioms is coherent in that it acts as if there is an underlying probability measure over models of L such that P(\u03c6) measures the probability of selecting a model (weighted by \u03bc) that models \u03c6.</p>\n<h3 id=\"Reflective_Probability\">Reflective Probability</h3>\n<p>Let's go back to considering our fixed theory T. Our goal is to allow T to \"talk about\" the \"subjective probability\" of its sentences: in other words, we want to extend the language L to include a coherent probability function P.</p>\n<p>We can't include just any P, obviously. Specifically, we want to embed a probability function that talks about the probability <em>of sentences of T</em>: In other words, it must map all sentences derived by T to probability 1, and then assign probability values to sentences not derived by T. (Otherwise, it clearly isn't talking about T.) It's easy to see that probability functions that assign probability 1 to all sentences of T correspond to probability functions derived from some probability measure over models of T.</p>\n<p>Pick a coherent probability function P that assigns probability 1 to all sentences of T. We now want to extend the language L to include a <em>symbol</em> P which is intended to represent the function P. Call the extended language L'.</p>\n<p>It's not enough to just shove P into L and call it a day. We know that P assigns \"sane\" probabilities to sentences of L, but we have no idea how P reacts to sentences of L': the fact that P was sane when it was talking about L does not mean that P will remain sane when it can start referring to itself.</p>\n<p>For example, imagine a probability function P such that <code>P(\u03c6)=1</code>, but <code>P(P('\u03c6')=1)=0</code>. This is legal, so long as P is coherent and <code>P(\u03c6)=1</code> is consistent with T. However, it's clearly not the probability function we're looking for, for it lies about itself: we would like the probability function to interpret the symbol P as the function P itself.</p>\n<p>We could simply require this property, and consider only P where</p>\n<pre><code>\u2200\u03c6\u2208L'. \u2200a,b\u2208Q. a&lt;P(\u03c6)&lt;b \u21d4 P(a&lt;P('\u03c6')&lt;b)=1\n</code></pre>\n<p>This translates to \"whenever the function P says that the probability of \u03c6 is between rational numbers a and b, the function P also says that the sentence <code>a &lt; P('\u03c6') &lt; b</code> has probability 1\". In other words, this narrows our search down to functions P that treat the symbol P exactly as the function P itself acts.</p>\n<p>Unfortunately, no such P exists. For imagine that there is such a P, and consider the sentence <code>G \u21d4 P('G')&lt;1</code>. Then</p>\n<pre><code>P(G)&lt;1 \u21d4 P(P('G')&lt;1)=1 \u21d4 P(G)=1\n</code></pre>\n<p>In other words, <code>P(G)&lt;1 \u21d4 P(G)=1</code>. Because P is coherent and has range [0, 1], this is a contradiction: no such P exists.</p>\n<p>I'll call this the \"strong reflective consistency\" requirement, and as shown above it is unsatisfiable for exactly the same reason that we can't define a predicate <code>True</code> such that <code>\u2200\u03c6\u2208L'. True(\u03c6) \u21d4 True(True('\u03c6'))</code>.</p>\n<p>So we must weaken our requirements. Fortunately, it turns out we don't have to weaken them very much. Instead of requiring that P can talk about itself <em>exactly</em>, we allow P to talk about itself with arbitrarily small (infinitesimal) error. In other words, we require</p>\n<pre><code>\u2200\u03c6\u2208L'. \u2200a,b\u2208Q. a&lt;P(\u03c6)&lt;b \u21d2 P(a&lt;P('\u03c6')&lt;b)=1\n</code></pre>\n<p>If there is such a P, we're in business: While sentences of the language L' cannot precisely assert the probability of a sentence, they can do so with arbitrarily small error. Before we discuss whether such a P exists, let's make a few notes and see an example.</p>\n<p>First, the above (weakened) \"reflection principle\" above implies the following:</p>\n<pre><code>\u2200\u03c6\u2208L'. \u2200a,b\u2208Q. \u00ac(a\u2264P('\u03c6')\u2264b) \u21d2 P(a\u2264P('\u03c6')\u2264b)=0\n</code></pre>\n<p>Because if P('\u03c6') is not in [a, b] then either <code>P(P('\u03c6')&lt;a)=1</code> or <code>P(P('\u03c6')&gt;b)=1</code>. This can be rephrased as</p>\n<pre><code>\u2200\u03c6\u2208L'. \u2200a,b\u2208Q. P(a\u2264P('\u03c6')\u2264b)&gt;0 \u21d2 a\u2264P('\u03c6')\u2264b\n</code></pre>\n<p>which is the \"other direction\" of our weak reflection principle.</p>\n<p>Second, an example: Consider the sentence <code>G\u21d4P('G')&lt;p</code> for some p\u2208(0,1]. A reflectively consistent P must assign <code>P(G)=p</code>: If it assigns <code>P(G)&gt;p</code> then <code>P(P('G')&gt;p)=1</code> so <code>P(G)=0</code> (contradiction) and if it assigns <code>P(G)&lt;p</code> then <code>P(P('G')&lt;p)=1</code> so <code>P(G)=1</code> (contradiction). If we required the \"strong\" reflection principle, then P could not exist, because <code>P(G)=p</code> would imply <code>P(P('G')&lt;p)=0</code> and thus <code>P(G)=0</code>, a contradiction. But with the weak reflection principle, no such contradiction can follow, because <code>P(G)=p</code> does <em>not</em> force <code>P(P('G')&lt;p)=0</code>: the <em>symbol</em> P is \"uncertain\" about the true value of the function P, and cannot prove that P('G') is precisely p.</p>\n<p>Models of L' with a reflectively consistent P will be able to show that P(G)\u2208[p-\u03b5,p+\u03b5] for arbitrarily small \u03b5, but <em>cannot</em> prove that <code>P(G)=p</code>. This is the mechanism by which consistency is maintained.</p>\n<h3 id=\"Finding_P\">Finding P</h3>\n<p>The question is, are there (weakly) reflectively consistent P? We know that there are <em>not</em> strongly reflectively consistent P, just as there are not consistent truth predicates. Before we can get excited about reflectively consistent P, we have to prove that there are some.</p>\n<p>I lack the knowledge to fully understand this proof, as it uses theorems from topology that I do not yet understand. I can, however, give you a sketch:</p>\n<p>Consider the set A of coherent probability distributions over L' that assign probability 1 to T. View it as a subset of the space [0,1]^L'. A is convex, closed, and (by Tychonoff's theorem) compact. A is also non-empty, because there is at least one model of T: we can make a probability distribution that assigns 1 to M and 0 to any other model.</p>\n<p>Given any P, we can construct the set of axioms that describe that P. For every \u03c6, we add every sentence <code>a&lt;P('\u03c6')&lt;b</code> for every rational interval (a, b) which does indeed contain P(\u03c6). For each P, call this set Rp. We say that a probability function \"obeys\" Rp if it assigns probability 1 to every sentence in Rp. We need to show that there is some P with obeys its own reflexivity axioms.</p>\n<p>We can define a function f : A \u2192 Powerset(A) such that f(P') = {P\u2208A : P(Rp')=1}. In other words, f takes probability functions P' to the set of probability functions that obey the reflectivity axioms of P'.</p>\n<p>Each f(P) is convex and non-empty by the same arguments as above. We show that f has a closed graph and apply Kukatani's fixed point theorem to show that f has a fixed point. This guarantees the existence of some P such that P\u2208f(P), which means P obeys its own reflexivity axioms.</p>\n<p>Thus, there exists a reflectively consistent P.</p>\n<p><em>Note: I don't understand Kukatani's fixed point theroem. I need to brush up on my topology. In the meantime, the point is that we can construct a function from probability functions P onto the set of probability functions obeying the reflectivity axioms of P, and that this function has a fixed point.</em></p>\n<p>Since f in general has a fixed point, we can say in general that reflectively consistent P do exist.</p>\n<h3 id=\"Knowing_your_limits\">Knowing your limits</h3>\n<p>Section 3.3 is fairly clear and worth a read. For posterity, I'll summarize it below.</p>\n<p>Note that a reflectively consistent P does not necessarily \"know\" it is reflectively consistent. Such a P <em>is</em> reflectively consistent, but it may not <em>claim</em> to be. In fact, if a reflectively consistent P assigns probability 1 to the general statement of its own reflective consistency, a contradiction can be derived.</p>\n<p>We can weaken the reflection principle further, but it is not yet known whether there is a version that both captures the interesting behavior of reflection and which is both true <em>of</em> P and asserted <em>in</em> P.</p>\n<p>I'll close with the closing quote of the paper:</p>\n<blockquote>\n<p>[this] work shows that the obstructions presented by the liar's paradox can be overcome by tolerating an infinitesimal error, and that Tarski's result on the undefinability of truth is in some sense an artifact of the infinite precision demanded by reasoning about complete certainty.</p>\n</blockquote>\n<h2 id=\"Discussion\">Discussion</h2>\n<p>I don't have much to add at this time. I clearly need to read up on topology, and am doing so. The result is very interesting, and I'm looking forward to playing around with other reflection principles.</p>\n<p>Next up is a walkthrough of&nbsp;<a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the L\u00f6bian Obstacle</a>.</p>\n<h2 id=\"Compiled_Notes\">Compiled Notes</h2>\n<ol>\n<li>\n<p>p3. I, like John Baez <a href=\"http://johncarlosbaez.wordpress.com/2013/03/31/probability-theory-and-the-undefinability-of-truth/\">here</a>, originally thought that axiom 3 was unnecessary. With a little help from Daniel Dewey, I realized that P is not restricted to [0, 1], nor even restricted to map logically equivalent sentences to the same probability. Now I'm not convinced that axioms 1, 2, and 3 are sufficient to force P to act like a probability function. If they are, I think the paper should either show that any P following the three axioms has range [0,1] or that it maps logically equivalent sentences to the same value. (There's some discussion about this in the comments.)</p>\n</li>\n<li>\n<p>p3. It's weird to \"fix a theory T\" at the beginning of section 2.1, and then \"Define T=\u222aTi\" on p3. It's not particularly confusing in context, but it irks the programmer in me.</p>\n</li>\n<li>\n<p>p3. I'm pretty sure that \"By axiom 3, P(T0)=1\" is a typo. Furthermore, it's not clear to me that we should treat the empty set like a tautological sentence. I would have appreciated a sentence after \"Let T0=\u2205\" to the effect of \"P(Ti) is the value of P on the conjunction of all Ti, which we can do because each Ti is finite\", with special treatment given to the case where Ti is empty.</p>\n</li>\n<li>\n<p>p4. I misparsed \"which contradicts P(G)\u2208[0,1]\" on the first pass. I took Range(P)=[0,1] as background knowledge and thought the claim should have been \"so P(G)\u2208[0,1) and P(G)=1, a contradiction\". I concluded there was a typo. This may have been a personal fluke. The meaning was clear, regardless.</p>\n</li>\n</ol>", "sections": [{"title": "Motivation", "anchor": "Motivation", "level": 1}, {"title": "Preliminaries", "anchor": "Preliminaries", "level": 2}, {"title": "Probability Predicates", "anchor": "Probability_Predicates", "level": 2}, {"title": "Reflective Probability", "anchor": "Reflective_Probability", "level": 2}, {"title": "Finding P", "anchor": "Finding_P", "level": 2}, {"title": "Knowing your limits", "anchor": "Knowing_your_limits", "level": 2}, {"title": "Discussion", "anchor": "Discussion", "level": 1}, {"title": "Compiled Notes", "anchor": "Compiled_Notes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "30 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-09T04:48:18.532Z", "modifiedAt": null, "url": null, "title": "The Statistician's Fallacy", "slug": "the-statistician-s-fallacy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:09.719Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TtKBkgfxJrH9NmkXW/the-statistician-s-fallacy", "pageUrlRelative": "/posts/TtKBkgfxJrH9NmkXW/the-statistician-s-fallacy", "linkUrl": "https://www.lesswrong.com/posts/TtKBkgfxJrH9NmkXW/the-statistician-s-fallacy", "postedAtFormatted": "Monday, December 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Statistician's%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Statistician's%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTtKBkgfxJrH9NmkXW%2Fthe-statistician-s-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Statistician's%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTtKBkgfxJrH9NmkXW%2Fthe-statistician-s-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTtKBkgfxJrH9NmkXW%2Fthe-statistician-s-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 920, "htmlBody": "<p style=\"text-align: justify;\">[<strong>Epistemic status </strong>|<strong>&nbsp;</strong>Contains generalization based on like three data points.]</p>\n<p style=\"text-align: justify;\">In grad school, I took a philosophy of science class that was based around looking for examples of bad reasoning in the scientific literature.&nbsp;The kinds of objections to published scientific studies we talked about were not stupid ones. The professor had a background in statistics, and as far as I could tell knew her stuff in that area (though she dismissed Bayesianism in favor of frequentism). And no, unlike&nbsp;<a href=\"http://www.patheos.com/blogs/hallq/2012/06/from-the-archivesplantingas-inexcusable-faults-review-of-where-the-conflict-really-lies/\"><em>some</em>&nbsp;of the professors in the department</a>, she wasn't an anti-evolutionist or anything like that.</p>\n<p style=\"text-align: justify;\">Instead she was convinced that cellphones cause cancer. In spite of the fact that there's <a href=\"http://www.cancer.gov/cancertopics/factsheet/Risk/cellphones\">scant</a> <a href=\"http://www.mayoclinic.com/health/cell-phones-and-cancer/AN01905\">evidence</a>&nbsp;for that claim, and there's <a href=\"http://www.csicop.org/si/show/power_line_panic_and_mobile_mania/\">no plausible physial mechanism for how that could happen.</a>&nbsp;This along with a number of other borderline-fringe beliefs that I won't get into here, but that was the big screaming red flag.*</p>\n<p style=\"text-align: justify;\">Over the course of the semester, I got a pretty good idea of what was going on. She had an agenda&mdash;it happened to be an environmentalist, populist, pro-\"natural\"-things agenda, but that's incidental. The problem was that when she saw a scientific study that seemed at odds with her agenda, she went looking for flaws. And often she could find them! Real flaws, not ones she was imagining! But people who've read the <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization\">rationalization sequence</a> will see a problem here...</p>\n<p style=\"text-align: justify;\">In my <a href=\"/lw/j8g/the_limits_of_intelligence_and_me_domain_expertise/\">last post</a>, I quoted Robin Hanson on the tendency of some physicists to be unduly dismissive of other fields. But based the above case and a couple others like it, I've come to suspect statistics may be even <em>worse</em> than physics in that way. That fluency in statistics sometimes causes a supercharged&nbsp;<a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">sophistication effect</a>.</p>\n<p style=\"text-align: justify;\">For example, some anthropogenic global warming skeptics make a big deal of alleged statistical errors in global warming research, but as I wrote in my post&nbsp;<a href=\"/lw/iu0/trusting_expert_consensus/\">Trusting Expert Consensus</a>:</p>\n<blockquote>\n<p style=\"text-align: justify;\">Michael Mann et al's so-called <a href=\"http://en.wikipedia.org/wiki/Hockey_stick_controversy\">\"hockey stick\"</a>&nbsp;graph has come under a lot of fire from skeptics, but (a) many other reconstructions have reached the same conclusion and (b) a panel formed by the National Research Council concluded that, while there were some problems with Mann et al's statistical analysis, these problems did not affect the conclusion. Furthermore, even if we didn't have the pre-1800 reconstructions, I understand that given what we know about CO2's heat-trapping properties, and given the increase in atmospheric CO2 levels due to burning fossil fuels, it would be surprising if humans <em>hadn't </em>caused significant warming.</p>\n</blockquote>\n<p style=\"text-align: justify;\">Most recently, I got into a Twitter argument with someone who claimed that \"IQ is demonstrably statistically meaningless\" and that this was widely accepted among statisticians. Not only did this set off my <a href=\"/lw/j09/academic_cliques/\">\"academic clique!\"</a>&nbsp;alarm bells, but I'd just come off doing a spurt of reading about intelligence, including the excellent&nbsp;<em><a href=\"http://www.amazon.com/Intelligence-A-Very-Short-Introduction/dp/0192893211\">Intelligence: A Very Short Introduction</a>.</em>&nbsp;The claim that IQ is meaningless was wildly contrary to what I understood was the consensus among people who study intelligence for a living.</p>\n<p style=\"text-align: justify;\">In response to my surprise, I got <a href=\"http://vserver1.cscs.lsa.umich.edu/~crshalizi/weblog/523.html\">an article</a>&nbsp;that contained lengthy and impressive-looking statistical arguments... but completely ignored a couple key points from the intelligence literature I'd read: first, that there's a strong correlation between IQ and real-world performance, and second that correlations between the components of intelligence we know how to test for turn out to be really strong. If IQ is actually made up of several independent factors, we haven't been able to find them. Maybe some people in intelligence research really did make the mistakes alleged, but there was more to intelligence research than the statistician who wrote the article let on.</p>\n<p style=\"text-align: justify;\">It would be fair to <a href=\"/lw/lv/every_cause_wants_to_be_a_cult/\">shout a warning about correspondence bias</a> before inferring anything from these cases. But consider two facts:</p>\n<ol>\n<li>Essentially all scientific fields rely heavily on statistics.</li>\n<li>There's a lot more to mastering a scientific discipline than learning statistics, which limits how well most scientists will ever master statistics.</li>\n</ol>\n<p style=\"text-align: justify;\">The first fact may make it tempting to think that if you know a lot of statistics, you're in a priviledged position to judge the validity of any scientific claim you come across. But the second fact means that if you've <em>specialized </em>in statistics, you'll probably be better at it than most scientists, even good scientists. So if you go scrutinizing their papers, there's a good chance you'll find clear mistakes in their stats, and an even better chance you'll find arguable ones.</p>\n<p style=\"text-align: justify;\">Bayesians will realize that, since there's a good chance that of happening <em>even when the conclusion is correct and well-supported by the evidence, </em>finding mistakes in the statistics is only weak evidence that the conclusion is wrong. Call it the statistician's fallacy: thinking that finding a mistake in the statistics is sufficient grounds to dismiss a finding.</p>\n<p style=\"text-align: justify;\">Oh, if you're dealing with a novel finding that experts in the field aren't sure what to make of yet, and the statistics turns out to be wrong, then that may be enough. You may have better things to do than investigate further. But when <a href=\"/lw/iu0/trusting_expert_consensus/\">a solid majority of the experts</a> agree on a conclusion, and you see flaws in their statistics, I think the default assumption should be that they still know the issue better than you and very likely the sum total of the available evidence <em>does </em>support the conclusion. Even if the specific statistical arguments youv'e seen from them are wrong.</p>\n<p style=\"text-align: justify;\"><em>*Note: I've done some Googling to try to find rebuttals to this link, and most of what I found confirms it. I did find some people talking about multi-photon effects and heating, but couldn't find defenses of these suggestions that rise beyond people saying,&nbsp;<a href=\"/lw/ml/but_theres_still_a_chance_right/\">\"well there's a chance.\"</a></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dJ6eJxJrCEget7Wb6": 1, "LDTSbmXtokYAsEq8e": 1, "5hpGj9nDLgokfghvR": 1, "bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TtKBkgfxJrH9NmkXW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 63, "extendedScore": null, "score": 0.000255, "legacy": true, "legacyId": "24929", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d693kbwauFNnq96WQ", "AdYdLP2sRqPMoe8fb", "R8YpYTq8LoD3k948L", "t9oXxwss8p6oXGg3W", "yEjaj7PWacno5EvWa", "q7Me34xvSG3Wm97As"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-09T09:08:48.835Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava Meetup VIII.", "slug": "meetup-bratislava-meetup-viii", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ms7DfsSYYpzE6xofh/meetup-bratislava-meetup-viii", "pageUrlRelative": "/posts/ms7DfsSYYpzE6xofh/meetup-bratislava-meetup-viii", "linkUrl": "https://www.lesswrong.com/posts/ms7DfsSYYpzE6xofh/meetup-bratislava-meetup-viii", "postedAtFormatted": "Monday, December 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava%20Meetup%20VIII.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%20Meetup%20VIII.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fms7DfsSYYpzE6xofh%2Fmeetup-bratislava-meetup-viii%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%20Meetup%20VIII.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fms7DfsSYYpzE6xofh%2Fmeetup-bratislava-meetup-viii", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fms7DfsSYYpzE6xofh%2Fmeetup-bratislava-meetup-viii", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/uk'>Bratislava Meetup VIII.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 December 2013 06:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Bistro The Peach, Heydukova 21, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The same place, free discussion.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/uk'>Bratislava Meetup VIII.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ms7DfsSYYpzE6xofh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.4581686377489437e-06, "legacy": true, "legacyId": "25045", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_VIII_\">Discussion article for the meetup : <a href=\"/meetups/uk\">Bratislava Meetup VIII.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 December 2013 06:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Bistro The Peach, Heydukova 21, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The same place, free discussion.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_VIII_1\">Discussion article for the meetup : <a href=\"/meetups/uk\">Bratislava Meetup VIII.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava Meetup VIII.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_VIII_", "level": 1}, {"title": "Discussion article for the meetup : Bratislava Meetup VIII.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_VIII_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-09T16:25:17.725Z", "modifiedAt": null, "url": null, "title": "Meetup : Book Mini-Review: Doug Hubbard's How to Measure Anything", "slug": "meetup-book-mini-review-doug-hubbard-s-how-to-measure", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:03.281Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rule_and_line", "createdAt": "2012-12-28T15:40:39.942Z", "isAdmin": false, "displayName": "rule_and_line"}, "userId": "FEtpkTTXzu5n6xs65", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pZceusegdfAnhgobD/meetup-book-mini-review-doug-hubbard-s-how-to-measure", "pageUrlRelative": "/posts/pZceusegdfAnhgobD/meetup-book-mini-review-doug-hubbard-s-how-to-measure", "linkUrl": "https://www.lesswrong.com/posts/pZceusegdfAnhgobD/meetup-book-mini-review-doug-hubbard-s-how-to-measure", "postedAtFormatted": "Monday, December 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Book%20Mini-Review%3A%20Doug%20Hubbard's%20How%20to%20Measure%20Anything&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Book%20Mini-Review%3A%20Doug%20Hubbard's%20How%20to%20Measure%20Anything%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpZceusegdfAnhgobD%2Fmeetup-book-mini-review-doug-hubbard-s-how-to-measure%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Book%20Mini-Review%3A%20Doug%20Hubbard's%20How%20to%20Measure%20Anything%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpZceusegdfAnhgobD%2Fmeetup-book-mini-review-doug-hubbard-s-how-to-measure", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpZceusegdfAnhgobD%2Fmeetup-book-mini-review-doug-hubbard-s-how-to-measure", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ul'>Book Mini-Review: Doug Hubbard's How to Measure Anything</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 December 2013 04:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">869 Stockton Street, Suite 1-2 , Jacksonville, FL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Folks who are enjoying this fine Jacksonville winter: come hang out on a Sunday afternoon!  I'll start things off with a mini-summary of a book I've been reading and how I apply some of the concepts in work and life.  With luck we'll rapidly move on to structured discussion, unstructured discussion, and social fun and games time!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ul'>Book Mini-Review: Doug Hubbard's How to Measure Anything</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pZceusegdfAnhgobD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 2, "extendedScore": null, "score": 1.4586156349268578e-06, "legacy": true, "legacyId": "25047", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Book_Mini_Review__Doug_Hubbard_s_How_to_Measure_Anything\">Discussion article for the meetup : <a href=\"/meetups/ul\">Book Mini-Review: Doug Hubbard's How to Measure Anything</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 December 2013 04:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">869 Stockton Street, Suite 1-2 , Jacksonville, FL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Folks who are enjoying this fine Jacksonville winter: come hang out on a Sunday afternoon!  I'll start things off with a mini-summary of a book I've been reading and how I apply some of the concepts in work and life.  With luck we'll rapidly move on to structured discussion, unstructured discussion, and social fun and games time!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Book_Mini_Review__Doug_Hubbard_s_How_to_Measure_Anything1\">Discussion article for the meetup : <a href=\"/meetups/ul\">Book Mini-Review: Doug Hubbard's How to Measure Anything</a></h2>", "sections": [{"title": "Discussion article for the meetup : Book Mini-Review: Doug Hubbard's How to Measure Anything", "anchor": "Discussion_article_for_the_meetup___Book_Mini_Review__Doug_Hubbard_s_How_to_Measure_Anything", "level": 1}, {"title": "Discussion article for the meetup : Book Mini-Review: Doug Hubbard's How to Measure Anything", "anchor": "Discussion_article_for_the_meetup___Book_Mini_Review__Doug_Hubbard_s_How_to_Measure_Anything1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-09T16:35:44.861Z", "modifiedAt": null, "url": null, "title": "Open thread for December 9 - 16, 2013", "slug": "open-thread-for-december-9-16-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:01.742Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uSQkQYQBa2r8ieinZ/open-thread-for-december-9-16-2013", "pageUrlRelative": "/posts/uSQkQYQBa2r8ieinZ/open-thread-for-december-9-16-2013", "linkUrl": "https://www.lesswrong.com/posts/uSQkQYQBa2r8ieinZ/open-thread-for-december-9-16-2013", "postedAtFormatted": "Monday, December 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%20for%20December%209%20-%2016%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%20for%20December%209%20-%2016%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuSQkQYQBa2r8ieinZ%2Fopen-thread-for-december-9-16-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%20for%20December%209%20-%2016%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuSQkQYQBa2r8ieinZ%2Fopen-thread-for-december-9-16-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuSQkQYQBa2r8ieinZ%2Fopen-thread-for-december-9-16-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uSQkQYQBa2r8ieinZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.4586263419641952e-06, "legacy": true, "legacyId": "25048", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 377, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-09T21:09:10.841Z", "modifiedAt": null, "url": null, "title": "Chocolate Ice Cream After All?", "slug": "chocolate-ice-cream-after-all", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:09.095Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pallas", "createdAt": "2013-11-18T00:30:55.630Z", "isAdmin": false, "displayName": "pallas"}, "userId": "LFWd5Q2mptz7d5MxZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iC4NueuGWSwBKPXPh/chocolate-ice-cream-after-all", "pageUrlRelative": "/posts/iC4NueuGWSwBKPXPh/chocolate-ice-cream-after-all", "linkUrl": "https://www.lesswrong.com/posts/iC4NueuGWSwBKPXPh/chocolate-ice-cream-after-all", "postedAtFormatted": "Monday, December 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Chocolate%20Ice%20Cream%20After%20All%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChocolate%20Ice%20Cream%20After%20All%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiC4NueuGWSwBKPXPh%2Fchocolate-ice-cream-after-all%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Chocolate%20Ice%20Cream%20After%20All%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiC4NueuGWSwBKPXPh%2Fchocolate-ice-cream-after-all", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiC4NueuGWSwBKPXPh%2Fchocolate-ice-cream-after-all", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6727, "htmlBody": "<p>I have collected some thoughts on decision theory and am wondering whether they are any good, or whether I&rsquo;m just thinking non-sense. I would really appreciate some critical feedback. Please be charitable in terms of language and writing style, as I am not a native English speaker and as this is the first time I am writing such an essay.</p>\n<h2>Overview</h2>\n<ul>\n<li>The classical notion of free will messes up our minds, especially in decision-theoretic problems. Once we come to see it as confused and reject it, we realize that our choices in some sense not only determine the future but also the past.</li>\n<li>If determining the past conflicts with our intuitions of how time behaves, then we need to adapt our intuitions.</li>\n<li>The A,B-Game shows us that, as far as the rejection of free will allows for it, it is in principle possible to <em>choose</em> our genes.</li>\n<li>Screening off only applies if we consider our action to be independent of the variable of interest &ndash; at least in expectation.</li>\n<li>When dealing with Newcomblike problems, we have to be clear about which forecasting powers are at work. Likewise, it turns out to be crucial to precisely point out which agent knows how much about the setting of the game.</li>\n<li>In the standard version of Newcomb&rsquo;s Soda, one should choose chocolate ice cream &ndash; unless the game were specified in a way that previous subjects did not (unlike us) know of any interdependence of soda and ice cream.</li>\n<li>Variations of Newcomb&rsquo;s Soda suggest that the evidential approach makes us better off.</li>\n<li>The analysis of Newcomb&rsquo;s Soda shows that its formulation fundamentally differs from the formulation of Solomon&rsquo;s Problem. &nbsp;</li>\n<li>Given all study-subjects make persistent precommitments, a proper use of evidential reasoning suggests <em>precommitting</em> to take chocolate ice cream. This is why Newcomb&rsquo;s Soda does not show that the evidential approach is dynamically inconsistent.</li>\n<li>The tickle defense does not apply to the standard medical version of Solomon&rsquo;s Problem. In versions where it applies, it does not tell us anything non-trivial.</li>\n<li>Evidential reasoning seems to be a winning approach not only in Newcomb&rsquo;s Problem, but also in Newcomb&rsquo;s Soda and in the medical version of Solomon&rsquo;s Problem. Therefore, we should consider a proper use of evidential reasoning as a potentially promising component when building the ultimate decision algorithm.</li>\n</ul>\n<p>In the standard formulation of Newcomb&rsquo;s Soda, the evidential approach suggests picking chocolate ice cream, since this makes it more probable that we will have been awarded the million dollars. Hence, it denies us the thousand dollars we actually could win if we only took vanilla ice cream. Admittedly, this may be counterintuitive. Common sense tells us that considering the thousand dollars, one could <em>change the outcome</em>, whereas one cannot change which type of soda one has drunk; therefore we have to make a decision that actually <em>affects</em> our outcome. Maybe the flaw in this kind of reasoning doesn&rsquo;t pose a problem to our intuitions as long as we deal with a &ldquo;causal-intuition-friendly&rdquo; setting of numbers. So let&rsquo;s consider various versions of this problem in order to thoroughly compare the &nbsp;two competing algorithmical traits. Let&rsquo;s find out which one actually <em>wins</em> and therefore should be implemented by rational agents.</p>\n<p>In this post, I will discuss Newcomblike problems and conclude that the arguments presented support an evidential approach. <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\" target=\"_blank\">Various</a> <a href=\"http://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\" target=\"_blank\">decision problems</a> have shown that plain evidential decision theory is not a winning strategy. I instead propose to include evidential reasoning in more elaborate decision theories, such as timeless decision theory or updateless decision theory, since they also need to come up with an answer in Newcomblike problems.<br />By looking at the strategies proposed in those problems, currently discussed decision theories produce outputs that can be grouped into evidential-like and causal-like. I am going to outline which of these two traits a winning decision theory must possess.<br />Let&rsquo;s consider the following excerpt by <a href=\"http://intelligence.org/files/TDT.pdf\" target=\"_blank\">Yudkowsky (2010)</a> about the <a href=\"/lw/gu1/decision_theory_faq/#medical-newcomb-problems\" target=\"_blank\">medical version of Solomon&rsquo;s Problem</a>:</p>\n<p>&ldquo;In the chewing-gum throat-abscess variant of Solomon&rsquo;s Problem, the dominant action is chewing gum, which leaves you better off whether or not you have the CGTA gene; but choosing to chew gum is <em>evidence</em> for possessing the CGTA gene, although it cannot <em>affect</em> the presence or absence of CGTA in any way.&rdquo;</p>\n<p>In what follows, I am going to elaborate on why I believe this point (in the otherwise brilliant paper) needs to be reconsidered. Furthermore, I will explore possible objections and have a look at other decision problems that might be of interest to the discussion.<br />But before we discuss classical Newcomblike problems, let&rsquo;s first have a look at the following thought experiment:</p>\n<p>&nbsp;</p>\n<h2>The school mark is already settled</h2>\n<p>Imagine you were going to school; it is the first day of the semester. Suppose you only care about getting the best marks. Now your math teacher tells you that he knows you very well and that this would be why he already wrote down the mark you will receive for the upcoming exam. To keep things simple, let&rsquo;s cut down your options to &ldquo;study as usual&rdquo; and &ldquo;don't study at all&rdquo;. What are you going to do? Should you learn as if you didn&rsquo;t know about the settled mark? Or should you not learn at all since the mark has already been written down?</p>\n<p>This is a tricky question because the answer to it depends on your credence in the teacher&rsquo;s <em>forecasting power</em>. Therefore let's consider the following two cases:</p>\n<ol>\n<li>Let's assume that the teacher is correct in 100% of the cases. Now we find ourselves in a problem that resembles Newcomb's Problem since our decision exactly determines the output of his prediction. Just as an agent that really wishes to win the most money should take only one box in Newcomb&rsquo;s Problem, you should learn for the exams as if you didn't know that the marks are already settled. (EDIT: For the record, one can point out a structural (but not relevant) difference between the two problems: Here, the logical equivalences \"learning\" &lt;--&gt; \"good mark\" and \"not learning\" &lt;--&gt; \"bad mark\" are part of the game's assumptions, while the teacher predicts in which of these two worlds we live in. In Newcomb's Problem, Omega predicts the logical equivalences of taking boxes and payoffs.)</li>\n<li>Now let's consider a situation where we assume a teacher having no forecasting power at all. In such a scenario the student's future effort behaves independently of the settled marks, that is no matter what input the student provides, the output of the teacher will have been <em>random</em>. Therefore, if we find ourselves in such a situation we shouldn't study for the exam and enjoy the gained spare time.</li>\n</ol>\n<p>(Of course we can also think of a case 3) where the teacher's prediction is wrong in 100% of all cases. Let&rsquo;s specify &ldquo;wrong&rdquo; since marks usually don&rsquo;t work in binaries, so let&rsquo;s go with &ldquo;wrong&rdquo; as the complementary mark. For instance, the best mark corresponds to the worst, the second best to the second worst and so on. In such a case not learning at all and returning an empty exam sheet would determine receiving the best marks. However, this scenario won't be of big interest to us.)<br />This thought experiment suggests that a deterministic world does not necessarily imply fatalism, since in expectation the fatalist (who wouldn't feel obligated to learn because the marks are \"already written down\") would lose in cases where the teacher predicts other than random. Generally, we can say that &ndash; beside the case 2) &ndash; in all the other cases the learning behaviour of the student is <em>relevant</em> for receiving a good mark.<br />This thought experiment does not only make it clear that determinism does not imply fatalism, but it even shows that fatalists tend to lose once they stop investing ressources in desriable outcomes. This will be important in subsequent sections. Now let us get to the actual topic of this article which already has been mentioned as an aside: Newcomblike problems.</p>\n<p>&nbsp;</p>\n<h2>Newcomb&rsquo;s Problem</h2>\n<p>The standard version of <a href=\"http://wiki.lesswrong.com/wiki/Newcomb's_problem\" target=\"_blank\">Newcomb&rsquo;s Problem</a> has been thoroughly discussed on Lesswrong. Many would agree that one-boxing is the correct solution, for one-boxing agents obtain a million dollars, while two-boxers only take home a thousand dollars. To clarify the structure of the problem: an agent chooses between two options, &ldquo;AB&ldquo; and &ldquo;B&ldquo;. When relatively considered, the option B &ldquo;costs&rdquo; a thousand dollars because one would abandon transparent box A containing this amount of money. As we play with the predictor Omega, who has an almost 100% &nbsp;forecasting power, our decision <em>determines</em> what past occured, that is we determine whether Omega put a million into box B or not. With determining I mean as much as &ldquo;<strong>being compatible with</strong>&rdquo;. Hence, choosing box B is compatible only with a past where Omega put a million into it.</p>\n<p>&nbsp;</p>\n<h2>Newcomb&rsquo;s Problem&rsquo;s Problem of Free Will</h2>\n<p>To many, Newcomb&rsquo;s Problem seems counterintuitive. People tend to think: &ldquo;We cannot change the past, as past events have already happened! So there&rsquo;s nothing we can do about it. Still, somehow the agents that only choose B become rich. How is this possible?&ldquo;<br />This uneasy feeling can be resolved by clarifing the notion of &ldquo;free will&rdquo;, i.e. by acknowledging that a world state X either logically implies (hard determinism) or probabilistically suggests (hard incompatibilism, stating that free will is impossible and complete determinism is false) another world state Y or a set of possible world states (Y1,Y2,Y3,..,Yn) &ndash; no matter if X precedes Y or vice versa. (Paul Almond has shown in his <a href=\"http://www.paul-almond.com/\" target=\"_blank\">paper</a> on decision theory &ndash; unfortunately his page has been down lately &ndash; that upholding this distinction does not affect the clarification of free will in decision-theoretic problems. Therefore, I chose to go with hard determinism.)</p>\n<p>The fog will lift once we accept the above. Since our action is a subset of a particular world state, the action itself is also implied by preceding world states, that is once we know all the facts about a preceding world state we can derive facts about subsequent world states.<br />If we look more closely, we cannot <em>really choose</em> in a way that people used to think. Common sense tells us that we confront a &ldquo;real choice&rdquo; if our decision is not just determined by external factors and also not picked at random, but governed by our free will. But what could this third case even mean? Despite its intuitive usefulness, the classical notion of choice seems to be an ill-defined term since it requires a problematic notion of free will, that is to say one that ought to be non-random but also not determined at once.<br />This is why I want to suggest a new definition of choice: Choosing is the way agents execute what they were determined to by other world states. Choosing has nothing to do with &ldquo;changing&rdquo; what did or is going to happen. The only thing that actually changes is the <em>perception</em> of what did or is going to happen, since executions produce new data points that call for updates.<br />So unless we could use a &ldquo;true&rdquo; random generator (which would only be possible if we did not assume complete determinism to be true) in order to make decisions, what we are going to do is &ldquo;planned&rdquo; and determined by preceding and subsequent world states.<br />If I take box B, then this determines a past world state where Omega has put a million dollars into this box. If I take both box A and B, then this determines a past world state where Omega has left box B empty. Therefore, when it comes to deciding, taking actions that determine (or are compatible with) not only desirable <em>future worlds</em>, but also desirable <em>past worlds</em> are the ones that make us win.<br />One may object now that we aren&rsquo;t &ldquo;really&ldquo; determining the past, but we only determine <em>our perception</em> of it. That&rsquo;s an interesting point. In the next section we are going to have a closer look on that. For now, I&rsquo;d like to bring the underlying perception of time into question. Because once I choose only box B, it seems that the million dollars I receive is not just an illusion of my map but it is really out there. Admittedly the past seems unswayable, but this example shows that maybe our conventional perception of time is misleading as it conflicts with the notion of us choosing what happened in the past.<br />How come self-proclaimed deterministic non-fatalists in fact <em>are</em> fatalists when they deal with the past? I&rsquo;d suggest to perceive time not as being divided into seperate caterogies like &ldquo;stuff that has passed &ldquo; and &ldquo;stuff that is about to happen&ldquo;, but rather as one dimension where every dot is just as <em>real</em> as any other and where the manifestation of one particular dot restrictively <em>determines</em> the set of possible manifestations other dots could embody. It is crucial to note that such a dot would describe the whole world in three spatial dimensions, while subsets of world states could still behave independently.</p>\n<p>Perceiving time without an inherent &ldquo;arrow&rdquo; is not new to <a href=\"http://en.wikipedia.org/wiki/Relativity_of_simultaneity\" target=\"_blank\">science</a> <a href=\"http://en.wikipedia.org/wiki/Eternalism_(philosophy_of_time)\" target=\"_blank\">and</a> <a href=\"http://en.wikipedia.org/wiki/Rietdijk-Putnam_argument\" target=\"_blank\">philosophy</a>, but still, readers of this post will probably need a compelling reason why this view would be more goal-tracking. Considering the Newcomb&rsquo;s Problem a reason can be given: Intuitively, the past seems much more &ldquo;settled&rdquo; to us than the future. But it seems to me that this notion is confounded as we often know more about the past than we know about the future. This could tempt us to project this disbalance of knowledge onto the universe such that we perceive the past as settled and unswayable in contrast to a shapeable future. However, such a conventional set of intuitions conflicts strongly with us picking only one box. These intuitions would tell us that we cannot affect the content of the box; it is already filled or empty since it has been prepared in the now <em>inaccessible</em> past.</p>\n<p>Changing the notion of time into one block would lead to &ldquo;better&rdquo; intuitions, because they directly suggested to choose one box, as this action is only compatible with a more desirable past. Therefore we might need to adapt our intution, <a href=\"/lw/hs/think_like_reality/\" target=\"_blank\">so that the universe looks normal</a> again. To illustrate the ideas discussed above and to put them into practice, I have constructed the following game:</p>\n<p>&nbsp;</p>\n<h2>The A,B-Game</h2>\n<p>You are confronted with Omega, a 100% correct predictor. In front of you, there are two buttons, A and B. You know that there are two kinds of agents. Agents with the gene G_A and agents with the gene G_B. Carriers of G_A are blessed with a life expectancy of 100 years whereas carriers of G_B die of cancer at the age of 40 on average. Suppose you are much younger than 40. Now Omega predicts that every agent who presses A is a carrier of G_A and every agent that presses B is a carrier of G_B. You can only press one button, which one should it be if you want to live for as long as possible?<br />People who prefer to live for a hundred years over forty years would press A. They would even pay a lot of money in order to be able to do so. Though one might say one cannot change or choose one&rsquo;s genes. Now we need to be clear about which definition of choice we make use of. Assuming the conventional one, I would agree that one could not choose one&rsquo;s genes, but for instance, when getting dressed, one could not choose one&rsquo;s jeans either, as the conventional understanding of choice requires an empty notion of non-random, not determined free will that is not applicable. Once we use the definition I introduced above, we can say that we choose our jeans. Likewise, we can choose our genes in the A,B-Game. If we one-box in Newcomb&rsquo;s Problem, we should also press A here, because the two problems are structurally identical (except for the labels &ldquo;box&rdquo; versus &ldquo;gene&rdquo;).<br />The notion of <em>objective ambiguity</em> of genes only stands if we believe in some sort of <em>objective ambiguity</em> about which choices will be made. When facing a correct predictor, those of us who believe in indeterministic objective ambiguity of choices have to bite the bullet that their genes would be objectively ambiguous. Such a model seems counterintuitive, but not contradictory. However, I don&rsquo;t feel forced to adapt this indeterministic view.</p>\n<p>Let us focus on the deterministic scenario again: In this case, our past already determined our choice, so there is only one way we will go and only one way we can go.<br />We don&rsquo;t know whether we are determined to do A or B. By &ldquo;choosing&rdquo; the one action that is <em>compatible</em> only with the more desirable past, we are better off. Just as we don&rsquo;t know in Newcomb&rsquo;s Problem whether B is empty or not, we have to behave in a way such that it must have been filled already. From our perspective, with little knowledge about the past, our choice determines the manifestation of our map of the past. Apparently, this is exactly what we do when making choices about the future. Taking actions determines the manifestation of our map of the future. Although the future is already settled, we don&rsquo;t know yet its exact manifestation. Therefore, from our perspective, it makes sense to act in ways that determine the most desirable futures. This does not automatically imply that some mysterious &ldquo;change&rdquo; is going to happen.<br />In both directions it feels like one would change the manifestation of other world states, but when we look more closely we cannot even spell out what that would mean. The word &ldquo;change&rdquo; only starts to become meaningful once we hypothetically compare our world with counterfactual ones (where we were not determined to do what we do in our world). In such a framework we could consistently claim that the content of box B &ldquo;changes&rdquo; depending on whether or not we choose only box B.</p>\n<p>&nbsp;</p>\n<h2>Screening off</h2>\n<p>Following this approach of determining one&rsquo;s perception of the world, the question arises, whether <em>every</em> change in perception is actually goal-tracking. We can ask ourselves, whether an agent should avoid new information if she knew that the new information had negative news value. For instance, if an agent, being suspected of having lung cancer and awaiting the results of her lung biopsy, seeks actions that make more desirable past world states more likely, then she should figure out a way so that she doesn&rsquo;t receive any mail, for instance by declaring an incorrect postal address. This naive approach obviously fails because of lack of proper use of Bayesian updating. The action &rdquo;avoiding to receive mail&rdquo; <a href=\"http://wiki.lesswrong.com/wiki/Screening_off\" target=\"_blank\">screens off</a> the desirable outcome so that once we know about this action we don&rsquo;t learn anything about the biopsy in (the very probable) case that we don&rsquo;t receive any mail.</p>\n<p>In the A,B-Game, this doesn&rsquo;t apply, since we believe Omega&rsquo;s prediction to be true when it says that A necessarily belongs to G _A and B to G_B. Generally, we can distinguish the cases by clarifying existing independencies: In the lung cancer case where we simply don&rsquo;t know better, we can assume that P(prevention|positive lab result)=P(prevention|negative lab result)=P(prevention). Hence, screening off applies. In the A,B-Game, we should believe that P(Press A|G_A)&gt;P(Press A)=P(Press A|G_A or G_B). We obtain this relevant piece of information thanks to Omega&rsquo;s forecasting power. Here, screening off does not apply.</p>\n<p>Subsequently, one might object that the statement P(Press A|G_A)&gt;P(Press A) leads to a conditional independence as well, at least in cases where not all the players that press A necessarily belong to G_A. Then you might be pressing A because of your reasoning R_1 which would screen off pressing A from G_A. A further objection could be that even if one could show a dependency between G_A and R_1, you might be choosing R_1 because of some meta-reasoning R_2 that again provides a reason not to press A. However, considering these objections more thoroughly, we realize that R_1 has to be <em>congruent</em> or at least evenly associated (in G_A as well as in G_B) with Pressing A. The same works for R_2. If this wasn&rsquo;t the case, then we would be talking about <em>another game</em>, a game where we knew, for instance, that 90% of the G_A carriers choose button A (without thinking) because of the gene and 10% of the G_B carriers would choose button A because of some sort of evidential reasoning. Knowing this, choosing A <em>out of evidential reasoning</em> would be foolish, since we already know that only G_B carriers could do that. Once we know this, evidential reasoners would suggest not to press A (unless B offers an even worse outcome). So these further objections fail as well, as they implicitly change the structure of the discussed problem. We can conclude that &nbsp;no screening off applies as long as an instance with forecasting power tells us that a particular action makes the desirable outcome likelier.<br />Now let&rsquo;s have a look at an alteration of the A,B-Game in order to figure out whether screening-off might apply here.</p>\n<p>&nbsp;</p>\n<h2>A Weak Omega in The A,B-Game</h2>\n<p>Thinking about the A,B-Game, what happens if we decreased Omega&rsquo;s <em>forecasting power</em>? Let&rsquo;s assume now that Omega&rsquo;s prediction is correct only in 90% of all cases. Should this fundamentally change our choice whether to press A or B because we only pressed A as a consequence of our <em>reasoning</em>?<br />To answer that, we need to be clear about why agents believe in Omega&rsquo;s predictions. They believe in Omega&rsquo;s prediction because they were correct so many times. This constitutes Omega&rsquo;s strong forecasting power. As we saw above, screening off only applies if the predicting instance (Omega, or us reading a study) has no forecasting power at all.<br />In the A,B-Game, as well as in the original Newcomb&rsquo;s Problem, we also have to take the predictions of a weaker Omega (with less forecasting power) into account, unless we face an Omega that happens to be right by chance (i.e. in 50% of the cases when considering a binary decision situation).</p>\n<p>If, in the standard A,B-Game, we consider pressing A to be important, and if we were willing to spend a large &nbsp;amount of money in order to be able to press A (suppose the button A would send a signal to cause a withdrawal from our bank account), then this amount should only <em>gradually</em> shrink once we decrease Omega&rsquo;s forecasting power. The question now arises whether we also had to &ldquo;choose&rdquo; the better genes in the medical version of Solomon&rsquo;s Problem and whether there might not be a fundamental difference between it and the original Newcomb&rsquo;s Problem.</p>\n<p>&nbsp;</p>\n<h2>Newcomb&rsquo;s versus Solomon&rsquo;s Problem</h2>\n<p>In order to uphold this convenient distinction, people tell me that &ldquo;you cannot change your genes&rdquo; though that&rsquo;s a bad argument since one could reply &ldquo;according to your definition of change, you cannot change the content of box B either, still you choose one-boxing&rdquo;. Further on, I quite often hear something like &ldquo;in Newcomb&rsquo;s Problem, we have to deal with <em>Omega</em> and that&rsquo;s something completely different than just reading a <em>study</em>&rdquo;. This &ndash; in contrast to the first &ndash; is a good point.</p>\n<p>In order to accept the forecasting power of a 100% correct Omega, we already have to presume induction to be legitimate. Or else one could say: &ldquo;Well, I see that Omega has been correct in 3^^^3 cases already, but why should I believe that it will be correct the next time?&rdquo;. As sophisticated this may sound, such an agent would lose terribly. So how do we deal with studies then? Do they have any forecasting power at all? It seems that this again depends on the setting of the game. Just as Omega&rsquo;s forecasting power can be set, the forecasting power of a study can be properly defined as well. It can be described by assigning values to the following two variables: its <em>descriptive power</em> and its <em>inductive power</em>. To settle them, we have to answer two questions: 1. How correct is the study's description of the population? 2. How representative is the population of the study to the future population of agents acting in knowledge of the study? Or in other words, to what degree can one consider the study subjects to be in one&rsquo;s reference class in order to make true predictions about one&rsquo;s behaviour and the outcome of the game? Once this is clear, we can then infer the forecasting power. How much forecasting power does the study have? Let&rsquo;s assume that the study we deal with is correct in what it describes. Those who wish can use a discounting factor. However, this is not important for subsequent arguments and would only make it more complicated.</p>\n<p>Considering the inductive power, it get&rsquo;s more tricky. Omega&rsquo;s predictions are <em>defined</em> to be correct. In contrast, the study&rsquo;s predictions have not been tested. Therefore we are quite uncertain about the study&rsquo;s forecasting power. It were 100% if and only if every factor involved was specified so that the total of them compel identical outcomes in the study and our game. Due to induction, we do have reason to assume a positive value of forecasting power. To identify its specific value (that discounts the forecasting power according to the specified conditions), we would need to settle every single factor that might be involved. So let&rsquo;s keep it simple by applying a 100% forecasting power. As long as there is a positive value of forecasting power, the basic point of the subsequent arguments (that presume a 100% forecasting power) will also hold when discounted.<br />Thinking about the inductive power of the study, there still is one thing that we need to specify: It is not clear what exactly previous subjects of the study knew.</p>\n<p>For instance in a case A), the subjects of the study knew nothing about the tendency of CGTA-carriers to chew gum. First, their genom was analyzed, then they had to decide whether or not to chew gum. In such a case, the subjects&lsquo; knowledge is quite different from those who play the medical version of Solomon&rsquo;s Problem. Therefore screening off applies. But does it apply to the same extent as in the avoiding-bad-news example mentioned above? That seems to be the case. In the avoiding-bad-news example, we assumed that there is no connection between the variables &bdquo;lung cancer&ldquo; and &bdquo;avoiding mail&ldquo;. In Solomon&rsquo;s Problem such an indepence can be settled as well. Then the variables &bdquo;having the gene CGTA&ldquo; and &bdquo;not chewing gum because of evidential reasoning&ldquo; are also assumed to be independent. Total screening off applies. Considering an evidential reasoner who knows that much, choosing not to chew gum would then be as irrational as declaring an incorrect postal address when awaiting biopsy results.</p>\n<p>Now let us consider a case B) where the subjects were introduced to the game just as we were. Then they would know about the tendency of CGTA-carriers to chew gum, and they themselves might have used evidential reasoning. In this scenario, screening off does not apply. This is why not chewing gum would be the winning strategy.<br />One might say that of course the study-subjects did not know of anything and that we should assume case A) a priori. I only partially agree with that. The screening off can already be weakend if, for instance, the subjects knew why the study was conducted. Maybe there was anecdotal evidence about heredity of a tendency to chew gum, which was about to be confirmed properly.<br />Without further clarification, one can plausibly assume a probability distribution over various intermediate cases between A and B where screening off becomes gradually fainter when getting closer to B. Of course there might also be cases where anecdotal evidence leads astray, but in order to cancel out the argument above, anecdotal evidence needs to be equalized with in expectation knowing nothing at all. But since it seems to be better (even though not much) than knowing nothing, it is not a priori clear that we have to assume case A right away. &nbsp;<br />So when compiling a medical version of Solomon&rsquo;s Problem, it is important to be very clear about what the subjects of the study were aware of.</p>\n<p>&nbsp;</p>\n<h2>What about Newcomb&rsquo;s Soda?</h2>\n<p>After exploring screening off and possible differences between Newcomb&rsquo;s Problem and Solomon&rsquo;s Problem (or rather between Omega and a study), let&rsquo;s investigate those questions in another game. My favourite of all Newcomblike problems is called <a href=\"/lw/gu1/decision_theory_faq/#newcombs-soda\" target=\"_blank\">Newcomb&rsquo;s Soda</a> and was introduced in <a href=\"http://intelligence.org/files/TDT.pdf\" target=\"_blank\">Yudkowsky (2010)</a>. Comparing Newcomb&rsquo;s Soda with Solomon&rsquo;s Problem, Yudkowsky writes:<br />&ldquo;Newcomb&rsquo;s Soda has the same structure as Solomon&rsquo;s Problem, except that instead of the outcome stemming from genes you possessed since birth, the outcome stems from a soda you will drink shortly. Both factors are in no way affected by your action nor by your decision, but your action provides <em>evidence</em> about which genetic allele you inherited or which soda you drank.&rdquo;</p>\n<p>Is there any relevant difference in structure between the two games?<br />In the previous section, we saw that once we settle that the study-subjects in Solomon&rsquo;s Problem don&rsquo;t know of any connection between the gene and chewing gum, screening off applies and one has good reasons to chew gum. Likewise, the screening off only applies in Newcomb&rsquo;s Soda if the subjects of the clinical test are completely unaware of any connection between the sodas and the ice creams. But is this really the case? Yudkowsky introduces the game as one big clinical test in which you are participating as a subject:</p>\n<p>&ldquo;You know that you will shortly be administered one of two sodas in a double-blind clinical test. After drinking your assigned soda, you will enter a room in which you find a chocolate ice cream and a vanilla ice cream. The first soda produces a strong but entirely subconscious desire for chocolate ice cream, and the second soda produces a strong subconscious desire for vanilla ice cream.&rdquo;</p>\n<p>This does not sound like previous subjects had no information about a connection between the sodas and the ice creams. Maybe you, and you alone, received those specific insights. If this were the case, it clearly had to be mentioned in the game&rsquo;s definition, since this factor is crucial when it comes to decision-making. Considering a game where the agent herself is a study-subject, without further specification, she wouldn&rsquo;t by default expect that other subjects knew less about the game than she did. Therefore let&rsquo;s assume in the following that all the subjects in the clinical test knew that the sodas cause a subconscious desire for a specific flavor of ice cream.</p>\n<p>&nbsp;</p>\n<h2>Newcomb&rsquo;s Soda in four variations</h2>\n<p>Let &ldquo;C&rdquo; be the causal approach which states that one has to choose vanilla ice cream in Newcomb&rsquo;s Soda. C only takes the $1,000 of the vanilla ice cream into account since one still can <em>change</em> the variable &ldquo;ice cream&rdquo;, whereas the variable &ldquo;soda&rdquo; is already <em>settled</em>. Let &ldquo;E&rdquo; be the evidential approach which suggests that one has to choose chocolate or vanilla ice cream in Newcomb&rsquo;s Soda &ndash; depending on the probabilities specified. E takes both the $1,000 of the vanilla ice cream and the $1,000,000 of the chocolate soda into account. In that case, one argument can outweigh the other. &nbsp;</p>\n<p>Let&rsquo;s compile a series of examples. We denote &ldquo;Ch&rdquo; for chocolate, &ldquo;V&rdquo; for vanilla, &ldquo;S&rdquo; for soda and &ldquo;I&rdquo; for ice cream. In all versions Ch-S will receive $1,000,000 and V-I will receive $1,000 and P(Ch-S)=P(V-S)=0.5. Furthermore we settle that P(Ch-I|Ch-S)=P(V-I|V-S) and call this term &ldquo;p&rdquo; in every version so we don&rsquo;t vary unnecessarily many parameters. As we are going to deal with large numbers, let&rsquo;s assume a linear monetary value utility function.</p>\n<p><strong>Version 1: </strong>Let us assume a case where the sodas are dosed homeopathically, so that no effect on the choice of ice creams can be observed. Ch-S and V-S choose from Ch-I and V-I randomly so that p=P(V-I|Ch-S)=P(Ch-I|V-S)=0.5. Both C and E choose V-I and win 0.5 *$1,001,000 + 0.5*$1000=$501,000 in expectation. C only considers the ice cream whereas E considers the soda as well, though in this case the soda doesn&rsquo;t change anything as the Ch-S are equally distributed over Ch-I and V-I.</p>\n<p><strong>Version 2: </strong>Here p=0.999999. Since P(Ch-S)=P(V-S)=0.5, one Ch-I in a million will have originated from V-S, whereas one V-I in a million will have originated from Ch-S. The other 999,999 Ch-I will have <em>determined</em> the desired past, Ch-S, due to their choice of Ch-I. So if we participated in this game a million times and tracked E that suggests choosing Ch-I each time, we overall could have expected to win 999,999*$1,000,000=$999,999,000,000. This is different to following C&rsquo;s advice. As C tells us that we cannot affect which soda we have drunk we would choose V-I each time and could expect to win 1,000,000*$1,000+$1,000,000=$1,001,000,000 in total. The second outcome, which C is responsible for, is 999 times worse than the first (which was suggested by E). In this version, E clearly outperforms C in helping us to make the most money.</p>\n<p><strong>Version 3: </strong>Now we have p=1. This version is equivalent to the standard version of the A,B-Game. What would C do? It seems that C ought to maintain its view that we <em>cannot</em> affect the soda. Therefore, only considering the ice cream-part of the outcome, C will suggest choosing V-I. This seems to be absurd: C leaves us disappointed with $1,000, whereas E makes us millionaires <em>every single time</em>.<br />A C-defender might say: &ldquo;Wait! Now you have changed the game. Now we are dealing with a probability of 1!&rdquo; The response would be : &ldquo;Interesting, I can make p get as close to 1 as I want as long as it <em>isn&rsquo;t</em> 1 and the rules of the game and my conclusions would still remain. For instance, we can think of a number like 0.999&hellip;(100^^^^^100 nines in a row). So tell me why exactly the probability change of 0.000&hellip;(100^^^^^100 -1 zeros in a row)1 should make you switch to Ch-I? But wait, why would you &ndash; as a defender of C &ndash; &nbsp;even consider Ch-I since it cannot affect your soda while it definitely prevents you from winning the $1,000 of the ice cream?&rdquo;</p>\n<p>The previous versions tried to exemplify why taking <em>both</em> arguments (the $1,000 and the $1,000,000) into account makes you better off at the one edge of the probability measure, whereas at the other edge, C and E produce the same outcomes. With a simple equation we can figure out for which p E would be indifferent about whether to choose Ch-I or V-I: solve(p*1,000,000=(1-p)*1,000,000+1,000,p). This gives us p=0.5005. So for 0.5005&lt;p&lt;=1 E does better than C and for 0&lt;=p&lt;=0.5005 E and C behave alike. Finally, let us consider the original version: &nbsp;</p>\n<p><strong>Version 4:</strong>&nbsp;Here we deal with p=0.9. According to the above we could already deduce that deciding according to E makes us better off, but let&rsquo;s have a closer look at it for the sake of completeness: In expectation, choosing V-I makes us win 0.1*$1,000,000+$1,000=$101,000, whereas Ch-I leaves us with 0.9*$1,000,000=$900,000 almost 9 times richer. After the insights above, it shouldn&rsquo;t surprise us too much that E clearly does better than C in the original version of Newcomb&rsquo;s Soda as well.</p>\n<p>The variations above illustrated that C had to eat V-I even if 99.9999% of C-S choose C-I and 99.9999% of V-S eat V-I. If you played it a million times, in expectation C-I would win the million 999,999 times and V-I just once. Can we <em>really</em> be indifferent about that? Wasn&rsquo;t it all about winning and losing? And who is winning here and who is losing?</p>\n<p>&nbsp;</p>\n<h2>Newcomb-Soda and Precommitments</h2>\n<p>Another excerpt from <a href=\"http://intelligence.org/files/TDT.pdf\" target=\"_blank\">Yudkowsky (2010)</a>:<br />&ldquo;An evidential agent would rather precommit to eating vanilla ice cream than precommit to eating chocolate, because such a precommitment made <em>in advance of drinking the soda</em> is not <em>evidence</em> about which soda will be assigned.&rdquo;<br />At first sight this seems intuitive. But if we look at the probabilities more closely suddenly a problem arises: Let&rsquo;s consider an agent that precommits (let&rsquo;s assume a 100% persistent mechanism) one&rsquo;s decision before a standard game (p=0.9) starts. Let&rsquo;s assume that he precommits &ndash; as suggested above &ndash; to choose V-I. What credence should he assign to P(Ch-S|V-I)? Is it 0.5 as if he didn&rsquo;t precommit at all or does something change? Basically, adding precommitments to the equation inhibits the effect of the sodas on the agent&rsquo;s decision. Again, we have to be clear about which agents are affected by this newly introduced variable. If we were the only ones who can precommit 100% persistently, then our game fundamentally differs from the previous subjects&rsquo; one. If they didn&rsquo;t precommit, we couldn&rsquo;t presuppose a forecasting power anymore because the previous subjects decided according to the soda&rsquo;s effect, whereas we now decide independently of that. In this case, E would suggest to precommit to V-I. However, this would constitute an entirely new game without any forecasting power. If all the agents of the study make persistent precommitments, then the forecasting power holds; the game doesn&rsquo;t change fundamentally. Hence, the way previous subjects behaved remains crucial to our decision-making. Let&rsquo;s now imagine that we were playing this game a million times. Each time we irrevocably precommit to V-I. In this case, if we consider ourselves to be sampled randomly among V-I, we can expect to originate from V-S 900,000 times. If we approach p to 1 we see that it gets desperately unlikely to originate from Ch-S once we precommit ourselves to V-I. So a rational agent following E should precommit Ch-I in advance of drinking the soda. Since E suggests Ch-I both during and before the game, this example doesn&rsquo;t show that E would be <a href=\"http://wiki.lesswrong.com/wiki/Dynamic_inconsistency\" target=\"_blank\"><em>dynamically inconsistent</em></a>.</p>\n<p>In the other game, where only we precommit persistently and the previous subjects don&rsquo;t, picking V-I doesn&rsquo;t make E dynamically inconsistent, as we would face another decision situation where no forecasting power applies. Of course we can also imagine intermediate cases. For instance one, where we make precommitments and the previous subjects were able to make them as well, but we don&rsquo;t know whether they did. The more uncertain we get &nbsp;about their precommitments, the more we approach the case where only we precommit while the forecasting power gradually weakens. Those cases are more complicated, but they do not show a dynamical inconsistency of E either.</p>\n<p>&nbsp;</p>\n<h2><span style=\"line-height: 15px;\">The tickle defense in Newcomblike problems</span></h2>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"line-height: 15px;\">In the last section I want to have a brief look at the tickle defense, which is sometimes used to defend evidential reasoning by offering a less controversial output. For instance, it states that in the medical version of Solomon&rsquo;s Problem an evidential reasoner should chew gum, since she can rule out having the gene as long as she doesn&rsquo;t feel an urge to chew gum. So chewing gum doesn&rsquo;t make it likelier to have the gene since she already has ruled it out.</span></p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"line-height: 15px;\">I believe that this argument fails since it changes the game. Suddenly, the gene doesn&rsquo;t cause you to &ldquo;choose chewing gum&rdquo; anymore but to &ldquo;feel an urge to choose chewing gum&rdquo;. Though I admit, in such a game a conditional independence would screen off the action &ldquo;not chewing gum&rdquo; from &ldquo;not having the gene&rdquo; &ndash; no matter what the previous subjects of the study knew. This is why it would be more attractive to chew gum. However, I don&rsquo;t see why this case should matter to us. In the original medical version of Solomon&rsquo;s Problem we are dealing with another game where this particular kind of screening off does not apply. As the gene causes one to &ldquo;choose chewing gum&rdquo; we can only rule it out by not doing so. However, this conclusion has to be treated with caution. For one thing, depending on the numbers, one can only diminish the probability of the undesirable event of having the gene &ndash; not rule it out completely; for another thing, the diminishment only works if the previous subjects were not ignorant of a depedence of the gene and chewing gum &ndash; at least in expectation. Therefore the tickle defense only trivially applies to a special version of the medical Solomon&rsquo;s Problem and fails to persuade proper evidential reasoners to do anything differently in the standard version. Depending on the specification of the previous subjects&rsquo; knowledge, an evidential reasoner would still chew or not chew gum.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iC4NueuGWSwBKPXPh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 1, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "24823", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I have collected some thoughts on decision theory and am wondering whether they are any good, or whether I\u2019m just thinking non-sense. I would really appreciate some critical feedback. Please be charitable in terms of language and writing style, as I am not a native English speaker and as this is the first time I am writing such an essay.</p>\n<h2 id=\"Overview\">Overview</h2>\n<ul>\n<li>The classical notion of free will messes up our minds, especially in decision-theoretic problems. Once we come to see it as confused and reject it, we realize that our choices in some sense not only determine the future but also the past.</li>\n<li>If determining the past conflicts with our intuitions of how time behaves, then we need to adapt our intuitions.</li>\n<li>The A,B-Game shows us that, as far as the rejection of free will allows for it, it is in principle possible to <em>choose</em> our genes.</li>\n<li>Screening off only applies if we consider our action to be independent of the variable of interest \u2013 at least in expectation.</li>\n<li>When dealing with Newcomblike problems, we have to be clear about which forecasting powers are at work. Likewise, it turns out to be crucial to precisely point out which agent knows how much about the setting of the game.</li>\n<li>In the standard version of Newcomb\u2019s Soda, one should choose chocolate ice cream \u2013 unless the game were specified in a way that previous subjects did not (unlike us) know of any interdependence of soda and ice cream.</li>\n<li>Variations of Newcomb\u2019s Soda suggest that the evidential approach makes us better off.</li>\n<li>The analysis of Newcomb\u2019s Soda shows that its formulation fundamentally differs from the formulation of Solomon\u2019s Problem. &nbsp;</li>\n<li>Given all study-subjects make persistent precommitments, a proper use of evidential reasoning suggests <em>precommitting</em> to take chocolate ice cream. This is why Newcomb\u2019s Soda does not show that the evidential approach is dynamically inconsistent.</li>\n<li>The tickle defense does not apply to the standard medical version of Solomon\u2019s Problem. In versions where it applies, it does not tell us anything non-trivial.</li>\n<li>Evidential reasoning seems to be a winning approach not only in Newcomb\u2019s Problem, but also in Newcomb\u2019s Soda and in the medical version of Solomon\u2019s Problem. Therefore, we should consider a proper use of evidential reasoning as a potentially promising component when building the ultimate decision algorithm.</li>\n</ul>\n<p>In the standard formulation of Newcomb\u2019s Soda, the evidential approach suggests picking chocolate ice cream, since this makes it more probable that we will have been awarded the million dollars. Hence, it denies us the thousand dollars we actually could win if we only took vanilla ice cream. Admittedly, this may be counterintuitive. Common sense tells us that considering the thousand dollars, one could <em>change the outcome</em>, whereas one cannot change which type of soda one has drunk; therefore we have to make a decision that actually <em>affects</em> our outcome. Maybe the flaw in this kind of reasoning doesn\u2019t pose a problem to our intuitions as long as we deal with a \u201ccausal-intuition-friendly\u201d setting of numbers. So let\u2019s consider various versions of this problem in order to thoroughly compare the &nbsp;two competing algorithmical traits. Let\u2019s find out which one actually <em>wins</em> and therefore should be implemented by rational agents.</p>\n<p>In this post, I will discuss Newcomblike problems and conclude that the arguments presented support an evidential approach. <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\" target=\"_blank\">Various</a> <a href=\"http://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\" target=\"_blank\">decision problems</a> have shown that plain evidential decision theory is not a winning strategy. I instead propose to include evidential reasoning in more elaborate decision theories, such as timeless decision theory or updateless decision theory, since they also need to come up with an answer in Newcomblike problems.<br>By looking at the strategies proposed in those problems, currently discussed decision theories produce outputs that can be grouped into evidential-like and causal-like. I am going to outline which of these two traits a winning decision theory must possess.<br>Let\u2019s consider the following excerpt by <a href=\"http://intelligence.org/files/TDT.pdf\" target=\"_blank\">Yudkowsky (2010)</a> about the <a href=\"/lw/gu1/decision_theory_faq/#medical-newcomb-problems\" target=\"_blank\">medical version of Solomon\u2019s Problem</a>:</p>\n<p>\u201cIn the chewing-gum throat-abscess variant of Solomon\u2019s Problem, the dominant action is chewing gum, which leaves you better off whether or not you have the CGTA gene; but choosing to chew gum is <em>evidence</em> for possessing the CGTA gene, although it cannot <em>affect</em> the presence or absence of CGTA in any way.\u201d</p>\n<p>In what follows, I am going to elaborate on why I believe this point (in the otherwise brilliant paper) needs to be reconsidered. Furthermore, I will explore possible objections and have a look at other decision problems that might be of interest to the discussion.<br>But before we discuss classical Newcomblike problems, let\u2019s first have a look at the following thought experiment:</p>\n<p>&nbsp;</p>\n<h2 id=\"The_school_mark_is_already_settled\">The school mark is already settled</h2>\n<p>Imagine you were going to school; it is the first day of the semester. Suppose you only care about getting the best marks. Now your math teacher tells you that he knows you very well and that this would be why he already wrote down the mark you will receive for the upcoming exam. To keep things simple, let\u2019s cut down your options to \u201cstudy as usual\u201d and \u201cdon't study at all\u201d. What are you going to do? Should you learn as if you didn\u2019t know about the settled mark? Or should you not learn at all since the mark has already been written down?</p>\n<p>This is a tricky question because the answer to it depends on your credence in the teacher\u2019s <em>forecasting power</em>. Therefore let's consider the following two cases:</p>\n<ol>\n<li>Let's assume that the teacher is correct in 100% of the cases. Now we find ourselves in a problem that resembles Newcomb's Problem since our decision exactly determines the output of his prediction. Just as an agent that really wishes to win the most money should take only one box in Newcomb\u2019s Problem, you should learn for the exams as if you didn't know that the marks are already settled. (EDIT: For the record, one can point out a structural (but not relevant) difference between the two problems: Here, the logical equivalences \"learning\" &lt;--&gt; \"good mark\" and \"not learning\" &lt;--&gt; \"bad mark\" are part of the game's assumptions, while the teacher predicts in which of these two worlds we live in. In Newcomb's Problem, Omega predicts the logical equivalences of taking boxes and payoffs.)</li>\n<li>Now let's consider a situation where we assume a teacher having no forecasting power at all. In such a scenario the student's future effort behaves independently of the settled marks, that is no matter what input the student provides, the output of the teacher will have been <em>random</em>. Therefore, if we find ourselves in such a situation we shouldn't study for the exam and enjoy the gained spare time.</li>\n</ol>\n<p>(Of course we can also think of a case 3) where the teacher's prediction is wrong in 100% of all cases. Let\u2019s specify \u201cwrong\u201d since marks usually don\u2019t work in binaries, so let\u2019s go with \u201cwrong\u201d as the complementary mark. For instance, the best mark corresponds to the worst, the second best to the second worst and so on. In such a case not learning at all and returning an empty exam sheet would determine receiving the best marks. However, this scenario won't be of big interest to us.)<br>This thought experiment suggests that a deterministic world does not necessarily imply fatalism, since in expectation the fatalist (who wouldn't feel obligated to learn because the marks are \"already written down\") would lose in cases where the teacher predicts other than random. Generally, we can say that \u2013 beside the case 2) \u2013 in all the other cases the learning behaviour of the student is <em>relevant</em> for receiving a good mark.<br>This thought experiment does not only make it clear that determinism does not imply fatalism, but it even shows that fatalists tend to lose once they stop investing ressources in desriable outcomes. This will be important in subsequent sections. Now let us get to the actual topic of this article which already has been mentioned as an aside: Newcomblike problems.</p>\n<p>&nbsp;</p>\n<h2 id=\"Newcomb_s_Problem\">Newcomb\u2019s Problem</h2>\n<p>The standard version of <a href=\"http://wiki.lesswrong.com/wiki/Newcomb's_problem\" target=\"_blank\">Newcomb\u2019s Problem</a> has been thoroughly discussed on Lesswrong. Many would agree that one-boxing is the correct solution, for one-boxing agents obtain a million dollars, while two-boxers only take home a thousand dollars. To clarify the structure of the problem: an agent chooses between two options, \u201cAB\u201c and \u201cB\u201c. When relatively considered, the option B \u201ccosts\u201d a thousand dollars because one would abandon transparent box A containing this amount of money. As we play with the predictor Omega, who has an almost 100% &nbsp;forecasting power, our decision <em>determines</em> what past occured, that is we determine whether Omega put a million into box B or not. With determining I mean as much as \u201c<strong>being compatible with</strong>\u201d. Hence, choosing box B is compatible only with a past where Omega put a million into it.</p>\n<p>&nbsp;</p>\n<h2 id=\"Newcomb_s_Problem_s_Problem_of_Free_Will\">Newcomb\u2019s Problem\u2019s Problem of Free Will</h2>\n<p>To many, Newcomb\u2019s Problem seems counterintuitive. People tend to think: \u201cWe cannot change the past, as past events have already happened! So there\u2019s nothing we can do about it. Still, somehow the agents that only choose B become rich. How is this possible?\u201c<br>This uneasy feeling can be resolved by clarifing the notion of \u201cfree will\u201d, i.e. by acknowledging that a world state X either logically implies (hard determinism) or probabilistically suggests (hard incompatibilism, stating that free will is impossible and complete determinism is false) another world state Y or a set of possible world states (Y1,Y2,Y3,..,Yn) \u2013 no matter if X precedes Y or vice versa. (Paul Almond has shown in his <a href=\"http://www.paul-almond.com/\" target=\"_blank\">paper</a> on decision theory \u2013 unfortunately his page has been down lately \u2013 that upholding this distinction does not affect the clarification of free will in decision-theoretic problems. Therefore, I chose to go with hard determinism.)</p>\n<p>The fog will lift once we accept the above. Since our action is a subset of a particular world state, the action itself is also implied by preceding world states, that is once we know all the facts about a preceding world state we can derive facts about subsequent world states.<br>If we look more closely, we cannot <em>really choose</em> in a way that people used to think. Common sense tells us that we confront a \u201creal choice\u201d if our decision is not just determined by external factors and also not picked at random, but governed by our free will. But what could this third case even mean? Despite its intuitive usefulness, the classical notion of choice seems to be an ill-defined term since it requires a problematic notion of free will, that is to say one that ought to be non-random but also not determined at once.<br>This is why I want to suggest a new definition of choice: Choosing is the way agents execute what they were determined to by other world states. Choosing has nothing to do with \u201cchanging\u201d what did or is going to happen. The only thing that actually changes is the <em>perception</em> of what did or is going to happen, since executions produce new data points that call for updates.<br>So unless we could use a \u201ctrue\u201d random generator (which would only be possible if we did not assume complete determinism to be true) in order to make decisions, what we are going to do is \u201cplanned\u201d and determined by preceding and subsequent world states.<br>If I take box B, then this determines a past world state where Omega has put a million dollars into this box. If I take both box A and B, then this determines a past world state where Omega has left box B empty. Therefore, when it comes to deciding, taking actions that determine (or are compatible with) not only desirable <em>future worlds</em>, but also desirable <em>past worlds</em> are the ones that make us win.<br>One may object now that we aren\u2019t \u201creally\u201c determining the past, but we only determine <em>our perception</em> of it. That\u2019s an interesting point. In the next section we are going to have a closer look on that. For now, I\u2019d like to bring the underlying perception of time into question. Because once I choose only box B, it seems that the million dollars I receive is not just an illusion of my map but it is really out there. Admittedly the past seems unswayable, but this example shows that maybe our conventional perception of time is misleading as it conflicts with the notion of us choosing what happened in the past.<br>How come self-proclaimed deterministic non-fatalists in fact <em>are</em> fatalists when they deal with the past? I\u2019d suggest to perceive time not as being divided into seperate caterogies like \u201cstuff that has passed \u201c and \u201cstuff that is about to happen\u201c, but rather as one dimension where every dot is just as <em>real</em> as any other and where the manifestation of one particular dot restrictively <em>determines</em> the set of possible manifestations other dots could embody. It is crucial to note that such a dot would describe the whole world in three spatial dimensions, while subsets of world states could still behave independently.</p>\n<p>Perceiving time without an inherent \u201carrow\u201d is not new to <a href=\"http://en.wikipedia.org/wiki/Relativity_of_simultaneity\" target=\"_blank\">science</a> <a href=\"http://en.wikipedia.org/wiki/Eternalism_(philosophy_of_time)\" target=\"_blank\">and</a> <a href=\"http://en.wikipedia.org/wiki/Rietdijk-Putnam_argument\" target=\"_blank\">philosophy</a>, but still, readers of this post will probably need a compelling reason why this view would be more goal-tracking. Considering the Newcomb\u2019s Problem a reason can be given: Intuitively, the past seems much more \u201csettled\u201d to us than the future. But it seems to me that this notion is confounded as we often know more about the past than we know about the future. This could tempt us to project this disbalance of knowledge onto the universe such that we perceive the past as settled and unswayable in contrast to a shapeable future. However, such a conventional set of intuitions conflicts strongly with us picking only one box. These intuitions would tell us that we cannot affect the content of the box; it is already filled or empty since it has been prepared in the now <em>inaccessible</em> past.</p>\n<p>Changing the notion of time into one block would lead to \u201cbetter\u201d intuitions, because they directly suggested to choose one box, as this action is only compatible with a more desirable past. Therefore we might need to adapt our intution, <a href=\"/lw/hs/think_like_reality/\" target=\"_blank\">so that the universe looks normal</a> again. To illustrate the ideas discussed above and to put them into practice, I have constructed the following game:</p>\n<p>&nbsp;</p>\n<h2 id=\"The_A_B_Game\">The A,B-Game</h2>\n<p>You are confronted with Omega, a 100% correct predictor. In front of you, there are two buttons, A and B. You know that there are two kinds of agents. Agents with the gene G_A and agents with the gene G_B. Carriers of G_A are blessed with a life expectancy of 100 years whereas carriers of G_B die of cancer at the age of 40 on average. Suppose you are much younger than 40. Now Omega predicts that every agent who presses A is a carrier of G_A and every agent that presses B is a carrier of G_B. You can only press one button, which one should it be if you want to live for as long as possible?<br>People who prefer to live for a hundred years over forty years would press A. They would even pay a lot of money in order to be able to do so. Though one might say one cannot change or choose one\u2019s genes. Now we need to be clear about which definition of choice we make use of. Assuming the conventional one, I would agree that one could not choose one\u2019s genes, but for instance, when getting dressed, one could not choose one\u2019s jeans either, as the conventional understanding of choice requires an empty notion of non-random, not determined free will that is not applicable. Once we use the definition I introduced above, we can say that we choose our jeans. Likewise, we can choose our genes in the A,B-Game. If we one-box in Newcomb\u2019s Problem, we should also press A here, because the two problems are structurally identical (except for the labels \u201cbox\u201d versus \u201cgene\u201d).<br>The notion of <em>objective ambiguity</em> of genes only stands if we believe in some sort of <em>objective ambiguity</em> about which choices will be made. When facing a correct predictor, those of us who believe in indeterministic objective ambiguity of choices have to bite the bullet that their genes would be objectively ambiguous. Such a model seems counterintuitive, but not contradictory. However, I don\u2019t feel forced to adapt this indeterministic view.</p>\n<p>Let us focus on the deterministic scenario again: In this case, our past already determined our choice, so there is only one way we will go and only one way we can go.<br>We don\u2019t know whether we are determined to do A or B. By \u201cchoosing\u201d the one action that is <em>compatible</em> only with the more desirable past, we are better off. Just as we don\u2019t know in Newcomb\u2019s Problem whether B is empty or not, we have to behave in a way such that it must have been filled already. From our perspective, with little knowledge about the past, our choice determines the manifestation of our map of the past. Apparently, this is exactly what we do when making choices about the future. Taking actions determines the manifestation of our map of the future. Although the future is already settled, we don\u2019t know yet its exact manifestation. Therefore, from our perspective, it makes sense to act in ways that determine the most desirable futures. This does not automatically imply that some mysterious \u201cchange\u201d is going to happen.<br>In both directions it feels like one would change the manifestation of other world states, but when we look more closely we cannot even spell out what that would mean. The word \u201cchange\u201d only starts to become meaningful once we hypothetically compare our world with counterfactual ones (where we were not determined to do what we do in our world). In such a framework we could consistently claim that the content of box B \u201cchanges\u201d depending on whether or not we choose only box B.</p>\n<p>&nbsp;</p>\n<h2 id=\"Screening_off\">Screening off</h2>\n<p>Following this approach of determining one\u2019s perception of the world, the question arises, whether <em>every</em> change in perception is actually goal-tracking. We can ask ourselves, whether an agent should avoid new information if she knew that the new information had negative news value. For instance, if an agent, being suspected of having lung cancer and awaiting the results of her lung biopsy, seeks actions that make more desirable past world states more likely, then she should figure out a way so that she doesn\u2019t receive any mail, for instance by declaring an incorrect postal address. This naive approach obviously fails because of lack of proper use of Bayesian updating. The action \u201davoiding to receive mail\u201d <a href=\"http://wiki.lesswrong.com/wiki/Screening_off\" target=\"_blank\">screens off</a> the desirable outcome so that once we know about this action we don\u2019t learn anything about the biopsy in (the very probable) case that we don\u2019t receive any mail.</p>\n<p>In the A,B-Game, this doesn\u2019t apply, since we believe Omega\u2019s prediction to be true when it says that A necessarily belongs to G _A and B to G_B. Generally, we can distinguish the cases by clarifying existing independencies: In the lung cancer case where we simply don\u2019t know better, we can assume that P(prevention|positive lab result)=P(prevention|negative lab result)=P(prevention). Hence, screening off applies. In the A,B-Game, we should believe that P(Press A|G_A)&gt;P(Press A)=P(Press A|G_A or G_B). We obtain this relevant piece of information thanks to Omega\u2019s forecasting power. Here, screening off does not apply.</p>\n<p>Subsequently, one might object that the statement P(Press A|G_A)&gt;P(Press A) leads to a conditional independence as well, at least in cases where not all the players that press A necessarily belong to G_A. Then you might be pressing A because of your reasoning R_1 which would screen off pressing A from G_A. A further objection could be that even if one could show a dependency between G_A and R_1, you might be choosing R_1 because of some meta-reasoning R_2 that again provides a reason not to press A. However, considering these objections more thoroughly, we realize that R_1 has to be <em>congruent</em> or at least evenly associated (in G_A as well as in G_B) with Pressing A. The same works for R_2. If this wasn\u2019t the case, then we would be talking about <em>another game</em>, a game where we knew, for instance, that 90% of the G_A carriers choose button A (without thinking) because of the gene and 10% of the G_B carriers would choose button A because of some sort of evidential reasoning. Knowing this, choosing A <em>out of evidential reasoning</em> would be foolish, since we already know that only G_B carriers could do that. Once we know this, evidential reasoners would suggest not to press A (unless B offers an even worse outcome). So these further objections fail as well, as they implicitly change the structure of the discussed problem. We can conclude that &nbsp;no screening off applies as long as an instance with forecasting power tells us that a particular action makes the desirable outcome likelier.<br>Now let\u2019s have a look at an alteration of the A,B-Game in order to figure out whether screening-off might apply here.</p>\n<p>&nbsp;</p>\n<h2 id=\"A_Weak_Omega_in_The_A_B_Game\">A Weak Omega in The A,B-Game</h2>\n<p>Thinking about the A,B-Game, what happens if we decreased Omega\u2019s <em>forecasting power</em>? Let\u2019s assume now that Omega\u2019s prediction is correct only in 90% of all cases. Should this fundamentally change our choice whether to press A or B because we only pressed A as a consequence of our <em>reasoning</em>?<br>To answer that, we need to be clear about why agents believe in Omega\u2019s predictions. They believe in Omega\u2019s prediction because they were correct so many times. This constitutes Omega\u2019s strong forecasting power. As we saw above, screening off only applies if the predicting instance (Omega, or us reading a study) has no forecasting power at all.<br>In the A,B-Game, as well as in the original Newcomb\u2019s Problem, we also have to take the predictions of a weaker Omega (with less forecasting power) into account, unless we face an Omega that happens to be right by chance (i.e. in 50% of the cases when considering a binary decision situation).</p>\n<p>If, in the standard A,B-Game, we consider pressing A to be important, and if we were willing to spend a large &nbsp;amount of money in order to be able to press A (suppose the button A would send a signal to cause a withdrawal from our bank account), then this amount should only <em>gradually</em> shrink once we decrease Omega\u2019s forecasting power. The question now arises whether we also had to \u201cchoose\u201d the better genes in the medical version of Solomon\u2019s Problem and whether there might not be a fundamental difference between it and the original Newcomb\u2019s Problem.</p>\n<p>&nbsp;</p>\n<h2 id=\"Newcomb_s_versus_Solomon_s_Problem\">Newcomb\u2019s versus Solomon\u2019s Problem</h2>\n<p>In order to uphold this convenient distinction, people tell me that \u201cyou cannot change your genes\u201d though that\u2019s a bad argument since one could reply \u201caccording to your definition of change, you cannot change the content of box B either, still you choose one-boxing\u201d. Further on, I quite often hear something like \u201cin Newcomb\u2019s Problem, we have to deal with <em>Omega</em> and that\u2019s something completely different than just reading a <em>study</em>\u201d. This \u2013 in contrast to the first \u2013 is a good point.</p>\n<p>In order to accept the forecasting power of a 100% correct Omega, we already have to presume induction to be legitimate. Or else one could say: \u201cWell, I see that Omega has been correct in 3^^^3 cases already, but why should I believe that it will be correct the next time?\u201d. As sophisticated this may sound, such an agent would lose terribly. So how do we deal with studies then? Do they have any forecasting power at all? It seems that this again depends on the setting of the game. Just as Omega\u2019s forecasting power can be set, the forecasting power of a study can be properly defined as well. It can be described by assigning values to the following two variables: its <em>descriptive power</em> and its <em>inductive power</em>. To settle them, we have to answer two questions: 1. How correct is the study's description of the population? 2. How representative is the population of the study to the future population of agents acting in knowledge of the study? Or in other words, to what degree can one consider the study subjects to be in one\u2019s reference class in order to make true predictions about one\u2019s behaviour and the outcome of the game? Once this is clear, we can then infer the forecasting power. How much forecasting power does the study have? Let\u2019s assume that the study we deal with is correct in what it describes. Those who wish can use a discounting factor. However, this is not important for subsequent arguments and would only make it more complicated.</p>\n<p>Considering the inductive power, it get\u2019s more tricky. Omega\u2019s predictions are <em>defined</em> to be correct. In contrast, the study\u2019s predictions have not been tested. Therefore we are quite uncertain about the study\u2019s forecasting power. It were 100% if and only if every factor involved was specified so that the total of them compel identical outcomes in the study and our game. Due to induction, we do have reason to assume a positive value of forecasting power. To identify its specific value (that discounts the forecasting power according to the specified conditions), we would need to settle every single factor that might be involved. So let\u2019s keep it simple by applying a 100% forecasting power. As long as there is a positive value of forecasting power, the basic point of the subsequent arguments (that presume a 100% forecasting power) will also hold when discounted.<br>Thinking about the inductive power of the study, there still is one thing that we need to specify: It is not clear what exactly previous subjects of the study knew.</p>\n<p>For instance in a case A), the subjects of the study knew nothing about the tendency of CGTA-carriers to chew gum. First, their genom was analyzed, then they had to decide whether or not to chew gum. In such a case, the subjects\u2018 knowledge is quite different from those who play the medical version of Solomon\u2019s Problem. Therefore screening off applies. But does it apply to the same extent as in the avoiding-bad-news example mentioned above? That seems to be the case. In the avoiding-bad-news example, we assumed that there is no connection between the variables \u201elung cancer\u201c and \u201eavoiding mail\u201c. In Solomon\u2019s Problem such an indepence can be settled as well. Then the variables \u201ehaving the gene CGTA\u201c and \u201enot chewing gum because of evidential reasoning\u201c are also assumed to be independent. Total screening off applies. Considering an evidential reasoner who knows that much, choosing not to chew gum would then be as irrational as declaring an incorrect postal address when awaiting biopsy results.</p>\n<p>Now let us consider a case B) where the subjects were introduced to the game just as we were. Then they would know about the tendency of CGTA-carriers to chew gum, and they themselves might have used evidential reasoning. In this scenario, screening off does not apply. This is why not chewing gum would be the winning strategy.<br>One might say that of course the study-subjects did not know of anything and that we should assume case A) a priori. I only partially agree with that. The screening off can already be weakend if, for instance, the subjects knew why the study was conducted. Maybe there was anecdotal evidence about heredity of a tendency to chew gum, which was about to be confirmed properly.<br>Without further clarification, one can plausibly assume a probability distribution over various intermediate cases between A and B where screening off becomes gradually fainter when getting closer to B. Of course there might also be cases where anecdotal evidence leads astray, but in order to cancel out the argument above, anecdotal evidence needs to be equalized with in expectation knowing nothing at all. But since it seems to be better (even though not much) than knowing nothing, it is not a priori clear that we have to assume case A right away. &nbsp;<br>So when compiling a medical version of Solomon\u2019s Problem, it is important to be very clear about what the subjects of the study were aware of.</p>\n<p>&nbsp;</p>\n<h2 id=\"What_about_Newcomb_s_Soda_\">What about Newcomb\u2019s Soda?</h2>\n<p>After exploring screening off and possible differences between Newcomb\u2019s Problem and Solomon\u2019s Problem (or rather between Omega and a study), let\u2019s investigate those questions in another game. My favourite of all Newcomblike problems is called <a href=\"/lw/gu1/decision_theory_faq/#newcombs-soda\" target=\"_blank\">Newcomb\u2019s Soda</a> and was introduced in <a href=\"http://intelligence.org/files/TDT.pdf\" target=\"_blank\">Yudkowsky (2010)</a>. Comparing Newcomb\u2019s Soda with Solomon\u2019s Problem, Yudkowsky writes:<br>\u201cNewcomb\u2019s Soda has the same structure as Solomon\u2019s Problem, except that instead of the outcome stemming from genes you possessed since birth, the outcome stems from a soda you will drink shortly. Both factors are in no way affected by your action nor by your decision, but your action provides <em>evidence</em> about which genetic allele you inherited or which soda you drank.\u201d</p>\n<p>Is there any relevant difference in structure between the two games?<br>In the previous section, we saw that once we settle that the study-subjects in Solomon\u2019s Problem don\u2019t know of any connection between the gene and chewing gum, screening off applies and one has good reasons to chew gum. Likewise, the screening off only applies in Newcomb\u2019s Soda if the subjects of the clinical test are completely unaware of any connection between the sodas and the ice creams. But is this really the case? Yudkowsky introduces the game as one big clinical test in which you are participating as a subject:</p>\n<p>\u201cYou know that you will shortly be administered one of two sodas in a double-blind clinical test. After drinking your assigned soda, you will enter a room in which you find a chocolate ice cream and a vanilla ice cream. The first soda produces a strong but entirely subconscious desire for chocolate ice cream, and the second soda produces a strong subconscious desire for vanilla ice cream.\u201d</p>\n<p>This does not sound like previous subjects had no information about a connection between the sodas and the ice creams. Maybe you, and you alone, received those specific insights. If this were the case, it clearly had to be mentioned in the game\u2019s definition, since this factor is crucial when it comes to decision-making. Considering a game where the agent herself is a study-subject, without further specification, she wouldn\u2019t by default expect that other subjects knew less about the game than she did. Therefore let\u2019s assume in the following that all the subjects in the clinical test knew that the sodas cause a subconscious desire for a specific flavor of ice cream.</p>\n<p>&nbsp;</p>\n<h2 id=\"Newcomb_s_Soda_in_four_variations\">Newcomb\u2019s Soda in four variations</h2>\n<p>Let \u201cC\u201d be the causal approach which states that one has to choose vanilla ice cream in Newcomb\u2019s Soda. C only takes the $1,000 of the vanilla ice cream into account since one still can <em>change</em> the variable \u201cice cream\u201d, whereas the variable \u201csoda\u201d is already <em>settled</em>. Let \u201cE\u201d be the evidential approach which suggests that one has to choose chocolate or vanilla ice cream in Newcomb\u2019s Soda \u2013 depending on the probabilities specified. E takes both the $1,000 of the vanilla ice cream and the $1,000,000 of the chocolate soda into account. In that case, one argument can outweigh the other. &nbsp;</p>\n<p>Let\u2019s compile a series of examples. We denote \u201cCh\u201d for chocolate, \u201cV\u201d for vanilla, \u201cS\u201d for soda and \u201cI\u201d for ice cream. In all versions Ch-S will receive $1,000,000 and V-I will receive $1,000 and P(Ch-S)=P(V-S)=0.5. Furthermore we settle that P(Ch-I|Ch-S)=P(V-I|V-S) and call this term \u201cp\u201d in every version so we don\u2019t vary unnecessarily many parameters. As we are going to deal with large numbers, let\u2019s assume a linear monetary value utility function.</p>\n<p><strong>Version 1: </strong>Let us assume a case where the sodas are dosed homeopathically, so that no effect on the choice of ice creams can be observed. Ch-S and V-S choose from Ch-I and V-I randomly so that p=P(V-I|Ch-S)=P(Ch-I|V-S)=0.5. Both C and E choose V-I and win 0.5 *$1,001,000 + 0.5*$1000=$501,000 in expectation. C only considers the ice cream whereas E considers the soda as well, though in this case the soda doesn\u2019t change anything as the Ch-S are equally distributed over Ch-I and V-I.</p>\n<p><strong>Version 2: </strong>Here p=0.999999. Since P(Ch-S)=P(V-S)=0.5, one Ch-I in a million will have originated from V-S, whereas one V-I in a million will have originated from Ch-S. The other 999,999 Ch-I will have <em>determined</em> the desired past, Ch-S, due to their choice of Ch-I. So if we participated in this game a million times and tracked E that suggests choosing Ch-I each time, we overall could have expected to win 999,999*$1,000,000=$999,999,000,000. This is different to following C\u2019s advice. As C tells us that we cannot affect which soda we have drunk we would choose V-I each time and could expect to win 1,000,000*$1,000+$1,000,000=$1,001,000,000 in total. The second outcome, which C is responsible for, is 999 times worse than the first (which was suggested by E). In this version, E clearly outperforms C in helping us to make the most money.</p>\n<p><strong>Version 3: </strong>Now we have p=1. This version is equivalent to the standard version of the A,B-Game. What would C do? It seems that C ought to maintain its view that we <em>cannot</em> affect the soda. Therefore, only considering the ice cream-part of the outcome, C will suggest choosing V-I. This seems to be absurd: C leaves us disappointed with $1,000, whereas E makes us millionaires <em>every single time</em>.<br>A C-defender might say: \u201cWait! Now you have changed the game. Now we are dealing with a probability of 1!\u201d The response would be : \u201cInteresting, I can make p get as close to 1 as I want as long as it <em>isn\u2019t</em> 1 and the rules of the game and my conclusions would still remain. For instance, we can think of a number like 0.999\u2026(100^^^^^100 nines in a row). So tell me why exactly the probability change of 0.000\u2026(100^^^^^100 -1 zeros in a row)1 should make you switch to Ch-I? But wait, why would you \u2013 as a defender of C \u2013 &nbsp;even consider Ch-I since it cannot affect your soda while it definitely prevents you from winning the $1,000 of the ice cream?\u201d</p>\n<p>The previous versions tried to exemplify why taking <em>both</em> arguments (the $1,000 and the $1,000,000) into account makes you better off at the one edge of the probability measure, whereas at the other edge, C and E produce the same outcomes. With a simple equation we can figure out for which p E would be indifferent about whether to choose Ch-I or V-I: solve(p*1,000,000=(1-p)*1,000,000+1,000,p). This gives us p=0.5005. So for 0.5005&lt;p&lt;=1 E does better than C and for 0&lt;=p&lt;=0.5005 E and C behave alike. Finally, let us consider the original version: &nbsp;</p>\n<p><strong>Version 4:</strong>&nbsp;Here we deal with p=0.9. According to the above we could already deduce that deciding according to E makes us better off, but let\u2019s have a closer look at it for the sake of completeness: In expectation, choosing V-I makes us win 0.1*$1,000,000+$1,000=$101,000, whereas Ch-I leaves us with 0.9*$1,000,000=$900,000 almost 9 times richer. After the insights above, it shouldn\u2019t surprise us too much that E clearly does better than C in the original version of Newcomb\u2019s Soda as well.</p>\n<p>The variations above illustrated that C had to eat V-I even if 99.9999% of C-S choose C-I and 99.9999% of V-S eat V-I. If you played it a million times, in expectation C-I would win the million 999,999 times and V-I just once. Can we <em>really</em> be indifferent about that? Wasn\u2019t it all about winning and losing? And who is winning here and who is losing?</p>\n<p>&nbsp;</p>\n<h2 id=\"Newcomb_Soda_and_Precommitments\">Newcomb-Soda and Precommitments</h2>\n<p>Another excerpt from <a href=\"http://intelligence.org/files/TDT.pdf\" target=\"_blank\">Yudkowsky (2010)</a>:<br>\u201cAn evidential agent would rather precommit to eating vanilla ice cream than precommit to eating chocolate, because such a precommitment made <em>in advance of drinking the soda</em> is not <em>evidence</em> about which soda will be assigned.\u201d<br>At first sight this seems intuitive. But if we look at the probabilities more closely suddenly a problem arises: Let\u2019s consider an agent that precommits (let\u2019s assume a 100% persistent mechanism) one\u2019s decision before a standard game (p=0.9) starts. Let\u2019s assume that he precommits \u2013 as suggested above \u2013 to choose V-I. What credence should he assign to P(Ch-S|V-I)? Is it 0.5 as if he didn\u2019t precommit at all or does something change? Basically, adding precommitments to the equation inhibits the effect of the sodas on the agent\u2019s decision. Again, we have to be clear about which agents are affected by this newly introduced variable. If we were the only ones who can precommit 100% persistently, then our game fundamentally differs from the previous subjects\u2019 one. If they didn\u2019t precommit, we couldn\u2019t presuppose a forecasting power anymore because the previous subjects decided according to the soda\u2019s effect, whereas we now decide independently of that. In this case, E would suggest to precommit to V-I. However, this would constitute an entirely new game without any forecasting power. If all the agents of the study make persistent precommitments, then the forecasting power holds; the game doesn\u2019t change fundamentally. Hence, the way previous subjects behaved remains crucial to our decision-making. Let\u2019s now imagine that we were playing this game a million times. Each time we irrevocably precommit to V-I. In this case, if we consider ourselves to be sampled randomly among V-I, we can expect to originate from V-S 900,000 times. If we approach p to 1 we see that it gets desperately unlikely to originate from Ch-S once we precommit ourselves to V-I. So a rational agent following E should precommit Ch-I in advance of drinking the soda. Since E suggests Ch-I both during and before the game, this example doesn\u2019t show that E would be <a href=\"http://wiki.lesswrong.com/wiki/Dynamic_inconsistency\" target=\"_blank\"><em>dynamically inconsistent</em></a>.</p>\n<p>In the other game, where only we precommit persistently and the previous subjects don\u2019t, picking V-I doesn\u2019t make E dynamically inconsistent, as we would face another decision situation where no forecasting power applies. Of course we can also imagine intermediate cases. For instance one, where we make precommitments and the previous subjects were able to make them as well, but we don\u2019t know whether they did. The more uncertain we get &nbsp;about their precommitments, the more we approach the case where only we precommit while the forecasting power gradually weakens. Those cases are more complicated, but they do not show a dynamical inconsistency of E either.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_tickle_defense_in_Newcomblike_problems\"><span style=\"line-height: 15px;\">The tickle defense in Newcomblike problems</span></h2>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"line-height: 15px;\">In the last section I want to have a brief look at the tickle defense, which is sometimes used to defend evidential reasoning by offering a less controversial output. For instance, it states that in the medical version of Solomon\u2019s Problem an evidential reasoner should chew gum, since she can rule out having the gene as long as she doesn\u2019t feel an urge to chew gum. So chewing gum doesn\u2019t make it likelier to have the gene since she already has ruled it out.</span></p>\n<p style=\"line-height: 1.5; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"line-height: 15px;\">I believe that this argument fails since it changes the game. Suddenly, the gene doesn\u2019t cause you to \u201cchoose chewing gum\u201d anymore but to \u201cfeel an urge to choose chewing gum\u201d. Though I admit, in such a game a conditional independence would screen off the action \u201cnot chewing gum\u201d from \u201cnot having the gene\u201d \u2013 no matter what the previous subjects of the study knew. This is why it would be more attractive to chew gum. However, I don\u2019t see why this case should matter to us. In the original medical version of Solomon\u2019s Problem we are dealing with another game where this particular kind of screening off does not apply. As the gene causes one to \u201cchoose chewing gum\u201d we can only rule it out by not doing so. However, this conclusion has to be treated with caution. For one thing, depending on the numbers, one can only diminish the probability of the undesirable event of having the gene \u2013 not rule it out completely; for another thing, the diminishment only works if the previous subjects were not ignorant of a depedence of the gene and chewing gum \u2013 at least in expectation. Therefore the tickle defense only trivially applies to a special version of the medical Solomon\u2019s Problem and fails to persuade proper evidential reasoners to do anything differently in the standard version. Depending on the specification of the previous subjects\u2019 knowledge, an evidential reasoner would still chew or not chew gum.</span></p>", "sections": [{"title": "Overview", "anchor": "Overview", "level": 1}, {"title": "The school mark is already settled", "anchor": "The_school_mark_is_already_settled", "level": 1}, {"title": "Newcomb\u2019s Problem", "anchor": "Newcomb_s_Problem", "level": 1}, {"title": "Newcomb\u2019s Problem\u2019s Problem of Free Will", "anchor": "Newcomb_s_Problem_s_Problem_of_Free_Will", "level": 1}, {"title": "The A,B-Game", "anchor": "The_A_B_Game", "level": 1}, {"title": "Screening off", "anchor": "Screening_off", "level": 1}, {"title": "A Weak Omega in The A,B-Game", "anchor": "A_Weak_Omega_in_The_A_B_Game", "level": 1}, {"title": "Newcomb\u2019s versus Solomon\u2019s Problem", "anchor": "Newcomb_s_versus_Solomon_s_Problem", "level": 1}, {"title": "What about Newcomb\u2019s Soda?", "anchor": "What_about_Newcomb_s_Soda_", "level": 1}, {"title": "Newcomb\u2019s Soda in four variations", "anchor": "Newcomb_s_Soda_in_four_variations", "level": 1}, {"title": "Newcomb-Soda and Precommitments", "anchor": "Newcomb_Soda_and_Precommitments", "level": 1}, {"title": "The tickle defense in Newcomblike problems", "anchor": "The_tickle_defense_in_Newcomblike_problems", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "78 comments"}], "headingsCount": 14}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tWLFWAndSZSYN6rPB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-11T00:29:52.608Z", "modifiedAt": null, "url": null, "title": "LINK: AI Researcher Yann LeCun on AI function", "slug": "link-ai-researcher-yann-lecun-on-ai-function", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:31.466Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rqj8tStj6KPTJzfbK/link-ai-researcher-yann-lecun-on-ai-function", "pageUrlRelative": "/posts/rqj8tStj6KPTJzfbK/link-ai-researcher-yann-lecun-on-ai-function", "linkUrl": "https://www.lesswrong.com/posts/rqj8tStj6KPTJzfbK/link-ai-researcher-yann-lecun-on-ai-function", "postedAtFormatted": "Wednesday, December 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20AI%20Researcher%20Yann%20LeCun%20on%20AI%20function&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20AI%20Researcher%20Yann%20LeCun%20on%20AI%20function%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frqj8tStj6KPTJzfbK%2Flink-ai-researcher-yann-lecun-on-ai-function%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20AI%20Researcher%20Yann%20LeCun%20on%20AI%20function%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frqj8tStj6KPTJzfbK%2Flink-ai-researcher-yann-lecun-on-ai-function", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frqj8tStj6KPTJzfbK%2Flink-ai-researcher-yann-lecun-on-ai-function", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Yann_LeCun\">Yann LeCun</a>, <a href=\"https://www.facebook.com/yann.lecun/posts/10151728212367143\">now of Facebook</a>, was <a href=\"http://www.theregister.co.uk/2013/12/10/facebook_artificial_intelligence_hire/\">interviewed by The Register</a>. It is interesting that his view of AI is apparently that of a prediction tool:</p>\n<p style=\"padding-left: 30px;\">\"In some ways you could say intelligence is all about prediction,\" he explained. \"What you can identify in intelligence is it can predict what is going to happen in the world with more accuracy and more time horizon than others.\"</p>\n<p>rather than of a world optimizer. This is not very surprising, given his background in handwriting and image recognition. This \"AI as intelligence augmentation\" view appears to be prevalent among the AI researchers in general.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rqj8tStj6KPTJzfbK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 2, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "25061", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-11T06:26:18.484Z", "modifiedAt": null, "url": null, "title": "Circular belief updating", "slug": "circular-belief-updating", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:37.889Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "irrational", "createdAt": "2011-10-04T19:46:41.913Z", "isAdmin": false, "displayName": "irrational"}, "userId": "TaHr6NuudyhaHevgQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eBjKQN7Sbnkpzo3FR/circular-belief-updating", "pageUrlRelative": "/posts/eBjKQN7Sbnkpzo3FR/circular-belief-updating", "linkUrl": "https://www.lesswrong.com/posts/eBjKQN7Sbnkpzo3FR/circular-belief-updating", "postedAtFormatted": "Wednesday, December 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Circular%20belief%20updating&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACircular%20belief%20updating%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeBjKQN7Sbnkpzo3FR%2Fcircular-belief-updating%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Circular%20belief%20updating%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeBjKQN7Sbnkpzo3FR%2Fcircular-belief-updating", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeBjKQN7Sbnkpzo3FR%2Fcircular-belief-updating", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 642, "htmlBody": "<p>This article is going to be in the form of a story, since I want to lay out all the premises in a clear way. There's a related question about religious belief.</p>\n<p>&nbsp;</p>\n<p>Let's suppose that there's a country called Faerie. I have a book about this country which describes all people living there as rational individuals (in a traditional sense). Furthermore, it states that some people in Faerie believe that there may be some individuals there known as sorcerers. No one has ever seen one, but they may or may not interfere in people's lives in subtle ways. Sorcerers are believed to be such that there can't be more than one of them around and they can't act outside of Faerie. There are 4 common belief systems present in Faerie:</p>\n<ol>\n<li>Some people believe there's a sorcerer called Bright who (among other things) likes people to believe in him and may be manipulating people or events to do so. He is not believed to be universally successful.</li>\n<li>Or, there may be a sorcerer named Invisible, who interferes with people only in such ways as to provide no information about whether he exists or not.</li>\n<li>Or, there may be an (obviously evil) sorcerer named Dark, who would prefer that people don't believe he exists, and interferes with events or people for this purpose, likewise not universally successfully.</li>\n<li>Or, there may either be no sorcerers at all, or perhaps some other sorcerers that no one knows about, or perhaps some other state of things hold, such as that there are multiple sorcerers, or these sorcerers don't obey the above rules. However, everyone who lives in Faerie and is in this category simply believes there's no such thing as a sorcerer.</li>\n</ol>\n<p>This is completely exhaustive, because everyone believes there can be at most one sorcerer. Of course, some individuals within each group have different ideas about what their sorcerer is like, but within each group they all absolutely agree with their dogma as stated above.</p>\n<p>Since I don't believe in sorcery, a priori I assign very high probability for case 4, and very low (and equal) probability for the other 3.</p>\n<p>I can't visit Faerie, but I am permitted to do a scientific phone poll. I call some random person, named Bob. It turns out he believes in Bright. Since P(Bob believes in Bright | case 1 is true) is higher than the unconditional probability, I believe I should adjust the probability of case 1 up, by Bayes rule. Does everyone agree? Likewise, the probability of case 3 should go up, since disbelief in Dark is evidence for existence of Dark in exactly the same way, although perhaps to a smaller degree. I also think the case 2 and case 4 have to lose some probability, since it adds up to 1. If I further call a second person, Daisy, who turns out to believe in Dark, I should adjust all probabilities in the opposite direction. I am not asking either of them about the actual evidence they have, just what they believe.</p>\n<p>I think this is straightforward so far. Here's the confusing part. It turns out that both Bob and Daisy are themselves aware of this argument. So, Bob says, one of the reasons he believes in Bright, is because that's positive evidence for Bright's existence. And Daisy believes in Dark despite that being evidence against his existence (presumably because there's some other evidence that's overwhelming).</p>\n<p>Here are my questions:</p>\n<ol>\n<li>Is it sane for Bob and Daisy to be in such a positive or negative feedback loop? How is this resolved?</li>\n<li>If Bob and Daisy took the evidence provided by their belief into account already, how does this affect my own evidence updating? Should I take it into account regardless, or not at all, or to a smaller degree?</li>\n</ol>\n<p>I am looking forward to your thoughts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eBjKQN7Sbnkpzo3FR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 12, "extendedScore": null, "score": 1.4609555502102323e-06, "legacy": true, "legacyId": "25062", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-12T00:18:53.375Z", "modifiedAt": null, "url": null, "title": "Karma awards for proofreaders of the Less Wrong Sequences ebook", "slug": "karma-awards-for-proofreaders-of-the-less-wrong-sequences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:13.203Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexvermeer", "createdAt": "2010-08-13T16:28:34.576Z", "isAdmin": false, "displayName": "alexvermeer"}, "userId": "3bK6aDQviGG3ovuDJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XRoatbM7rbBCcF9Nq/karma-awards-for-proofreaders-of-the-less-wrong-sequences", "pageUrlRelative": "/posts/XRoatbM7rbBCcF9Nq/karma-awards-for-proofreaders-of-the-less-wrong-sequences", "linkUrl": "https://www.lesswrong.com/posts/XRoatbM7rbBCcF9Nq/karma-awards-for-proofreaders-of-the-less-wrong-sequences", "postedAtFormatted": "Thursday, December 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Karma%20awards%20for%20proofreaders%20of%20the%20Less%20Wrong%20Sequences%20ebook&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKarma%20awards%20for%20proofreaders%20of%20the%20Less%20Wrong%20Sequences%20ebook%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXRoatbM7rbBCcF9Nq%2Fkarma-awards-for-proofreaders-of-the-less-wrong-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Karma%20awards%20for%20proofreaders%20of%20the%20Less%20Wrong%20Sequences%20ebook%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXRoatbM7rbBCcF9Nq%2Fkarma-awards-for-proofreaders-of-the-less-wrong-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXRoatbM7rbBCcF9Nq%2Fkarma-awards-for-proofreaders-of-the-less-wrong-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 199, "htmlBody": "<p>MIRI is gathering a bunch of Eliezer&rsquo;s <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">writings</a> into a nicely-edited ebook, currently titled <em>The Hard Part is Actually Changing Your Mind</em>. This book will ultimately be released in various digital formats (Kindle MOBI, EPUB, and PDF). Much of the initial work for this project is complete. What we need now are volunteers to review the book's chapters to:</p>\n<ul>\n<li>verify that all the content has been correctly transferred (text, equations, and images),</li>\n<li>proofread for any typographical errors (spelling, punctuation, layout, etc.),</li>\n<li>verify all internal and external links,</li>\n<li>and more.</li>\n</ul>\n<p>This project has been added to Youtopia, MIRI&rsquo;s volunteer system. (<a href=\"http://mirivolunteers.org/\">Click &ldquo;Register as a Volunteer&rdquo; here to sign up</a>. Already signed up? <a href=\"https://www.youtopia.com/profile\">Go here</a>.)</p>\n<p><strong>LW Karma Bonus</strong></p>\n<p>For this special project, every point earned in Youtopia will also earn you 3 karma on LW!</p>\n<p>Points are awarded based on the amount of time spent proofreading the book. For example, an hour of work logged in Youtopia earns you 10 points, which will also get you 30 LW karma. Karma is awarded by admins in a publicly-accountable way: all manual karma additions are listed <a href=\"/karma/\">here</a>.</p>\n<p>Questions about this project can be directed to <a href=\"mailto:alexv@intelligence.org\">alexv@intelligence.org</a> or in the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XRoatbM7rbBCcF9Nq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 1.4620581259912202e-06, "legacy": true, "legacyId": "25063", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-12T05:10:56.208Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 28, chapter 99-101", "slug": "harry-potter-and-the-methods-of-rationality-discussion-14", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:01.324Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "palladias", "createdAt": "2012-04-03T13:45:53.766Z", "isAdmin": false, "displayName": "palladias"}, "userId": "Bv2LXWzZf96WGpqJ5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K6rGFyi4QhPDSPFYr/harry-potter-and-the-methods-of-rationality-discussion-14", "pageUrlRelative": "/posts/K6rGFyi4QhPDSPFYr/harry-potter-and-the-methods-of-rationality-discussion-14", "linkUrl": "https://www.lesswrong.com/posts/K6rGFyi4QhPDSPFYr/harry-potter-and-the-methods-of-rationality-discussion-14", "postedAtFormatted": "Thursday, December 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2028%2C%20chapter%2099-101&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2028%2C%20chapter%2099-101%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK6rGFyi4QhPDSPFYr%2Fharry-potter-and-the-methods-of-rationality-discussion-14%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2028%2C%20chapter%2099-101%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK6rGFyi4QhPDSPFYr%2Fharry-potter-and-the-methods-of-rationality-discussion-14", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK6rGFyi4QhPDSPFYr%2Fharry-potter-and-the-methods-of-rationality-discussion-14", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 234, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is a new thread to discuss Eliezer Yudkowsky&rsquo;s&nbsp;<em><a style=\"color: #8a8a8b;\" href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em>&nbsp;and anything related to it. This thread is intended for discussing&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/chapter/99\">chapter 99</a>, <a href=\"http://hpmor.com/chapter/100\">100</a>, and <a href=\"http://hpmor.com/chapter/101\">101</a>.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/igc/harry_potter_and_the_methods_of_rationality/\">The previous thread&nbsp;</a>is at nearly 500 comments.&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">There is now a site dedicated to the story at&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/notes/\">authors notes</a>&nbsp;and all sorts of other goodies. AdeleneDawner has kept an&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author&rsquo;s Notes</a>. (This goes up to the notes for chapter 76, and is now not updating. The authors notes from chapter 77 onwards are on hpmor.com.)&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The first 5 discussion threads are on the main page under the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/tag/harry_potter/\">discussion section</a>&nbsp;using its separate tag system.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Also:&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2ab/harry_potter_and_the_methods_of_rationality\">1</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2ie/harry_potter_and_the_methods_of_rationality\">2</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2nm/harry_potter_and_the_methods_of_rationality\">3</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2tr/harry_potter_and_the_methods_of_rationality\">4</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/30g/harry_potter_and_the_methods_of_rationality\">5</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">6</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">7</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/797/harry_potter_and_the_methods_of_rationality/\">8</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/7jd/harry_potter_and_the_methods_of_rationality\">9</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/ams/harry_potter_and_the_methods_of_rationality\">10</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/axe/harry_potter_and_the_methods_of_rationality/\">11</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">12</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/b7s/harry_potter_and_the_methods_of_rationality/\">13</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/bfo/harry_potter_and_the_methods_of_rationality/\">14</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/bmx/harry_potter_and_the_methods_of_rationality/\">15</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/bto/harry_potter_and_the_methods_of_rationality/\">16</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/fyv/harry_potter_and_the_methods_of_rationality/\">17</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/g1q/harry_potter_and_the_methods_of_rationality/\">18</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/huq/harry_potter_and_the_methods_of_rationality/\">19</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/hvg/harry_potter_and_the_methods_of_rationality/\">20</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/hwf/harry_potter_and_the_methods_of_rationality/\">21</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/hws/harry_potter_and_the_methods_of_rationality/\">22</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/hxg/harry_potter_and_the_methods_of_rationality/\">23</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/i19/harry_potter_and_the_methods_of_rationality/\">24</a>, &nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/i4r/harry_potter_and_the_methods_of_rationality/\">25</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/ibr/harry_potter_and_the_methods_of_rationality/\">26</a>,&nbsp;<a href=\"/r/discussion/lw/igc/harry_potter_and_the_methods_of_rationality/\">27</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>Spoiler Warning</strong>: this thread is full of spoilers. With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\">You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).</p>\n<p style=\"margin: 0px 0px 1em;\">If there is evidence for X in MOR and/or canon then it&rsquo;s fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that &ldquo;Eliezer said X is true&rdquo; unless you use rot13.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K6rGFyi4QhPDSPFYr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "25064", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 368, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tbQZxoPfFdB3ZJz3c", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL", "8yEdpDpGgvDWHeodM", "K4JBpAxhvstdnNbeg", "xK6Pswbozev6pv4A6", "pBTcCB5uJTzADdMm4", "XN4WDRSPFo9iGEuk3", "QkhX5YeuYHzPW7Waz", "4sY9rqAqty8rHWGSW", "35GjH7tDvNJWSHQ3H", "Pxiu5SG8gjhCh2jYd", "CEd85FLRbQWsbkrmf", "CcnpbKuRaYMjpFmQq", "smKK6yrKBehxvQq5i", "bMxxf7Wtic298LcNx", "uBpSaxteqitApiJJs", "Ey8yGkFnT7Gcgnt5r", "4hKoG4e248yuXtG47"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-12T17:56:30.191Z", "modifiedAt": null, "url": null, "title": "Luck I: Finding White Swans", "slug": "luck-i-finding-white-swans", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:06.430Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fowlertm", "createdAt": "2012-01-07T20:35:26.490Z", "isAdmin": false, "displayName": "fowlertm"}, "userId": "gDAt4FezxH8dDr5EY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iptYS9nZ8kGCmBujc/luck-i-finding-white-swans", "pageUrlRelative": "/posts/iptYS9nZ8kGCmBujc/luck-i-finding-white-swans", "linkUrl": "https://www.lesswrong.com/posts/iptYS9nZ8kGCmBujc/luck-i-finding-white-swans", "postedAtFormatted": "Thursday, December 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Luck%20I%3A%20Finding%20White%20Swans&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALuck%20I%3A%20Finding%20White%20Swans%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiptYS9nZ8kGCmBujc%2Fluck-i-finding-white-swans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Luck%20I%3A%20Finding%20White%20Swans%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiptYS9nZ8kGCmBujc%2Fluck-i-finding-white-swans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiptYS9nZ8kGCmBujc%2Fluck-i-finding-white-swans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2387, "htmlBody": "<p><em>Quoth the Master, great in Wisdom, to the Novice: \"Ye, carry with thee all thy days a cheque folded up in your wallet. &nbsp;For there may be many situations in which thou shalt have need of it.\"</em></p>\n<p><em>And the Novice, of high intelligence but lesser wisdom, replied, saying unto the Master: \"Of what situations dost thou speak?\" &nbsp;</em></p>\n<p><em>To which the Master replied: \"imagine that thou dost come upon a nice piece of land, and wish to make a down payment on it. The real estate market moveth quickly in these troubled economic times, and you may soon find your opportunity dried up like dead leaves in summer. &nbsp;What would you do?\" &nbsp;</em><em>The Master, you see, did dabble in real estate development a l</em><em>ittle</em><em>, and his knowledge was deep in these matters. &nbsp;</em></p>\n<p><em>The Novice thought for a moment, saying: \"But always I carry with me a credit card. &nbsp;Surely this is sufficient for my purposes.\"</em></p>\n<p><em>And the Master replied: \"Thou knoweth not the ways of commerce. &nbsp;Thinketh thee that all dealings are conducted within feet of a machine that can read credit cards?!\"</em></p>\n<p><em>The Novice knew the ways of Traditional Rationality and Skepticism, and felt it his duty to take the opposite stance to the Master, lest he unthinkingly obey an authority figure. &nbsp;Undeterred, he replied, saying unto the Master: \"But always I carry with me cash. Surely this is sufficient for my purposes.\"</em></p>\n<p><em>Upon hearing this, the Master did reply, incredulously: \"Would thee carry with thee always an amount of cash equal to the reasonable asking price of a down payment for a piece of land?!\" &nbsp;&nbsp;</em></p>\n<p><em>And lo, the Novice did understand, though he could not put it into these words, that the Master did speak of a certain </em>stance<em>&nbsp;with respect to the unknown. &nbsp;The swirling chaos of reality may be impossible to predict, but there are things an aspiring empirimancer can do to make it more likely that ve will have good fortune. </em></p>\n<p><em>Verily, know that that which people call 'luck' is not the smile of a beneficent god, but the outcome of how some people interact with chance. &nbsp;</em></p>\n<p>&nbsp;</p>\n<p>______________________________________________________________________________________________________________</p>\n<p>&nbsp;</p>\n<p>Consider for a moment two real people, whom we will call ''Martin\" and \"Brenda\", that considers themselves lucky and unlucky, respectively. Both are part of the group of exceptionally lucky/unlucky people which psychologist <a href=\"http://richardwiseman.wordpress.com/\">Dr. Richard Wiseman</a> has assembled to try and scientifically study the phenomenon of luck.</p>\n<p>(The following is taken from his book \"<a href=\"http://www.amazon.com/The-Luck-Factor-Richard-Wiseman/dp/0786869143/ref=sr_1_1?ie=UTF8&amp;qid=1386984935&amp;sr=8-1&amp;keywords=the+luck+factor\">The Luck Factor</a>\", and interested parties should go there for more information.)&nbsp;</p>\n<p>As part of the research, both people were placed in identical, fortuitous circumstances, but both handled the situation very differently. The setting: a small coffee shop, arranged so that there were four tables with a confederate (someone who knows about the experiment) sitting at each table. One of these confederates was a wealthy businessman, the kind of person that, should you happen to meet him in real life and make a good impression, could set you up with a well-paying job. All the confederates were told to act the same way for both Brenda and Martin. On the street right outside the coffee shop, the researchers placed a &pound;5 note. &nbsp;</p>\n<p>Brenda and Martin were told to go to the coffee shop at different times, and their behavior was covertly filmed. Martin noticed the money sitting on the street and picked it up. When he went into the coffee shop he sat down next to the businessman and struck up a conversation, even offering to buy him a coffee. &nbsp;Brenda walked past the money, never noticing it, and sat quietly in the shop without talking to anyone.</p>\n<p><strong>Fortune favors the...?</strong></p>\n<p>There are obvious differences in Brenda and Martin's&nbsp;behavior, but are they indicative of more far-reaching differences in how lucky and unlucky people live their lives? First, let's discuss what <em>doesn't</em>&nbsp;differentiate lucky from unlucky people. Wiseman, having assembled his initial group of subjects, tested them on two traits which could have an impact on luck: intelligence and psychic ability. Determining that intelligence wasn't a factor was as easy as administering an intelligence test. Psychic ability was ruled out by having both lucky and unlucky people pick lottery numbers, with the result being that neither group was more successful than the other. &nbsp;</p>\n<p>Wiseman further tested for differences in personality using the Five Factor Model of Personality, which you will recall breaks personality up into Openness,&nbsp;Conscientiousness, Extraversion,&nbsp;Agreeableness, and Neuroticism (the acronym OCEAN makes for easy recall) . Lucky and unlucky people showed no differences in Conscientiousness or Agreeableness, but did show differences in Openness, Extraversion, and Neuroticism. &nbsp;It is here that an interesting picture began to emerge. &nbsp;</p>\n<p>Ultimately, Wiseman was able to break luck down into four overarching principles and twelve subprinciples, summarized here:</p>\n<p>&nbsp;</p>\n<p><em>Principle One: Maximize the number of chance opportunities you have in life.</em></p>\n<p><em>&nbsp; sub-principle one: lucky people maintain a network of contacts with other people.</em></p>\n<p><em>&nbsp; sub-principle two: lucky people are more relaxed and less neurotic than unlucky people</em></p>\n<p><em></em><em>&nbsp; sub-principle three: lucky people have a strong drive towards novelty, and strive to introduce variety into their routines.</em></p>\n<p><em>Principle Two: Use your intuition to make important decisions.</em></p>\n<p><em>&nbsp; sub-principle one: pay attention to your hunches.</em></p>\n<p><em>&nbsp; sub-principle two: try and make your intuition more accurate.</em></p>\n<p><em>Principle Three: Expect good fortune. &nbsp;</em></p>\n<p><em>&nbsp; sub-principle one: lucky people believe their lucky will continue.</em></p>\n<p><em>&nbsp; sub-principle two: lucky people attempt to achieve their goals and persist through difficulty.</em></p>\n<p><em>&nbsp; sub-principle three: lucky people think their interactions will be positive and successful.</em></p>\n<p><em>Principle Four: Turn bad luck into good.</em></p>\n<p><em>&nbsp; sub-principle one: lucky people see the silver lining in bad situations.</em></p>\n<p><em>&nbsp; sub-principle two: lucky people believe that things will work out for them in the long run.</em></p>\n<p><em>&nbsp; sub-principle three: lucky people spend less time brooding over bad luck.</em></p>\n<p><em>&nbsp; sub-principle four: lucky people are more proactive in learning from their mistakes and preventing further bad luck. &nbsp;</em></p>\n<p>&nbsp;</p>\n<p>I suspect that LWers will have a unique set of reactions to and problems with each of these principles, so let's take them one at a time. &nbsp;In this essay, I will examine the first two.&nbsp;</p>\n<p><strong>Facing up to randomness</strong>&nbsp;</p>\n<p>First, how would you go about increasing the likelihood of positive chance encounters? Well, you could start spending more time talking to strangers and making friends with people. &nbsp;Indeed, one of the important differences between unlucky and lucky people is that lucky people are more outgoing, more friendly and open in their body language (lucky people smiled and made eye contact far, far more often), and keep in touch with people they meet longer. The age-old adage 'it's not what you know, but who you know' has more than a grain of truth in it, and a great way to get to know the right people is by simply getting to know more people, period. The chances of any given person being the contact you need are pretty slim, but the odds improve with every person you get to know.&nbsp;</p>\n<p>This actually works on several levels. Since the complexity of the world greatly exceeds the cognitive abilities of any one person, cultivating a strong social network positions you to take advantage of the&nbsp;knowledge&nbsp;and experience of others. Even if you are so much smarter than person X that they can't compete with you along any dimension, they may still have information you don't, or they may know somebody who knows somebody who can help you out.</p>\n<p>Moreover, I'm sure everyone is familiar with the experience of struggling with a problem, only to have a random conversation (with a stranger or a friend) shake loose a key insight. This can happen locally inside your own head when you have the necessary raw material laying around but haven't seen a certain connection. In this situation you would have eventually hit upon the insight but the process has been expedited. &nbsp;More valuable still is when two or more people enter a conversation that produces an insight that nobody had the necessary components to produce for themselves; I think this is part of what Matt Ridley means when he talks about <a href=\"http://www.ted.com/talks/matt_ridley_when_ideas_have_sex.html\">ideas having sex</a>. &nbsp; &nbsp;&nbsp;</p>\n<p>So you're doing your best to meet more people and flex your extroversion muscles. Next, you might try and be more spontaneous and random in your life. Wiseman notes that many lucky people have a strong orientation towards variety and novel experiences. &nbsp;Some of them, facing an important decision like which car to buy, will do something like list their options on a piece of paper and then roll a die.&nbsp;</p>\n<p>You don't need to go quite this far; it's also acceptable to shop different places, take different routes to work, or pick a new part of the city to explore every month. The takeaway here is that it's difficult to have positive chance encounters if you always do the same thing.</p>\n<p>One of my favorite examples of someone positioning themselves to benefit from chance comes from HPMoR, when Harry and Hermione first read all the titles of the books in the library and then read all the tables of contents. &nbsp;From their point of view the books in the library are a vast store of unknown information, any bit of which they might need at a given time. Since reading every single book isn't an option, familiarizing themselves with the information in a systematic way means creating many potential sources of insight while simultaneously reducing the cost of doing future research. Hacker Eric Raymond made related&nbsp;<a href=\"http://esr.ibiblio.org/?p=4699\">point</a> in the context of winning table-top board games:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>I made chance work for me. Pay attention, because I am about to reveal why there is a large class of games (notably pick-up-and-carry games like Empire Builder, network-building games like Power Grid, and more generally games with a large variety of paths to the win condition) at which I am extremely difficult to beat. The technique is replicable.</p>\n<p>I have a rule: when in doubt, play to maximize the breadth of your option tree. Actually, you should often choose option-maximizing moves over moves with a slightly higher immediate payoff, especially early in the game and most especially if the effect of investing in options is cumulative.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>What's the common thread between extroversion, skimming the library shelves, and beating your friends at boardgames? <em>Certain actions and certain states of mind make it more likely you'll benefit from white swans.</em></p>\n<p>(Clever readers may be saying to themselves: \"okay, but doesn't all this also make the chances of encountering black swans higher as well?\" We will address these concerns when we talk about principles three and four.)</p>\n<p><strong>Attitude matters</strong>&nbsp;</p>\n<p>We've covered extraversion and openness, but the lucky people Dr. Wiseman interviewed were also more relaxed and less neurotic than the unlucky ones. This has obvious consequences for when you are trying to meet new people, but research also hints that being less anxious may make you more likely to notice things you aren't specifically looking for. This is probably why several of Dr. Wiseman's lucky participants remarked on how often they found money on the street, found great opportunities while listening to the radio or reading the newspaper, and in general stumbled over opportunities in places where other people simply failed to notice them. &nbsp;</p>\n<p>This attitude undergirds and complements much of what I discussed in the previous section; while you are trying to maximize your pathways to victory, don't forget that constantly worrying and mentally spinning your tires will make you less likely to see a chance opportunity.&nbsp;</p>\n<p><strong>Pump your intuition</strong></p>\n<p>Lucky people tend to have strong intuitions, and they have a habit of paying careful attention to them. &nbsp;I'm sure you're skeptical of this advice, as I was when I first started reading this section. Given present company I don't think I need to reiterate all the billion ways intuition can be derailed and misleading. That said, placing intuition and rationality as orthogonal to one another is a good example of the <a href=\"http://measureofdoubt.com/2011/11/26/the-straw-vulcan-hollywoods-illogical-approach-to-logical-decisionmaking/\">straw vulcan of rationality</a>. Intuitions are of course not always wrong, and in some cases may be the only source of information a person has to go off of.</p>\n<p>Two things put a little nuance on the proposition that you should listen to your intuitions. The first is that, as far as I can tell, lucky people don't trust their intuitions immediately and absolutely. They don't stand at a busy intersection, blindfolded, and trust their gut to tell them when it's safe to cross. Rather, their hunches act more like yellow traffic lights, telling them that they should proceed with caution here or do a bit more research there. In other words, it sounds to me like lucky people treat their intuitions in a pretty rational manner, as data points, to be used but not relied upon in isolation unless there is just nothing else available.&nbsp;</p>\n<p>The other thing is that many lucky people take steps to sharpen their intuitions, utilizing quiet solitude or meditation. Dr. Wiseman goes into precious little detail about this, including just a few anecdotal descriptions of people's efforts to clear their mind. The rationalist community will be familiar with more quantitative methods like <a href=\"http://predictionbook.com/\">predictionbook</a>, and googling for 'improving your intuitions' turned up about as much garbage as you'd probably expect. &nbsp;If anyone has leads to legitimate research on improving intuition, I'd be happy to add an addendum.&nbsp;</p>\n<p><strong>Suggested exercises</strong></p>\n<p>Throughout the book Dr. Wiseman includes exercises which are meant to help people utilize the principles uncovered in his research to become luckier. Here are the suggested exercises for the topics discussed in this post:</p>\n<p>-To enhance your extraversion, strike up a conversation with four people you either don't know or don't know well. Do this each week for a month. Additionally, every week make contact with a person you haven't spoken to in a while.</p>\n<p>-To relax, find a quiet place and picture yourself in a beautiful, calming scene. Make sure to visualize each and every detail of the location, including whatever sounds and smells are around you. When you've got the scene in place, visualize the tension leaving your body in the form of a liquid flowing out of you, starting with your head. once you feel sufficiently relaxed, slowly open your eyes.</p>\n<p>-Inject some randomness in your life by making a list of 6 new experiences. These can be anything from trying a new type of food to taking a class on a subject you've always been interested in. &nbsp;Number them 1 to 6, roll a die, and then do whatever corresponds to the number you rolled.&nbsp;</p>\n<p>This essay can also be found at <a href=\"http://rulerstothesky.wordpress.com/2013/12/12/luck-i-finding-white-swans/\">Rulers To the Sky</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "fkABsGCJZ6y9qConW": 1, "nwcnHxrxcgnwJ878t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iptYS9nZ8kGCmBujc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 39, "extendedScore": null, "score": 1.4631467573342067e-06, "legacy": true, "legacyId": "22717", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Quoth the Master, great in Wisdom, to the Novice: \"Ye, carry with thee all thy days a cheque folded up in your wallet. &nbsp;For there may be many situations in which thou shalt have need of it.\"</em></p>\n<p><em>And the Novice, of high intelligence but lesser wisdom, replied, saying unto the Master: \"Of what situations dost thou speak?\" &nbsp;</em></p>\n<p><em>To which the Master replied: \"imagine that thou dost come upon a nice piece of land, and wish to make a down payment on it. The real estate market moveth quickly in these troubled economic times, and you may soon find your opportunity dried up like dead leaves in summer. &nbsp;What would you do?\" &nbsp;</em><em>The Master, you see, did dabble in real estate development a l</em><em>ittle</em><em>, and his knowledge was deep in these matters. &nbsp;</em></p>\n<p><em>The Novice thought for a moment, saying: \"But always I carry with me a credit card. &nbsp;Surely this is sufficient for my purposes.\"</em></p>\n<p><em>And the Master replied: \"Thou knoweth not the ways of commerce. &nbsp;Thinketh thee that all dealings are conducted within feet of a machine that can read credit cards?!\"</em></p>\n<p><em>The Novice knew the ways of Traditional Rationality and Skepticism, and felt it his duty to take the opposite stance to the Master, lest he unthinkingly obey an authority figure. &nbsp;Undeterred, he replied, saying unto the Master: \"But always I carry with me cash. Surely this is sufficient for my purposes.\"</em></p>\n<p><em>Upon hearing this, the Master did reply, incredulously: \"Would thee carry with thee always an amount of cash equal to the reasonable asking price of a down payment for a piece of land?!\" &nbsp;&nbsp;</em></p>\n<p><em>And lo, the Novice did understand, though he could not put it into these words, that the Master did speak of a certain </em>stance<em>&nbsp;with respect to the unknown. &nbsp;The swirling chaos of reality may be impossible to predict, but there are things an aspiring empirimancer can do to make it more likely that ve will have good fortune. </em></p>\n<p><em>Verily, know that that which people call 'luck' is not the smile of a beneficent god, but the outcome of how some people interact with chance. &nbsp;</em></p>\n<p>&nbsp;</p>\n<p>______________________________________________________________________________________________________________</p>\n<p>&nbsp;</p>\n<p>Consider for a moment two real people, whom we will call ''Martin\" and \"Brenda\", that considers themselves lucky and unlucky, respectively. Both are part of the group of exceptionally lucky/unlucky people which psychologist <a href=\"http://richardwiseman.wordpress.com/\">Dr. Richard Wiseman</a> has assembled to try and scientifically study the phenomenon of luck.</p>\n<p>(The following is taken from his book \"<a href=\"http://www.amazon.com/The-Luck-Factor-Richard-Wiseman/dp/0786869143/ref=sr_1_1?ie=UTF8&amp;qid=1386984935&amp;sr=8-1&amp;keywords=the+luck+factor\">The Luck Factor</a>\", and interested parties should go there for more information.)&nbsp;</p>\n<p>As part of the research, both people were placed in identical, fortuitous circumstances, but both handled the situation very differently. The setting: a small coffee shop, arranged so that there were four tables with a confederate (someone who knows about the experiment) sitting at each table. One of these confederates was a wealthy businessman, the kind of person that, should you happen to meet him in real life and make a good impression, could set you up with a well-paying job. All the confederates were told to act the same way for both Brenda and Martin. On the street right outside the coffee shop, the researchers placed a \u00a35 note. &nbsp;</p>\n<p>Brenda and Martin were told to go to the coffee shop at different times, and their behavior was covertly filmed. Martin noticed the money sitting on the street and picked it up. When he went into the coffee shop he sat down next to the businessman and struck up a conversation, even offering to buy him a coffee. &nbsp;Brenda walked past the money, never noticing it, and sat quietly in the shop without talking to anyone.</p>\n<p><strong id=\"Fortune_favors_the____\">Fortune favors the...?</strong></p>\n<p>There are obvious differences in Brenda and Martin's&nbsp;behavior, but are they indicative of more far-reaching differences in how lucky and unlucky people live their lives? First, let's discuss what <em>doesn't</em>&nbsp;differentiate lucky from unlucky people. Wiseman, having assembled his initial group of subjects, tested them on two traits which could have an impact on luck: intelligence and psychic ability. Determining that intelligence wasn't a factor was as easy as administering an intelligence test. Psychic ability was ruled out by having both lucky and unlucky people pick lottery numbers, with the result being that neither group was more successful than the other. &nbsp;</p>\n<p>Wiseman further tested for differences in personality using the Five Factor Model of Personality, which you will recall breaks personality up into Openness,&nbsp;Conscientiousness, Extraversion,&nbsp;Agreeableness, and Neuroticism (the acronym OCEAN makes for easy recall) . Lucky and unlucky people showed no differences in Conscientiousness or Agreeableness, but did show differences in Openness, Extraversion, and Neuroticism. &nbsp;It is here that an interesting picture began to emerge. &nbsp;</p>\n<p>Ultimately, Wiseman was able to break luck down into four overarching principles and twelve subprinciples, summarized here:</p>\n<p>&nbsp;</p>\n<p><em>Principle One: Maximize the number of chance opportunities you have in life.</em></p>\n<p><em>&nbsp; sub-principle one: lucky people maintain a network of contacts with other people.</em></p>\n<p><em>&nbsp; sub-principle two: lucky people are more relaxed and less neurotic than unlucky people</em></p>\n<p><em></em><em>&nbsp; sub-principle three: lucky people have a strong drive towards novelty, and strive to introduce variety into their routines.</em></p>\n<p><em>Principle Two: Use your intuition to make important decisions.</em></p>\n<p><em>&nbsp; sub-principle one: pay attention to your hunches.</em></p>\n<p><em>&nbsp; sub-principle two: try and make your intuition more accurate.</em></p>\n<p><em>Principle Three: Expect good fortune. &nbsp;</em></p>\n<p><em>&nbsp; sub-principle one: lucky people believe their lucky will continue.</em></p>\n<p><em>&nbsp; sub-principle two: lucky people attempt to achieve their goals and persist through difficulty.</em></p>\n<p><em>&nbsp; sub-principle three: lucky people think their interactions will be positive and successful.</em></p>\n<p><em>Principle Four: Turn bad luck into good.</em></p>\n<p><em>&nbsp; sub-principle one: lucky people see the silver lining in bad situations.</em></p>\n<p><em>&nbsp; sub-principle two: lucky people believe that things will work out for them in the long run.</em></p>\n<p><em>&nbsp; sub-principle three: lucky people spend less time brooding over bad luck.</em></p>\n<p><em>&nbsp; sub-principle four: lucky people are more proactive in learning from their mistakes and preventing further bad luck. &nbsp;</em></p>\n<p>&nbsp;</p>\n<p>I suspect that LWers will have a unique set of reactions to and problems with each of these principles, so let's take them one at a time. &nbsp;In this essay, I will examine the first two.&nbsp;</p>\n<p><strong>Facing up to randomness</strong>&nbsp;</p>\n<p>First, how would you go about increasing the likelihood of positive chance encounters? Well, you could start spending more time talking to strangers and making friends with people. &nbsp;Indeed, one of the important differences between unlucky and lucky people is that lucky people are more outgoing, more friendly and open in their body language (lucky people smiled and made eye contact far, far more often), and keep in touch with people they meet longer. The age-old adage 'it's not what you know, but who you know' has more than a grain of truth in it, and a great way to get to know the right people is by simply getting to know more people, period. The chances of any given person being the contact you need are pretty slim, but the odds improve with every person you get to know.&nbsp;</p>\n<p>This actually works on several levels. Since the complexity of the world greatly exceeds the cognitive abilities of any one person, cultivating a strong social network positions you to take advantage of the&nbsp;knowledge&nbsp;and experience of others. Even if you are so much smarter than person X that they can't compete with you along any dimension, they may still have information you don't, or they may know somebody who knows somebody who can help you out.</p>\n<p>Moreover, I'm sure everyone is familiar with the experience of struggling with a problem, only to have a random conversation (with a stranger or a friend) shake loose a key insight. This can happen locally inside your own head when you have the necessary raw material laying around but haven't seen a certain connection. In this situation you would have eventually hit upon the insight but the process has been expedited. &nbsp;More valuable still is when two or more people enter a conversation that produces an insight that nobody had the necessary components to produce for themselves; I think this is part of what Matt Ridley means when he talks about <a href=\"http://www.ted.com/talks/matt_ridley_when_ideas_have_sex.html\">ideas having sex</a>. &nbsp; &nbsp;&nbsp;</p>\n<p>So you're doing your best to meet more people and flex your extroversion muscles. Next, you might try and be more spontaneous and random in your life. Wiseman notes that many lucky people have a strong orientation towards variety and novel experiences. &nbsp;Some of them, facing an important decision like which car to buy, will do something like list their options on a piece of paper and then roll a die.&nbsp;</p>\n<p>You don't need to go quite this far; it's also acceptable to shop different places, take different routes to work, or pick a new part of the city to explore every month. The takeaway here is that it's difficult to have positive chance encounters if you always do the same thing.</p>\n<p>One of my favorite examples of someone positioning themselves to benefit from chance comes from HPMoR, when Harry and Hermione first read all the titles of the books in the library and then read all the tables of contents. &nbsp;From their point of view the books in the library are a vast store of unknown information, any bit of which they might need at a given time. Since reading every single book isn't an option, familiarizing themselves with the information in a systematic way means creating many potential sources of insight while simultaneously reducing the cost of doing future research. Hacker Eric Raymond made related&nbsp;<a href=\"http://esr.ibiblio.org/?p=4699\">point</a> in the context of winning table-top board games:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>I made chance work for me. Pay attention, because I am about to reveal why there is a large class of games (notably pick-up-and-carry games like Empire Builder, network-building games like Power Grid, and more generally games with a large variety of paths to the win condition) at which I am extremely difficult to beat. The technique is replicable.</p>\n<p>I have a rule: when in doubt, play to maximize the breadth of your option tree. Actually, you should often choose option-maximizing moves over moves with a slightly higher immediate payoff, especially early in the game and most especially if the effect of investing in options is cumulative.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>What's the common thread between extroversion, skimming the library shelves, and beating your friends at boardgames? <em>Certain actions and certain states of mind make it more likely you'll benefit from white swans.</em></p>\n<p>(Clever readers may be saying to themselves: \"okay, but doesn't all this also make the chances of encountering black swans higher as well?\" We will address these concerns when we talk about principles three and four.)</p>\n<p><strong>Attitude matters</strong>&nbsp;</p>\n<p>We've covered extraversion and openness, but the lucky people Dr. Wiseman interviewed were also more relaxed and less neurotic than the unlucky ones. This has obvious consequences for when you are trying to meet new people, but research also hints that being less anxious may make you more likely to notice things you aren't specifically looking for. This is probably why several of Dr. Wiseman's lucky participants remarked on how often they found money on the street, found great opportunities while listening to the radio or reading the newspaper, and in general stumbled over opportunities in places where other people simply failed to notice them. &nbsp;</p>\n<p>This attitude undergirds and complements much of what I discussed in the previous section; while you are trying to maximize your pathways to victory, don't forget that constantly worrying and mentally spinning your tires will make you less likely to see a chance opportunity.&nbsp;</p>\n<p><strong id=\"Pump_your_intuition\">Pump your intuition</strong></p>\n<p>Lucky people tend to have strong intuitions, and they have a habit of paying careful attention to them. &nbsp;I'm sure you're skeptical of this advice, as I was when I first started reading this section. Given present company I don't think I need to reiterate all the billion ways intuition can be derailed and misleading. That said, placing intuition and rationality as orthogonal to one another is a good example of the <a href=\"http://measureofdoubt.com/2011/11/26/the-straw-vulcan-hollywoods-illogical-approach-to-logical-decisionmaking/\">straw vulcan of rationality</a>. Intuitions are of course not always wrong, and in some cases may be the only source of information a person has to go off of.</p>\n<p>Two things put a little nuance on the proposition that you should listen to your intuitions. The first is that, as far as I can tell, lucky people don't trust their intuitions immediately and absolutely. They don't stand at a busy intersection, blindfolded, and trust their gut to tell them when it's safe to cross. Rather, their hunches act more like yellow traffic lights, telling them that they should proceed with caution here or do a bit more research there. In other words, it sounds to me like lucky people treat their intuitions in a pretty rational manner, as data points, to be used but not relied upon in isolation unless there is just nothing else available.&nbsp;</p>\n<p>The other thing is that many lucky people take steps to sharpen their intuitions, utilizing quiet solitude or meditation. Dr. Wiseman goes into precious little detail about this, including just a few anecdotal descriptions of people's efforts to clear their mind. The rationalist community will be familiar with more quantitative methods like <a href=\"http://predictionbook.com/\">predictionbook</a>, and googling for 'improving your intuitions' turned up about as much garbage as you'd probably expect. &nbsp;If anyone has leads to legitimate research on improving intuition, I'd be happy to add an addendum.&nbsp;</p>\n<p><strong id=\"Suggested_exercises\">Suggested exercises</strong></p>\n<p>Throughout the book Dr. Wiseman includes exercises which are meant to help people utilize the principles uncovered in his research to become luckier. Here are the suggested exercises for the topics discussed in this post:</p>\n<p>-To enhance your extraversion, strike up a conversation with four people you either don't know or don't know well. Do this each week for a month. Additionally, every week make contact with a person you haven't spoken to in a while.</p>\n<p>-To relax, find a quiet place and picture yourself in a beautiful, calming scene. Make sure to visualize each and every detail of the location, including whatever sounds and smells are around you. When you've got the scene in place, visualize the tension leaving your body in the form of a liquid flowing out of you, starting with your head. once you feel sufficiently relaxed, slowly open your eyes.</p>\n<p>-Inject some randomness in your life by making a list of 6 new experiences. These can be anything from trying a new type of food to taking a class on a subject you've always been interested in. &nbsp;Number them 1 to 6, roll a die, and then do whatever corresponds to the number you rolled.&nbsp;</p>\n<p>This essay can also be found at <a href=\"http://rulerstothesky.wordpress.com/2013/12/12/luck-i-finding-white-swans/\">Rulers To the Sky</a>.</p>", "sections": [{"title": "Fortune favors the...?", "anchor": "Fortune_favors_the____", "level": 1}, {"title": "Pump your intuition", "anchor": "Pump_your_intuition", "level": 1}, {"title": "Suggested exercises", "anchor": "Suggested_exercises", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "52 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-12T22:16:00.190Z", "modifiedAt": null, "url": null, "title": "Truth Tables", "slug": "truth-tables", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:06.700Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "johnlawrenceaspden", "createdAt": "2012-04-09T10:41:59.681Z", "isAdmin": false, "displayName": "johnlawrenceaspden"}, "userId": "NjYTzHAEAz44HitHH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/onMFFBbmCcwqdqxJj/truth-tables", "pageUrlRelative": "/posts/onMFFBbmCcwqdqxJj/truth-tables", "linkUrl": "https://www.lesswrong.com/posts/onMFFBbmCcwqdqxJj/truth-tables", "postedAtFormatted": "Thursday, December 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Truth%20Tables&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATruth%20Tables%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FonMFFBbmCcwqdqxJj%2Ftruth-tables%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Truth%20Tables%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FonMFFBbmCcwqdqxJj%2Ftruth-tables", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FonMFFBbmCcwqdqxJj%2Ftruth-tables", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1372, "htmlBody": "<p><em>Summary: There's a short program that can run all possible programs, OMG</em></p>\n<p><em>Alternative Summary: Just about any universe that can exist must contain ours, OMG.<br /></em></p>\n<p><em>I was thinking about diagonalization arguments. Does this make any sense? Can anyone debug it for me?</em></p>\n<h1>Self Evident Truths</h1>\n<p><br />The universe is computable.<br /><br />All computations can be performed by Turing Machines.<br /><br />The mind is made out of atoms.<br /><br />The name of the empty string is epsilon.</p>\n<h1>Truths</h1>\n<p><br />Binary Strings are enumerable : epsilon, 0, 1, 00, 01, 10, 11, 000, ....<br /><br />There's a way of converting binary strings to Turing machines and back again. It gets all Turing machines.</p>\n<h1>Consequence</h1>\n<p><br />Consider tables TT(n), which are numbered by binary strings along the top and side, representing turing machines and inputs respectively.<br /><br />Construct a series of finite triangular tables which are defined iteratively as follows:</p>\n<h2>TT(1)<br /></h2>\n<p>To construct the first table, we need one element, the top left, which corresponds to the turing machine epsilon working on the input string epsilon. <br /><br />Convert the string epsilon to its corresponding turing machine, which has no states and no transition rules, and which therefore halts immediately without accepting.</p>\n<p><br />The value of TT(1)(epsilon,epsilon) is therefore F0 (Fail after no steps).</p>\n<pre>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; eps<br />eps F0 <br /></pre>\n<p>That's it for TT(1).</p>\n<p>&nbsp;</p>\n<h2>TT(2)<br /></h2>\n<p>TT(2) will have three cells, the topleftmost three<br /><br />To construct the second table, consider again the element at the top left. Since it represents a halted state, copy it verbatim from the last table.</p>\n<p><br />Then consider the element corresponding to (epsilon,0) , or&nbsp; row epsilon, column 0, or (TM(epsilon), \"0\")<br /><br />Again TM(epsilon) is either a machine with no states, or an invalid specification, and so it halts immediately on the input 0.<br /><br />For the third step in constructing the second triangle, consider (TM(\"0\"), epsilon).<br /><br />TM(\"0\") is another dud. It halts immediately without accepting.<br /><br />So TT(2) is the table</p>\n<pre>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; eps 0</pre>\n<pre>eps F0&nbsp; F0</pre>\n<pre>0&nbsp;&nbsp;&nbsp; F0</pre>\n<p>&nbsp;</p>\n<h2>Towards Infinity...<br /></h2>\n<p>It should be reasonably clear how to continue the construction of these tables <br /><br />For instance TT(3) looks like</p>\n<pre>&nbsp;&nbsp;&nbsp;&nbsp; eps 0&nbsp; 1</pre>\n<pre>eps&nbsp; F0&nbsp; F0 F0</pre>\n<pre>0&nbsp;&nbsp;&nbsp; F0&nbsp; F0</pre>\n<pre>1&nbsp;&nbsp;&nbsp; F0 </pre>\n<p><br />Eventually we will reach a row whose string represents a TM which does something other than halt immediately without accepting.<br /><br />As an example of what to do then, consider the string 1001111, which is the 207th string.<br /><br />In the usual encoding, this string will represent the TM with start state 0, and transition function delta(0,B)=(0,0,L).<br /><br />The first time we consider the 207th row will be when we calculate TT(207), the 207th triangular table.</p>\n<p>The first cell we consider in that row will be (\"1001111\", epsilon). Since 1001111 represents a valid machine, we construct the Instantaneous ID (epsilon,0,epsilon), which represents a machine in state 0 with a blank tape.</p>\n<p>That's the value of TT(207)(\"1001111\",epsilon).</p>\n<p>When we construct TT(208), we take that ID from TT(207), and execute one step. In this case, the machine head moves one step left, writing a zero, and so the instantaneous ID becomes <br />(epsilon,0,0) (State zero, tape reads ...0...., head just to the left of the zero)<br /><br />And so on. The values of TT(n)(\"1001111\",epsilon) are undefined for n&lt;206, since the tables are not that large, but for n=207 onwards, they are:<br />(epsilon,0,epsilon), (epsilon,0,0), (epsilon,0,00), (epsilon,0,000), and so on, with the tape head moving ever leftward, leaving a run of zeros behind it.<br /><br />Other strings may represent TMs that do things that are even more interesting.</p>\n<h2><br /></h2>\n<h2>But Not Beyond, (Or Even As Far As)<br /></h2>\n<p>&nbsp;</p>\n<p>As we construct the successive tables, they become larger and larger.&nbsp; Some cells<br />will stabilize after a finite number of steps, either accepting or<br />rejecting their input strings, at which point their contents become<br />either F??? or A??? where ??? is the number of steps taken.<br /><br />Some cells will loop. By comparing the instantaneous state of the table with the state of the cells in all previous tables, loops can be detected. We can mark them as loops, continue computing as before, or re-use the values in the previous tables to avoid performing the computations again. <br /><br />And some cells, like the ones at (\"1001111\", epsilon)&nbsp; will keep producing new instantaneous IDs, without looping.<br /><br />But every TT(n) is a finitely computable object. Indeed the program to compute them all is very short and will run on any computer worthy of the name.</p>\n<h2>I Find This A Bit Worrying, Because:</h2>\n<p><br />As we continue to construct the successive tables, we will perform every conceivable computation.<br /><br />We will simulate in precise detail every possible world, universe and multiverse. Even though our computer is not quantum, we will simulate all quantum computations.<br /><br />In particular, if you believe that you are a computation, or that simulation of your brain is equivalent to your existence, then you will be present in the computation, with exactly as much free will, and with your behaviour as precisely determined, as it ever was or will be.<br /><br />Some of you will live in universes in which artificial intelligences rise and successfully paperclip everything.<br /><br />Some of you will live in universes where friendly AIs are built, if that is possible.<br /><br />It will not be possible for these copies to tell which copy they are, and so they will not be able to tell what is about to happen, or what has happened. Any more than you can.<br /><br />There will be hells and paradises.<br /><br />In some universes, copies of you will set programs running to calculate the successive triangular tables TT(n), and they will keep adding memory to their computers as needed (only actually one extra storage location per step of computation, at worst).<br /><br />And so the sequence of finite truth tables will contain itself, as well as everything else. Everything that has ever happened will happen again. You will be reborn and live and possibly die.</p>\n<p>You will not know whether you are in the 'base universe', or in the computation. If that even means anything.</p>\n<p>And every computation that occurs as part of this great computation is utterly \"Beyond the Reach of God\".</p>\n<h1>Exercises</h1>\n<p><br />1/ It will take me about a day, a packet of cigars and a machine full of coffee to write this program and start it running. That is what I am doing now. When I start it running, will I have done a bad thing? If someone were to stop me before the program started running, would that make any difference to anything important?<br /><br />2/ Can anything except what is computed by this program be said to exist in any sense? Continua, and sets of all sets, and so on, are very problematic. And if the human mind is itself a computation , contained in a universe which itself is a computation, how can we think of or interact with any non-computable thing?<br /><br />3/ Does the ultimate Truth Table, which the finite tables TT(n) approach as the process continues, exist? What does that question mean? The values of many of its cells are determined. Many of them are not computable. The ratio is unclear.<br /><br />4/ We can perform the initial stages of this computation with a finite computer with finite memory. At no point does the amount of memory required become infinite. If the computational power of the universe is infinite, then it can contain not only itself but every other thing. If the computation power of the universe is finite, where does that number come from?<br /><br />5/ The program is very short. Any randomly chosen computation has a good chance of being it. It would probably be very hard to construct an interesting universe which did not contain every possible universe and person.</p>\n<p>6/ Does it make any sense to talk about 'not being part of this computation'?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "onMFFBbmCcwqdqxJj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -8, "extendedScore": null, "score": 1.4634140869099767e-06, "legacy": true, "legacyId": "25072", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Summary: There's a short program that can run all possible programs, OMG</em></p>\n<p><em>Alternative Summary: Just about any universe that can exist must contain ours, OMG.<br></em></p>\n<p><em>I was thinking about diagonalization arguments. Does this make any sense? Can anyone debug it for me?</em></p>\n<h1 id=\"Self_Evident_Truths\">Self Evident Truths</h1>\n<p><br>The universe is computable.<br><br>All computations can be performed by Turing Machines.<br><br>The mind is made out of atoms.<br><br>The name of the empty string is epsilon.</p>\n<h1 id=\"Truths\">Truths</h1>\n<p><br>Binary Strings are enumerable : epsilon, 0, 1, 00, 01, 10, 11, 000, ....<br><br>There's a way of converting binary strings to Turing machines and back again. It gets all Turing machines.</p>\n<h1 id=\"Consequence\">Consequence</h1>\n<p><br>Consider tables TT(n), which are numbered by binary strings along the top and side, representing turing machines and inputs respectively.<br><br>Construct a series of finite triangular tables which are defined iteratively as follows:</p>\n<h2 id=\"TT_1_\">TT(1)<br></h2>\n<p>To construct the first table, we need one element, the top left, which corresponds to the turing machine epsilon working on the input string epsilon. <br><br>Convert the string epsilon to its corresponding turing machine, which has no states and no transition rules, and which therefore halts immediately without accepting.</p>\n<p><br>The value of TT(1)(epsilon,epsilon) is therefore F0 (Fail after no steps).</p>\n<pre>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; eps<br>eps F0 <br></pre>\n<p>That's it for TT(1).</p>\n<p>&nbsp;</p>\n<h2 id=\"TT_2_\">TT(2)<br></h2>\n<p>TT(2) will have three cells, the topleftmost three<br><br>To construct the second table, consider again the element at the top left. Since it represents a halted state, copy it verbatim from the last table.</p>\n<p><br>Then consider the element corresponding to (epsilon,0) , or&nbsp; row epsilon, column 0, or (TM(epsilon), \"0\")<br><br>Again TM(epsilon) is either a machine with no states, or an invalid specification, and so it halts immediately on the input 0.<br><br>For the third step in constructing the second triangle, consider (TM(\"0\"), epsilon).<br><br>TM(\"0\") is another dud. It halts immediately without accepting.<br><br>So TT(2) is the table</p>\n<pre>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; eps 0</pre>\n<pre>eps F0&nbsp; F0</pre>\n<pre>0&nbsp;&nbsp;&nbsp; F0</pre>\n<p>&nbsp;</p>\n<h2 id=\"Towards_Infinity___\">Towards Infinity...<br></h2>\n<p>It should be reasonably clear how to continue the construction of these tables <br><br>For instance TT(3) looks like</p>\n<pre>&nbsp;&nbsp;&nbsp;&nbsp; eps 0&nbsp; 1</pre>\n<pre>eps&nbsp; F0&nbsp; F0 F0</pre>\n<pre>0&nbsp;&nbsp;&nbsp; F0&nbsp; F0</pre>\n<pre>1&nbsp;&nbsp;&nbsp; F0 </pre>\n<p><br>Eventually we will reach a row whose string represents a TM which does something other than halt immediately without accepting.<br><br>As an example of what to do then, consider the string 1001111, which is the 207th string.<br><br>In the usual encoding, this string will represent the TM with start state 0, and transition function delta(0,B)=(0,0,L).<br><br>The first time we consider the 207th row will be when we calculate TT(207), the 207th triangular table.</p>\n<p>The first cell we consider in that row will be (\"1001111\", epsilon). Since 1001111 represents a valid machine, we construct the Instantaneous ID (epsilon,0,epsilon), which represents a machine in state 0 with a blank tape.</p>\n<p>That's the value of TT(207)(\"1001111\",epsilon).</p>\n<p>When we construct TT(208), we take that ID from TT(207), and execute one step. In this case, the machine head moves one step left, writing a zero, and so the instantaneous ID becomes <br>(epsilon,0,0) (State zero, tape reads ...0...., head just to the left of the zero)<br><br>And so on. The values of TT(n)(\"1001111\",epsilon) are undefined for n&lt;206, since the tables are not that large, but for n=207 onwards, they are:<br>(epsilon,0,epsilon), (epsilon,0,0), (epsilon,0,00), (epsilon,0,000), and so on, with the tape head moving ever leftward, leaving a run of zeros behind it.<br><br>Other strings may represent TMs that do things that are even more interesting.</p>\n<h2><br></h2>\n<h2 id=\"But_Not_Beyond___Or_Even_As_Far_As_\">But Not Beyond, (Or Even As Far As)<br></h2>\n<p>&nbsp;</p>\n<p>As we construct the successive tables, they become larger and larger.&nbsp; Some cells<br>will stabilize after a finite number of steps, either accepting or<br>rejecting their input strings, at which point their contents become<br>either F??? or A??? where ??? is the number of steps taken.<br><br>Some cells will loop. By comparing the instantaneous state of the table with the state of the cells in all previous tables, loops can be detected. We can mark them as loops, continue computing as before, or re-use the values in the previous tables to avoid performing the computations again. <br><br>And some cells, like the ones at (\"1001111\", epsilon)&nbsp; will keep producing new instantaneous IDs, without looping.<br><br>But every TT(n) is a finitely computable object. Indeed the program to compute them all is very short and will run on any computer worthy of the name.</p>\n<h2 id=\"I_Find_This_A_Bit_Worrying__Because_\">I Find This A Bit Worrying, Because:</h2>\n<p><br>As we continue to construct the successive tables, we will perform every conceivable computation.<br><br>We will simulate in precise detail every possible world, universe and multiverse. Even though our computer is not quantum, we will simulate all quantum computations.<br><br>In particular, if you believe that you are a computation, or that simulation of your brain is equivalent to your existence, then you will be present in the computation, with exactly as much free will, and with your behaviour as precisely determined, as it ever was or will be.<br><br>Some of you will live in universes in which artificial intelligences rise and successfully paperclip everything.<br><br>Some of you will live in universes where friendly AIs are built, if that is possible.<br><br>It will not be possible for these copies to tell which copy they are, and so they will not be able to tell what is about to happen, or what has happened. Any more than you can.<br><br>There will be hells and paradises.<br><br>In some universes, copies of you will set programs running to calculate the successive triangular tables TT(n), and they will keep adding memory to their computers as needed (only actually one extra storage location per step of computation, at worst).<br><br>And so the sequence of finite truth tables will contain itself, as well as everything else. Everything that has ever happened will happen again. You will be reborn and live and possibly die.</p>\n<p>You will not know whether you are in the 'base universe', or in the computation. If that even means anything.</p>\n<p>And every computation that occurs as part of this great computation is utterly \"Beyond the Reach of God\".</p>\n<h1 id=\"Exercises\">Exercises</h1>\n<p><br>1/ It will take me about a day, a packet of cigars and a machine full of coffee to write this program and start it running. That is what I am doing now. When I start it running, will I have done a bad thing? If someone were to stop me before the program started running, would that make any difference to anything important?<br><br>2/ Can anything except what is computed by this program be said to exist in any sense? Continua, and sets of all sets, and so on, are very problematic. And if the human mind is itself a computation , contained in a universe which itself is a computation, how can we think of or interact with any non-computable thing?<br><br>3/ Does the ultimate Truth Table, which the finite tables TT(n) approach as the process continues, exist? What does that question mean? The values of many of its cells are determined. Many of them are not computable. The ratio is unclear.<br><br>4/ We can perform the initial stages of this computation with a finite computer with finite memory. At no point does the amount of memory required become infinite. If the computational power of the universe is infinite, then it can contain not only itself but every other thing. If the computation power of the universe is finite, where does that number come from?<br><br>5/ The program is very short. Any randomly chosen computation has a good chance of being it. It would probably be very hard to construct an interesting universe which did not contain every possible universe and person.</p>\n<p>6/ Does it make any sense to talk about 'not being part of this computation'?</p>", "sections": [{"title": "Self Evident Truths", "anchor": "Self_Evident_Truths", "level": 1}, {"title": "Truths", "anchor": "Truths", "level": 1}, {"title": "Consequence", "anchor": "Consequence", "level": 1}, {"title": "TT(1)", "anchor": "TT_1_", "level": 2}, {"title": "TT(2)", "anchor": "TT_2_", "level": 2}, {"title": "Towards Infinity...", "anchor": "Towards_Infinity___", "level": 2}, {"title": "But Not Beyond, (Or Even As Far As)", "anchor": "But_Not_Beyond___Or_Even_As_Far_As_", "level": 2}, {"title": "I Find This A Bit Worrying, Because:", "anchor": "I_Find_This_A_Bit_Worrying__Because_", "level": 2}, {"title": "Exercises", "anchor": "Exercises", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-13T03:23:55.984Z", "modifiedAt": null, "url": null, "title": "Walkthrough of the Tiling Agents for Self-Modifying AI paper", "slug": "walkthrough-of-the-tiling-agents-for-self-modifying-ai-paper", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:35.622Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QGrX3qK3qxQYK9D4C/walkthrough-of-the-tiling-agents-for-self-modifying-ai-paper", "pageUrlRelative": "/posts/QGrX3qK3qxQYK9D4C/walkthrough-of-the-tiling-agents-for-self-modifying-ai-paper", "linkUrl": "https://www.lesswrong.com/posts/QGrX3qK3qxQYK9D4C/walkthrough-of-the-tiling-agents-for-self-modifying-ai-paper", "postedAtFormatted": "Friday, December 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Walkthrough%20of%20the%20Tiling%20Agents%20for%20Self-Modifying%20AI%20paper&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWalkthrough%20of%20the%20Tiling%20Agents%20for%20Self-Modifying%20AI%20paper%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQGrX3qK3qxQYK9D4C%2Fwalkthrough-of-the-tiling-agents-for-self-modifying-ai-paper%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Walkthrough%20of%20the%20Tiling%20Agents%20for%20Self-Modifying%20AI%20paper%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQGrX3qK3qxQYK9D4C%2Fwalkthrough-of-the-tiling-agents-for-self-modifying-ai-paper", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQGrX3qK3qxQYK9D4C%2Fwalkthrough-of-the-tiling-agents-for-self-modifying-ai-paper", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6434, "htmlBody": "<p>This is my walkthrough of&nbsp;<a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the L&ouml;bian Obstacle</a>. It's meant to summarize the paper and provide a slightly-less-technical introduction to the content. I've also collected a number of typos and suggestions, which can be found at the end of the post.</p>\n<p><span style=\"font-size: 2em;\">Motivation</span></p>\n<p>We want to be able to consider agents which build slightly better versions of themselves, which build slightly better versions of themselves, and so on. This is referred to as an agent \"tiling\" itself. This introduces a question: how can the parent agent trust its descendants?</p>\n<p>We'll be dealing with logical systems, which are crisply deterministic. In such a setting, child agents can be \"absolutely trustworthy\": for example, if the child uses the same methods of proof and the same axioms as the parent, then the parent should be able to trust the child unconditionally: or, at least, the parent should trust the child at least as much as it trusts itself.</p>\n<p>This turns out to be a huge problem, because modern logical systems <em>don't</em>&nbsp;trust themselves (or any logical systems of equal strength), to say nothing of (supposedly more powerful) successor systems.</p>\n<p>This problem of trust is rooted in a theorem stated by the mathematician L&ouml;b.</p>\n<p>Assuming that we can surmount such difficulties, there are a number of other principles that we want in our tiling agents, such as the Vingean principle (the parent shouldn't have to know exactly what the children will do) and the Naturalistic principle (agents should think of themselves as part of the world). These will be discussed in detail later in the paper.</p>\n<p><a id=\"more\"></a></p>\n<h1>Glossary</h1>\n<ul>\n<li>I'll use <span>@x</span> to mean \"the execution of x\". (The paper uses x\u0304 instead.)</li>\n<li>I'll use <span>[x]</span> denotes the G&ouml;del number of <span>x</span> in some suitable encoding. (The paper uses &nbsp;&lceil;x&rceil; instead).</li>\n<li><span>\u25a1T[x]</span> is read \"There is a proof of x in T\". It's the same as \"<span>&exist;y. Prooves_T(y, [x])\"</span>. The T is omitted when it's clear from the context.</li>\n<li>A0, A1, &hellip; will denote agents. Note that A0 is the <em>child</em> and A1 is the <em>parent</em>. I personally would have done things the other way around, but I'll stick to the paper's descending style.</li>\n<li>G will be used to denote a world-state in which the agent's goals have been achieved.</li>\n<li>&empty; will be used to denote a world-state wherein nothing of import has been changed by the agent. For more specificity, see the paper.</li>\n<li><span>A \u22a9 x</span> is read \"A has cognitively concluded x\" or \"A believes x\".</li>\n<li>PA is Peano Arithmetic, ZF is Zermelo-Fraenkel set theory.</li>\n<li><span>T</span> will denote a theory. In general, assume that T is at least as powerful as PA.</li>\n<li><span>T+1</span> is <span>T &cup; {&not;\u25a1T[&perp;]}</span> (T plus the fact that T is consistent). T+2 is (T+1)+1, etc.</li>\n</ul>\n<p>Here's a few reminders, just in case you're rusty:</p>\n<ul>\n<li>&phi;[x\\y] denotes \"&phi;, with all instances of <span>x</span> replaced by <span>y</span>\". The slash disambiguates this from quoting.</li>\n<li><span>\u225c</span> means \"is defined to be\".</li>\n<li><span>T \u22a2 &phi;</span> is pronounced \"T derives &phi;\".</li>\n</ul>\n<p>So, for example, <span>T \u22a2 \u25a1T[&phi;]</span> is read \"T knows that there is a T-proof of &phi;.\"</p>\n<p>In general, I'll be less formal than the paper. If I've left any ambiguities, see the paper for clarification.</p>\n<h1>Trusting Yourself</h1>\n<p>Imagine that we've got a parent agent A1. It wants to achieve the goal G. Among the things it can do is create more agents. What we want to do is find an agent A0 within the actions of A1 such that:</p>\n<pre><span>A1 \u22a9 @A0 &rarr; (&forall;action &isin; Acts0. @action &rarr; \u25a10[@action &rarr; (&empty; &or; G)])\n</span></pre>\n<p>Translated, this reads \"A1 believes that, if it constructs A0, then for all actions available to A0, that action will only be taken if A0 has proven that taking the action leads to either nothing of import happening, or the goal being achieved.\"</p>\n<p>(We also require <span>A1 \u22a9 @A0 &rarr; ((&forall;action &isin; Acts0. &not;@action) &rarr; &empty;)</span>, which says \"if A0 takes no actions then nothing will happen\".)</p>\n<p>We would further like A1 to be able to trust A0. Obviously, we don't want A1 to trust its children unconditionally. But assuming that A0's proofs use the same mechanism as A1's proofs, and assuming that A1 verifies that A0's proof system works <em>exactly</em> like A1's own proof system, we would like to have the following:</p>\n<pre><span>A1 \u22a9 @A0 &rarr; (&forall;action &isin; Acts0. @action &rarr; (&empty; &or; G))\n</span></pre>\n<p>ie, A1 believes that, if it constructs A0, then any action taken by A0 <em>actually will</em> either cause nothing to happen or achieve the goal.</p>\n<p>This is an important distinction, make sure you understand it before proceeding. We want A1 to <em>unwrap</em>&nbsp;A0's proofs, to look at things inside the&nbsp;\u25a10 and conclude that they actually are true. We want A1 to look at&nbsp;\u25a10[&phi;] and actually conclude &phi;.</p>\n<p>Unfortunately, this is impossible&nbsp;&mdash; at least, it's impossible when A1 has concluded that A0's proof system is identical to A1s proof system. Why? Because A1 can't even trust its own formalism. This is the famed L&ouml;b's theorem.</p>\n<h1>The L&ouml;bstacle</h1>\n<p>Intuitively, L&ouml;b's theorem claims \"No system T can trust its own proofs until it witnesses them.\" More formally, it states that in a theory T at least as powerful as PA, <span>\u25a1T[&phi;] &rarr; &phi; &hArr; &phi;</span>: T can only deduce &phi; from a T-proof of &phi; if &phi; is actually true.</p>\n<p>You might expect that there are <em>some</em> sentences &psi; which T cannot prove, but for which T knows \"if I <em>could</em> prove &psi;, then &psi; would be true\". This is not the case! There are <em>no</em> sentences that T cannot prove, for which T will trust \"but <em>if I could prove it</em>, then it would be true\". T does not trust its own proofs until it witnesses them. Here's why:</p>\n<p>Any T of sufficient power can define a sentence <span>L &harr; \u25a1[L]&rarr;&phi;</span> (L is true iff a proof of L implies &phi;.) T can deduce the following about such an L for&nbsp;<em>any&nbsp;</em>&phi;.</p>\n<ol>\n<li><code>L &rarr; (\u25a1[L]&rarr;&phi;)</code> (from the definition of L)</li>\n<li><code>\u25a1[L&rarr;(\u25a1[L]&rarr;&phi;)]</code> (from 1 because T can prove the things it knows: <span>T \u22a2 x &rArr; T \u22a2 \u25a1[x]</span>)</li>\n<li><code>\u25a1[L]&rarr;\u25a1[\u25a1[L]&rarr;&phi;]</code> (from 2 because provability distributes over implication: <span>T \u22a2 \u25a1[x&rarr;y] &rArr; T \u22a2 \u25a1[x]&rarr;\u25a1[y]</span>)</li>\n<li><code>\u25a1[L]&rarr;(\u25a1[\u25a1[L]]&rarr;\u25a1[&phi;])</code> (from 3 because provability distributes over implication)</li>\n<li><code>\u25a1[L]&rarr;\u25a1[\u25a1[L]]</code> (because T can prove things it knows)</li>\n<li><code>\u25a1[L]&rarr;\u25a1[&phi;]</code> (from 4 and 5)</li>\n</ol>\n<p>Note that &phi; here is general. This means that for <em>any</em> &phi;, there is a sentence L such that &phi; is provable when L is provable, and also &phi; is true when L is provable. Now, <em>if</em> T had <span>\u25a1[&phi;]&rarr;&phi;</span> then:</p>\n<ul>\n<li><code>\u25a1[&phi;]&rarr;&phi;</code> (assumption)</li>\n<li><code>\u25a1[L]&rarr;&phi;</code> (from 6 and assumption)</li>\n<li><code>L</code> (<span>L &harr; \u25a1[L]&rarr;&phi; by definition,</span>&nbsp;and we just deduced <span>\u25a1[L]&rarr;&phi;</span>)</li>\n<li><code>\u25a1[L]</code> (T can prove things it knows)</li>\n<li><code>&phi;</code></li>\n</ul>\n<p>We have now proved <span>&phi;</span>&nbsp;<em>in general</em>: such a T can prove anything. Notice what this means: any system of sufficient strength can derive a proof of any &phi; <em>from the mere fact that if provable then it is true.</em></p>\n<p>(Fun fact: pick <span>&phi;=&perp;</span> to get a proof of the second incompleteness theorem. <span>&not;\u25a1[&perp;]</span> means <span>\u25a1[&perp;]&rarr;&perp;</span>, so if you can prove <span>&not;\u25a1[&perp;]</span> (that you don't prove a contradiction) then you prove &perp; (a contradiction).)</p>\n<p>Conclusion: A system cannot trust, in general, that \u25a1[&phi;] implies &phi;. It can't know that its proofs are valid.</p>\n<p>This is going to make our lives difficult: our tiling agents will be generating situations where the parent agent verifies \"my child will only execute actions that they've proven (in T) lead to good outcomes\". By L&ouml;b's theorem, such agents, running on T, <em>cannot</em> conclude \"and so my child, also running on T, will only execute actions that lead to good outcomes\" &mdash; because it cannot trust its own proof system.</p>\n<h1>Finite Descent Problems</h1>\n<p>A system <em>can</em> prove weaker systems consistent. For example, T+1 can prove T consistent, and T+2 can prove T+1 consistent, etc. You might think that we can work around the problem by having our initial system run on T+n for large n. But this fails.</p>\n<p>First of all, if you pick a large finite n then your system has a finite \"telomere\", a trust-capacity that is running out. And even if you try to start with T+&omega;, it has to bequeath unto its child system a T+n where n is an ordinal less than &omega;. Though such an n may be arbitrarily large, it must be finite, and then we have the finite fuse again. In general, no matter how infinitely large an ordinal you start with, it will still suffer from a finite descent problem (because the ordinals are well-ordered).</p>\n<p>And secondly, T+1 is far more powerful than T. Take, for example, the <a href=\"http://math.andrej.com/2008/02/02/the-hydra-game/\">hydra game</a>. It turns out (spoiler alert) you can't lose: any strategy beats every hydra.</p>\n<p>Now consider PA. Given an h denoting the height of the hydra, PA can prove that any strategy beats hydras of height h. But PA <em>cannot</em> conclude that any strategy beats every hydra. Letting <span>&psi;(h)</span> denote \"hydras of height h can be killed by any strategy\", it is the case that for all h,<span>&nbsp;PA \u22a2 &psi;(h)</span>. But PA does NOT derive <span>&forall;h. &psi;(h)</span>! PA cannot rule out the possibility that there is <em>some</em> height at which its proofs will fail: it only trusts proofs that it <em>witnesses</em>.</p>\n<p>PA+1, on the other hand, has <span>PA+1 \u22a2 &forall;h. &psi;(h)</span> because it knows that PA is consistent, and thus that there is no height at which PA's proofs start failing. Thus, in a very real sense, there are problems (like the hydra problem, or like checking whether a Goldstein sequence terminates) that can be solved in PA+1 but not PA.</p>\n<p>In fact, an agent's strength is directly related to which ordinals it can well-order: the ability to well-order high ordinals directly translates into mathematical power. Child agents running on weaker mathematical systems correspond to a very real loss in capabilities.</p>\n<p>Having each agent build a weaker child is untenable, and by L&ouml;b's theorem we can't have the child agent run on the same system as the parent (because the parent can't trust its own system). Yet we demand away to tile agents without having their trust drop below a certain baseline. Is this even possible? Experience with G&ouml;delian problems may lead you to think that the answer is \"No\", but as we will see, we can gain a few technical victories.</p>\n<h1>Failed Solutions</h1>\n<p>Here are a few ideas to get things rolling:</p>\n<h3 id=\"recursivediving\">Recursive Diving</h3>\n<p>Define <span>Safe(x)</span> to be <span>(@x -&gt; &empty; &or; G)</span>: in other words, <span>Safe(x)</span> means \"executing x leads to either no change, or the goal being achieved.\"</p>\n<p>Instead of requiring the agent to know \u25a1[Safe(x)] &rarr; Safe(x), we instead define a proxy for safety. This proxy is <span>&psi;(x) &harr; Safe(x) &or; \u25a1[&psi;(x)]</span>. Then the condition of action is <span>&psi;(x)</span>.</p>\n<p>The idea is that instead of requiring the system to accept that a proof of safety makes things safe, we instead ask it to either know that an action is safe, or be able to prove &psi; about the action (which means either we can prove the action safe or we can prove we can prove &psi; about the action (which means either we can prove we can prove the action safe, or ...)). Na&iuml;vely, you might expect that the only way out of this recursive dive is for the system to accept Safe(x) at some depth of proof. But that's not what happens.</p>\n<p>In fact, &psi;(x) is accepted immediately, regardless of the safety of the action. &psi; is in fact a Henkin sentence: It is of the form <span>&psi;(x) &harr; \u25a1[&psi;(x)]</span> (with a dangling <span>&or; Safe(x)</span> that we can ignore). Henkin sentences are universally true. Our system can ignore the safety constraint, skip the recursive descent, and conclude any action is OK. This makes it&hellip; rather unsuited to the real world.</p>\n<h3 id=\"bucktossing\">Buck Tossing</h3>\n<p>Instead of requiring \u25a1T[&phi;] &rarr; &phi;, we construct a tower of theories. Each theory trusts the next theory, but not itself. So T0 has <span>\u25a1T1[&phi;] &rarr; &phi;</span>, and T1 has <span>\u25a1T2[&phi;] &rarr; &phi;</span>. This allows parents to trust their children without ever trusting themselves (and thus becoming inconsistent.)</p>\n<p>Problem is, if T0 is as strong as PA then it can deduce that proofs in T0 are valid in T1: it just bumps all the appropriate 'n' up by 1. Then T0 deduces that \u25a1T0[&phi;] &rarr; \u25a1T1[&phi;], from which it deduces &phi;. Triumphant, T0 proudly concludes \u25a1T0[&phi;]&rarr;&phi;, and immediately bursts into inconsistent flames.</p>\n<h3 id=\"proofstacking\">Proof Stacking</h3>\n<p>It doesn't matter whether T trusts itself. WE trust T. If Safe(x), or T proves Safe(x), or T proves that T proves Safe(x), or if there is <em>any</em> depth of proofiness such that Safe(x), then the agent may do x.</p>\n<p>Let \u25a1^k[x] stand for \"T proves that T proves that &hellip; (k times) &hellip; that Safe(x)\". (Assume k nonzero throughout.) Because this stacking has a regular structure, there should be a compact formula expressing this. So what if we make T's criterion of action be (Safe(x) &or; \u25a1^k[x])?</p>\n<p>Then, when a child concludes (Safe(x) &or; \u25a1^k[x]), the parent concludes that \u25a1[Safe(x) &or; \u25a1^k[Safe(x)]].</p>\n<p>Problem is, \u25a1[Safe(x) &or; \u25a1^k[Safe(x)]] doesn't reduce to (Safe(x) &or; \u25a1^j[Safe(x)]), no matter how much we want it to. This is because \u25a1^k[&phi;] is actually a formula like (&exist; k. StackOProofs(k, &phi;)), and \u25a1[&exist; k. StackOProofs(k, x)] does not reduce to (&exist; j. StackOProofs(j, x)), precisely because T does not trust that \u25a1[&phi;]&rarr;&phi;. So this technique doesn't really dodge the L&ouml;bstacle at all.</p>\n<hr />\n<p>Is it completely hopeless? No! Just mostly hopeless. We can, it turns out, achieve a few technical victories.</p>\n<h1>Technically Solutions</h1>\n<h3 id=\"infinitedescent\">Infinite Descent</h3>\n<p>Idea 2 above (buck-tossing) is reparable. The idea, you remember, is that T0 trusts T1, which trusts T2, etc. The problem was that T0 could deduce that, by bumping all the n in a proof, things true in T0 were true in T1. It could then \"lift\" its proofs into T1, prove itself consistent, and become inconsistent.</p>\n<p>This can be fixed, if we somehow find a way to allow Tn to trust Tn+1 without being able to trust that it can lift its own proofs upwards. For example, instead of having as an axiom of Tn that <span>Tn+1[&phi;]&rarr;&phi;</span>, we could guard the axiom by a formula &psi;. Thus, the new axiom becomes <span>&psi;(n) &rarr; (Tn+1[&phi;] &rarr; &phi;)</span>.</p>\n<p>Now, if only we could find a &psi; such that &psi; is true for all n, and T can prove &psi;(n) for each individual n, but such that T cannot prove &forall;n. &psi;(n)&hellip;</p>\n<p>And we can! We were just talking about such formulas earlier. We need a formula that T-0 can prove for every n, but that it can't prove it can prove for every in. (In other words, for all n we need&nbsp;<span>T \u22a2 &psi;(n)</span>, but we also need that T can NOT derive&nbsp;<span>&forall;n. &psi;(n)</span>.)</p>\n<p>In order to do this, we're going to need a theory <em>more powerful</em> than T: a theory Z such that T cannot prove Z consistent. (Why? We'll get to that later.) For concreteness, if T is PA then ZF is more powerful in the appropriate manner.</p>\n<p>Now, here's an example guardian formula: <span>&psi;(n)&equiv;&not;ProofZ(n, [&perp;])</span> or, in english, &psi;(n)=\"n does not encode a proof of contradiction in Z\".</p>\n<p>For every n, T0 can prove &psi;(n). But T0 cannot prove that <span>&forall;n. &psi;(n)</span>. Thus, it can no longer \"lift\" its proofs into the higher system: \"increasing all the n's\" is no longer a safe operation. T0 does not expect &psi; to hold forever.</p>\n<p>We can prove this system consistent. I'll sketch the proof; see the paper for details. We'll construct a sound system T+ that trusts T and reasons about T0. We'll show that T+ does not prove T0 inconsistent, and that if T0 were consistent then T+ would know.</p>\n<p>T+ is built from T, a new symbol \"TRUE\", and axioms stating both that T is consistent and TRUE is a truth predicate for T. We note that T+ is strong enough to reason about T0 and proofs in T0. T+ reasons as follows:</p>\n<p>Suppose T0 is inconsistent. Then \u25a1T0[&perp;]. Consider the proof. It has a bad line. The bad line can't come from the axioms of TRUE, nor from the use of T, as those are trusted. Therefore, it must come from the special axiom, and be of the form <span>&psi;(n) &rarr; (\u25a1Tn[&phi;(x)] &rarr; &phi;(x))</span>. In order for this to be the point of failure, &psi;(n) must be true, \u25a1Tn[&phi;] must be true, and &phi; must be false. Consider the Tn proof of &phi;. Recurse: its first bad line must also come from a &psi;(n) axiom, and its first bad line must come from a &psi;(n) axiom, and so on and so forth. Thus, &psi;(n) must be true for all n. So, <span>\u25a1T0[&perp;]&rarr;(&forall;n. &psi;(n))</span>.</p>\n<p>This reasoning shows that T+ knows that if T0 is inconsistent then &psi; is true for all n. Furthermore, notice that if &psi; is true for all n then Z is consistent.</p>\n<p>If there were a proof of contradiction in T0, the proof could be fed to T+, and then T+ would be able to prove the consistency of Z.</p>\n<p>We've established that T (and thus T+) cannot prove the consistency of Z. Therefore, T0 is consistent.</p>\n<p>This is nice, but it's not without problems. For example, <span>T0 \u22a2 (&forall;n. &psi;(n)) &rarr; &perp;</span>: T0 knows that if &psi; is true for all n, then it can bump its proofs upwards. It follows that <span>T0 \u22a2 &exist;n. &not;&psi;(n): T0 believes that some n encodes a proof of contradiction in Z.</span>&nbsp;Concretely, when T=PA and Z=ZF, T thinks that ZF is inconsistent. This is&hellip; not the type of behavior that we strive for in real world agents, to say the least.</p>\n<p>Also, this means that T0 has only nonstandard models: T0 is consistent, and thus has a model, but as we've seen, T0 only has models where T is consistent but ZF is inconsistent. This, too, is a hinderance.</p>\n<p>Regardless, the paper has shown that you can tile agents without letting their trust fall below a certain minimum. (All of T0, T1, T2, ... trust T.) This system doesn't trust itself, and thinks ZF is inconsistent, but we've still shattered the L&ouml;bstacle. Via <a title=\"Technically correct\" href=\"http://imgur.com/OldYeT6\" target=\"_blank\">technical tomfoolery</a>, we have a mathematical system that can trust a another system of the same power.</p>\n<p>This is a cause for hope.</p>\n<p><em>Question: What if we use <span>&psi;(n)&equiv;&not;ProofT(n,&not;\u25a1T[&perp;])</span> (ie &psi;(n) means \"n does not encode a proof of T's consistency\"). The existing proof of consistency fails because T+ proves T consistent, but can we prove T0's consistency by other means? The fact that&nbsp;<span>T0 \u22a2 &exist;n. &not;&psi;(n)&nbsp;</span>in this case means that T0 believes T is consistent. This is far nicer than T believing Z to be inconsistent.&nbsp;</em><em>I suspect that such a T0 can reason something like \"If T is consistent then so am I\", thus dooming itself, but I haven't had time to play with this yet.</em></p>\n<p><em>Update: This does fail. The actual proof path is T&nbsp;</em><em>\u22a2&nbsp;</em><em>\u25a1(</em><em>&forall;n. Tn</em>&rarr;<em>&perp;)</em>&rarr;(<em>&forall;n. Tn</em>&rarr;<em>&perp;), so by&nbsp;L&ouml;b's theorem T proves all such Tn inconsistent. In other words, if T could prove all Tn consistent then they would all know it, and would be inconsistent: and so, by&nbsp;L&ouml;b's theorem, they are.</em></p>\n<h3 id=\"parametricpolymorphism\">Parametric Polymorphism</h3>\n<p>The trick that made the above approach work was that we gave the theory a trust condition that (from the perspective of the theory) \"might run out\". Only from outside the theory do we know that the trust will not run out.</p>\n<p>The second approach &mdash; parametric polymorphism &mdash; makes that technique somewhat more explicit.</p>\n<p>In parametric polymorphism, we require that the theory T under consideration can be modeled by the natural numbers. (This gives us some nice properties that we'll need to use for the consistency proof.)</p>\n<p>We start by adding a new term 'k' to our logical language. This k, the \"number of ineffable mystery\", will be represented by a number (when N is the model), but T has no knowledge about k.</p>\n<p>Remember that the goal is to have something like <span>\u25a1[&phi;] &rarr; &phi;</span> (though we can never achieve this). Instead, we add an axiom that <span>(k &gt; 0) &rarr; \u25a1Tk[&phi;] &rarr; &phi;[k\\k-1]</span>. In other words, <em>so long as k is positive</em> a proof in Tk of &phi; implies that &phi; is true after replacing all k by k-1.</p>\n<p>T, in general, cannot prove that Tk[&phi;]&rarr;&phi; even for &phi; that do not mention k, becuase Tk doesn't know whether k is positive. However, it can prove that <em>when</em> Tk is nonzero it can believe &phi; \"at the next k-step\". Proofs in Tk will need numerous guards concerning the positivity of Tk, but <em>in the positive branches</em> it will be able to trust Tk proofs: and outside the system, we know that the positive branches will always be the ones taken.</p>\n<p>Before we get excited, we have to check whether Tk is consistent. As a matter of fact, it is. We can show this by presenting a model of Tk: this we can do trivially. {N, k=0} (the naturals, with k modeled as 0) is trivially a model of Tk: N is a model of T, and when k=0 the new axiom schema is trivially true.</p>\n<p>Models where k=0 are not exactly useful to us, though: in these models, the k=0 branches of the proofs are taken, and Tk never achieves self-trust. What we want is to show that there are models of Tk where k is arbitrarily large. We can prove this via induction.</p>\n<p>We've already got a base case: {N, k=0} is a model of Tk. For the induction step, we need to show that if {N, k=n} is a model of Tk then {N, k=n+1} is a model of Tk. To show this, note:</p>\n<ol>\n<li>If {N, k=n+1} \u22a7 \u25a1Tk[&phi;] then {N, k=n} \u22a7 \u25a1Tk[&phi;]</li>\n<li>If {N, k=n} \u22a7 \u25a1Tk[&phi;] then {N, k=n} \u22a7 &phi;</li>\n<li>If {N, k=n} \u22a7 &phi; then {N, k=n+1} \u22a7 &phi;[k\\k-1]</li>\n</ol>\n<p>(1) is true because \u25a1Tk has no access to k.</p>\n<p>(2) is true because if \u25a1Tk[&phi;] is a proof <em>modeled</em><em>&nbsp;a standard model</em> then &phi; is semantically derivable within Tk by a <em>standard</em>&nbsp;(ie classical) proof. Whenever &Gamma; &rarr; &Delta; via a classical proof, all models of &Gamma; are also models of &Delta;. Since {N, k=n} is a model of Tk, it is also a model of all things provable in Tk via standard proof.</p>\n<p>(3) is trivial by substitution.</p>\n<p>We can extend these points to cover formulas free in one variable by noting that, because we are dealing with N as our model, every term x corresponds to a finite constant c.</p>\n<p>Combining these three points we get that if {N, k=n+1} \u22a7 \u25a1Tk[&phi;] then {N, k=n+1} \u22a7 &phi;[k\\k-1], which completes the induction step.</p>\n<p>We conclude that {N, k=n} is a model of Tk for any n.</p>\n<p>We can use this to construct tiling agents. The proof is technical (there's a bit of finagling to do because when the child has proven an action safe for k steps, the parent concludes that it is safe for k-1 steps). The proof in the paper still has a number of typos. (See the end of the post.) I won't go over the proof in detail here. It suffices to say that when the child proves an action safe for k steps, the parent concludes that it's safe for at least k-1 steps and thus (conditional upon k being positive) A1 can construct A0. Outside the system, we know that k can be arbitrarily large, and so conclude that the system is safe for an arbitrary number of steps.</p>\n<p>This system, like the last, would not be the best choice as a belief-system for a rational agent. It's not clear what k represents to the agent, nor what it would mean for a rational agent to \"actually believe\" Tk. As before, Tk is not presented as a realistic candidate for the mental system of a real agent. Rather, it is presented as yet another technical victory over the L&ouml;bstacle.</p>\n<p>With that done, we can talk about self-modifying agents on a more general level, and will do so in the next few sections.</p>\n<h2 id=\"probabilisticreflection\">Probabilistic Reflection</h2>\n<p>The paper mentions the <a href=\"http://intelligence.org/wp-content/uploads/2013/03/Christiano-et-al-Naturalistic-reflection-early-draft.pdf\">Probabilistic Reflection</a> results, summarizes them, and notes that we don't yet have a formalism for agents using a probabilistic reflection principle. (We don't even yet know what knowledge of probabilistic reflection can appear inside a probabilistically reflective system).</p>\n<p>A few days ago I did a that&nbsp;<a href=\"/lw/jbe/walkthrough_of_definability_of_truth_in/\">write-up of that paper</a>, which may be helpful if you're looking for more walkthroughs such as this one.</p>\n<p><span style=\"font-size: 2em;\">What Self-Modifying Agents Need</span></p>\n<p>There are three principles that we really want our self-modifying agents to adhere to. I'll summarize these sections briefly, as they are not difficult to grasp. See the paper for more.</p>\n<h3 id=\"thevingeanprinciple\">The Vingean principle</h3>\n<p>Agents shouldn't have to predict precisely what their children do. If the parent knows precisely what the child will do &mdash; for example, by proving which action the child will take &mdash; then the child is hardly 'smarter' than the parent. In our logical systems, we follow the Vingean principle by requiring that the parent only discuss the child's actions within quantifiers. We can have a set representing the child-actions, and we can quantify over that set, but we can't have particular child-actions known ahead of time.</p>\n<p>The parent may verify properties about all&nbsp;actions, e.g. by looking at the action system and verifying that the motors only activate when presented with a valid proof. The parent may not e.g. simulate the child's exact action.</p>\n<h3 id=\"thetilingprinciple\">The tiling principle</h3>\n<p>Agents should be able to approve the construction of agents similar to themselves. This is what we've been searching for formalisms to allow in the above sections. This portion of the paper also discusses a formalism where the agent is divided into an untrusted proof suggestion component &Sigma; and a trusted proof verification component &Delta;. This has some nice properties (especially in monotonic logic, where things that &Sigma; knows but isn't telling &Delta; can't cause &Delta; to allow something it shouldn't), but would quickly break down outside the realm of logic.</p>\n<h3 id=\"thenaturalisticprinciple\">The naturalistic principle</h3>\n<p>Agents should think of themselves as part of the world. There shouldn't be a Cartesian boundary between \"world-stuff\" and \"mind-stuff\". Otherwise, you have to deal with weird boundary problems (attaching a new CPU requires things to cross the world/mind boundary).</p>\n<p>Furthermore, having different rules for internal actions and external actions is not particularly safe. For example, if certain internal modifications are prohibited an agent could build an external thing that happens to apply the desired internal changes.</p>\n<p>But <em>if you do</em> allow a cartesian boundary then the L&ouml;bstacle is easy to avoid. You simply set as your criterion of action to be</p>\n<pre><span>&psi;(action) =</span></pre>\n<pre style=\"padding-left: 30px;\"><span style=\"font-size: 10px;\">(externalOK(action) &rarr; (@action &rarr; G)) &and;</span></pre>\n<pre style=\"padding-left: 30px;\"><span style=\"font-size: 10px;\">(internalOk(action) &rarr; (@action &rarr; (&forall; action' in Acts. @action' &rarr; \u25a1[&psi;(action')])))</span></pre>\n<p>Translation: \"The condition for action is this: [when action is externalOK, if we execute it then it achieves the goal] and [when action is internalOk, if we execute it then it leaves us in a state where we only take actions that meet this condition].\"</p>\n<p><em>Note: There is a bit of ambiguity here. I believe that '<span>Acts'</span>&nbsp;is meant to refer to the new action-set available after taking the internal action.</em></p>\n<p>This is similar to our first failed solution, except that the Cartesian boundary allows &psi; to avoid becoming a Henkin sentence.</p>\n<p><em>Note: On gut instinct, I'd consider exploring <span>&psi;(x) = @x &rarr; (G &or; (&forall;y &isin; ActsX. @y &rarr; \u25a1[&psi;(y)]))</span>, which is a version of the above that does not have a&nbsp;Cartesian&nbsp;boundary and does not immediately succumb to the Henkin-sentence problem.</em></p>\n<h1>Sensors</h1>\n<p>Then we try to add sensors, and everything blows up.</p>\n<p>That's not exactly the case, but adding sensors and unknown variables certainly doesn't make the formalism any prettier.</p>\n<p>Section six is long and somewhat involved, though it's saying something essentially simple. I'll summarize:</p>\n<p>We want to allow our agents to have sensors, and indeed construct new sensors. We'll say things like</p>\n<pre><span>A0 \u22a9 @sl &rArr; @Sl\n</span></pre>\n<p>to mean \"A0 believes @sl only when sensor S takes on the state @Sl\".</p>\n<p>This causes a trust problem. We still want A1 to trust A0, despite the fact that A0 will have access to sensor data that A1 knows not. We can patch our trust framework by changing A0's criterion of action to be</p>\n<pre><span>@bi &rArr; A0 \u22a9 (@sl &and; @bi) &rarr; (&empty; &or; G)\n</span></pre>\n<p>(If bi is taken then A0 believes that, with the sensors in state sl, taking the action bi leads to either &empty; or G). Then, if this is known for every possible combination of sensor states, A1 can retain trust in A0.</p>\n<p>This is messy, though. Additional sensors lead to exponentially more computing power required (on A1's part) to trust A0. This seems pathological. What we <em>want</em> is for A1 to reason that, since it can verify that A0 \u22a9 @sl only when @Sl (i.e. A1 trusts that the sensors take on state sl only when the world is actually in state Sl), anything that A0 does based on the sensor data is trusted.</p>\n<p>And we can get this property &mdash; or something like it. However, this means that A1 must be able to express things like \"A0's reasoning is such that, if we add only \"true\" sensor-data, it does not disturb our trust in A0.\" So A1 needs to not only trust A0, but must also trust A0 after A0 has had new \"true axioms\" added in the form of (trusted) sensor data.</p>\n<p>Formalizing this requires full-on set theory, which can formalize things like \"In all models X, all logical tautologies hold\".</p>\n<p>We then create a language in which there is a special set &tau; (the \"territory\") encoding a model of the environment. We then express A1's trust as follows:</p>\n<pre><span>A1 \u22a9 @A0 &rarr; &forall;[&phi;]. (&tau; \u22a7&tau; [A0 \u22a9 \"&phi;\"]) &rarr; (&tau; \u22a7&tau; [&phi;])\n</span></pre>\n<p>Translation: A1 believes that, if it constructs A0, then for all quoted sentences <em>about the territory</em>, if it is true <em>in the territory</em> that A0 believes some A0-encoding of &phi;, then it is actually true in the territory that &phi;.</p>\n<p>Notice the two levels of quoting. A1 believes that A0's beliefs (<span>\"&phi;\"</span>) correspond to statements true in the territory (<span>[&phi;]</span>).</p>\n<p>This loosely represents how a programmer might reason about a program: A programmer (A1), who thinks that there is an object (&phi;) in the environment, will have an internal representation (<span>[</span>&phi;<span>]</span>) of the object. They will also know that the program (A2) has an object corresponding to Fred (<span>\"</span>&phi;<span>\"</span>), and they will retain a mental map between their representation of the object (<span>[</span>&phi;<span>]</span>) and their representation of the program's representation of the object (<span>[\"</span>&phi;<span>\"]</span>).</p>\n<p>The point is that we deploy set theory to represent this sort of logic. This logic, however, allows us to reason about sensors thusly (replacing \u22a7&tau; with \u22a7):</p>\n<pre><span>A1 \u22a9 @A0 &rarr; &forall;[sl]. (&tau; \u22a7 [A0 \u22a9 \"@sl\"]) &rarr; (&tau; \u22a7 [@Sl])\n</span></pre>\n<p>Which basically says \"A0 only believes @sl when, in reality, the sensor actually is in state l.\"</p>\n<p>This is what we wanted, but it does not come cheap: We had to deploy full ZF in order to allow A1 to reason as we wanted. Worse, A2 (the parent of A1) must reason in ZF+1 in order to trust A1 (because it must believe that there is some model of ZF: otherwise A1 is meaningless). A3 must reason is ZF+2, and so on.</p>\n<p>It is noted that this can probably be addressed via parametric polymorphism (ie A2 uses ZFk, and also trusts A1 to use ZFk), though no proof has been written out yet. A simplification of the above mechanism (without resorting to set theory, or at the least using a weaker set theory) is desired.</p>\n<h1>Probability and Expected Utility</h1>\n<p>Adding expected utility satisfaction is somewhat difficult.</p>\n<p>Probabilistic reasoning is non-monotonic. This leads to difficulties. For example, if you have an untrusted proof-suggester &Sigma; and a proof-verifier &Delta;, then so long as &Delta; works you can't go wrong &mdash; in logic. At worst, &Sigma; withholds a good proof and the system does nothing. But with probabilistic reasoning, &Sigma; can withhold information from &Delta; with catastrophic results.</p>\n<p>For example, imagine that the condition of action is <span>E[U] &ge; $20</span>, where E[U] is the expected utility. If we have a fair coin and a bet with payout of $50 on H and $10 on T, then the agent will take the bet. But if we later learn that the coin landed T, we should no longer take the bet: new information can invalidate actions that otherwise would have been taken. If the suggestion-device is untrusted, this can be particularly dangerous, because &Sigma; can withhold information maliciously.</p>\n<p>We can hack our way around this problem by requiring that every probabilistic variable be observed by A0 before it may choose an action. A1, knowing this, can proceed to trust A0. A formalization of this process is given on pages 35-36. The result is a technical success &mdash; the parent can still trust the child so long as the child observes all data before acting &mdash; but this formalism falls short on many counts.</p>\n<p>For example, the given formalism cannot assign prior probabilities to variables. (In the given formalism, probabilistic variables are left unassigned until the observations comes in. If the system tried to assign <span>&nu;=0.7</span> as a prior probability and later assigned <span>&nu;=1</span> according to sensor data, the system would arrive at the contradictory <span>0.7=1</span>.)</p>\n<p>This is obviously a very rudimentary formalization of utility satisfaction, and there is much work to be done.</p>\n<h1>Utility Management</h1>\n<p>The above section dealt with utility <em>satisfaction</em>, not utility maximization. This was intentional, as perfect utility maximization is impossible in all but the most trivial of situations.</p>\n<p>Utility satisfaction, however, is far from ideal. To take an example from the paper, consider an agent built to satisfy at least E[U]=2*10^15. If it finds an action that guarantees E[U]=10^16, then the agent may decide to randomize between a 30% probability of taking the 10^16, a 60% probability of doing nothing, and a 10% probability of burning 10^16 utils. This is&hellip; not ideal, to say the least.</p>\n<blockquote>\n<p>You get what you pay for, no more than you asked for, and exactly what you proved.</p>\n</blockquote>\n<p>An alternative means of utility management is a \"meliorizing\" criteria of action, wherein an agent picks an action and then decides whether to swap its current action for another as opportunities arise. This strategy is not without its flaws. To lift another example from the paper, imagine an agent built to save humanity from a deadly asteroid. Imagine it starts with the default action of \"do nothing\". If the first action it comes across is \"immediately take an action which will save 1000 humans, then shut down\", it will take that action and then shut down &mdash; forgoing the opportunity to save billions of human lives. Also not ideal.</p>\n<p>That said, meliorization at least shows a bit of promise, whereas satisficing is untenable and maximization is unrealistic. It's worth noting, though, that we don't even understand &mdash; not even at the highest possible level &mdash; how a probabilistic rational agent should go about pursuing its goals. This is the paper's ending note:</p>\n<blockquote>\n<p>The problem is very wide open indeed.</p>\n</blockquote>\n<h1>Edits</h1>\n<h3 id=\"typos\">Typos</h3>\n<p>I'm pretty confident about most of these, but some may be due to a misunderstanding, and are suffixed with (?).</p>\n<ul>\n<li>p10. &psi; &harr; &phi;(x) &or; \u25a1&tau;[&psi;(x)], this should be &psi;(x) &harr; &hellip; (?)</li>\n<li>p17. (4.13) The leading A_t^0 has a stray 't' after it.</li>\n<li>p17. (4.14) is missing a b_{i,u}\\bar inside the \u25a1_0, before the \"&rarr; &forall;&upsilon;\".</li>\n<li>p17. (4.14) is missing a b_{i,t}\\bar before the \"&rarr; &forall;&nu;\".</li>\n<li>p18. (4.23) there is a K that should be a &kappa;.</li>\n<li>p18. (4.24) there is an extraneous (A_t^0 &rarr;) after the \u22a9</li>\n<li>p18. (4.24) there should be a -1 after &forall;&nu;&le;&upsilon;+&kappa;. (Alternatively, change the &le; to &lt;) (?)</li>\n<li>p18. (4.25) there is an extraneous (A_t^0 &rarr;) after the \u22a9</li>\n<li>p18. (4.25) there should be a -1 after &forall;&upsilon;&le;t+&kappa;. (Alternatively, change the &le; to &lt;) (?)</li>\n<li>p18. (4.26) there should be a -1 after &forall;&upsilon;&le;t+&kappa;. (Alternatively, change the &le; to &lt;) (?)</li>\n<li>p28. \"If we were in the shoes of A1 building A1 &hellip;\" should be \"&hellip; building A0 &hellip;\".</li>\n<li>p29. \"&hellip; it should not disturb our trust in A1.\" should be \"&hellip; in A0.\"</li>\n<li>p31. (6.9)&nbsp;(&tau; \u22a7 @A0) should be just @A0 (?)</li>\n</ul>\n<h3 id=\"nitpicks\">Nitpicks</h3>\n<ul>\n<li>p10. \"Either x implies (&empty; &or; G), or &hellip;\" should read \"Either the execution of x implies (&empty; or G), or &hellip;\"</li>\n<li>p13. You use &equiv; to introduce &psi; and \u225c to introduce T-n. Later (p27) you use = to introduce a different &psi;. Are these differences intentional?</li>\n<li>p19. Using P() to denote the function P seems somewhat strange. The probabilistic reflection paper simply calls it P. Other texts I've read use the style P(&mdash;). I haven't seen the style P() before. (This may just be inexperience.)</li>\n<li>In infinite descent, why do the theories walk downwards? Why not use T0, T1, T2, &hellip;? Walking downwards causes a bit of awkward syntax (T-(n+1)) that seems unnecessary.</li>\n</ul>\n<p>And while I'm on the subject, it felt foreign that A0 is the <em>youngest</em> agent. I would expect A0 to be the oldest agent. In one sense, I appreciated that A0 was the youngling: it lends itself well to the idea that we have this gigantic tower of agents building agents building agents, and then at the very end of the tower we have the final agent that affects the environment (which is why A0 is special). However, one of the nice things about this formalism is that it doesn't really treat environment-actions differently from agent-creating-actions, so we don't&nbsp;<em>need</em> this distinction. I found it counter-intuitive to consider the \"end\" of the tiling schema, preferring to think of it from the other direction: we start with an agent running on trusted math (agent zero) and let it tile indefinitely.</p>\n<p>All in all, it's not a particularly bad choice, it just felt unnatural to me.</p>\n<h3 id=\"structure\">Structure</h3>\n<p>Section 5.2 seems like a non-sequitur. 5.1 and 5.3 discuss principles that agents should have. By contrast, 5.2 mentions tiling and gives examples of how trust can be decomposed. It was weird to be introduced to tiling after spending section 4 proving that tiling is possible while preserving trust. Also, the discussion about decomposition of trust didn't feel at home in a section detailing the tiling principle.</p>\n<p>I'd recommend moving section 5 up to before section 3 or possibly before section 2. This would give us a better idea of what we're looking for in self-modifying agents before we are introduced to the obstacles. I'd also suggest breaking \"decomposition of trust\" out of the \"tiling principle\" section. 5.2 feels like it's trying to introduce one concept while explaining another.</p>\n<h3 id=\"confusions\">Confusions</h3>\n<ul>\n<li>p17. Syntax question: Why are (4.15) and (4.17) indented more than everything else?</li>\n<li>p27. I'm confused about the \"split decision criterion\". <span>Acts</span> denotes the set of actions that are available after the execution of the (internalOk) action <span>act</span>, yes?</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ksdiAMKfgSyEeKMo6": 1, "6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QGrX3qK3qxQYK9D4C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 28, "extendedScore": null, "score": 7.4e-05, "legacy": true, "legacyId": "25066", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is my walkthrough of&nbsp;<a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the L\u00f6bian Obstacle</a>. It's meant to summarize the paper and provide a slightly-less-technical introduction to the content. I've also collected a number of typos and suggestions, which can be found at the end of the post.</p>\n<p><span style=\"font-size: 2em;\">Motivation</span></p>\n<p>We want to be able to consider agents which build slightly better versions of themselves, which build slightly better versions of themselves, and so on. This is referred to as an agent \"tiling\" itself. This introduces a question: how can the parent agent trust its descendants?</p>\n<p>We'll be dealing with logical systems, which are crisply deterministic. In such a setting, child agents can be \"absolutely trustworthy\": for example, if the child uses the same methods of proof and the same axioms as the parent, then the parent should be able to trust the child unconditionally: or, at least, the parent should trust the child at least as much as it trusts itself.</p>\n<p>This turns out to be a huge problem, because modern logical systems <em>don't</em>&nbsp;trust themselves (or any logical systems of equal strength), to say nothing of (supposedly more powerful) successor systems.</p>\n<p>This problem of trust is rooted in a theorem stated by the mathematician L\u00f6b.</p>\n<p>Assuming that we can surmount such difficulties, there are a number of other principles that we want in our tiling agents, such as the Vingean principle (the parent shouldn't have to know exactly what the children will do) and the Naturalistic principle (agents should think of themselves as part of the world). These will be discussed in detail later in the paper.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"Glossary\">Glossary</h1>\n<ul>\n<li>I'll use <span>@x</span> to mean \"the execution of x\". (The paper uses x\u0304 instead.)</li>\n<li>I'll use <span>[x]</span> denotes the G\u00f6del number of <span>x</span> in some suitable encoding. (The paper uses &nbsp;\u2308x\u2309 instead).</li>\n<li><span>\u25a1T[x]</span> is read \"There is a proof of x in T\". It's the same as \"<span>\u2203y. Prooves_T(y, [x])\"</span>. The T is omitted when it's clear from the context.</li>\n<li>A0, A1, \u2026 will denote agents. Note that A0 is the <em>child</em> and A1 is the <em>parent</em>. I personally would have done things the other way around, but I'll stick to the paper's descending style.</li>\n<li>G will be used to denote a world-state in which the agent's goals have been achieved.</li>\n<li>\u2205 will be used to denote a world-state wherein nothing of import has been changed by the agent. For more specificity, see the paper.</li>\n<li><span>A \u22a9 x</span> is read \"A has cognitively concluded x\" or \"A believes x\".</li>\n<li>PA is Peano Arithmetic, ZF is Zermelo-Fraenkel set theory.</li>\n<li><span>T</span> will denote a theory. In general, assume that T is at least as powerful as PA.</li>\n<li><span>T+1</span> is <span>T \u222a {\u00ac\u25a1T[\u22a5]}</span> (T plus the fact that T is consistent). T+2 is (T+1)+1, etc.</li>\n</ul>\n<p>Here's a few reminders, just in case you're rusty:</p>\n<ul>\n<li>\u03c6[x\\y] denotes \"\u03c6, with all instances of <span>x</span> replaced by <span>y</span>\". The slash disambiguates this from quoting.</li>\n<li><span>\u225c</span> means \"is defined to be\".</li>\n<li><span>T \u22a2 \u03c6</span> is pronounced \"T derives \u03c6\".</li>\n</ul>\n<p>So, for example, <span>T \u22a2 \u25a1T[\u03c6]</span> is read \"T knows that there is a T-proof of \u03c6.\"</p>\n<p>In general, I'll be less formal than the paper. If I've left any ambiguities, see the paper for clarification.</p>\n<h1 id=\"Trusting_Yourself\">Trusting Yourself</h1>\n<p>Imagine that we've got a parent agent A1. It wants to achieve the goal G. Among the things it can do is create more agents. What we want to do is find an agent A0 within the actions of A1 such that:</p>\n<pre><span>A1 \u22a9 @A0 \u2192 (\u2200action \u2208 Acts0. @action \u2192 \u25a10[@action \u2192 (\u2205 \u2228 G)])\n</span></pre>\n<p>Translated, this reads \"A1 believes that, if it constructs A0, then for all actions available to A0, that action will only be taken if A0 has proven that taking the action leads to either nothing of import happening, or the goal being achieved.\"</p>\n<p>(We also require <span>A1 \u22a9 @A0 \u2192 ((\u2200action \u2208 Acts0. \u00ac@action) \u2192 \u2205)</span>, which says \"if A0 takes no actions then nothing will happen\".)</p>\n<p>We would further like A1 to be able to trust A0. Obviously, we don't want A1 to trust its children unconditionally. But assuming that A0's proofs use the same mechanism as A1's proofs, and assuming that A1 verifies that A0's proof system works <em>exactly</em> like A1's own proof system, we would like to have the following:</p>\n<pre><span>A1 \u22a9 @A0 \u2192 (\u2200action \u2208 Acts0. @action \u2192 (\u2205 \u2228 G))\n</span></pre>\n<p>ie, A1 believes that, if it constructs A0, then any action taken by A0 <em>actually will</em> either cause nothing to happen or achieve the goal.</p>\n<p>This is an important distinction, make sure you understand it before proceeding. We want A1 to <em>unwrap</em>&nbsp;A0's proofs, to look at things inside the&nbsp;\u25a10 and conclude that they actually are true. We want A1 to look at&nbsp;\u25a10[\u03c6] and actually conclude \u03c6.</p>\n<p>Unfortunately, this is impossible&nbsp;\u2014 at least, it's impossible when A1 has concluded that A0's proof system is identical to A1s proof system. Why? Because A1 can't even trust its own formalism. This is the famed L\u00f6b's theorem.</p>\n<h1 id=\"The_L_bstacle\">The L\u00f6bstacle</h1>\n<p>Intuitively, L\u00f6b's theorem claims \"No system T can trust its own proofs until it witnesses them.\" More formally, it states that in a theory T at least as powerful as PA, <span>\u25a1T[\u03c6] \u2192 \u03c6 \u21d4 \u03c6</span>: T can only deduce \u03c6 from a T-proof of \u03c6 if \u03c6 is actually true.</p>\n<p>You might expect that there are <em>some</em> sentences \u03c8 which T cannot prove, but for which T knows \"if I <em>could</em> prove \u03c8, then \u03c8 would be true\". This is not the case! There are <em>no</em> sentences that T cannot prove, for which T will trust \"but <em>if I could prove it</em>, then it would be true\". T does not trust its own proofs until it witnesses them. Here's why:</p>\n<p>Any T of sufficient power can define a sentence <span>L \u2194 \u25a1[L]\u2192\u03c6</span> (L is true iff a proof of L implies \u03c6.) T can deduce the following about such an L for&nbsp;<em>any&nbsp;</em>\u03c6.</p>\n<ol>\n<li><code>L \u2192 (\u25a1[L]\u2192\u03c6)</code> (from the definition of L)</li>\n<li><code>\u25a1[L\u2192(\u25a1[L]\u2192\u03c6)]</code> (from 1 because T can prove the things it knows: <span>T \u22a2 x \u21d2 T \u22a2 \u25a1[x]</span>)</li>\n<li><code>\u25a1[L]\u2192\u25a1[\u25a1[L]\u2192\u03c6]</code> (from 2 because provability distributes over implication: <span>T \u22a2 \u25a1[x\u2192y] \u21d2 T \u22a2 \u25a1[x]\u2192\u25a1[y]</span>)</li>\n<li><code>\u25a1[L]\u2192(\u25a1[\u25a1[L]]\u2192\u25a1[\u03c6])</code> (from 3 because provability distributes over implication)</li>\n<li><code>\u25a1[L]\u2192\u25a1[\u25a1[L]]</code> (because T can prove things it knows)</li>\n<li><code>\u25a1[L]\u2192\u25a1[\u03c6]</code> (from 4 and 5)</li>\n</ol>\n<p>Note that \u03c6 here is general. This means that for <em>any</em> \u03c6, there is a sentence L such that \u03c6 is provable when L is provable, and also \u03c6 is true when L is provable. Now, <em>if</em> T had <span>\u25a1[\u03c6]\u2192\u03c6</span> then:</p>\n<ul>\n<li><code>\u25a1[\u03c6]\u2192\u03c6</code> (assumption)</li>\n<li><code>\u25a1[L]\u2192\u03c6</code> (from 6 and assumption)</li>\n<li><code>L</code> (<span>L \u2194 \u25a1[L]\u2192\u03c6 by definition,</span>&nbsp;and we just deduced <span>\u25a1[L]\u2192\u03c6</span>)</li>\n<li><code>\u25a1[L]</code> (T can prove things it knows)</li>\n<li><code>\u03c6</code></li>\n</ul>\n<p>We have now proved <span>\u03c6</span>&nbsp;<em>in general</em>: such a T can prove anything. Notice what this means: any system of sufficient strength can derive a proof of any \u03c6 <em>from the mere fact that if provable then it is true.</em></p>\n<p>(Fun fact: pick <span>\u03c6=\u22a5</span> to get a proof of the second incompleteness theorem. <span>\u00ac\u25a1[\u22a5]</span> means <span>\u25a1[\u22a5]\u2192\u22a5</span>, so if you can prove <span>\u00ac\u25a1[\u22a5]</span> (that you don't prove a contradiction) then you prove \u22a5 (a contradiction).)</p>\n<p>Conclusion: A system cannot trust, in general, that \u25a1[\u03c6] implies \u03c6. It can't know that its proofs are valid.</p>\n<p>This is going to make our lives difficult: our tiling agents will be generating situations where the parent agent verifies \"my child will only execute actions that they've proven (in T) lead to good outcomes\". By L\u00f6b's theorem, such agents, running on T, <em>cannot</em> conclude \"and so my child, also running on T, will only execute actions that lead to good outcomes\" \u2014 because it cannot trust its own proof system.</p>\n<h1 id=\"Finite_Descent_Problems\">Finite Descent Problems</h1>\n<p>A system <em>can</em> prove weaker systems consistent. For example, T+1 can prove T consistent, and T+2 can prove T+1 consistent, etc. You might think that we can work around the problem by having our initial system run on T+n for large n. But this fails.</p>\n<p>First of all, if you pick a large finite n then your system has a finite \"telomere\", a trust-capacity that is running out. And even if you try to start with T+\u03c9, it has to bequeath unto its child system a T+n where n is an ordinal less than \u03c9. Though such an n may be arbitrarily large, it must be finite, and then we have the finite fuse again. In general, no matter how infinitely large an ordinal you start with, it will still suffer from a finite descent problem (because the ordinals are well-ordered).</p>\n<p>And secondly, T+1 is far more powerful than T. Take, for example, the <a href=\"http://math.andrej.com/2008/02/02/the-hydra-game/\">hydra game</a>. It turns out (spoiler alert) you can't lose: any strategy beats every hydra.</p>\n<p>Now consider PA. Given an h denoting the height of the hydra, PA can prove that any strategy beats hydras of height h. But PA <em>cannot</em> conclude that any strategy beats every hydra. Letting <span>\u03c8(h)</span> denote \"hydras of height h can be killed by any strategy\", it is the case that for all h,<span>&nbsp;PA \u22a2 \u03c8(h)</span>. But PA does NOT derive <span>\u2200h. \u03c8(h)</span>! PA cannot rule out the possibility that there is <em>some</em> height at which its proofs will fail: it only trusts proofs that it <em>witnesses</em>.</p>\n<p>PA+1, on the other hand, has <span>PA+1 \u22a2 \u2200h. \u03c8(h)</span> because it knows that PA is consistent, and thus that there is no height at which PA's proofs start failing. Thus, in a very real sense, there are problems (like the hydra problem, or like checking whether a Goldstein sequence terminates) that can be solved in PA+1 but not PA.</p>\n<p>In fact, an agent's strength is directly related to which ordinals it can well-order: the ability to well-order high ordinals directly translates into mathematical power. Child agents running on weaker mathematical systems correspond to a very real loss in capabilities.</p>\n<p>Having each agent build a weaker child is untenable, and by L\u00f6b's theorem we can't have the child agent run on the same system as the parent (because the parent can't trust its own system). Yet we demand away to tile agents without having their trust drop below a certain baseline. Is this even possible? Experience with G\u00f6delian problems may lead you to think that the answer is \"No\", but as we will see, we can gain a few technical victories.</p>\n<h1 id=\"Failed_Solutions\">Failed Solutions</h1>\n<p>Here are a few ideas to get things rolling:</p>\n<h3 id=\"Recursive_Diving\">Recursive Diving</h3>\n<p>Define <span>Safe(x)</span> to be <span>(@x -&gt; \u2205 \u2228 G)</span>: in other words, <span>Safe(x)</span> means \"executing x leads to either no change, or the goal being achieved.\"</p>\n<p>Instead of requiring the agent to know \u25a1[Safe(x)] \u2192 Safe(x), we instead define a proxy for safety. This proxy is <span>\u03c8(x) \u2194 Safe(x) \u2228 \u25a1[\u03c8(x)]</span>. Then the condition of action is <span>\u03c8(x)</span>.</p>\n<p>The idea is that instead of requiring the system to accept that a proof of safety makes things safe, we instead ask it to either know that an action is safe, or be able to prove \u03c8 about the action (which means either we can prove the action safe or we can prove we can prove \u03c8 about the action (which means either we can prove we can prove the action safe, or ...)). Na\u00efvely, you might expect that the only way out of this recursive dive is for the system to accept Safe(x) at some depth of proof. But that's not what happens.</p>\n<p>In fact, \u03c8(x) is accepted immediately, regardless of the safety of the action. \u03c8 is in fact a Henkin sentence: It is of the form <span>\u03c8(x) \u2194 \u25a1[\u03c8(x)]</span> (with a dangling <span>\u2228 Safe(x)</span> that we can ignore). Henkin sentences are universally true. Our system can ignore the safety constraint, skip the recursive descent, and conclude any action is OK. This makes it\u2026 rather unsuited to the real world.</p>\n<h3 id=\"Buck_Tossing\">Buck Tossing</h3>\n<p>Instead of requiring \u25a1T[\u03c6] \u2192 \u03c6, we construct a tower of theories. Each theory trusts the next theory, but not itself. So T0 has <span>\u25a1T1[\u03c6] \u2192 \u03c6</span>, and T1 has <span>\u25a1T2[\u03c6] \u2192 \u03c6</span>. This allows parents to trust their children without ever trusting themselves (and thus becoming inconsistent.)</p>\n<p>Problem is, if T0 is as strong as PA then it can deduce that proofs in T0 are valid in T1: it just bumps all the appropriate 'n' up by 1. Then T0 deduces that \u25a1T0[\u03c6] \u2192 \u25a1T1[\u03c6], from which it deduces \u03c6. Triumphant, T0 proudly concludes \u25a1T0[\u03c6]\u2192\u03c6, and immediately bursts into inconsistent flames.</p>\n<h3 id=\"Proof_Stacking\">Proof Stacking</h3>\n<p>It doesn't matter whether T trusts itself. WE trust T. If Safe(x), or T proves Safe(x), or T proves that T proves Safe(x), or if there is <em>any</em> depth of proofiness such that Safe(x), then the agent may do x.</p>\n<p>Let \u25a1^k[x] stand for \"T proves that T proves that \u2026 (k times) \u2026 that Safe(x)\". (Assume k nonzero throughout.) Because this stacking has a regular structure, there should be a compact formula expressing this. So what if we make T's criterion of action be (Safe(x) \u2228 \u25a1^k[x])?</p>\n<p>Then, when a child concludes (Safe(x) \u2228 \u25a1^k[x]), the parent concludes that \u25a1[Safe(x) \u2228 \u25a1^k[Safe(x)]].</p>\n<p>Problem is, \u25a1[Safe(x) \u2228 \u25a1^k[Safe(x)]] doesn't reduce to (Safe(x) \u2228 \u25a1^j[Safe(x)]), no matter how much we want it to. This is because \u25a1^k[\u03c6] is actually a formula like (\u2203 k. StackOProofs(k, \u03c6)), and \u25a1[\u2203 k. StackOProofs(k, x)] does not reduce to (\u2203 j. StackOProofs(j, x)), precisely because T does not trust that \u25a1[\u03c6]\u2192\u03c6. So this technique doesn't really dodge the L\u00f6bstacle at all.</p>\n<hr>\n<p>Is it completely hopeless? No! Just mostly hopeless. We can, it turns out, achieve a few technical victories.</p>\n<h1 id=\"Technically_Solutions\">Technically Solutions</h1>\n<h3 id=\"Infinite_Descent\">Infinite Descent</h3>\n<p>Idea 2 above (buck-tossing) is reparable. The idea, you remember, is that T0 trusts T1, which trusts T2, etc. The problem was that T0 could deduce that, by bumping all the n in a proof, things true in T0 were true in T1. It could then \"lift\" its proofs into T1, prove itself consistent, and become inconsistent.</p>\n<p>This can be fixed, if we somehow find a way to allow Tn to trust Tn+1 without being able to trust that it can lift its own proofs upwards. For example, instead of having as an axiom of Tn that <span>Tn+1[\u03c6]\u2192\u03c6</span>, we could guard the axiom by a formula \u03c8. Thus, the new axiom becomes <span>\u03c8(n) \u2192 (Tn+1[\u03c6] \u2192 \u03c6)</span>.</p>\n<p>Now, if only we could find a \u03c8 such that \u03c8 is true for all n, and T can prove \u03c8(n) for each individual n, but such that T cannot prove \u2200n. \u03c8(n)\u2026</p>\n<p>And we can! We were just talking about such formulas earlier. We need a formula that T-0 can prove for every n, but that it can't prove it can prove for every in. (In other words, for all n we need&nbsp;<span>T \u22a2 \u03c8(n)</span>, but we also need that T can NOT derive&nbsp;<span>\u2200n. \u03c8(n)</span>.)</p>\n<p>In order to do this, we're going to need a theory <em>more powerful</em> than T: a theory Z such that T cannot prove Z consistent. (Why? We'll get to that later.) For concreteness, if T is PA then ZF is more powerful in the appropriate manner.</p>\n<p>Now, here's an example guardian formula: <span>\u03c8(n)\u2261\u00acProofZ(n, [\u22a5])</span> or, in english, \u03c8(n)=\"n does not encode a proof of contradiction in Z\".</p>\n<p>For every n, T0 can prove \u03c8(n). But T0 cannot prove that <span>\u2200n. \u03c8(n)</span>. Thus, it can no longer \"lift\" its proofs into the higher system: \"increasing all the n's\" is no longer a safe operation. T0 does not expect \u03c8 to hold forever.</p>\n<p>We can prove this system consistent. I'll sketch the proof; see the paper for details. We'll construct a sound system T+ that trusts T and reasons about T0. We'll show that T+ does not prove T0 inconsistent, and that if T0 were consistent then T+ would know.</p>\n<p>T+ is built from T, a new symbol \"TRUE\", and axioms stating both that T is consistent and TRUE is a truth predicate for T. We note that T+ is strong enough to reason about T0 and proofs in T0. T+ reasons as follows:</p>\n<p>Suppose T0 is inconsistent. Then \u25a1T0[\u22a5]. Consider the proof. It has a bad line. The bad line can't come from the axioms of TRUE, nor from the use of T, as those are trusted. Therefore, it must come from the special axiom, and be of the form <span>\u03c8(n) \u2192 (\u25a1Tn[\u03c6(x)] \u2192 \u03c6(x))</span>. In order for this to be the point of failure, \u03c8(n) must be true, \u25a1Tn[\u03c6] must be true, and \u03c6 must be false. Consider the Tn proof of \u03c6. Recurse: its first bad line must also come from a \u03c8(n) axiom, and its first bad line must come from a \u03c8(n) axiom, and so on and so forth. Thus, \u03c8(n) must be true for all n. So, <span>\u25a1T0[\u22a5]\u2192(\u2200n. \u03c8(n))</span>.</p>\n<p>This reasoning shows that T+ knows that if T0 is inconsistent then \u03c8 is true for all n. Furthermore, notice that if \u03c8 is true for all n then Z is consistent.</p>\n<p>If there were a proof of contradiction in T0, the proof could be fed to T+, and then T+ would be able to prove the consistency of Z.</p>\n<p>We've established that T (and thus T+) cannot prove the consistency of Z. Therefore, T0 is consistent.</p>\n<p>This is nice, but it's not without problems. For example, <span>T0 \u22a2 (\u2200n. \u03c8(n)) \u2192 \u22a5</span>: T0 knows that if \u03c8 is true for all n, then it can bump its proofs upwards. It follows that <span>T0 \u22a2 \u2203n. \u00ac\u03c8(n): T0 believes that some n encodes a proof of contradiction in Z.</span>&nbsp;Concretely, when T=PA and Z=ZF, T thinks that ZF is inconsistent. This is\u2026 not the type of behavior that we strive for in real world agents, to say the least.</p>\n<p>Also, this means that T0 has only nonstandard models: T0 is consistent, and thus has a model, but as we've seen, T0 only has models where T is consistent but ZF is inconsistent. This, too, is a hinderance.</p>\n<p>Regardless, the paper has shown that you can tile agents without letting their trust fall below a certain minimum. (All of T0, T1, T2, ... trust T.) This system doesn't trust itself, and thinks ZF is inconsistent, but we've still shattered the L\u00f6bstacle. Via <a title=\"Technically correct\" href=\"http://imgur.com/OldYeT6\" target=\"_blank\">technical tomfoolery</a>, we have a mathematical system that can trust a another system of the same power.</p>\n<p>This is a cause for hope.</p>\n<p><em>Question: What if we use <span>\u03c8(n)\u2261\u00acProofT(n,\u00ac\u25a1T[\u22a5])</span> (ie \u03c8(n) means \"n does not encode a proof of T's consistency\"). The existing proof of consistency fails because T+ proves T consistent, but can we prove T0's consistency by other means? The fact that&nbsp;<span>T0 \u22a2 \u2203n. \u00ac\u03c8(n)&nbsp;</span>in this case means that T0 believes T is consistent. This is far nicer than T believing Z to be inconsistent.&nbsp;</em><em>I suspect that such a T0 can reason something like \"If T is consistent then so am I\", thus dooming itself, but I haven't had time to play with this yet.</em></p>\n<p><em>Update: This does fail. The actual proof path is T&nbsp;</em><em>\u22a2&nbsp;</em><em>\u25a1(</em><em>\u2200n. Tn</em>\u2192<em>\u22a5)</em>\u2192(<em>\u2200n. Tn</em>\u2192<em>\u22a5), so by&nbsp;L\u00f6b's theorem T proves all such Tn inconsistent. In other words, if T could prove all Tn consistent then they would all know it, and would be inconsistent: and so, by&nbsp;L\u00f6b's theorem, they are.</em></p>\n<h3 id=\"Parametric_Polymorphism\">Parametric Polymorphism</h3>\n<p>The trick that made the above approach work was that we gave the theory a trust condition that (from the perspective of the theory) \"might run out\". Only from outside the theory do we know that the trust will not run out.</p>\n<p>The second approach \u2014 parametric polymorphism \u2014 makes that technique somewhat more explicit.</p>\n<p>In parametric polymorphism, we require that the theory T under consideration can be modeled by the natural numbers. (This gives us some nice properties that we'll need to use for the consistency proof.)</p>\n<p>We start by adding a new term 'k' to our logical language. This k, the \"number of ineffable mystery\", will be represented by a number (when N is the model), but T has no knowledge about k.</p>\n<p>Remember that the goal is to have something like <span>\u25a1[\u03c6] \u2192 \u03c6</span> (though we can never achieve this). Instead, we add an axiom that <span>(k &gt; 0) \u2192 \u25a1Tk[\u03c6] \u2192 \u03c6[k\\k-1]</span>. In other words, <em>so long as k is positive</em> a proof in Tk of \u03c6 implies that \u03c6 is true after replacing all k by k-1.</p>\n<p>T, in general, cannot prove that Tk[\u03c6]\u2192\u03c6 even for \u03c6 that do not mention k, becuase Tk doesn't know whether k is positive. However, it can prove that <em>when</em> Tk is nonzero it can believe \u03c6 \"at the next k-step\". Proofs in Tk will need numerous guards concerning the positivity of Tk, but <em>in the positive branches</em> it will be able to trust Tk proofs: and outside the system, we know that the positive branches will always be the ones taken.</p>\n<p>Before we get excited, we have to check whether Tk is consistent. As a matter of fact, it is. We can show this by presenting a model of Tk: this we can do trivially. {N, k=0} (the naturals, with k modeled as 0) is trivially a model of Tk: N is a model of T, and when k=0 the new axiom schema is trivially true.</p>\n<p>Models where k=0 are not exactly useful to us, though: in these models, the k=0 branches of the proofs are taken, and Tk never achieves self-trust. What we want is to show that there are models of Tk where k is arbitrarily large. We can prove this via induction.</p>\n<p>We've already got a base case: {N, k=0} is a model of Tk. For the induction step, we need to show that if {N, k=n} is a model of Tk then {N, k=n+1} is a model of Tk. To show this, note:</p>\n<ol>\n<li>If {N, k=n+1} \u22a7 \u25a1Tk[\u03c6] then {N, k=n} \u22a7 \u25a1Tk[\u03c6]</li>\n<li>If {N, k=n} \u22a7 \u25a1Tk[\u03c6] then {N, k=n} \u22a7 \u03c6</li>\n<li>If {N, k=n} \u22a7 \u03c6 then {N, k=n+1} \u22a7 \u03c6[k\\k-1]</li>\n</ol>\n<p>(1) is true because \u25a1Tk has no access to k.</p>\n<p>(2) is true because if \u25a1Tk[\u03c6] is a proof <em>modeled</em><em>&nbsp;a standard model</em> then \u03c6 is semantically derivable within Tk by a <em>standard</em>&nbsp;(ie classical) proof. Whenever \u0393 \u2192 \u0394 via a classical proof, all models of \u0393 are also models of \u0394. Since {N, k=n} is a model of Tk, it is also a model of all things provable in Tk via standard proof.</p>\n<p>(3) is trivial by substitution.</p>\n<p>We can extend these points to cover formulas free in one variable by noting that, because we are dealing with N as our model, every term x corresponds to a finite constant c.</p>\n<p>Combining these three points we get that if {N, k=n+1} \u22a7 \u25a1Tk[\u03c6] then {N, k=n+1} \u22a7 \u03c6[k\\k-1], which completes the induction step.</p>\n<p>We conclude that {N, k=n} is a model of Tk for any n.</p>\n<p>We can use this to construct tiling agents. The proof is technical (there's a bit of finagling to do because when the child has proven an action safe for k steps, the parent concludes that it is safe for k-1 steps). The proof in the paper still has a number of typos. (See the end of the post.) I won't go over the proof in detail here. It suffices to say that when the child proves an action safe for k steps, the parent concludes that it's safe for at least k-1 steps and thus (conditional upon k being positive) A1 can construct A0. Outside the system, we know that k can be arbitrarily large, and so conclude that the system is safe for an arbitrary number of steps.</p>\n<p>This system, like the last, would not be the best choice as a belief-system for a rational agent. It's not clear what k represents to the agent, nor what it would mean for a rational agent to \"actually believe\" Tk. As before, Tk is not presented as a realistic candidate for the mental system of a real agent. Rather, it is presented as yet another technical victory over the L\u00f6bstacle.</p>\n<p>With that done, we can talk about self-modifying agents on a more general level, and will do so in the next few sections.</p>\n<h2 id=\"Probabilistic_Reflection\">Probabilistic Reflection</h2>\n<p>The paper mentions the <a href=\"http://intelligence.org/wp-content/uploads/2013/03/Christiano-et-al-Naturalistic-reflection-early-draft.pdf\">Probabilistic Reflection</a> results, summarizes them, and notes that we don't yet have a formalism for agents using a probabilistic reflection principle. (We don't even yet know what knowledge of probabilistic reflection can appear inside a probabilistically reflective system).</p>\n<p>A few days ago I did a that&nbsp;<a href=\"/lw/jbe/walkthrough_of_definability_of_truth_in/\">write-up of that paper</a>, which may be helpful if you're looking for more walkthroughs such as this one.</p>\n<p><span style=\"font-size: 2em;\">What Self-Modifying Agents Need</span></p>\n<p>There are three principles that we really want our self-modifying agents to adhere to. I'll summarize these sections briefly, as they are not difficult to grasp. See the paper for more.</p>\n<h3 id=\"The_Vingean_principle\">The Vingean principle</h3>\n<p>Agents shouldn't have to predict precisely what their children do. If the parent knows precisely what the child will do \u2014 for example, by proving which action the child will take \u2014 then the child is hardly 'smarter' than the parent. In our logical systems, we follow the Vingean principle by requiring that the parent only discuss the child's actions within quantifiers. We can have a set representing the child-actions, and we can quantify over that set, but we can't have particular child-actions known ahead of time.</p>\n<p>The parent may verify properties about all&nbsp;actions, e.g. by looking at the action system and verifying that the motors only activate when presented with a valid proof. The parent may not e.g. simulate the child's exact action.</p>\n<h3 id=\"The_tiling_principle\">The tiling principle</h3>\n<p>Agents should be able to approve the construction of agents similar to themselves. This is what we've been searching for formalisms to allow in the above sections. This portion of the paper also discusses a formalism where the agent is divided into an untrusted proof suggestion component \u03a3 and a trusted proof verification component \u0394. This has some nice properties (especially in monotonic logic, where things that \u03a3 knows but isn't telling \u0394 can't cause \u0394 to allow something it shouldn't), but would quickly break down outside the realm of logic.</p>\n<h3 id=\"The_naturalistic_principle\">The naturalistic principle</h3>\n<p>Agents should think of themselves as part of the world. There shouldn't be a Cartesian boundary between \"world-stuff\" and \"mind-stuff\". Otherwise, you have to deal with weird boundary problems (attaching a new CPU requires things to cross the world/mind boundary).</p>\n<p>Furthermore, having different rules for internal actions and external actions is not particularly safe. For example, if certain internal modifications are prohibited an agent could build an external thing that happens to apply the desired internal changes.</p>\n<p>But <em>if you do</em> allow a cartesian boundary then the L\u00f6bstacle is easy to avoid. You simply set as your criterion of action to be</p>\n<pre><span>\u03c8(action) =</span></pre>\n<pre style=\"padding-left: 30px;\"><span style=\"font-size: 10px;\">(externalOK(action) \u2192 (@action \u2192 G)) \u2227</span></pre>\n<pre style=\"padding-left: 30px;\"><span style=\"font-size: 10px;\">(internalOk(action) \u2192 (@action \u2192 (\u2200 action' in Acts. @action' \u2192 \u25a1[\u03c8(action')])))</span></pre>\n<p>Translation: \"The condition for action is this: [when action is externalOK, if we execute it then it achieves the goal] and [when action is internalOk, if we execute it then it leaves us in a state where we only take actions that meet this condition].\"</p>\n<p><em>Note: There is a bit of ambiguity here. I believe that '<span>Acts'</span>&nbsp;is meant to refer to the new action-set available after taking the internal action.</em></p>\n<p>This is similar to our first failed solution, except that the Cartesian boundary allows \u03c8 to avoid becoming a Henkin sentence.</p>\n<p><em>Note: On gut instinct, I'd consider exploring <span>\u03c8(x) = @x \u2192 (G \u2228 (\u2200y \u2208 ActsX. @y \u2192 \u25a1[\u03c8(y)]))</span>, which is a version of the above that does not have a&nbsp;Cartesian&nbsp;boundary and does not immediately succumb to the Henkin-sentence problem.</em></p>\n<h1 id=\"Sensors\">Sensors</h1>\n<p>Then we try to add sensors, and everything blows up.</p>\n<p>That's not exactly the case, but adding sensors and unknown variables certainly doesn't make the formalism any prettier.</p>\n<p>Section six is long and somewhat involved, though it's saying something essentially simple. I'll summarize:</p>\n<p>We want to allow our agents to have sensors, and indeed construct new sensors. We'll say things like</p>\n<pre><span>A0 \u22a9 @sl \u21d2 @Sl\n</span></pre>\n<p>to mean \"A0 believes @sl only when sensor S takes on the state @Sl\".</p>\n<p>This causes a trust problem. We still want A1 to trust A0, despite the fact that A0 will have access to sensor data that A1 knows not. We can patch our trust framework by changing A0's criterion of action to be</p>\n<pre><span>@bi \u21d2 A0 \u22a9 (@sl \u2227 @bi) \u2192 (\u2205 \u2228 G)\n</span></pre>\n<p>(If bi is taken then A0 believes that, with the sensors in state sl, taking the action bi leads to either \u2205 or G). Then, if this is known for every possible combination of sensor states, A1 can retain trust in A0.</p>\n<p>This is messy, though. Additional sensors lead to exponentially more computing power required (on A1's part) to trust A0. This seems pathological. What we <em>want</em> is for A1 to reason that, since it can verify that A0 \u22a9 @sl only when @Sl (i.e. A1 trusts that the sensors take on state sl only when the world is actually in state Sl), anything that A0 does based on the sensor data is trusted.</p>\n<p>And we can get this property \u2014 or something like it. However, this means that A1 must be able to express things like \"A0's reasoning is such that, if we add only \"true\" sensor-data, it does not disturb our trust in A0.\" So A1 needs to not only trust A0, but must also trust A0 after A0 has had new \"true axioms\" added in the form of (trusted) sensor data.</p>\n<p>Formalizing this requires full-on set theory, which can formalize things like \"In all models X, all logical tautologies hold\".</p>\n<p>We then create a language in which there is a special set \u03c4 (the \"territory\") encoding a model of the environment. We then express A1's trust as follows:</p>\n<pre><span>A1 \u22a9 @A0 \u2192 \u2200[\u03c6]. (\u03c4 \u22a7\u03c4 [A0 \u22a9 \"\u03c6\"]) \u2192 (\u03c4 \u22a7\u03c4 [\u03c6])\n</span></pre>\n<p>Translation: A1 believes that, if it constructs A0, then for all quoted sentences <em>about the territory</em>, if it is true <em>in the territory</em> that A0 believes some A0-encoding of \u03c6, then it is actually true in the territory that \u03c6.</p>\n<p>Notice the two levels of quoting. A1 believes that A0's beliefs (<span>\"\u03c6\"</span>) correspond to statements true in the territory (<span>[\u03c6]</span>).</p>\n<p>This loosely represents how a programmer might reason about a program: A programmer (A1), who thinks that there is an object (\u03c6) in the environment, will have an internal representation (<span>[</span>\u03c6<span>]</span>) of the object. They will also know that the program (A2) has an object corresponding to Fred (<span>\"</span>\u03c6<span>\"</span>), and they will retain a mental map between their representation of the object (<span>[</span>\u03c6<span>]</span>) and their representation of the program's representation of the object (<span>[\"</span>\u03c6<span>\"]</span>).</p>\n<p>The point is that we deploy set theory to represent this sort of logic. This logic, however, allows us to reason about sensors thusly (replacing \u22a7\u03c4 with \u22a7):</p>\n<pre><span>A1 \u22a9 @A0 \u2192 \u2200[sl]. (\u03c4 \u22a7 [A0 \u22a9 \"@sl\"]) \u2192 (\u03c4 \u22a7 [@Sl])\n</span></pre>\n<p>Which basically says \"A0 only believes @sl when, in reality, the sensor actually is in state l.\"</p>\n<p>This is what we wanted, but it does not come cheap: We had to deploy full ZF in order to allow A1 to reason as we wanted. Worse, A2 (the parent of A1) must reason in ZF+1 in order to trust A1 (because it must believe that there is some model of ZF: otherwise A1 is meaningless). A3 must reason is ZF+2, and so on.</p>\n<p>It is noted that this can probably be addressed via parametric polymorphism (ie A2 uses ZFk, and also trusts A1 to use ZFk), though no proof has been written out yet. A simplification of the above mechanism (without resorting to set theory, or at the least using a weaker set theory) is desired.</p>\n<h1 id=\"Probability_and_Expected_Utility\">Probability and Expected Utility</h1>\n<p>Adding expected utility satisfaction is somewhat difficult.</p>\n<p>Probabilistic reasoning is non-monotonic. This leads to difficulties. For example, if you have an untrusted proof-suggester \u03a3 and a proof-verifier \u0394, then so long as \u0394 works you can't go wrong \u2014 in logic. At worst, \u03a3 withholds a good proof and the system does nothing. But with probabilistic reasoning, \u03a3 can withhold information from \u0394 with catastrophic results.</p>\n<p>For example, imagine that the condition of action is <span>E[U] \u2265 $20</span>, where E[U] is the expected utility. If we have a fair coin and a bet with payout of $50 on H and $10 on T, then the agent will take the bet. But if we later learn that the coin landed T, we should no longer take the bet: new information can invalidate actions that otherwise would have been taken. If the suggestion-device is untrusted, this can be particularly dangerous, because \u03a3 can withhold information maliciously.</p>\n<p>We can hack our way around this problem by requiring that every probabilistic variable be observed by A0 before it may choose an action. A1, knowing this, can proceed to trust A0. A formalization of this process is given on pages 35-36. The result is a technical success \u2014 the parent can still trust the child so long as the child observes all data before acting \u2014 but this formalism falls short on many counts.</p>\n<p>For example, the given formalism cannot assign prior probabilities to variables. (In the given formalism, probabilistic variables are left unassigned until the observations comes in. If the system tried to assign <span>\u03bd=0.7</span> as a prior probability and later assigned <span>\u03bd=1</span> according to sensor data, the system would arrive at the contradictory <span>0.7=1</span>.)</p>\n<p>This is obviously a very rudimentary formalization of utility satisfaction, and there is much work to be done.</p>\n<h1 id=\"Utility_Management\">Utility Management</h1>\n<p>The above section dealt with utility <em>satisfaction</em>, not utility maximization. This was intentional, as perfect utility maximization is impossible in all but the most trivial of situations.</p>\n<p>Utility satisfaction, however, is far from ideal. To take an example from the paper, consider an agent built to satisfy at least E[U]=2*10^15. If it finds an action that guarantees E[U]=10^16, then the agent may decide to randomize between a 30% probability of taking the 10^16, a 60% probability of doing nothing, and a 10% probability of burning 10^16 utils. This is\u2026 not ideal, to say the least.</p>\n<blockquote>\n<p>You get what you pay for, no more than you asked for, and exactly what you proved.</p>\n</blockquote>\n<p>An alternative means of utility management is a \"meliorizing\" criteria of action, wherein an agent picks an action and then decides whether to swap its current action for another as opportunities arise. This strategy is not without its flaws. To lift another example from the paper, imagine an agent built to save humanity from a deadly asteroid. Imagine it starts with the default action of \"do nothing\". If the first action it comes across is \"immediately take an action which will save 1000 humans, then shut down\", it will take that action and then shut down \u2014 forgoing the opportunity to save billions of human lives. Also not ideal.</p>\n<p>That said, meliorization at least shows a bit of promise, whereas satisficing is untenable and maximization is unrealistic. It's worth noting, though, that we don't even understand \u2014 not even at the highest possible level \u2014 how a probabilistic rational agent should go about pursuing its goals. This is the paper's ending note:</p>\n<blockquote>\n<p>The problem is very wide open indeed.</p>\n</blockquote>\n<h1 id=\"Edits\">Edits</h1>\n<h3 id=\"Typos\">Typos</h3>\n<p>I'm pretty confident about most of these, but some may be due to a misunderstanding, and are suffixed with (?).</p>\n<ul>\n<li>p10. \u03c8 \u2194 \u03c6(x) \u2228 \u25a1\u03c4[\u03c8(x)], this should be \u03c8(x) \u2194 \u2026 (?)</li>\n<li>p17. (4.13) The leading A_t^0 has a stray 't' after it.</li>\n<li>p17. (4.14) is missing a b_{i,u}\\bar inside the \u25a1_0, before the \"\u2192 \u2200\u03c5\".</li>\n<li>p17. (4.14) is missing a b_{i,t}\\bar before the \"\u2192 \u2200\u03bd\".</li>\n<li>p18. (4.23) there is a K that should be a \u03ba.</li>\n<li>p18. (4.24) there is an extraneous (A_t^0 \u2192) after the \u22a9</li>\n<li>p18. (4.24) there should be a -1 after \u2200\u03bd\u2264\u03c5+\u03ba. (Alternatively, change the \u2264 to &lt;) (?)</li>\n<li>p18. (4.25) there is an extraneous (A_t^0 \u2192) after the \u22a9</li>\n<li>p18. (4.25) there should be a -1 after \u2200\u03c5\u2264t+\u03ba. (Alternatively, change the \u2264 to &lt;) (?)</li>\n<li>p18. (4.26) there should be a -1 after \u2200\u03c5\u2264t+\u03ba. (Alternatively, change the \u2264 to &lt;) (?)</li>\n<li>p28. \"If we were in the shoes of A1 building A1 \u2026\" should be \"\u2026 building A0 \u2026\".</li>\n<li>p29. \"\u2026 it should not disturb our trust in A1.\" should be \"\u2026 in A0.\"</li>\n<li>p31. (6.9)&nbsp;(\u03c4 \u22a7 @A0) should be just @A0 (?)</li>\n</ul>\n<h3 id=\"Nitpicks\">Nitpicks</h3>\n<ul>\n<li>p10. \"Either x implies (\u2205 \u2228 G), or \u2026\" should read \"Either the execution of x implies (\u2205 or G), or \u2026\"</li>\n<li>p13. You use \u2261 to introduce \u03c8 and \u225c to introduce T-n. Later (p27) you use = to introduce a different \u03c8. Are these differences intentional?</li>\n<li>p19. Using P() to denote the function P seems somewhat strange. The probabilistic reflection paper simply calls it P. Other texts I've read use the style P(\u2014). I haven't seen the style P() before. (This may just be inexperience.)</li>\n<li>In infinite descent, why do the theories walk downwards? Why not use T0, T1, T2, \u2026? Walking downwards causes a bit of awkward syntax (T-(n+1)) that seems unnecessary.</li>\n</ul>\n<p>And while I'm on the subject, it felt foreign that A0 is the <em>youngest</em> agent. I would expect A0 to be the oldest agent. In one sense, I appreciated that A0 was the youngling: it lends itself well to the idea that we have this gigantic tower of agents building agents building agents, and then at the very end of the tower we have the final agent that affects the environment (which is why A0 is special). However, one of the nice things about this formalism is that it doesn't really treat environment-actions differently from agent-creating-actions, so we don't&nbsp;<em>need</em> this distinction. I found it counter-intuitive to consider the \"end\" of the tiling schema, preferring to think of it from the other direction: we start with an agent running on trusted math (agent zero) and let it tile indefinitely.</p>\n<p>All in all, it's not a particularly bad choice, it just felt unnatural to me.</p>\n<h3 id=\"Structure\">Structure</h3>\n<p>Section 5.2 seems like a non-sequitur. 5.1 and 5.3 discuss principles that agents should have. By contrast, 5.2 mentions tiling and gives examples of how trust can be decomposed. It was weird to be introduced to tiling after spending section 4 proving that tiling is possible while preserving trust. Also, the discussion about decomposition of trust didn't feel at home in a section detailing the tiling principle.</p>\n<p>I'd recommend moving section 5 up to before section 3 or possibly before section 2. This would give us a better idea of what we're looking for in self-modifying agents before we are introduced to the obstacles. I'd also suggest breaking \"decomposition of trust\" out of the \"tiling principle\" section. 5.2 feels like it's trying to introduce one concept while explaining another.</p>\n<h3 id=\"Confusions\">Confusions</h3>\n<ul>\n<li>p17. Syntax question: Why are (4.15) and (4.17) indented more than everything else?</li>\n<li>p27. I'm confused about the \"split decision criterion\". <span>Acts</span> denotes the set of actions that are available after the execution of the (internalOk) action <span>act</span>, yes?</li>\n</ul>", "sections": [{"title": "Glossary", "anchor": "Glossary", "level": 1}, {"title": "Trusting Yourself", "anchor": "Trusting_Yourself", "level": 1}, {"title": "The L\u00f6bstacle", "anchor": "The_L_bstacle", "level": 1}, {"title": "Finite Descent Problems", "anchor": "Finite_Descent_Problems", "level": 1}, {"title": "Failed Solutions", "anchor": "Failed_Solutions", "level": 1}, {"title": "Recursive Diving", "anchor": "Recursive_Diving", "level": 3}, {"title": "Buck Tossing", "anchor": "Buck_Tossing", "level": 3}, {"title": "Proof Stacking", "anchor": "Proof_Stacking", "level": 3}, {"title": "Technically Solutions", "anchor": "Technically_Solutions", "level": 1}, {"title": "Infinite Descent", "anchor": "Infinite_Descent", "level": 3}, {"title": "Parametric Polymorphism", "anchor": "Parametric_Polymorphism", "level": 3}, {"title": "Probabilistic Reflection", "anchor": "Probabilistic_Reflection", "level": 2}, {"title": "The Vingean principle", "anchor": "The_Vingean_principle", "level": 3}, {"title": "The tiling principle", "anchor": "The_tiling_principle", "level": 3}, {"title": "The naturalistic principle", "anchor": "The_naturalistic_principle", "level": 3}, {"title": "Sensors", "anchor": "Sensors", "level": 1}, {"title": "Probability and Expected Utility", "anchor": "Probability_and_Expected_Utility", "level": 1}, {"title": "Utility Management", "anchor": "Utility_Management", "level": 1}, {"title": "Edits", "anchor": "Edits", "level": 1}, {"title": "Typos", "anchor": "Typos", "level": 3}, {"title": "Nitpicks", "anchor": "Nitpicks", "level": 3}, {"title": "Structure", "anchor": "Structure", "level": 3}, {"title": "Confusions", "anchor": "Confusions", "level": 3}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 25}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kFDikC8kbukAhSnbe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-13T16:45:42.389Z", "modifiedAt": null, "url": null, "title": "New LW Meetups: Leipzig, Utrecht", "slug": "new-lw-meetups-leipzig-utrecht", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u97qcQK7DBFy3AHXH/new-lw-meetups-leipzig-utrecht", "pageUrlRelative": "/posts/u97qcQK7DBFy3AHXH/new-lw-meetups-leipzig-utrecht", "linkUrl": "https://www.lesswrong.com/posts/u97qcQK7DBFy3AHXH/new-lw-meetups-leipzig-utrecht", "postedAtFormatted": "Friday, December 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetups%3A%20Leipzig%2C%20Utrecht&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetups%3A%20Leipzig%2C%20Utrecht%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu97qcQK7DBFy3AHXH%2Fnew-lw-meetups-leipzig-utrecht%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetups%3A%20Leipzig%2C%20Utrecht%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu97qcQK7DBFy3AHXH%2Fnew-lw-meetups-leipzig-utrecht", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu97qcQK7DBFy3AHXH%2Fnew-lw-meetups-leipzig-utrecht", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 613, "htmlBody": "<p><strong>This summary was posted to LW main on December 6th. The following week's summary is <a href=\"/lw/jci/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/u6\">[Leipzig] Secular Solstice Celebration! (And the Inauguration of the LW Leipzig Community):&nbsp;<span class=\"date\">21 December 2013 05:05PM</span></a></li>\n<li><a href=\"/meetups/tv\">Mumbai Meetup:&nbsp;<span class=\"date\">15 December 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/tz\">Newcastle-upon-Tyne meetup, December:&nbsp;<span class=\"date\">07 December 2013 12:00PM</span></a></li>\n<li><a href=\"/meetups/u4\">Utrecht:&nbsp;<span class=\"date\">14 December 2013 02:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/tf\">Berlin:&nbsp;<span class=\"date\">01 January 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/ub\">Frankfurt meetup::&nbsp;<span class=\"date\">08 December 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ty\">Helsinki Meetup:&nbsp;<span class=\"date\">15 December 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/uf\">Moscow, The First Winter One:&nbsp;<span class=\"date\">08 December 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/u1\">Munich Meetup:&nbsp;<span class=\"date\">07 December 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ud\">Saint Petersburg, Russia. On discussions and some social skills:&nbsp;<span class=\"date\">08 December 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/u3\">San Francisco / App Academy meetup [LOCATION CHANGE]:&nbsp;<span class=\"date\">07 December 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/ug\">Urbana-Champaign fun and games:&nbsp;<span class=\"date\">07 December 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ue\">[Vienna] The Return of the Rationalists!:&nbsp;<span class=\"date\">14 December 2013 03:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">07 December 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/ua\">Bay Area Solstice:&nbsp;<span class=\"date\">07 December 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/uc\">Boston/Cambridge - The Attention Economy:&nbsp;<span class=\"date\">08 December 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ti\">Brussels monthly meetup: time!:&nbsp;<span class=\"date\">14 December 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/u7\">London Practical Meetup - Calibration Training!:&nbsp;<span class=\"date\">08 December 2013 02:00AM</span></a></li>\n<li><a href=\"/meetups/uh\">Washington DC Fermi Estimates Meetup:&nbsp;<span class=\"date\">08 December 2013 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u97qcQK7DBFy3AHXH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.464558248242646e-06, "legacy": true, "legacyId": "25011", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AtNk66CwotT6AKWN5", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-13T21:53:10.881Z", "modifiedAt": null, "url": null, "title": "an ethical puzzle about brain emulation", "slug": "an-ethical-puzzle-about-brain-emulation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.104Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "asr", "createdAt": "2011-06-02T01:42:05.591Z", "isAdmin": false, "displayName": "asr"}, "userId": "w2EyaugHx6wxwdbva", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9Pz4Hg8qmATFkux4q/an-ethical-puzzle-about-brain-emulation", "pageUrlRelative": "/posts/9Pz4Hg8qmATFkux4q/an-ethical-puzzle-about-brain-emulation", "linkUrl": "https://www.lesswrong.com/posts/9Pz4Hg8qmATFkux4q/an-ethical-puzzle-about-brain-emulation", "postedAtFormatted": "Friday, December 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20an%20ethical%20puzzle%20about%20brain%20emulation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Aan%20ethical%20puzzle%20about%20brain%20emulation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Pz4Hg8qmATFkux4q%2Fan-ethical-puzzle-about-brain-emulation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=an%20ethical%20puzzle%20about%20brain%20emulation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Pz4Hg8qmATFkux4q%2Fan-ethical-puzzle-about-brain-emulation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Pz4Hg8qmATFkux4q%2Fan-ethical-puzzle-about-brain-emulation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 491, "htmlBody": "<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">I've been thinking about ethics and brain emulations for a while and now have realized I am confused.&nbsp; Here are five scenarios. I am pretty sure the first is morally problematic, and pretty sure the last is completely innocuous. But I can't find a clean way to partition the intermediate cases.</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">A) We grab John Smith off the street, scan his brain, torture him, and then by some means, restore him to a mental and physical state as though the torture never happened.</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">B)&nbsp; We scan John Smith's brain, and then run a detailed simulation of the brain being tortured for ten seconds, and over again. If we attached appropriate hardware to the appropriate simulated neurons, we would hear the simulation screaming.</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">C) We store, on disk, each timestep of the simulation in scenario B. Then we sequentially load each timestep into memory, and overwrite it.&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">D) The same as C, except that each timestep is encrypted with a secure symmetric cipher, say, AES. The key used for encryption has been lost. (Edit: The key length is much smaller than the size of the stored state and there's only one possible valid decryption.)</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">E) The same as D, except we have encrypted each timestep with a one time pad.</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">I take for granted that scenario A is bad: one oughtn't be inflicting pain, even if there's no permanent record or consequence of the pain.&nbsp; And I can't think of any moral reason to distinguish a supercomputer simulation of a brain from the traditional implementation made of neurons and synapses. So that says that B should be equally immoral.</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">Scenario C is just B with an implementation tweak -- instead of _calculating_ each subsequent step, we're just playing it back from storage. The simulated brain has the same sequence of states as in B and the same outputs.</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">Scenario D is just C with a different data format. &nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">Scenario E is just D with a different encryption.</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">Now here I am confused. Scenario E is just repeatedly writing random bytes to memory. This cannot possibly have any moral significance!&nbsp; D and E are indistinguishable to any practical algorithm. (By definition, secure encryption produces bytes that \"look random\" to any adversary that doesn't know the key).&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">Either torture in case A is actually not immoral or two of these adjacent scenarios are morally distinct. But none of those options seem appealing.&nbsp; I don't see a simple clean way to resolve the paradox here. Thoughts?</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica;\">As an aside: Scenarios C,D and E aren't so far beyond current technology as you might expect.&nbsp; Wikipedia tells me that the brain has ~120 trillion synapses.&nbsp; Most of the storage cost will be the per-timestep data, not the underlying topology. If we need one byte per synapse per timestep, that's 120TB/timestep. If we have a timestep every millisecond, that's 120 PB/second. That's a lot of data, but it's not unthinkably beyond what's commercially available today, So this isn't a Chinese-Room case where the premise can't possibly be realized, physically.</p>\n<p>&nbsp;</p>\n<p style=\"margin: 0px; font-size: 12px; font-family: Helvetica; min-height: 14px;\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9Pz4Hg8qmATFkux4q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 21, "extendedScore": null, "score": 1.4648755507267096e-06, "legacy": true, "legacyId": "25075", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-14T03:09:58.366Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC fun and games meetup", "slug": "meetup-washington-dc-fun-and-games-meetup-10", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ikTjHcFeRa2cuahZx/meetup-washington-dc-fun-and-games-meetup-10", "pageUrlRelative": "/posts/ikTjHcFeRa2cuahZx/meetup-washington-dc-fun-and-games-meetup-10", "linkUrl": "https://www.lesswrong.com/posts/ikTjHcFeRa2cuahZx/meetup-washington-dc-fun-and-games-meetup-10", "postedAtFormatted": "Saturday, December 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FikTjHcFeRa2cuahZx%2Fmeetup-washington-dc-fun-and-games-meetup-10%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FikTjHcFeRa2cuahZx%2Fmeetup-washington-dc-fun-and-games-meetup-10", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FikTjHcFeRa2cuahZx%2Fmeetup-washington-dc-fun-and-games-meetup-10", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/um'>Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 December 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/um'>Washington DC fun and games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ikTjHcFeRa2cuahZx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4652025948604445e-06, "legacy": true, "legacyId": "25076", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/um\">Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 December 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/um\">Washington DC fun and games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-14T15:19:19.627Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Fun and Games", "slug": "meetup-urbana-champaign-fun-and-games-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rvSEvrFoRcEhsvuRZ/meetup-urbana-champaign-fun-and-games-2", "pageUrlRelative": "/posts/rvSEvrFoRcEhsvuRZ/meetup-urbana-champaign-fun-and-games-2", "linkUrl": "https://www.lesswrong.com/posts/rvSEvrFoRcEhsvuRZ/meetup-urbana-champaign-fun-and-games-2", "postedAtFormatted": "Saturday, December 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Fun%20and%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Fun%20and%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrvSEvrFoRcEhsvuRZ%2Fmeetup-urbana-champaign-fun-and-games-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Fun%20and%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrvSEvrFoRcEhsvuRZ%2Fmeetup-urbana-champaign-fun-and-games-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrvSEvrFoRcEhsvuRZ%2Fmeetup-urbana-champaign-fun-and-games-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/un'>Urbana-Champaign: Fun and Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 December 2013 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">300 S Goodwin Ave Apt 102, Urbana, IL.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There will also probably be HPMOR discussion.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/un'>Urbana-Champaign: Fun and Games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rvSEvrFoRcEhsvuRZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.4659560455288727e-06, "legacy": true, "legacyId": "25077", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Fun_and_Games\">Discussion article for the meetup : <a href=\"/meetups/un\">Urbana-Champaign: Fun and Games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 December 2013 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">300 S Goodwin Ave Apt 102, Urbana, IL.</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There will also probably be HPMOR discussion.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Fun_and_Games1\">Discussion article for the meetup : <a href=\"/meetups/un\">Urbana-Champaign: Fun and Games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Fun and Games", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Fun_and_Games", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Fun and Games", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Fun_and_Games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-14T16:55:24.033Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels monthly meetup: Futurology!", "slug": "meetup-brussels-monthly-meetup-futurology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:33.152Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/piwHxpPreuDkfaHva/meetup-brussels-monthly-meetup-futurology", "pageUrlRelative": "/posts/piwHxpPreuDkfaHva/meetup-brussels-monthly-meetup-futurology", "linkUrl": "https://www.lesswrong.com/posts/piwHxpPreuDkfaHva/meetup-brussels-monthly-meetup-futurology", "postedAtFormatted": "Saturday, December 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20monthly%20meetup%3A%20Futurology!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20monthly%20meetup%3A%20Futurology!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpiwHxpPreuDkfaHva%2Fmeetup-brussels-monthly-meetup-futurology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20monthly%20meetup%3A%20Futurology!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpiwHxpPreuDkfaHva%2Fmeetup-brussels-monthly-meetup-futurology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpiwHxpPreuDkfaHva%2Fmeetup-brussels-monthly-meetup-futurology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/uo'>Brussels monthly meetup: Futurology!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 January 2014 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>When will self-driving cars become the norm? How soon until an AI passes the Turing Test, and how soon afterwards will we all be out of a job? Which organization will be the first to unleash uncontrolled all-consuming nanomachines upon the world? That is what we'll discuss this month (until the conversation goes wildly off-topic, as it usually does). If you've read anything interesting recently about new technologies and future trends, please share.</p>\n\n<p>Fermi estimates and predictionbook.com will be involved.</p>\n\n<p>In other news, and for at least the next six months, the Brussels meetup group has a page on Meetup.com. It's for recruitment purposes only, but the meetups will be announced there too. If you find that more convenient than subscribing to our Google Group, feel free to join it (link below).</p>\n\n<p>We will meet at 1 pm at La Fleur en papier dor\u00e9, close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out this one minute form, to share your contact information: <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform</a>\nThe Brussels meetups use a Google Group: <a href=\"https://groups.google.com/forum/#!forum/lesswrong-brussels\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/lesswrong-brussels</a>\nAnd, for meetups announcements only, a Meetup Group:\n<a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">http://www.meetup.com/LWBrussels/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/uo'>Brussels monthly meetup: Futurology!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "piwHxpPreuDkfaHva", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4660553440573812e-06, "legacy": true, "legacyId": "25078", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_monthly_meetup__Futurology_\">Discussion article for the meetup : <a href=\"/meetups/uo\">Brussels monthly meetup: Futurology!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 January 2014 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>When will self-driving cars become the norm? How soon until an AI passes the Turing Test, and how soon afterwards will we all be out of a job? Which organization will be the first to unleash uncontrolled all-consuming nanomachines upon the world? That is what we'll discuss this month (until the conversation goes wildly off-topic, as it usually does). If you've read anything interesting recently about new technologies and future trends, please share.</p>\n\n<p>Fermi estimates and predictionbook.com will be involved.</p>\n\n<p>In other news, and for at least the next six months, the Brussels meetup group has a page on Meetup.com. It's for recruitment purposes only, but the meetups will be announced there too. If you find that more convenient than subscribing to our Google Group, feel free to join it (link below).</p>\n\n<p>We will meet at 1 pm at La Fleur en papier dor\u00e9, close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out this one minute form, to share your contact information: <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform</a>\nThe Brussels meetups use a Google Group: <a href=\"https://groups.google.com/forum/#!forum/lesswrong-brussels\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/lesswrong-brussels</a>\nAnd, for meetups announcements only, a Meetup Group:\n<a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">http://www.meetup.com/LWBrussels/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_monthly_meetup__Futurology_1\">Discussion article for the meetup : <a href=\"/meetups/uo\">Brussels monthly meetup: Futurology!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels monthly meetup: Futurology!", "anchor": "Discussion_article_for_the_meetup___Brussels_monthly_meetup__Futurology_", "level": 1}, {"title": "Discussion article for the meetup : Brussels monthly meetup: Futurology!", "anchor": "Discussion_article_for_the_meetup___Brussels_monthly_meetup__Futurology_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-14T18:39:40.101Z", "modifiedAt": null, "url": null, "title": "Defining causal isomorphism", "slug": "defining-causal-isomorphism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:07.444Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badtheatre", "createdAt": "2013-07-17T18:02:18.233Z", "isAdmin": false, "displayName": "badtheatre"}, "userId": "FPyYxyYXR7TekKos7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sPDYqeyZg9LC5vRdi/defining-causal-isomorphism", "pageUrlRelative": "/posts/sPDYqeyZg9LC5vRdi/defining-causal-isomorphism", "linkUrl": "https://www.lesswrong.com/posts/sPDYqeyZg9LC5vRdi/defining-causal-isomorphism", "postedAtFormatted": "Saturday, December 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Defining%20causal%20isomorphism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADefining%20causal%20isomorphism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsPDYqeyZg9LC5vRdi%2Fdefining-causal-isomorphism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Defining%20causal%20isomorphism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsPDYqeyZg9LC5vRdi%2Fdefining-causal-isomorphism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsPDYqeyZg9LC5vRdi%2Fdefining-causal-isomorphism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>I previously <a href=\"/lw/icy/how_sure_are_you_that_brain_emulations_would_be/9one\">posted</a> this question in another discussion, but it didn't get any replies so, since I now have enough karma, I've decided to make it my first \"article\".&nbsp;</p>\n<blockquote>\n<p>This brings up something that has been on my mind for a long time. What are the necessary and sufficient conditions for two computations to be (homeo?)morphic? This could mean a lot of things, but specifically I'd like to capture the notion of being able to contain a consciousness, so what I'm asking is, what we would have to prove in order to say program A contains a consciousness --&gt; program B contains a consciousness. \"pointwise\" isomorphism, if you're saying what I think, seems too strict. On the other hand, allowing any invertible function to be a _morphism doesn't seem strict enough. For one thing we can put any reversible computation in 1-1 correspondence with a program that merely stores a copy of the initial state of the first program and ticks off the natural numbers. Restricting our functions by, say, resource complexity, also seems to lead to both similar and unrelated issues...</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Any takers?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sPDYqeyZg9LC5vRdi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 1.466163125502245e-06, "legacy": true, "legacyId": "25079", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-14T22:15:08.908Z", "modifiedAt": null, "url": null, "title": "Examples in Mathematics", "slug": "examples-in-mathematics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:07.158Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kgalias", "createdAt": "2012-04-08T18:58:57.805Z", "isAdmin": false, "displayName": "kgalias"}, "userId": "grKD3C8Gt2vNtfBzt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9C2LPafGPQcmSR3HK/examples-in-mathematics", "pageUrlRelative": "/posts/9C2LPafGPQcmSR3HK/examples-in-mathematics", "linkUrl": "https://www.lesswrong.com/posts/9C2LPafGPQcmSR3HK/examples-in-mathematics", "postedAtFormatted": "Saturday, December 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Examples%20in%20Mathematics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExamples%20in%20Mathematics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9C2LPafGPQcmSR3HK%2Fexamples-in-mathematics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Examples%20in%20Mathematics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9C2LPafGPQcmSR3HK%2Fexamples-in-mathematics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9C2LPafGPQcmSR3HK%2Fexamples-in-mathematics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 529, "htmlBody": "<p>&nbsp;</p>\n<p>After reading <a href=\"http://intelligence.org/2013/12/13/aaronson/\">Luke's interview with Scott Aaronson</a>, I've decided to come back to an issue that's been bugging me.</p>\n<p>Specifically, in the answer to Luke's question about object-level tactics, Scott says (under 3):</p>\n<blockquote>\n<p>Sometimes, when you set out to prove some mathematical conjecture, your first instinct is just to throw an arsenal of theory at it. (..)&nbsp;Rather than looking for &ldquo;general frameworks,&rdquo; I look for easy special cases and simple sanity checks, for stuff I can try out using high-school algebra or maybe a five-line computer program, just to get a feel for the problem.</p>\n</blockquote>\n<p>In a similar vein, there's the Halmos quote which has been heavily upvoted in the November Rationality Quotes:</p>\n<blockquote>\n<p>A good stack of examples, as large as possible, is indispensable for a thorough understanding of any concept, and when I want to learn something new, I make it my first job to build one.</p>\n</blockquote>\n<p>Every time I see an opinion expressing a similar sentiment, I can't help but contrast it with the opinions and practices of two wildly successful (very) theoretical mathematicians:</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Alexander_Grothendieck\"><strong>Alexander Grothendieck</strong></a></p>\n<blockquote>\n<p>One striking characteristic of Grothendieck's mode of thinking is that it seemed to rely so little on examples. This can be seen in the legend of the so-called \"Grothendieck prime\". In a mathematical conversation, someone suggested to Grothendieck that they should consider a particular prime number. &ldquo;You mean an actual number?&rdquo; Grothendieck asked. The other person replied, yes, an actual prime number. Grothendieck suggested, &ldquo;All right, take 57.&rdquo; But Grothendieck must have known that 57 is not prime, right? Absolutely not, said David Mumford of Brown University. &ldquo;He doesn&rsquo;t think concretely.\" Consider by contrast the Indian mathematician Ramanujan, who was intimately familiar with properties of many numbers, some of them huge. That way of thinking represents a world antipodal to that of Grothendieck. \"He never really worked on examples,\" Mumford observed. \"I only understand things through examples and then gradually make them more abstract. I don't think it helped Grothendieck in the least to look at an example. He really got control of the situation by thinking of it in absolutely the most abstract way possible. It's just very strange. That's the way his mind worked.\"</p>\n</blockquote>\n<p>(from <a href=\"http://www.ams.org/notices/200410/fea-grothendieck-part2.pdf\">Allyn Jackson's account of Grothendieck's life</a>).</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Maxim_Kontsevich\"><strong>Maxim Kontsevich</strong></a></p>\n<blockquote>\n<p>Saito: This is one typical point of your work. But I \ufb01nd that in much of your work, by hearing one symptom you capture the central point of the problem and then give some general big framework. That&rsquo;s my general impression of what you are doing.</p>\n<p>Kontsevich: Yeah, I really don&rsquo;t work on examples at such a level.</p>\n<p>Saito: How can you work in that way?</p>\n<p>Kontsevich: For myself sometimes I work on one or two examples, but...</p>\n<p>Saito: You already keep some examples in mind, but still you construct theory.</p>\n<p>Kontsevich: Yes. And generally I \ufb01nd examples sometimes to be misleading. [Laughter]. Because often the properties of examples are too special, you cannot see general properties if you constantly work too much on concrete examples.</p>\n</blockquote>\n<p>(from the <a href=\"http://www.ipmu.jp/webfm_send/70\">IPMU interview</a>).</p>\n<p>Are they fooling themselves, or is there something to be learned? Perhaps it's possible to mention&nbsp;<a href=\"https://www.dpmms.cam.ac.uk/~wtg10/2cultures.pdf\">Gowers' Two Cultures</a>&nbsp;in the answer.</p>\n<p>&nbsp;</p>\n<h6><span style=\"font-weight: normal;\">P.S. First content post here, I would appreciate feedback.</span></h6>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9C2LPafGPQcmSR3HK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 22, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "25080", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-15T08:32:51.311Z", "modifiedAt": null, "url": null, "title": "Naturalistic trust among AIs: The parable of the thesis advisor's theorem", "slug": "naturalistic-trust-among-ais-the-parable-of-the-thesis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.720Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s3PjeZuXmw5jziNDT/naturalistic-trust-among-ais-the-parable-of-the-thesis", "pageUrlRelative": "/posts/s3PjeZuXmw5jziNDT/naturalistic-trust-among-ais-the-parable-of-the-thesis", "linkUrl": "https://www.lesswrong.com/posts/s3PjeZuXmw5jziNDT/naturalistic-trust-among-ais-the-parable-of-the-thesis", "postedAtFormatted": "Sunday, December 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Naturalistic%20trust%20among%20AIs%3A%20The%20parable%20of%20the%20thesis%20advisor's%20theorem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANaturalistic%20trust%20among%20AIs%3A%20The%20parable%20of%20the%20thesis%20advisor's%20theorem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs3PjeZuXmw5jziNDT%2Fnaturalistic-trust-among-ais-the-parable-of-the-thesis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Naturalistic%20trust%20among%20AIs%3A%20The%20parable%20of%20the%20thesis%20advisor's%20theorem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs3PjeZuXmw5jziNDT%2Fnaturalistic-trust-among-ais-the-parable-of-the-thesis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs3PjeZuXmw5jziNDT%2Fnaturalistic-trust-among-ais-the-parable-of-the-thesis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1887, "htmlBody": "<p>Eliezer and Marcello's article on <a href=\"http://intelligence.org/files/TilingAgents.pdf\">tiling agents and the L&ouml;bian obstacle</a> discusses several things that you intuitively would expect a rational agent to be able to do that, because of L&ouml;b's theorem, are problematic for an agent using logical reasoning. One of these desiderata is <em>naturalistic trust</em>: Imagine that you build an AI that uses PA for its mathematical reasoning, and this AI happens to find in its environment an automated theorem prover which, the AI carefully establishes, <em>also</em> uses PA for its reasoning. Our AI looks at the theorem prover's display and sees that it flashes a particular lemma that would be very useful for our AI in its own reasoning; the fact that it's on the prover's display means that the prover has just completed a formal proof of this lemma. Can our AI now use the lemma? Well, even if it can establish in its own PA-based reasoning module that <em>there exists a proof</em> of the lemma, by L&ouml;b's theorem this doesn't imply in PA that the lemma is in fact true; as Eliezer would put it, our agent treats proofs checked inside the boundaries of its own head different from proofs checked somewhere in the environment. (The above isn't fully formal, but the formal details can be filled in.)</p>\n<p>At the <a href=\"http://intelligence.org/2013/07/24/miris-december-2013-workshop/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=miris-december-2013-workshop\">MIRI's December workshop</a> (which started today), we've been discussing <a href=\"http://arxiv-web.arxiv.org/abs/1312.3626\">a suggestion by Nik Weaver</a> for how to handle this problem. Nik starts from a simple suggestion (which he doesn't consider to be <em>entirely</em> sufficient, and his linked paper is mostly about a much more involved proposal that addresses some remaining problems, but the simple idea will suffice for this post): Presumably there's some instrumental reason that our AI proves things; suppose that in particular, the AI will only take an action after it has proven that it is \"safe\" to take this action (e.g., the action doesn't blow up the planet). Nik suggests to relax this a bit: The AI will only take an action after it has (i) proven in PA that taking the action is safe; <strong>OR</strong> (ii) proven in PA that it's provable in PA that the action is safe; <strong>OR</strong> (iii) proven in PA that it's provable in PA that it's provable in PA that the action is safe; etc.</p>\n<p>Now suppose that our AI sees that lemma, A, flashing on the theorem prover's display, and suppose that our AI can prove that A implies that action X is safe. Then our AI can also prove that <em>it's provable</em> that A -&gt; safe(X), and it can prove that A is provable because it has established that the theorem prover works correctly; thus, it can prove that <em>it's provable</em> that safe(X), and therefore take action X.</p>\n<p>Even if the theorem prover has only proved that A is provable, so that the AI only knows that it's provable that A is provable, it can use the same sort of reasoning to prove that it's provable that it's provable that safe(X), and again take action X.</p>\n<p>But on hearing this, Eliezer and I had the same skeptical reaction: It seems that our AI, in an informal sense, \"trusts\" that A is true if it finds (i) a proof of A, or (ii) a proof that A is provable, or -- etc. Now suppose that the theorem prover our AI is looking at flashes statements on its display after it has established that they are \"trustworthy\" in this sense -- if it has found a proof, or a proof that there is a proof, etc. Then when A flashes on the display, our AI can only prove that there exists some <em>n</em> such that it's \"provable^<em>n</em>\" that A, and that's <em>not</em> enough for it to use the lemma. If the theorem prover flashed <em>n</em> on its screen together with A, everything would be fine and dandy; but if the AI doesn't know <em>n</em>, it's not able to use the theorem prover's work. So it still seems that the AI is unwilling to \"trust\" another system that reasons just like the AI itself.</p>\n<p>I want to try to shed some light on this obstacle by giving an intuition for why the AI's behavior here could, in some sense, be considered to be the right thing to do. Let me tell you a little story.</p>\n<p style=\"padding-left: 30px;\">One day you talk with a bright young mathematician about a mathematical problem that's been bothering you, and she suggests that it's an easy consequence of a theorem in cohistonomical tomolopy. You haven't heard of this theorem before, and find it rather surprising, so you ask for the proof.</p>\n<p style=\"padding-left: 30px;\">\"Well,\" she says, \"I've heard it from my thesis advisor.\"</p>\n<p style=\"padding-left: 30px;\">\"Oh,\" you say, \"fair enough. Um--\"</p>\n<p style=\"padding-left: 30px;\">\"Yes?\"</p>\n<p style=\"padding-left: 30px;\">\"You're sure that your advisor checked it carefully, right?\"</p>\n<p style=\"padding-left: 30px;\">\"Ah! Yeah, I made quite sure of that. In fact, I established very carefully that my thesis advisor uses exactly the same system of mathematical reasoning that I use myself, and only states theorems after she has checked the proof beyond any doubt, so as a rational agent I am compelled to accept anything as true that she's convinced herself of.\"</p>\n<p style=\"padding-left: 30px;\">\"Oh, I see! Well, fair enough. I'd still like to understand why this theorem is true, though. You wouldn't happen to know your advisor's proof, would you?\"</p>\n<p style=\"padding-left: 30px;\">\"Ah, as a matter of fact, I do! She's heard it from her thesis advisor.\"</p>\n<p style=\"padding-left: 30px;\">\"...\"</p>\n<p style=\"padding-left: 30px;\">\"Something the matter?\"</p>\n<p style=\"padding-left: 30px;\">\"Er, have you considered...\"</p>\n<p style=\"padding-left: 30px;\">\"<em>Oh!</em> I'm glad you asked! In fact, I've been curious myself, and yes, it <em>does</em> happen to be the case that there's an infinitely descending chain of thesis advisors all of which have established the truth of this theorem solely by having heard it from the previous advisor in the chain.\" <em>(This parable takes place in a world without a big bang -- human history stretches infinitely far into the past.)</em> \"But never to worry -- they've all checked very carefully that the previous person in the chain used the same formal system as themselves. Of course, that was obvious by induction -- my advisor wouldn't have accepted it from her advisor without checking his reasoning first, and he would have accepted it from his advisor without checking, etc.\"</p>\n<p style=\"padding-left: 30px;\">\"Uh, doesn't it bother you that nobody has ever, like, actually <em>proven</em> the theorem?\"</p>\n<p style=\"padding-left: 30px;\">\"Whatever in the world are you talking about? I've proven it myself! In fact, I just told you that <em>infinitely many</em> people have each proved it in slightly different ways -- for example my own proof made use of the fact that my advisor had proven the theorem, whereas her proof used <em>her</em> advisor instead...\"</p>\n<p>This can't <em>literally</em> happen with a sound proof system, but the reason is that that a system like PA can only accept things as true if they have been proven in a system <em>weaker</em> than PA -- i.e., because we have L&ouml;b's theorem. Our mathematician's advisor would have to use a weaker system than the mathematician herself, and the advisor's advisor a weaker system still; this sequence would have to terminate after a finite time <em>(I don't have a formal proof of this, but&nbsp;I'm fairly sure you can turn the above story into a formal proof that something like this has to be true of sound proof systems)</em>, and so <em>someone</em> will actually have to have proved the actual theorem on the object level.</p>\n<p>So here's my intuition: A satisfactory solution of the problems around the L&ouml;bian obstacle will have to make sure that the buck doesn't get passed on indefinitely -- you can accept a theorem because someone reasoning like you has established that someone else reasoning like you has proven the theorem, but there can only be a finite number of links between you and someone who has actually done the object-level proof. We know how to do this by decreasing the mathematical strength of the proof system, and that's not satisfactory, but my intuition is that a satisfactory solution will still have to make sure that there's <em>something</em> that decreases when you go up the chain of thesis advisors, and when that thing reaches zero you've found the thesis advisor that has actually proven the theorem. (I sense <a href=\"http://en.wikipedia.org/wiki/Ordinal_number\">ordinals</a> entering the picture.)</p>\n<p>...aaaand in fact, I <em>can</em> now tell you one way to do <em>something</em> like this: Nik's idea, which I was talking about above. Remember how our AI \"trusts\" the theorem prover that flashes the number <em>n</em> which says how many times you have to iterate \"that it's provable in PA that\", but doesn't \"trust\" the prover that's exactly the same except it doesn't tell you this number? That's the thing that decreases. If the theorem prover actually establishes A by observing a <em>different</em> theorem prover flashing A and the number 1584, then it can flash A, but only with a number at least 1585. And hence, if you go 1585 thesis advisors up the chain, you find the gal who actually proved A.</p>\n<p>The cool thing about Nik's idea is that it <em>doesn't</em> change mathematical strength while going down the chain. In fact, it's not hard to show that if PA proves a sentence A, then it also proves that PA proves A; and the other way, we believe that everything that PA proves is actually true, so if PA proves <em>PA proves A</em>, then it follows that PA proves A.</p>\n<p>I can guess what Eliezer's reaction to my argument here might be: The problem I've been describing can only occur in infinitely large worlds, which have all sorts of other problems, like utilities not converging and stuff.</p>\n<p style=\"padding-left: 30px;\"><span class=\"userContent\">We settled for a large finite TV screen, but we could have had an arbitrarily larger finite TV screen. <a class=\"_58cn\" href=\"https://www.facebook.com/hashtag/infiniteworldproblems\">#infiniteworldproblems</a><br /> <br /> We have Porsches for every natural number, but at every time t we have to trade down the Porsche with number t for a BMW. #infiniteworldproblems<br /> <br /> We have ever-rising expectations for our standard of living, but the limit of our expectations doesn't equal our expectation of the limit. #infiniteworldproblems </span></p>\n<p style=\"padding-left: 30px;\"><span class=\"userContent\"><em>-- <a href=\"https://www.facebook.com/yudkowsky/posts/10152063086689228\">Eliezer</a>, not coincidentally after talking to me</em></span></p>\n<p><span class=\"userContent\">I'm not going to be able to resolve that argument in this post, but briefly: I agree that we <em>probably</em> live in a finite world, and that finite worlds have many properties that make them nice to handle mathematically, but we can formally reason about infinite worlds of the kind I'm talking about here using standard, extremely well-understood mathematics.</span></p>\n<p><span class=\"userContent\">Because proof systems like PA (or more conveniently ZFC) allow us to formalize this standard mathematical reasoning, a solution to the L&ouml;bian obstacle has to \"work\" properly in these infinite worlds, or we would be able to turn our story of the thesis advisors' proof that 0=1 into a formal proof of an inconsistency in PA, say. To be concrete, consider the system PA*, which consists of PA + the axiom schema \"if PA* proves phi, then phi\" for every formula phi; this is easily seen to be inconsistent by L&ouml;b's theorem, but if we didn't know that yet, we could translate the story of the thesis advisors (which are using PA* as their proof system this time) into a formal proof of the inconsistency of PA*.</span></p>\n<p><span class=\"userContent\">Therefore, thinking intuitively in terms of infinite worlds can give us insight into why many approaches to the L&ouml;bian family of problems fail -- as long as we make sure that these infinite worlds, and their properties that we're using in our arguments, really <em>can</em> be formalized in standard mathematics, of course.<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "6nS8oYmSMuFMaiowF": 1, "KN9KEMgyBHjcAyc26": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s3PjeZuXmw5jziNDT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 36, "extendedScore": null, "score": 1.4670248935701236e-06, "legacy": true, "legacyId": "25082", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-15T17:40:08.775Z", "modifiedAt": null, "url": null, "title": "Luck II: Expecting White Swans", "slug": "luck-ii-expecting-white-swans", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.423Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fowlertm", "createdAt": "2012-01-07T20:35:26.490Z", "isAdmin": false, "displayName": "fowlertm"}, "userId": "gDAt4FezxH8dDr5EY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4eHMNevX38Hg6sqBy/luck-ii-expecting-white-swans", "pageUrlRelative": "/posts/4eHMNevX38Hg6sqBy/luck-ii-expecting-white-swans", "linkUrl": "https://www.lesswrong.com/posts/4eHMNevX38Hg6sqBy/luck-ii-expecting-white-swans", "postedAtFormatted": "Sunday, December 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Luck%20II%3A%20Expecting%20White%20Swans&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALuck%20II%3A%20Expecting%20White%20Swans%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4eHMNevX38Hg6sqBy%2Fluck-ii-expecting-white-swans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Luck%20II%3A%20Expecting%20White%20Swans%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4eHMNevX38Hg6sqBy%2Fluck-ii-expecting-white-swans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4eHMNevX38Hg6sqBy%2Fluck-ii-expecting-white-swans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2209, "htmlBody": "<p>When we left off <a href=\"/lw/hj1/luck_i_finding_white_swans/\"><span class=\"s2\">last time</span></a>, we had discussed two of the four principles which Dr. Richard Wiseman believes accounts for the differences between lucky and unlucky people: do more to maximize the number of chance opportunities you have and take steps to improve your intuition. In this post we will discuss the third principle: expect good luck. We will talk a little about principle four, but it's going to get its own post.&nbsp;</p>\n<p class=\"p1\"><span class=\"s1\">But before all that, I want to spend the first bit of this post clearing up some of the confusion which resulted from my less-than-perfect presentation. My most&nbsp;persistent&nbsp;critic, Lumifer, made a number of claims which I think are worth addressing. &nbsp;As I understand it, here is the meat of the criticism:</span></p>\n<blockquote>\n<p class=\"p1\"><span class=\"s1\">1) Luck should be defined as the benefit one receives from events they have no control over, not the result of systematic differences between lucky and unlucky people.</span></p>\n<p class=\"p1\"><span class=\"s1\">2) The luck being discussed in the post was entirely the result of self-perception. More neurotic people are of course going to think themselves less lucky and are going to be less open to novel experiences. Whether or not this is true is another matter. &nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">3) Related to point 2), perhaps lucky people are simply the people located in the thin strip on the right of a certain bell curve, and so they (naturally) consider themselves lucky. In other words, maybe the arrow of causality is pointing: [objectively lucky] --&gt; [acts differently], rather than the reverse. This point will crop up repeatedly. &nbsp;&nbsp;</span></p>\n</blockquote>\n<p class=\"p1\"><span class=\"s1\">We could argue definitions all day, but let it be known that when I talk about luck I mean the differences between lucky and unlucky people which are a result of differences in their behavior. When I talk about lucky people I mean people who self-describe as lucky and for whom there is weak anecdotal evidence of their luck. After reading Dr. Wiseman's book I have a high confidence level that such behavioral differences exist, a high confidence level that they matter, and a slightly less high confidence level that they can be taught.</span></p>\n<p class=\"p1\"><span class=\"s1\">Of course, there isn't much a person can do to prevent their being killed by a meteorite, and nothing at all a person can do to stop themselves being born with Down Syndrome. But taken to an extreme, points 2) and 3) seem equivalent to saying that there is simply <em>nothing </em>a person can do to increase the likelihood that they will not be made a fool of by Lady Luck. That seems unwarranted to me, not least because Dr. Wiseman appears to have been able to <a href=\"http://www.richardwiseman.com/research/moreluck.html\"><span class=\"s2\">teach some people the skill of luck</span></a>. </span></p>\n<p class=\"p1\"><span class=\"s1\">There are better and worse ways of improving your bench press, better and worse ways of learning a foreign language...why wouldn't there be better and worse ways of improving the odds that you'll be exposed to positive random events (or, alternatively, decreasing the odds that you'll be exposed to negative randomness)? &nbsp;I think this case is bolstered by the fact that, at least according to the testimony of lucky people, their good fortune is spread out among many different areas of their life. It would be one thing if these 'lucky' people had gotten a break in their career or lucked out in their choice of marriage partners, but many of them seem to be lucky almost across the board. &nbsp;This still doesn't rule out pure,&nbsp;unadulterated&nbsp;chance, but I think it makes person-specific causes more plausible. &nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">Granted, Dr. Wiseman's evidence comes mostly in the form of anecdotes, which is not particularly strong evidence. But it's more than <em>no</em> evidence. Establishing that people who think they are lucky really are objectively lucky at anything like p &lt; .05 would require a monumental longitudinal study which, to my knowledge, no one has even come close to doing. Nevertheless, it's my impression that Dr. Wiseman made an honest effort at epistemic cleanliness, utilizing numerous questionnaires, tests, interviews, and actual experiments to tease apart causal threads, establishing that there may well be&nbsp;behaviors&nbsp;which lead to more luck.</span></p>\n<p class=\"p1\"><span class=\"s1\">It's not a mathematical proof, but I think there is a good dose of truth to it, and I think it's useful.</span></p>\n<p class=\"p1\"><span class=\"s1\">With that out of the way, you'll recall that the four principles and twelve sub-principles are:</span></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p3\"><span class=\"s1\"><em>Principle One: Maximize the number of chance opportunities you have in life.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle one: lucky people maintain a network of contacts with other people.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle two: lucky people are more relaxed and less neurotic than unlucky people</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle three: lucky people have a strong drive towards novelty, and strive to introduce variety into their routines.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>Principle Two: Use your intuition to make important decisions.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle one: pay attention to your hunches.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle two: try and make your intuition more accurate.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>Principle Three: Expect good fortune. &nbsp;</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle one: lucky people believe their luck will continue.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle two: lucky people attempt to achieve their goals and persist through difficulty.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle three: lucky people think their interactions will be positive and successful.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>Principle Four: Turn bad luck into good.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle one: lucky people see the silver lining in bad situations.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle two: lucky people believe that things will work out for them in the long run.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle three: lucky people spend less time brooding over bad luck.</em></span></p>\n<p class=\"p4\"><span class=\"s1\"><em>&nbsp; sub-principle four: lucky people are more proactive in learning from their mistakes and preventing further bad luck.&nbsp;</em></span><span class=\"s3\">&nbsp;&nbsp;</span></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p2\"><strong>Great Expectations</strong></p>\n<p class=\"p1\">If you look at principle three and four, you'll see that most of the sub-principles have to do with what lucky people think will happen in the future. When given a set of questionnaires which tested respondents belief that they would experience positive and negative events in the future, we again find stark differences between lucky and unlucky people. Overwhelmingly, lucky people were more likely than unlucky people to believe they would have a good time on vacation, be admired for their accomplishments, develop good relationships with their families, etc. Conversely, unlucky people were more likely to believe that they would become overweight later in life, decide that they&rsquo;d chosen the wrong career, be mugged, etc.&nbsp;</p>\n<p class=\"p1\"><em>Maybe</em> this is straight forward inductive inference: if you've mostly had bad or good luck in the past, it makes sense to believe that this will continue into the future.&nbsp;But if psychology were this crisp and simple, life would be a lot easier. Besides all the heuristics and biases that cloud thinking, our expectations about the future feed back into the causal matrix which determines our behaviors, influencing both <em>what actually happens to us</em> as well as <em>how we interpret what happens to us</em>. Each of these will be important to our discussion.</p>\n<p class=\"p1\"><strong>Making self-fulfillment work for you (?)</strong></p>\n<p class=\"p1\">So what results when two groups of people vary in terms of their expectations for the future if we grant that these expectations exert some influence (however small) on what happens to them?&nbsp;</p>\n<p class=\"p1\">Dr. Wiseman believes that lucky people's positive expectations account for the fact that they are often very persistent in the face of adversity, and that this leads to self-fulfilling prophecies of success. When he gave three lucky and three unlucky people a very difficult puzzle to solve, two of the lucky people spent significantly longer working on the puzzle than the unlucky people, around 20 minutes vs. 1 hour +, respectively. (one of the lucky people miscounted the number of puzzle pieces and, believing one to be missing and thus the puzzle to be impossible, didn't even begin)! Quotes from interviews with lucky and unlucky people offer evidence that lucky people often spend more time chasing their ambitions while unlucky people have in some cases stopped even trying. &nbsp;</p>\n<p class=\"p1\">But are lucky people more persistent because of their beliefs that the future is bright, or could it be the case that lucky people were simply more&nbsp;persistent as a matter of their personal psychology?&nbsp;</p>\n<p class=\"p1\">A more clear-cut example comes from the realm of interpersonal interaction. Here, it turns out, we have good evidence for the power of self-fulfilling prophecies. Several famous studies have demonstrated that the beliefs you have when you enter into an interaction can profoundly shape the course of that interaction. <a href=\"https://business.missouri.edu/sites/default/files/publication/dougherty_turban_callender_1994_jap.pdf\">Dougherty et al., (1994)</a> found that, when people interviewing candidates for a job had high expectations for the candidates, they were friendly, and the candidates thus made a better impression. Still more powerfully, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3131&amp;rep=rep1&amp;type=pdf\">Snyder et al., (1977)</a>&nbsp;demonstrated that when men thought they were talking to an attractive woman, not only did they act more warmly towards the woman, and not only did she respond more sociably, but other people listening to only the woman's part of the conversation <em>also</em> thought she was more attractive.&nbsp;</p>\n<p class=\"p1\">Did Dr. Wiseman's research yield any new insights into this area? Anecdotes included in the book paint a picture of lucky people's ability to quickly form warm and close relationships with people, allegedly on the basis of their expectations that other people will be interesting, funny, etc.&nbsp;</p>\n<p class=\"p1\">Perhaps lucky people's beliefs that their interactions will be positive actually lead to positive interactions, and independent research indicates that there is something to this. But recall from my last post that&nbsp;lucky people also&nbsp;smile, make better eye contact, and have friendlier body language than unlucky people, and maybe this accounts for their good experiences with people. Or they could have just always been lucky with respect to their interactions and thus believe this state of affairs will continue. Unfortunately, I feel that Dr. Wiseman's work did little to clarify these underlying issues.</p>\n<p class=\"p1\">That said, I do think that there are two valuable things to learn here: 1) don't give up hope too early, and 2) people's expectations of others powerfully influence how their interactions unfold.&nbsp;</p>\n<p class=\"p1\">Remember how during the last essay I said that some people may worry that principle one ('maximize the number of chance opportunities you have') might also expose you to a lot of black swans? Well, persistence is one reason why this isn't such a big problem. With enough hard work, a gray or even black swan encounter can be made into a white swan (though I freely admit any rational person has to know when to give up). There is a bigger reason than this, though, but it'll have to wait for the next post because this one has gotten long enough.&nbsp;</p>\n<p class=\"p1\"><strong>Suggested Exercises&nbsp;</strong></p>\n<p class=\"p1\">As with the first two principles, Dr. Wiseman recommends the following exercises:</p>\n<p class=\"p1\">-Begin each day with positive affirmations, of the \"I know that I will be lucky in the future\" variety.</p>\n<p class=\"p1\">-Make a list of your short, medium, and long-term goals, reviewing the list periodically. This helps establish high expectations for the future.</p>\n<p class=\"p1\">-To maintain motivation, write down the costs and benefits associated with achieving a goal. Having a concrete analysis to look at should help you persist, assuming that the benefits actually do outweigh the costs.</p>\n<p class=\"p1\">-With a potentially difficult situation on the horizon, like a date or job interview, spend a few minutes visualizing yourself confidently and successfully navigating it.</p>\n<p class=\"p1\"><strong>Criticisms and open questions</strong></p>\n<p class=\"p1\">I'll come right out and say it: I thought this section was weaker than the others, and less useful to readers of this blog. There's so much mushy-headed nonsense out there about how 'perception is reality' and you should 'visualize your way into wealth' that when I read the the title of principle three ('expect good luck') my eyes glazed over a bit.</p>\n<p class=\"p1\">Still. Goals, emotions, expectations. These are as much a part of the fabric of the world as chairs are, and we can no more ignore them than we can any of the other threads in that tapestry. If it is true that what I will think will happen affects what will happen, even if those expectations aren't based on anything particularly rational, then I want to believe that that is the case, and plan my life accordingly.</p>\n<p class=\"p1\">So I ask:</p>\n<p class=\"p1\">1) Might there be domains where there is a slight negative expected utility for accuracy of belief, at least at the levels of rationality attainable by humans now (see: discussions of the <a href=\"http://wiki.lesswrong.com/wiki/Valley_of_bad_rationality\">valley of bad rationality</a>)? For a true master of the mature art of human rationality, a person who has a detailed self-model and very accurate probability estimates, there would presumably be no reason to fiddle with expectations; these would flow naturally from their beliefs about the world. But since I don't yet have anything like that, maybe it's a good idea for me to <em>purposefully try to make myself believe that the future will be good.</em></p>\n<p class=\"p1\">2) Can a person have a belief in self-fulfilling belief? If you know about self-fulfilling prophecies, does that make you better or worse at making them happen?&nbsp;</p>\n<p class=\"p1\">3) Let's say I'm an objectively, physically unattractive person, but because of positive attention I&nbsp;received&nbsp;during childhood I believe myself to be attractive and thus have moderate success in dating. Is my belief in my own attractiveness warranted? Does the answer change if, instead of being based on childhood experiences, I believe I'm attractive because I chanted \"I am attractive and deserving of love\" ten times before I left the house every morning?</p>\n<p class=\"p1\">4) Is it ethical to exploit this knowledge, even if you're doing it to make another person more successful? When, if ever, is it appropriate to put down the mantle of rationality and let people believe silly things (or even actively encourage them)? One possible example: when giving a pep talk to&nbsp;beleaguered troops in the minutes before a battle.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "nwcnHxrxcgnwJ878t": 1, "fkABsGCJZ6y9qConW": 1, "YyGDbZhGtws5hEPda": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4eHMNevX38Hg6sqBy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "25068", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>When we left off <a href=\"/lw/hj1/luck_i_finding_white_swans/\"><span class=\"s2\">last time</span></a>, we had discussed two of the four principles which Dr. Richard Wiseman believes accounts for the differences between lucky and unlucky people: do more to maximize the number of chance opportunities you have and take steps to improve your intuition. In this post we will discuss the third principle: expect good luck. We will talk a little about principle four, but it's going to get its own post.&nbsp;</p>\n<p class=\"p1\"><span class=\"s1\">But before all that, I want to spend the first bit of this post clearing up some of the confusion which resulted from my less-than-perfect presentation. My most&nbsp;persistent&nbsp;critic, Lumifer, made a number of claims which I think are worth addressing. &nbsp;As I understand it, here is the meat of the criticism:</span></p>\n<blockquote>\n<p class=\"p1\"><span class=\"s1\">1) Luck should be defined as the benefit one receives from events they have no control over, not the result of systematic differences between lucky and unlucky people.</span></p>\n<p class=\"p1\"><span class=\"s1\">2) The luck being discussed in the post was entirely the result of self-perception. More neurotic people are of course going to think themselves less lucky and are going to be less open to novel experiences. Whether or not this is true is another matter. &nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">3) Related to point 2), perhaps lucky people are simply the people located in the thin strip on the right of a certain bell curve, and so they (naturally) consider themselves lucky. In other words, maybe the arrow of causality is pointing: [objectively lucky] --&gt; [acts differently], rather than the reverse. This point will crop up repeatedly. &nbsp;&nbsp;</span></p>\n</blockquote>\n<p class=\"p1\"><span class=\"s1\">We could argue definitions all day, but let it be known that when I talk about luck I mean the differences between lucky and unlucky people which are a result of differences in their behavior. When I talk about lucky people I mean people who self-describe as lucky and for whom there is weak anecdotal evidence of their luck. After reading Dr. Wiseman's book I have a high confidence level that such behavioral differences exist, a high confidence level that they matter, and a slightly less high confidence level that they can be taught.</span></p>\n<p class=\"p1\"><span class=\"s1\">Of course, there isn't much a person can do to prevent their being killed by a meteorite, and nothing at all a person can do to stop themselves being born with Down Syndrome. But taken to an extreme, points 2) and 3) seem equivalent to saying that there is simply <em>nothing </em>a person can do to increase the likelihood that they will not be made a fool of by Lady Luck. That seems unwarranted to me, not least because Dr. Wiseman appears to have been able to <a href=\"http://www.richardwiseman.com/research/moreluck.html\"><span class=\"s2\">teach some people the skill of luck</span></a>. </span></p>\n<p class=\"p1\"><span class=\"s1\">There are better and worse ways of improving your bench press, better and worse ways of learning a foreign language...why wouldn't there be better and worse ways of improving the odds that you'll be exposed to positive random events (or, alternatively, decreasing the odds that you'll be exposed to negative randomness)? &nbsp;I think this case is bolstered by the fact that, at least according to the testimony of lucky people, their good fortune is spread out among many different areas of their life. It would be one thing if these 'lucky' people had gotten a break in their career or lucked out in their choice of marriage partners, but many of them seem to be lucky almost across the board. &nbsp;This still doesn't rule out pure,&nbsp;unadulterated&nbsp;chance, but I think it makes person-specific causes more plausible. &nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">Granted, Dr. Wiseman's evidence comes mostly in the form of anecdotes, which is not particularly strong evidence. But it's more than <em>no</em> evidence. Establishing that people who think they are lucky really are objectively lucky at anything like p &lt; .05 would require a monumental longitudinal study which, to my knowledge, no one has even come close to doing. Nevertheless, it's my impression that Dr. Wiseman made an honest effort at epistemic cleanliness, utilizing numerous questionnaires, tests, interviews, and actual experiments to tease apart causal threads, establishing that there may well be&nbsp;behaviors&nbsp;which lead to more luck.</span></p>\n<p class=\"p1\"><span class=\"s1\">It's not a mathematical proof, but I think there is a good dose of truth to it, and I think it's useful.</span></p>\n<p class=\"p1\"><span class=\"s1\">With that out of the way, you'll recall that the four principles and twelve sub-principles are:</span></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p3\"><span class=\"s1\"><em>Principle One: Maximize the number of chance opportunities you have in life.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle one: lucky people maintain a network of contacts with other people.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle two: lucky people are more relaxed and less neurotic than unlucky people</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle three: lucky people have a strong drive towards novelty, and strive to introduce variety into their routines.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>Principle Two: Use your intuition to make important decisions.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle one: pay attention to your hunches.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle two: try and make your intuition more accurate.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>Principle Three: Expect good fortune. &nbsp;</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle one: lucky people believe their luck will continue.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle two: lucky people attempt to achieve their goals and persist through difficulty.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle three: lucky people think their interactions will be positive and successful.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>Principle Four: Turn bad luck into good.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle one: lucky people see the silver lining in bad situations.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle two: lucky people believe that things will work out for them in the long run.</em></span></p>\n<p class=\"p3\"><span class=\"s1\"><em>&nbsp; sub-principle three: lucky people spend less time brooding over bad luck.</em></span></p>\n<p class=\"p4\"><span class=\"s1\"><em>&nbsp; sub-principle four: lucky people are more proactive in learning from their mistakes and preventing further bad luck.&nbsp;</em></span><span class=\"s3\">&nbsp;&nbsp;</span></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p2\"><strong id=\"Great_Expectations\">Great Expectations</strong></p>\n<p class=\"p1\">If you look at principle three and four, you'll see that most of the sub-principles have to do with what lucky people think will happen in the future. When given a set of questionnaires which tested respondents belief that they would experience positive and negative events in the future, we again find stark differences between lucky and unlucky people. Overwhelmingly, lucky people were more likely than unlucky people to believe they would have a good time on vacation, be admired for their accomplishments, develop good relationships with their families, etc. Conversely, unlucky people were more likely to believe that they would become overweight later in life, decide that they\u2019d chosen the wrong career, be mugged, etc.&nbsp;</p>\n<p class=\"p1\"><em>Maybe</em> this is straight forward inductive inference: if you've mostly had bad or good luck in the past, it makes sense to believe that this will continue into the future.&nbsp;But if psychology were this crisp and simple, life would be a lot easier. Besides all the heuristics and biases that cloud thinking, our expectations about the future feed back into the causal matrix which determines our behaviors, influencing both <em>what actually happens to us</em> as well as <em>how we interpret what happens to us</em>. Each of these will be important to our discussion.</p>\n<p class=\"p1\"><strong id=\"Making_self_fulfillment_work_for_you____\">Making self-fulfillment work for you (?)</strong></p>\n<p class=\"p1\">So what results when two groups of people vary in terms of their expectations for the future if we grant that these expectations exert some influence (however small) on what happens to them?&nbsp;</p>\n<p class=\"p1\">Dr. Wiseman believes that lucky people's positive expectations account for the fact that they are often very persistent in the face of adversity, and that this leads to self-fulfilling prophecies of success. When he gave three lucky and three unlucky people a very difficult puzzle to solve, two of the lucky people spent significantly longer working on the puzzle than the unlucky people, around 20 minutes vs. 1 hour +, respectively. (one of the lucky people miscounted the number of puzzle pieces and, believing one to be missing and thus the puzzle to be impossible, didn't even begin)! Quotes from interviews with lucky and unlucky people offer evidence that lucky people often spend more time chasing their ambitions while unlucky people have in some cases stopped even trying. &nbsp;</p>\n<p class=\"p1\">But are lucky people more persistent because of their beliefs that the future is bright, or could it be the case that lucky people were simply more&nbsp;persistent as a matter of their personal psychology?&nbsp;</p>\n<p class=\"p1\">A more clear-cut example comes from the realm of interpersonal interaction. Here, it turns out, we have good evidence for the power of self-fulfilling prophecies. Several famous studies have demonstrated that the beliefs you have when you enter into an interaction can profoundly shape the course of that interaction. <a href=\"https://business.missouri.edu/sites/default/files/publication/dougherty_turban_callender_1994_jap.pdf\">Dougherty et al., (1994)</a> found that, when people interviewing candidates for a job had high expectations for the candidates, they were friendly, and the candidates thus made a better impression. Still more powerfully, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3131&amp;rep=rep1&amp;type=pdf\">Snyder et al., (1977)</a>&nbsp;demonstrated that when men thought they were talking to an attractive woman, not only did they act more warmly towards the woman, and not only did she respond more sociably, but other people listening to only the woman's part of the conversation <em>also</em> thought she was more attractive.&nbsp;</p>\n<p class=\"p1\">Did Dr. Wiseman's research yield any new insights into this area? Anecdotes included in the book paint a picture of lucky people's ability to quickly form warm and close relationships with people, allegedly on the basis of their expectations that other people will be interesting, funny, etc.&nbsp;</p>\n<p class=\"p1\">Perhaps lucky people's beliefs that their interactions will be positive actually lead to positive interactions, and independent research indicates that there is something to this. But recall from my last post that&nbsp;lucky people also&nbsp;smile, make better eye contact, and have friendlier body language than unlucky people, and maybe this accounts for their good experiences with people. Or they could have just always been lucky with respect to their interactions and thus believe this state of affairs will continue. Unfortunately, I feel that Dr. Wiseman's work did little to clarify these underlying issues.</p>\n<p class=\"p1\">That said, I do think that there are two valuable things to learn here: 1) don't give up hope too early, and 2) people's expectations of others powerfully influence how their interactions unfold.&nbsp;</p>\n<p class=\"p1\">Remember how during the last essay I said that some people may worry that principle one ('maximize the number of chance opportunities you have') might also expose you to a lot of black swans? Well, persistence is one reason why this isn't such a big problem. With enough hard work, a gray or even black swan encounter can be made into a white swan (though I freely admit any rational person has to know when to give up). There is a bigger reason than this, though, but it'll have to wait for the next post because this one has gotten long enough.&nbsp;</p>\n<p class=\"p1\"><strong id=\"Suggested_Exercises_\">Suggested Exercises&nbsp;</strong></p>\n<p class=\"p1\">As with the first two principles, Dr. Wiseman recommends the following exercises:</p>\n<p class=\"p1\">-Begin each day with positive affirmations, of the \"I know that I will be lucky in the future\" variety.</p>\n<p class=\"p1\">-Make a list of your short, medium, and long-term goals, reviewing the list periodically. This helps establish high expectations for the future.</p>\n<p class=\"p1\">-To maintain motivation, write down the costs and benefits associated with achieving a goal. Having a concrete analysis to look at should help you persist, assuming that the benefits actually do outweigh the costs.</p>\n<p class=\"p1\">-With a potentially difficult situation on the horizon, like a date or job interview, spend a few minutes visualizing yourself confidently and successfully navigating it.</p>\n<p class=\"p1\"><strong id=\"Criticisms_and_open_questions\">Criticisms and open questions</strong></p>\n<p class=\"p1\">I'll come right out and say it: I thought this section was weaker than the others, and less useful to readers of this blog. There's so much mushy-headed nonsense out there about how 'perception is reality' and you should 'visualize your way into wealth' that when I read the the title of principle three ('expect good luck') my eyes glazed over a bit.</p>\n<p class=\"p1\">Still. Goals, emotions, expectations. These are as much a part of the fabric of the world as chairs are, and we can no more ignore them than we can any of the other threads in that tapestry. If it is true that what I will think will happen affects what will happen, even if those expectations aren't based on anything particularly rational, then I want to believe that that is the case, and plan my life accordingly.</p>\n<p class=\"p1\">So I ask:</p>\n<p class=\"p1\">1) Might there be domains where there is a slight negative expected utility for accuracy of belief, at least at the levels of rationality attainable by humans now (see: discussions of the <a href=\"http://wiki.lesswrong.com/wiki/Valley_of_bad_rationality\">valley of bad rationality</a>)? For a true master of the mature art of human rationality, a person who has a detailed self-model and very accurate probability estimates, there would presumably be no reason to fiddle with expectations; these would flow naturally from their beliefs about the world. But since I don't yet have anything like that, maybe it's a good idea for me to <em>purposefully try to make myself believe that the future will be good.</em></p>\n<p class=\"p1\">2) Can a person have a belief in self-fulfilling belief? If you know about self-fulfilling prophecies, does that make you better or worse at making them happen?&nbsp;</p>\n<p class=\"p1\">3) Let's say I'm an objectively, physically unattractive person, but because of positive attention I&nbsp;received&nbsp;during childhood I believe myself to be attractive and thus have moderate success in dating. Is my belief in my own attractiveness warranted? Does the answer change if, instead of being based on childhood experiences, I believe I'm attractive because I chanted \"I am attractive and deserving of love\" ten times before I left the house every morning?</p>\n<p class=\"p1\">4) Is it ethical to exploit this knowledge, even if you're doing it to make another person more successful? When, if ever, is it appropriate to put down the mantle of rationality and let people believe silly things (or even actively encourage them)? One possible example: when giving a pep talk to&nbsp;beleaguered troops in the minutes before a battle.</p>", "sections": [{"title": "Great Expectations", "anchor": "Great_Expectations", "level": 1}, {"title": "Making self-fulfillment work for you (?)", "anchor": "Making_self_fulfillment_work_for_you____", "level": 1}, {"title": "Suggested Exercises\u00a0", "anchor": "Suggested_Exercises_", "level": 1}, {"title": "Criticisms and open questions", "anchor": "Criticisms_and_open_questions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "87 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iptYS9nZ8kGCmBujc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-16T00:00:30.334Z", "modifiedAt": null, "url": null, "title": "Meetup : Meetup: Less Wrong Israel Meetup (Tel Aviv) with special guest Tomer Kagan", "slug": "meetup-meetup-less-wrong-israel-meetup-tel-aviv-with-special", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:10.642Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SoftFlare", "createdAt": "2012-11-09T00:22:21.187Z", "isAdmin": false, "displayName": "SoftFlare"}, "userId": "dSdSRiHQPaFuBXhG6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EjiuAsLNQBJYuNrwJ/meetup-meetup-less-wrong-israel-meetup-tel-aviv-with-special", "pageUrlRelative": "/posts/EjiuAsLNQBJYuNrwJ/meetup-meetup-less-wrong-israel-meetup-tel-aviv-with-special", "linkUrl": "https://www.lesswrong.com/posts/EjiuAsLNQBJYuNrwJ/meetup-meetup-less-wrong-israel-meetup-tel-aviv-with-special", "postedAtFormatted": "Monday, December 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meetup%3A%20Less%20Wrong%20Israel%20Meetup%20(Tel%20Aviv)%20with%20special%20guest%20Tomer%20Kagan&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meetup%3A%20Less%20Wrong%20Israel%20Meetup%20(Tel%20Aviv)%20with%20special%20guest%20Tomer%20Kagan%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEjiuAsLNQBJYuNrwJ%2Fmeetup-meetup-less-wrong-israel-meetup-tel-aviv-with-special%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meetup%3A%20Less%20Wrong%20Israel%20Meetup%20(Tel%20Aviv)%20with%20special%20guest%20Tomer%20Kagan%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEjiuAsLNQBJYuNrwJ%2Fmeetup-meetup-less-wrong-israel-meetup-tel-aviv-with-special", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEjiuAsLNQBJYuNrwJ%2Fmeetup-meetup-less-wrong-israel-meetup-tel-aviv-with-special", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 358, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/up'>Meetup: Less Wrong Israel Meetup (Tel Aviv) with special guest Tomer Kagan</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 December 2013 08:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">weizman 18, givatayim, Israel</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>* UPDATE *</strong></p>\n\n<p>Sadly, Tomer had to move his flight to an earlier date, and so he will not be able to attend.</p>\n\n<p>Instead, our main talk will be Solving real problems with economics by Aur Saraf .</p>\n\n<p>Furthermore, we will be hosting another special guest, Joshua Fox, a MIRI Research Associate and representative in Israel who will tell us a little about his job at MIRI and AI research.</p>\n\n<p><strong>* Previous meetup info below *</strong></p>\n\n<p>We're very proud to host special guest Tomer Kagan from Quixey for this upcoming event on Thursday, December 26th.\nTomer is the CEO and Co-Founder of Quixey. Quixey is a company dedicated to helping users find and use apps in their everyday lives. In 2011 he was listed in Forbes 30 Under 30.</p>\n\n<p>He currently sits on the board of the Machine Intelligence Research Institute and is also an advisor to Innovation Endeavors, Mucker Lab Seed Fund, and Upwest Labs.</p>\n\n<p>A lifelong entrepreneur, Tomer was the Founder and CEO of Your Logo Here, a branding solutions provider for companies such as Google and Slide. In his spare time he is an advisor to a local AZA chapter - a high school aged Jewish leadership-focused youth organization.</p>\n\n<p>Our program is:</p>\n\n<p>20:00-20:15: Assembly</p>\n\n<p>20:15-21:00: Main Talk</p>\n\n<p>21:00-22:00: Dinner &amp; Discussion</p>\n\n<p>22:00-23:00: Rump Session (minitalks)</p>\n\n<p>23:00-: End of official programming</p>\n\n<p>Main Talk: Tomer Kagan</p>\n\n<p>Backup Talk: Aur Saraf - Solving real problems with economics</p>\n\n<p>Rump Session: Each participant will give a 4-minute talk (+3 minute encore if we applaud hard enough). Giving a talk isn't mandatory, but it's highly recommended. Not confident that what you have to say is relevant to our interests? Unsure about your public speaking skills? Doesn't matter - in the rump session, anything goes. - Note, you don't have to prepare a talk to come! Speaking at the rump session is completely only if you want to.</p>\n\n<p>Feel free to contact me (Gal Hochberg) at hochbergg@gmail.com or at 0545330678 for any further information</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/up'>Meetup: Less Wrong Israel Meetup (Tel Aviv) with special guest Tomer Kagan</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EjiuAsLNQBJYuNrwJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "25084", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meetup__Less_Wrong_Israel_Meetup__Tel_Aviv__with_special_guest_Tomer_Kagan\">Discussion article for the meetup : <a href=\"/meetups/up\">Meetup: Less Wrong Israel Meetup (Tel Aviv) with special guest Tomer Kagan</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 December 2013 08:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">weizman 18, givatayim, Israel</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong id=\"__UPDATE__\">* UPDATE *</strong></p>\n\n<p>Sadly, Tomer had to move his flight to an earlier date, and so he will not be able to attend.</p>\n\n<p>Instead, our main talk will be Solving real problems with economics by Aur Saraf .</p>\n\n<p>Furthermore, we will be hosting another special guest, Joshua Fox, a MIRI Research Associate and representative in Israel who will tell us a little about his job at MIRI and AI research.</p>\n\n<p><strong id=\"__Previous_meetup_info_below__\">* Previous meetup info below *</strong></p>\n\n<p>We're very proud to host special guest Tomer Kagan from Quixey for this upcoming event on Thursday, December 26th.\nTomer is the CEO and Co-Founder of Quixey. Quixey is a company dedicated to helping users find and use apps in their everyday lives. In 2011 he was listed in Forbes 30 Under 30.</p>\n\n<p>He currently sits on the board of the Machine Intelligence Research Institute and is also an advisor to Innovation Endeavors, Mucker Lab Seed Fund, and Upwest Labs.</p>\n\n<p>A lifelong entrepreneur, Tomer was the Founder and CEO of Your Logo Here, a branding solutions provider for companies such as Google and Slide. In his spare time he is an advisor to a local AZA chapter - a high school aged Jewish leadership-focused youth organization.</p>\n\n<p>Our program is:</p>\n\n<p>20:00-20:15: Assembly</p>\n\n<p>20:15-21:00: Main Talk</p>\n\n<p>21:00-22:00: Dinner &amp; Discussion</p>\n\n<p>22:00-23:00: Rump Session (minitalks)</p>\n\n<p>23:00-: End of official programming</p>\n\n<p>Main Talk: Tomer Kagan</p>\n\n<p>Backup Talk: Aur Saraf - Solving real problems with economics</p>\n\n<p>Rump Session: Each participant will give a 4-minute talk (+3 minute encore if we applaud hard enough). Giving a talk isn't mandatory, but it's highly recommended. Not confident that what you have to say is relevant to our interests? Unsure about your public speaking skills? Doesn't matter - in the rump session, anything goes. - Note, you don't have to prepare a talk to come! Speaking at the rump session is completely only if you want to.</p>\n\n<p>Feel free to contact me (Gal Hochberg) at hochbergg@gmail.com or at 0545330678 for any further information</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meetup__Less_Wrong_Israel_Meetup__Tel_Aviv__with_special_guest_Tomer_Kagan1\">Discussion article for the meetup : <a href=\"/meetups/up\">Meetup: Less Wrong Israel Meetup (Tel Aviv) with special guest Tomer Kagan</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meetup: Less Wrong Israel Meetup (Tel Aviv) with special guest Tomer Kagan", "anchor": "Discussion_article_for_the_meetup___Meetup__Less_Wrong_Israel_Meetup__Tel_Aviv__with_special_guest_Tomer_Kagan", "level": 1}, {"title": "* UPDATE *", "anchor": "__UPDATE__", "level": 2}, {"title": "* Previous meetup info below *", "anchor": "__Previous_meetup_info_below__", "level": 2}, {"title": "Discussion article for the meetup : Meetup: Less Wrong Israel Meetup (Tel Aviv) with special guest Tomer Kagan", "anchor": "Discussion_article_for_the_meetup___Meetup__Less_Wrong_Israel_Meetup__Tel_Aviv__with_special_guest_Tomer_Kagan1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-16T17:18:56.648Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, How Can Visual Image Be Rational", "slug": "meetup-moscow-how-can-visual-image-be-rational", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2vjwsWbJ2GomYqAua/meetup-moscow-how-can-visual-image-be-rational", "pageUrlRelative": "/posts/2vjwsWbJ2GomYqAua/meetup-moscow-how-can-visual-image-be-rational", "linkUrl": "https://www.lesswrong.com/posts/2vjwsWbJ2GomYqAua/meetup-moscow-how-can-visual-image-be-rational", "postedAtFormatted": "Monday, December 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20How%20Can%20Visual%20Image%20Be%20Rational&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20How%20Can%20Visual%20Image%20Be%20Rational%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2vjwsWbJ2GomYqAua%2Fmeetup-moscow-how-can-visual-image-be-rational%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20How%20Can%20Visual%20Image%20Be%20Rational%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2vjwsWbJ2GomYqAua%2Fmeetup-moscow-how-can-visual-image-be-rational", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2vjwsWbJ2GomYqAua%2Fmeetup-moscow-how-can-visual-image-be-rational", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 261, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/uq'>Moscow, How Can Visual Image Be Rational</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 December 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at the same second entrance, but we will go to the new room at 16:00. So please do not be late.</p>\n\n<p>This Sunday we will have visual image and created expression workshop. You can ask to analyse you own image, you can also bring some photos. Or you can send these photos for us in advance at lw@lesswrong.ru.</p>\n\n<p>In the second section we will have something from the following list:</p>\n\n<ul>\n<li><p>Belief investigation</p></li>\n<li><p>Training game</p></li>\n<li><p>Dutch booking</p></li>\n<li><p>Moderated discussion</p></li>\n<li><p>Useful ideas sharing</p></li>\n<li><p>Tabletop games</p></li>\n</ul>\n\n<p>You can read a bit more in <a href=\"http://umneem.org/%D0%BD%D0%BE%D0%B2%D0%BE%D1%81%D1%82%D0%B8/68?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_meet_up_notice+20131222_meet_up&amp;utm_content=20131222_meet_up&amp;utm_campaign=moscow_meetups\">Russian notice</a>.</p>\n\n<p><strong>If you are going for the first time:</strong>\nWe gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.\nWe start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the second entrance and then move to the new room.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/uq'>Moscow, How Can Visual Image Be Rational</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2vjwsWbJ2GomYqAua", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4690619828658707e-06, "legacy": true, "legacyId": "25085", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__How_Can_Visual_Image_Be_Rational\">Discussion article for the meetup : <a href=\"/meetups/uq\">Moscow, How Can Visual Image Be Rational</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 December 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at the same second entrance, but we will go to the new room at 16:00. So please do not be late.</p>\n\n<p>This Sunday we will have visual image and created expression workshop. You can ask to analyse you own image, you can also bring some photos. Or you can send these photos for us in advance at lw@lesswrong.ru.</p>\n\n<p>In the second section we will have something from the following list:</p>\n\n<ul>\n<li><p>Belief investigation</p></li>\n<li><p>Training game</p></li>\n<li><p>Dutch booking</p></li>\n<li><p>Moderated discussion</p></li>\n<li><p>Useful ideas sharing</p></li>\n<li><p>Tabletop games</p></li>\n</ul>\n\n<p>You can read a bit more in <a href=\"http://umneem.org/%D0%BD%D0%BE%D0%B2%D0%BE%D1%81%D1%82%D0%B8/68?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_meet_up_notice+20131222_meet_up&amp;utm_content=20131222_meet_up&amp;utm_campaign=moscow_meetups\">Russian notice</a>.</p>\n\n<p><strong>If you are going for the first time:</strong>\nWe gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.\nWe start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the second entrance and then move to the new room.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__How_Can_Visual_Image_Be_Rational1\">Discussion article for the meetup : <a href=\"/meetups/uq\">Moscow, How Can Visual Image Be Rational</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, How Can Visual Image Be Rational", "anchor": "Discussion_article_for_the_meetup___Moscow__How_Can_Visual_Image_Be_Rational", "level": 1}, {"title": "Discussion article for the meetup : Moscow, How Can Visual Image Be Rational", "anchor": "Discussion_article_for_the_meetup___Moscow__How_Can_Visual_Image_Be_Rational1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-16T20:34:00.260Z", "modifiedAt": null, "url": null, "title": "Meetup : West Los Angeles - Resolutions", "slug": "meetup-west-los-angeles-resolutions", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kmTtNqNwQrPktmcoj/meetup-west-los-angeles-resolutions", "pageUrlRelative": "/posts/kmTtNqNwQrPktmcoj/meetup-west-los-angeles-resolutions", "linkUrl": "https://www.lesswrong.com/posts/kmTtNqNwQrPktmcoj/meetup-west-los-angeles-resolutions", "postedAtFormatted": "Monday, December 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20Los%20Angeles%20-%20Resolutions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20Los%20Angeles%20-%20Resolutions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmTtNqNwQrPktmcoj%2Fmeetup-west-los-angeles-resolutions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20Los%20Angeles%20-%20Resolutions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmTtNqNwQrPktmcoj%2Fmeetup-west-los-angeles-resolutions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmTtNqNwQrPktmcoj%2Fmeetup-west-los-angeles-resolutions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 237, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ur'>West Los Angeles - Resolutions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 December 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Suite 312, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, December 18.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion:</strong></p>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary;</em> this will be generally accessible and useful to everyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>We will talk about new years resolutions and other hacks to get your future self to listen to your present self. I will provide an abstract game theoretic model to explain why it is hard to get yourselves to get along, and propose a solution to this problem that hopefully all your future selves can agree on. Then I will explain the way new years resolutions work for most people and explain how you can use a different strategy to make them work for you much more effectively. Hopefully this will lead to a discussion about how my proposal can be made even more effective, and how it can be extended to everyday life. (As opposed to every year)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ur'>West Los Angeles - Resolutions</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kmTtNqNwQrPktmcoj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.4692643600533317e-06, "legacy": true, "legacyId": "25086", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_Los_Angeles___Resolutions\">Discussion article for the meetup : <a href=\"/meetups/ur\">West Los Angeles - Resolutions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 December 2013 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Suite 312, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, December 18.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong id=\"Discussion_\">Discussion:</strong></p>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary;</em> this will be generally accessible and useful to everyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>We will talk about new years resolutions and other hacks to get your future self to listen to your present self. I will provide an abstract game theoretic model to explain why it is hard to get yourselves to get along, and propose a solution to this problem that hopefully all your future selves can agree on. Then I will explain the way new years resolutions work for most people and explain how you can use a different strategy to make them work for you much more effectively. Hopefully this will lead to a discussion about how my proposal can be made even more effective, and how it can be extended to everyday life. (As opposed to every year)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_Los_Angeles___Resolutions1\">Discussion article for the meetup : <a href=\"/meetups/ur\">West Los Angeles - Resolutions</a></h2>", "sections": [{"title": "Discussion article for the meetup : West Los Angeles - Resolutions", "anchor": "Discussion_article_for_the_meetup___West_Los_Angeles___Resolutions", "level": 1}, {"title": "Discussion:", "anchor": "Discussion_", "level": 2}, {"title": "Discussion article for the meetup : West Los Angeles - Resolutions", "anchor": "Discussion_article_for_the_meetup___West_Los_Angeles___Resolutions1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-16T22:44:28.024Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, December 16-31", "slug": "group-rationality-diary-december-16-31", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:28.096Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WfewSr59eJfwZyYS4/group-rationality-diary-december-16-31", "pageUrlRelative": "/posts/WfewSr59eJfwZyYS4/group-rationality-diary-december-16-31", "linkUrl": "https://www.lesswrong.com/posts/WfewSr59eJfwZyYS4/group-rationality-diary-december-16-31", "postedAtFormatted": "Monday, December 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20December%2016-31&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20December%2016-31%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWfewSr59eJfwZyYS4%2Fgroup-rationality-diary-december-16-31%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20December%2016-31%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWfewSr59eJfwZyYS4%2Fgroup-rationality-diary-december-16-31", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWfewSr59eJfwZyYS4%2Fgroup-rationality-diary-december-16-31", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">This is the public group instrumental rationality diary for December 16-31.</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Immediate past diary: &nbsp;<a href=\"/lw/j97/group_rationality_diary_december_115/\">December 1-15</a>&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\"><a href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WfewSr59eJfwZyYS4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "25087", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YnxSxHnDvj3neYFzy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-17T06:56:27.729Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston Winter Solstice", "slug": "meetup-boston-winter-solstice", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cutB4qfXq7W4wunpi/meetup-boston-winter-solstice", "pageUrlRelative": "/posts/cutB4qfXq7W4wunpi/meetup-boston-winter-solstice", "linkUrl": "https://www.lesswrong.com/posts/cutB4qfXq7W4wunpi/meetup-boston-winter-solstice", "postedAtFormatted": "Tuesday, December 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20Winter%20Solstice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20Winter%20Solstice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcutB4qfXq7W4wunpi%2Fmeetup-boston-winter-solstice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20Winter%20Solstice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcutB4qfXq7W4wunpi%2Fmeetup-boston-winter-solstice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcutB4qfXq7W4wunpi%2Fmeetup-boston-winter-solstice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/us\">Boston Winter Solstice</a>&nbsp;(<a style=\"font-size: 16px;\" href=\"/r/discussion/lw/jeb/ritual_report_boston_solstice_celebration/\">Ritual Report</a>)</h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">21 December 2013 06:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Citadel, 98 Elm St Apt 1, Somerville, MA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The winter solstice is a secular holiday. Last Saturday was New York's large-scale celebration, designed by Ray Arnold, and this Saturday we're having a smaller and cozier local one, and putting our own spin on it. This involves sing-alongs, speeches, and contemplating our place in an uncaring universe. It has a fairly intense emotional arc, including some rather dark parts, and is quite inspirational overall.</p>\n<p>Dinner (potluck) will start at 6 pm. The ceremony will start at 7:30 pm and last about two hours.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/us\">Boston Winter Solstice</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cutB4qfXq7W4wunpi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "25090", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston_Winter_Solstice__Ritual_Report_\">Discussion article for the meetup : <a href=\"/meetups/us\">Boston Winter Solstice</a>&nbsp;(<a style=\"font-size: 16px;\" href=\"/r/discussion/lw/jeb/ritual_report_boston_solstice_celebration/\">Ritual Report</a>)</h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">21 December 2013 06:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Citadel, 98 Elm St Apt 1, Somerville, MA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The winter solstice is a secular holiday. Last Saturday was New York's large-scale celebration, designed by Ray Arnold, and this Saturday we're having a smaller and cozier local one, and putting our own spin on it. This involves sing-alongs, speeches, and contemplating our place in an uncaring universe. It has a fairly intense emotional arc, including some rather dark parts, and is quite inspirational overall.</p>\n<p>Dinner (potluck) will start at 6 pm. The ceremony will start at 7:30 pm and last about two hours.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Boston_Winter_Solstice\">Discussion article for the meetup : <a href=\"/meetups/us\">Boston Winter Solstice</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston Winter Solstice\u00a0(Ritual Report)", "anchor": "Discussion_article_for_the_meetup___Boston_Winter_Solstice__Ritual_Report_", "level": 1}, {"title": "Discussion article for the meetup : Boston Winter Solstice", "anchor": "Discussion_article_for_the_meetup___Boston_Winter_Solstice", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["r8zyX8zfQ2i4DAy66"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-17T07:03:56.976Z", "modifiedAt": null, "url": null, "title": "[LINK] David Deutsch on why we don't have AGI yet \"Creative Blocks\"", "slug": "link-david-deutsch-on-why-we-don-t-have-agi-yet-creative", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:28.734Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "harshhpareek", "createdAt": "2011-09-06T17:26:15.810Z", "isAdmin": false, "displayName": "harshhpareek"}, "userId": "Y7zNnpyJrotdfsMvQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7eEXKwFWiR2zx8x2k/link-david-deutsch-on-why-we-don-t-have-agi-yet-creative", "pageUrlRelative": "/posts/7eEXKwFWiR2zx8x2k/link-david-deutsch-on-why-we-don-t-have-agi-yet-creative", "linkUrl": "https://www.lesswrong.com/posts/7eEXKwFWiR2zx8x2k/link-david-deutsch-on-why-we-don-t-have-agi-yet-creative", "postedAtFormatted": "Tuesday, December 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20David%20Deutsch%20on%20why%20we%20don't%20have%20AGI%20yet%20%22Creative%20Blocks%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20David%20Deutsch%20on%20why%20we%20don't%20have%20AGI%20yet%20%22Creative%20Blocks%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7eEXKwFWiR2zx8x2k%2Flink-david-deutsch-on-why-we-don-t-have-agi-yet-creative%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20David%20Deutsch%20on%20why%20we%20don't%20have%20AGI%20yet%20%22Creative%20Blocks%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7eEXKwFWiR2zx8x2k%2Flink-david-deutsch-on-why-we-don-t-have-agi-yet-creative", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7eEXKwFWiR2zx8x2k%2Flink-david-deutsch-on-why-we-don-t-have-agi-yet-creative", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 555, "htmlBody": "<p>Folks here should be familiar with most of these arguments. Putting some interesting quotes below:</p>\n<p>http://aeon.co/magazine/being-human/david-deutsch-artificial-intelligence/</p>\n<p>\"Creative blocks: The very laws of physics imply that artificial intelligence must be possible. What's holding us up?\"</p>\n<blockquote>\n<p>Remember the significance attributed to Skynet&rsquo;s becoming &lsquo;self-aware&rsquo;? [...] The fact is that present-day software developers could straightforwardly program a computer to have &lsquo;self-awareness&rsquo; in the behavioural sense &mdash; for example, to pass the &lsquo;mirror test&rsquo; of being able to use a mirror to infer facts about itself &mdash; if they wanted to. [...] AGIs will indeed be capable of self-awareness &mdash; but that is because they will be General</p>\n</blockquote>\n<blockquote>\n<p>Some hope to learn how we can rig their programming to make [AGIs] constitutionally unable to harm humans (as in Isaac Asimov&rsquo;s &lsquo;laws of robotics&rsquo;), or to prevent them from acquiring the theory that the universe should be converted into paper clips (as imagined by Nick Bostrom). None of these are the real problem. It has always been the case that a single exceptionally creative person can be thousands of times as productive &mdash; economically, intellectually or whatever &mdash; as most people; and that such a person could do enormous harm were he to turn his powers to evil instead of good.[...] The battle between good and evil ideas is as old as our species and will go on regardless of the hardware on which it is running</p>\n</blockquote>\n<p>He also says confusing things about induction being inadequate for creativity which I'm guessing he couldn't support well in this short essay (perhaps he explains better in his books). Not quoting here. His attack on Bayesianism as an explanation for intelligence is valid and interesting, but could be wrong. Given what we know about neural networks, something like this does happen in the brain, and possibly even at a concept level.&nbsp;</p>\n<blockquote>\n<p>The doctrine assumes that minds work by assigning probabilities to their ideas and modifying those probabilities in the light of experience as a way of choosing how to act. This is especially perverse when it comes to an AGI&rsquo;s values &mdash; the moral and aesthetic ideas that inform its choices and intentions &mdash; for it allows only a behaviouristic model of them, in which values that are &lsquo;rewarded&rsquo; by &lsquo;experience&rsquo; are &lsquo;reinforced&rsquo; and come to dominate behaviour while those that are &lsquo;punished&rsquo; by &lsquo;experience&rsquo; are extinguished. As I argued above, that behaviourist, input-output model is appropriate for most computer programming other than AGI, but hopeless for AGI.</p>\n</blockquote>\n<p>His final conclusions are disagreeable. He somehow concludes that the principal bottleneck in AGI research is a philosophical one.&nbsp;</p>\n<blockquote></blockquote>\n<p>In his last paragraph, he makes the following controversial statement:</p>\n<blockquote>\n<p>For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees.</p>\n</blockquote>\n<p>This would be false if, for example, the mother controls gene expression while a foetus develops and helps shape the brain. We should be able to answer this question definitively once we can grow human babies completely in vitro. Another problem would be the impact of the cultural environment. A way to answer this question would be to see if our Stone Age ancestors would be classified as AGIs under a reasonable definition</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7eEXKwFWiR2zx8x2k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 4, "extendedScore": null, "score": 1.4699182724125026e-06, "legacy": true, "legacyId": "25091", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-17T15:08:57.063Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Social Southern Summer Solstice Celebration", "slug": "meetup-melbourne-social-southern-summer-solstice-celebration", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BraydenM", "createdAt": "2013-06-10T09:17:31.294Z", "isAdmin": false, "displayName": "BraydenM"}, "userId": "KBZHqcMC6z2rPZkZK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cL5dQqECgS3vrzZRH/meetup-melbourne-social-southern-summer-solstice-celebration", "pageUrlRelative": "/posts/cL5dQqECgS3vrzZRH/meetup-melbourne-social-southern-summer-solstice-celebration", "linkUrl": "https://www.lesswrong.com/posts/cL5dQqECgS3vrzZRH/meetup-melbourne-social-southern-summer-solstice-celebration", "postedAtFormatted": "Tuesday, December 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Social%20Southern%20Summer%20Solstice%20Celebration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Social%20Southern%20Summer%20Solstice%20Celebration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcL5dQqECgS3vrzZRH%2Fmeetup-melbourne-social-southern-summer-solstice-celebration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Social%20Southern%20Summer%20Solstice%20Celebration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcL5dQqECgS3vrzZRH%2Fmeetup-melbourne-social-southern-summer-solstice-celebration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcL5dQqECgS3vrzZRH%2Fmeetup-melbourne-social-southern-summer-solstice-celebration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 397, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ut'>Melbourne Social Southern Summer Solstice Celebration</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 December 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">54 Scenic Cres, Eltham North</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Welcome to the end of 2013, a time of feasts and of plenty, and a time to celebrate the successes of the previous year and prepare for the next one.</p>\n\n<p>Our traditional social meetup this month falls on the weekend of the 2013 Southern Summer Solstice. Lets take this opportunity to review our recent progress and bring a touch of ceremony into our lives.</p>\n\n<p>Instead of our usual establishment in Carlton, you are invited to join me at the special location of my family home, in Eltham.</p>\n\n<p>The focus for the evening will be on reflection and refocusing - What have we done in the past year that has lead to our successes? What could we have done differently? How can we improve our next year?</p>\n\n<p>We will have an opportunity to look at these from both an individual and societal level. There will be reflections on the progress of humanity, and participatory exercises on self improvement.</p>\n\n<p>In keeping with the tradition of feasting, we will have a pot-luck and BBQ dinner. All guests are encouraged to bring food to cook on the grill, a dish to share, or a contribution towards supplies. If you want to opt in, post what you are able to bring - be it meals, drinks, or deserts - to the Facebook event invitation (<a href=\"https://www.facebook.com/events/436330916489443\" rel=\"nofollow\">https://www.facebook.com/events/436330916489443</a>). Please RSVP by Friday morning so I can make final arrangements.</p>\n\n<p>Following the feast we can play some social party games, including a thematically appropriate Werewolf game, modified to include additional characters to make it even more interesting. If the clouds are clear I will bring out the telescope and we should have views of Venus, Jupiter's moons, our moon, and the milky way. We hope to end the evening with a mindfulness session.</p>\n\n<p>Come to 54 Scenic Crescent, Eltham North. Parking is available up the driveway, and we will offer lifts from Eltham train station. Arrive from 7pm if you want to help with the set up, and we will kick off formal proceedings and dinner at 8pm. Sunset will be 8:41PM, and the day length will be 14 hours 47 minutes.</p>\n\n<p>Let this be a ceremony to saviour, while the days remain long. I look forward to your attendance.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ut'>Melbourne Social Southern Summer Solstice Celebration</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cL5dQqECgS3vrzZRH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "25093", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Southern_Summer_Solstice_Celebration\">Discussion article for the meetup : <a href=\"/meetups/ut\">Melbourne Social Southern Summer Solstice Celebration</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 December 2013 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">54 Scenic Cres, Eltham North</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Welcome to the end of 2013, a time of feasts and of plenty, and a time to celebrate the successes of the previous year and prepare for the next one.</p>\n\n<p>Our traditional social meetup this month falls on the weekend of the 2013 Southern Summer Solstice. Lets take this opportunity to review our recent progress and bring a touch of ceremony into our lives.</p>\n\n<p>Instead of our usual establishment in Carlton, you are invited to join me at the special location of my family home, in Eltham.</p>\n\n<p>The focus for the evening will be on reflection and refocusing - What have we done in the past year that has lead to our successes? What could we have done differently? How can we improve our next year?</p>\n\n<p>We will have an opportunity to look at these from both an individual and societal level. There will be reflections on the progress of humanity, and participatory exercises on self improvement.</p>\n\n<p>In keeping with the tradition of feasting, we will have a pot-luck and BBQ dinner. All guests are encouraged to bring food to cook on the grill, a dish to share, or a contribution towards supplies. If you want to opt in, post what you are able to bring - be it meals, drinks, or deserts - to the Facebook event invitation (<a href=\"https://www.facebook.com/events/436330916489443\" rel=\"nofollow\">https://www.facebook.com/events/436330916489443</a>). Please RSVP by Friday morning so I can make final arrangements.</p>\n\n<p>Following the feast we can play some social party games, including a thematically appropriate Werewolf game, modified to include additional characters to make it even more interesting. If the clouds are clear I will bring out the telescope and we should have views of Venus, Jupiter's moons, our moon, and the milky way. We hope to end the evening with a mindfulness session.</p>\n\n<p>Come to 54 Scenic Crescent, Eltham North. Parking is available up the driveway, and we will offer lifts from Eltham train station. Arrive from 7pm if you want to help with the set up, and we will kick off formal proceedings and dinner at 8pm. Sunset will be 8:41PM, and the day length will be 14 hours 47 minutes.</p>\n\n<p>Let this be a ceremony to saviour, while the days remain long. I look forward to your attendance.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Southern_Summer_Solstice_Celebration1\">Discussion article for the meetup : <a href=\"/meetups/ut\">Melbourne Social Southern Summer Solstice Celebration</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Social Southern Summer Solstice Celebration", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Southern_Summer_Solstice_Celebration", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Social Southern Summer Solstice Celebration", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Southern_Summer_Solstice_Celebration1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-17T17:49:43.346Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal - How to Actually Change your Mind", "slug": "meetup-montreal-how-to-actually-change-your-mind", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bartimaeus", "createdAt": "2013-05-07T17:14:04.389Z", "isAdmin": false, "displayName": "bartimaeus"}, "userId": "mqWrbcZHzhfPLnJqg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3emaMD2ogEBcanGAT/meetup-montreal-how-to-actually-change-your-mind", "pageUrlRelative": "/posts/3emaMD2ogEBcanGAT/meetup-montreal-how-to-actually-change-your-mind", "linkUrl": "https://www.lesswrong.com/posts/3emaMD2ogEBcanGAT/meetup-montreal-how-to-actually-change-your-mind", "postedAtFormatted": "Tuesday, December 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20-%20How%20to%20Actually%20Change%20your%20Mind&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20-%20How%20to%20Actually%20Change%20your%20Mind%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3emaMD2ogEBcanGAT%2Fmeetup-montreal-how-to-actually-change-your-mind%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20-%20How%20to%20Actually%20Change%20your%20Mind%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3emaMD2ogEBcanGAT%2Fmeetup-montreal-how-to-actually-change-your-mind", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3emaMD2ogEBcanGAT%2Fmeetup-montreal-how-to-actually-change-your-mind", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 282, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/uu'>Montreal - How to Actually Change your Mind</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 January 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">459 rue McTavish Montreal Quebec Canada </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Montreal Less Wrong won't be meeting officially over the holidays due to several of us having busy schedules. However, send me a message on Less Wrong if you want to meet some fellow Less Wrongers over the holidays!\nIn January, we'll be having a meetup intended for newcomers to rationality. We'll look at some concrete 5-second level mental skills that can help us overcome mental obstacles to actually updating on evidence. Here's the description from the meetup.com page:\nPeople aren't very good at changing their minds when their ideas are challenged. Have you ever been in an argument with someone, and no matter how much evidence you showed them that they were wrong, they would just become more and more convinced that YOU are the one who's wrong? That is a common human trait called reactance, where trying to get people to change their minds polarizes them in the other direction.\nThat's one example out of several cognitive biases that can prevent us from realizing we're wrong, and this can sometimes have disastrous consequences. We'll take a look at some classic biases that might prevent us from updating our opinions, and try to pin down specific mental skills you can use to fix your thinking and actually update on evidence.\nLet me know if the date is good with everyone; the venue is subject to change once we have a better idea of how many people we'll be. Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/uu'>Montreal - How to Actually Change your Mind</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3emaMD2ogEBcanGAT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.4705891491958328e-06, "legacy": true, "legacyId": "25096", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal___How_to_Actually_Change_your_Mind\">Discussion article for the meetup : <a href=\"/meetups/uu\">Montreal - How to Actually Change your Mind</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 January 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">459 rue McTavish Montreal Quebec Canada </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Montreal Less Wrong won't be meeting officially over the holidays due to several of us having busy schedules. However, send me a message on Less Wrong if you want to meet some fellow Less Wrongers over the holidays!\nIn January, we'll be having a meetup intended for newcomers to rationality. We'll look at some concrete 5-second level mental skills that can help us overcome mental obstacles to actually updating on evidence. Here's the description from the meetup.com page:\nPeople aren't very good at changing their minds when their ideas are challenged. Have you ever been in an argument with someone, and no matter how much evidence you showed them that they were wrong, they would just become more and more convinced that YOU are the one who's wrong? That is a common human trait called reactance, where trying to get people to change their minds polarizes them in the other direction.\nThat's one example out of several cognitive biases that can prevent us from realizing we're wrong, and this can sometimes have disastrous consequences. We'll take a look at some classic biases that might prevent us from updating our opinions, and try to pin down specific mental skills you can use to fix your thinking and actually update on evidence.\nLet me know if the date is good with everyone; the venue is subject to change once we have a better idea of how many people we'll be. Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal___How_to_Actually_Change_your_Mind1\">Discussion article for the meetup : <a href=\"/meetups/uu\">Montreal - How to Actually Change your Mind</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal - How to Actually Change your Mind", "anchor": "Discussion_article_for_the_meetup___Montreal___How_to_Actually_Change_your_Mind", "level": 1}, {"title": "Discussion article for the meetup : Montreal - How to Actually Change your Mind", "anchor": "Discussion_article_for_the_meetup___Montreal___How_to_Actually_Change_your_Mind1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-17T20:41:28.303Z", "modifiedAt": null, "url": null, "title": "MIRI's Winter 2013 Matching Challenge", "slug": "miri-s-winter-2013-matching-challenge", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:37.691Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/arpzMZyzod3Nsqr4J/miri-s-winter-2013-matching-challenge", "pageUrlRelative": "/posts/arpzMZyzod3Nsqr4J/miri-s-winter-2013-matching-challenge", "linkUrl": "https://www.lesswrong.com/posts/arpzMZyzod3Nsqr4J/miri-s-winter-2013-matching-challenge", "postedAtFormatted": "Tuesday, December 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MIRI's%20Winter%202013%20Matching%20Challenge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMIRI's%20Winter%202013%20Matching%20Challenge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FarpzMZyzod3Nsqr4J%2Fmiri-s-winter-2013-matching-challenge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MIRI's%20Winter%202013%20Matching%20Challenge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FarpzMZyzod3Nsqr4J%2Fmiri-s-winter-2013-matching-challenge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FarpzMZyzod3Nsqr4J%2Fmiri-s-winter-2013-matching-challenge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 803, "htmlBody": "<p><strong>Update</strong>: The fundraiser has been completed! Details <a href=\"http://intelligence.org/2013/12/26/winter-2013-fundraiser-completed/\">here</a>. The original post follows...</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><small>(Cross-posted from <a href=\"http://intelligence.org/2013/12/02/2013-winter-matching-challenge/\">MIRI's blog</a>. <a href=\"http://intelligence.org/\">MIRI</a> maintains Less Wrong, with generous help from <a href=\"http://trikeapps.com/\">Trike Apps</a>, and much of the core content is written by salaried MIRI staff members.)</small></p>\n<p>Thanks to <a href=\"http://en.wikipedia.org/wiki/Peter_Thiel\">Peter Thiel</a>, every donation made to MIRI between now and January 15th, 2014 will be <strong>matched dollar-for-dollar</strong>!</p>\n<p>Also,&nbsp;<strong>gifts from \"new large donors\" will be matched 3x!</strong> That is, if you've given less than $5k to SIAI/MIRI ever, and you now give or pledge $5k or more, Thiel will donate $3 for every dollar you give or pledge.</p>\n<p>We don't know whether we'll be able to offer the 3:1 matching ever again, so if you're capable of giving $5k or more, we encourage you to take advantage of the opportunity while you can. Remember that:</p>\n<ul>\n<li>If you prefer to give monthly, no problem! If you pledge 6 months of monthly donations, your full 6-month pledge will be the donation amount to be matched. So if you give monthly, you can get 3:1 matching for only $834/mo (or $417/mo if you get matching from your employer).</li>\n<li>We accept <a href=\"http://en.wikipedia.org/wiki/Bitcoin\">Bitcoin</a> (BTC) and <a href=\"http://en.wikipedia.org/wiki/Ripple_(monetary_system)\">Ripple</a> (XRP), both of which have recently jumped in value. If the market value of your Bitcoin or Ripple is $5k or more on the day you make the donation, this will count for matching.</li>\n<li>If your employer matches your donations at 1:1 (check <a href=\"http://doublethedonation.com/miri\">here</a>), then you can take advantage of Thiel's 3:1 matching by giving as little as $2,500 (because it's $5k after corporate matching).</li>\n</ul>\n<p><em>Please email <a href=\"mailto: malo@intelligence.org\">malo@intelligence.org</a>&nbsp;if you intend on leveraging corporate matching or would like to pledge&nbsp;6 months of monthly donations, so that we can properly account for your contributions towards the fundraiser.</em></p>\n<p>Thiel's total match is capped at $250,000. The total amount raised will depend on how many people take advantage of 3:1 matching. We don't anticipate being able to hit the $250k cap without substantial use of 3:1 matching &mdash; so if you haven't given $5k thus far, please consider giving/pledging $5k or more during this drive. (If you'd like to know the total amount of your past donations to MIRI, just ask <a href=\"mailto: malo@intelligence.org\">malo@intelligence.org</a>.)</p>\n<p align=\"center\"><img src=\"https://intelligence.org/wp-content/uploads/2014/08/finished-winter-2013.png\" alt=\"\" /></p>\n<p style=\"text-align: center;\">Now is your chance to <strong>double or quadruple your impact</strong> in funding our <a href=\"http://intelligence.org/research/\">research program</a>.</p>\n<p align=\"center\"><big><a href=\"https://intelligence.org/donate/#donation-methods\">Donate Today</a></big></p>\n<p align=\"center\"><img class=\"img-rounded shadowed aligncenter\" src=\"https://intelligence.org/wp-content/uploads/2013/12/workshop-horizontal-3.jpg\" alt=\"\" /></p>\n<h3>Accomplishments Since Our July 2013 Fundraiser Launched:</h3>\n<ul>\n<li>Held three <strong><a href=\"http://intelligence.org/workshops/#past-workshops\">research workshops</a></strong>, including our <a href=\"http://intelligence.org/2013/08/30/miris-november-2013-workshop-in-oxford/\">first European workshop</a>.</li>\n<li><a href=\"http://intelligence.org/2013/10/01/upcoming-talks-at-harvard-and-mit/\"><strong>Talks</strong> at MIT and Harvard</a>, by Eliezer Yudkowsky and Paul Christiano.</li>\n<li>Yudkowsky is blogging more <strong>Open Problems in Friendly AI</strong>...&nbsp;<a href=\"https://www.facebook.com/groups/233397376818827/\">on Facebook</a>! (They're also being written up in a more conventional format.)</li>\n<li>New <strong>papers</strong>: (1)&nbsp;<a href=\"/r/discussion/lw/i8i/algorithmic_progress_in_six_domains/\">Algorithmic Progress in Six Domains</a>; (2)&nbsp;<a href=\"http://intelligence.org/2013/10/30/new-paper-embryo-selection-for-cognitive-enhancement/\">Embryo Selection for Cognitive Enhancement</a>; (3)&nbsp;<a href=\"http://intelligence.org/2013/11/27/new-paper-racing-to-the-precipice/\">Racing to the Precipice</a>; (4) <a href=\"http://intelligence.org/files/PredictingAGI.pdf\">Predicting AGI: What can we say when we know so little?</a></li>\n<li>New <strong>ebook</strong>: <a href=\"http://intelligence.org/ai-foom-debate/\"><em>The Hanson-Yudkowsky AI-Foom Debate</em></a>.</li>\n<li>New <strong>analyses</strong>: (1) <a href=\"http://intelligence.org/2013/11/04/from-philosophy-to-math-to-engineering/\">From Philosophy to Math to Engineering</a>; (2)&nbsp;<a href=\"http://intelligence.org/2013/09/12/how-well-will-policy-makers-handle-agi-initial-findings/\">How well will policy-makers handle AGI?</a> (3)&nbsp;<a href=\"http://intelligence.org/2013/09/04/how-effectively-can-we-plan-for-future-decades/\">How effectively can we plan for future decades?</a> (4)&nbsp;<a href=\"http://intelligence.org/2013/08/25/transparency-in-safety-critical-systems/\">Transparency in Safety-Critical Systems</a>; (5)&nbsp;<a href=\"http://intelligence.org/2013/10/03/proofs/\">Mathematical Proofs Improve But Don&rsquo;t Guarantee Security, Safety, and Friendliness</a>; (6)&nbsp;<a href=\"http://intelligence.org/2013/08/11/what-is-agi/\">What is AGI?</a> (7)&nbsp;<a href=\"http://intelligence.org/2013/07/31/ai-risk-and-the-security-mindset/\">AI Risk and the Security Mindset</a>; (8)&nbsp;<a href=\"http://intelligence.org/2013/10/18/richard-posner-on-ai-dangers/\">Richard Posner on AI Dangers</a>; (9)&nbsp;<a href=\"http://intelligence.org/2013/10/19/russell-and-norvig-on-friendly-ai/\">Russell and Norvig on Friendly AI</a>.</li>\n<li>New <strong>expert interviews</strong>: <a href=\"http://intelligence.org/2013/11/05/greg-morrisett-on-secure-and-reliable-systems-2/\">Greg Morrisett</a> (Harvard),&nbsp;<a href=\"http://intelligence.org/2013/11/01/robin-hanson/\">Robin Hanson</a> (GMU), <a href=\"http://intelligence.org/2013/09/25/paul-rosenbloom-interview/\">Paul Rosenbloom</a> (USC), <a href=\"http://intelligence.org/2013/08/31/stephen-hsu-on-cognitive-genomics/\">Stephen Hsu</a> (MSU),&nbsp;<a href=\"http://intelligence.org/2013/10/28/markus-schmidt-on-risks-from-novel-biotechnologies/\">Markus Schmidt</a> (Biofaction), <a href=\"http://intelligence.org/2013/09/06/laurent-orseau-on-agi/\">Laurent Orseau</a> (AgroParisTech), <a href=\"http://intelligence.org/2013/08/25/holden-karnofsky-interview/\">Holden Karnofsky</a> (GiveWell),&nbsp;<a href=\"http://intelligence.org/2013/10/25/bas-steunebrink-on-sleight/\">Bas Steunebrink</a> (IDSIA),&nbsp;<a href=\"http://intelligence.org/2013/10/21/hadi-esmaeilzadeh-on-dark-silicon/\">Hadi Esmaeilzadeh</a>&nbsp;(GIT), <a href=\"http://intelligence.org/2013/07/17/beckstead-interview/\">Nick Beckstead</a> (Oxford),&nbsp;<a href=\"http://intelligence.org/2013/08/04/benja-interview/\">Benja Fallenstein</a> (Bristol), <a href=\"http://intelligence.org/2013/07/15/roman-interview/\">Roman Yampolskiy</a> (U Louisville),&nbsp;<a href=\"http://intelligence.org/2013/10/18/ben-goertzel/\">Ben Goertzel</a> (Novamente), and <a href=\"http://intelligence.org/2013/07/12/james-miller-interview/\">James Miller</a> (Smith College).</li>\n<li>With <a href=\"http://www.leverageresearch.org/\">Leverage Research</a>, we held a San Francisco <strong>book launch party</strong> for James Barratt's&nbsp;<a href=\"http://www.amazon.com/Our-Final-Invention-Artificial-Intelligence-ebook/dp/B00CQYAWRY/\"><em>Our Final Invention</em></a>, which discusses MIRI's work at length. (If you live in the Bay Area and would like to be notified of local events, please tell malo@intelligence.org!)</li>\n</ul>\n<h3>How Will Marginal Funds Be Used?</h3>\n<ul>\n<li><strong>Hiring Friendly AI researchers</strong>, identified through our workshops, as they become available for full-time work at MIRI.</li>\n<li>Running <strong>more workshops</strong> (next one begins&nbsp;<a href=\"http://intelligence.org/2013/07/24/miris-december-2013-workshop/\">Dec. 14th</a>), to make concrete Friendly AI research progress, to introduce new researchers to open problems in Friendly AI, and to identify candidates for MIRI to hire.</li>\n<li>Describing more <strong>open problems in Friendly AI</strong>. Our current strategy is for Yudkowsky to explain them as quickly as possible via Facebook discussion, followed by more structured explanations written by others in collaboration with Yudkowsky.</li>\n<li>Improving humanity's <strong>strategic understanding</strong> of what to do about superintelligence. In the coming months this will include (1) additional interviews and analyses on our blog, (2) a reader's guide for Nick Bostrom's forthcoming&nbsp;<a href=\"http://ukcatalogue.oup.com/product/9780199678112.do\"><em>Superintelligence</em></a><a href=\"http://ukcatalogue.oup.com/product/9780199678112.do\"> book</a>, and (3) an introductory ebook currently titled&nbsp;<em>Smarter Than Us.</em></li>\n</ul>\n<p>Other projects are still being surveyed for likely cost and impact.</p>\n<p><small>We appreciate your support for our work! <a href=\"https://intelligence.org/donate/#donation-methods\">Donate now</a>, and seize a better than usual chance to move our work forward. If you have questions about donating, please contact Louie Helm at (510) 717-1477 or louie@intelligence.org. <a href=\"https://linkpeek.com\">Screenshot Service</a> provided by LinkPeek.com.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "arpzMZyzod3Nsqr4J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 34, "extendedScore": null, "score": 1.4707676663169741e-06, "legacy": true, "legacyId": "25094", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Update</strong>: The fundraiser has been completed! Details <a href=\"http://intelligence.org/2013/12/26/winter-2013-fundraiser-completed/\">here</a>. The original post follows...</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><small>(Cross-posted from <a href=\"http://intelligence.org/2013/12/02/2013-winter-matching-challenge/\">MIRI's blog</a>. <a href=\"http://intelligence.org/\">MIRI</a> maintains Less Wrong, with generous help from <a href=\"http://trikeapps.com/\">Trike Apps</a>, and much of the core content is written by salaried MIRI staff members.)</small></p>\n<p>Thanks to <a href=\"http://en.wikipedia.org/wiki/Peter_Thiel\">Peter Thiel</a>, every donation made to MIRI between now and January 15th, 2014 will be <strong>matched dollar-for-dollar</strong>!</p>\n<p>Also,&nbsp;<strong>gifts from \"new large donors\" will be matched 3x!</strong> That is, if you've given less than $5k to SIAI/MIRI ever, and you now give or pledge $5k or more, Thiel will donate $3 for every dollar you give or pledge.</p>\n<p>We don't know whether we'll be able to offer the 3:1 matching ever again, so if you're capable of giving $5k or more, we encourage you to take advantage of the opportunity while you can. Remember that:</p>\n<ul>\n<li>If you prefer to give monthly, no problem! If you pledge 6 months of monthly donations, your full 6-month pledge will be the donation amount to be matched. So if you give monthly, you can get 3:1 matching for only $834/mo (or $417/mo if you get matching from your employer).</li>\n<li>We accept <a href=\"http://en.wikipedia.org/wiki/Bitcoin\">Bitcoin</a> (BTC) and <a href=\"http://en.wikipedia.org/wiki/Ripple_(monetary_system)\">Ripple</a> (XRP), both of which have recently jumped in value. If the market value of your Bitcoin or Ripple is $5k or more on the day you make the donation, this will count for matching.</li>\n<li>If your employer matches your donations at 1:1 (check <a href=\"http://doublethedonation.com/miri\">here</a>), then you can take advantage of Thiel's 3:1 matching by giving as little as $2,500 (because it's $5k after corporate matching).</li>\n</ul>\n<p><em>Please email <a href=\"mailto: malo@intelligence.org\">malo@intelligence.org</a>&nbsp;if you intend on leveraging corporate matching or would like to pledge&nbsp;6 months of monthly donations, so that we can properly account for your contributions towards the fundraiser.</em></p>\n<p>Thiel's total match is capped at $250,000. The total amount raised will depend on how many people take advantage of 3:1 matching. We don't anticipate being able to hit the $250k cap without substantial use of 3:1 matching \u2014 so if you haven't given $5k thus far, please consider giving/pledging $5k or more during this drive. (If you'd like to know the total amount of your past donations to MIRI, just ask <a href=\"mailto: malo@intelligence.org\">malo@intelligence.org</a>.)</p>\n<p align=\"center\"><img src=\"https://intelligence.org/wp-content/uploads/2014/08/finished-winter-2013.png\" alt=\"\"></p>\n<p style=\"text-align: center;\">Now is your chance to <strong>double or quadruple your impact</strong> in funding our <a href=\"http://intelligence.org/research/\">research program</a>.</p>\n<p align=\"center\"><big><a href=\"https://intelligence.org/donate/#donation-methods\">Donate Today</a></big></p>\n<p align=\"center\"><img class=\"img-rounded shadowed aligncenter\" src=\"https://intelligence.org/wp-content/uploads/2013/12/workshop-horizontal-3.jpg\" alt=\"\"></p>\n<h3 id=\"Accomplishments_Since_Our_July_2013_Fundraiser_Launched_\">Accomplishments Since Our July 2013 Fundraiser Launched:</h3>\n<ul>\n<li>Held three <strong><a href=\"http://intelligence.org/workshops/#past-workshops\">research workshops</a></strong>, including our <a href=\"http://intelligence.org/2013/08/30/miris-november-2013-workshop-in-oxford/\">first European workshop</a>.</li>\n<li><a href=\"http://intelligence.org/2013/10/01/upcoming-talks-at-harvard-and-mit/\"><strong>Talks</strong> at MIT and Harvard</a>, by Eliezer Yudkowsky and Paul Christiano.</li>\n<li>Yudkowsky is blogging more <strong>Open Problems in Friendly AI</strong>...&nbsp;<a href=\"https://www.facebook.com/groups/233397376818827/\">on Facebook</a>! (They're also being written up in a more conventional format.)</li>\n<li>New <strong>papers</strong>: (1)&nbsp;<a href=\"/r/discussion/lw/i8i/algorithmic_progress_in_six_domains/\">Algorithmic Progress in Six Domains</a>; (2)&nbsp;<a href=\"http://intelligence.org/2013/10/30/new-paper-embryo-selection-for-cognitive-enhancement/\">Embryo Selection for Cognitive Enhancement</a>; (3)&nbsp;<a href=\"http://intelligence.org/2013/11/27/new-paper-racing-to-the-precipice/\">Racing to the Precipice</a>; (4) <a href=\"http://intelligence.org/files/PredictingAGI.pdf\">Predicting AGI: What can we say when we know so little?</a></li>\n<li>New <strong>ebook</strong>: <a href=\"http://intelligence.org/ai-foom-debate/\"><em>The Hanson-Yudkowsky AI-Foom Debate</em></a>.</li>\n<li>New <strong>analyses</strong>: (1) <a href=\"http://intelligence.org/2013/11/04/from-philosophy-to-math-to-engineering/\">From Philosophy to Math to Engineering</a>; (2)&nbsp;<a href=\"http://intelligence.org/2013/09/12/how-well-will-policy-makers-handle-agi-initial-findings/\">How well will policy-makers handle AGI?</a> (3)&nbsp;<a href=\"http://intelligence.org/2013/09/04/how-effectively-can-we-plan-for-future-decades/\">How effectively can we plan for future decades?</a> (4)&nbsp;<a href=\"http://intelligence.org/2013/08/25/transparency-in-safety-critical-systems/\">Transparency in Safety-Critical Systems</a>; (5)&nbsp;<a href=\"http://intelligence.org/2013/10/03/proofs/\">Mathematical Proofs Improve But Don\u2019t Guarantee Security, Safety, and Friendliness</a>; (6)&nbsp;<a href=\"http://intelligence.org/2013/08/11/what-is-agi/\">What is AGI?</a> (7)&nbsp;<a href=\"http://intelligence.org/2013/07/31/ai-risk-and-the-security-mindset/\">AI Risk and the Security Mindset</a>; (8)&nbsp;<a href=\"http://intelligence.org/2013/10/18/richard-posner-on-ai-dangers/\">Richard Posner on AI Dangers</a>; (9)&nbsp;<a href=\"http://intelligence.org/2013/10/19/russell-and-norvig-on-friendly-ai/\">Russell and Norvig on Friendly AI</a>.</li>\n<li>New <strong>expert interviews</strong>: <a href=\"http://intelligence.org/2013/11/05/greg-morrisett-on-secure-and-reliable-systems-2/\">Greg Morrisett</a> (Harvard),&nbsp;<a href=\"http://intelligence.org/2013/11/01/robin-hanson/\">Robin Hanson</a> (GMU), <a href=\"http://intelligence.org/2013/09/25/paul-rosenbloom-interview/\">Paul Rosenbloom</a> (USC), <a href=\"http://intelligence.org/2013/08/31/stephen-hsu-on-cognitive-genomics/\">Stephen Hsu</a> (MSU),&nbsp;<a href=\"http://intelligence.org/2013/10/28/markus-schmidt-on-risks-from-novel-biotechnologies/\">Markus Schmidt</a> (Biofaction), <a href=\"http://intelligence.org/2013/09/06/laurent-orseau-on-agi/\">Laurent Orseau</a> (AgroParisTech), <a href=\"http://intelligence.org/2013/08/25/holden-karnofsky-interview/\">Holden Karnofsky</a> (GiveWell),&nbsp;<a href=\"http://intelligence.org/2013/10/25/bas-steunebrink-on-sleight/\">Bas Steunebrink</a> (IDSIA),&nbsp;<a href=\"http://intelligence.org/2013/10/21/hadi-esmaeilzadeh-on-dark-silicon/\">Hadi Esmaeilzadeh</a>&nbsp;(GIT), <a href=\"http://intelligence.org/2013/07/17/beckstead-interview/\">Nick Beckstead</a> (Oxford),&nbsp;<a href=\"http://intelligence.org/2013/08/04/benja-interview/\">Benja Fallenstein</a> (Bristol), <a href=\"http://intelligence.org/2013/07/15/roman-interview/\">Roman Yampolskiy</a> (U Louisville),&nbsp;<a href=\"http://intelligence.org/2013/10/18/ben-goertzel/\">Ben Goertzel</a> (Novamente), and <a href=\"http://intelligence.org/2013/07/12/james-miller-interview/\">James Miller</a> (Smith College).</li>\n<li>With <a href=\"http://www.leverageresearch.org/\">Leverage Research</a>, we held a San Francisco <strong>book launch party</strong> for James Barratt's&nbsp;<a href=\"http://www.amazon.com/Our-Final-Invention-Artificial-Intelligence-ebook/dp/B00CQYAWRY/\"><em>Our Final Invention</em></a>, which discusses MIRI's work at length. (If you live in the Bay Area and would like to be notified of local events, please tell malo@intelligence.org!)</li>\n</ul>\n<h3 id=\"How_Will_Marginal_Funds_Be_Used_\">How Will Marginal Funds Be Used?</h3>\n<ul>\n<li><strong>Hiring Friendly AI researchers</strong>, identified through our workshops, as they become available for full-time work at MIRI.</li>\n<li>Running <strong>more workshops</strong> (next one begins&nbsp;<a href=\"http://intelligence.org/2013/07/24/miris-december-2013-workshop/\">Dec. 14th</a>), to make concrete Friendly AI research progress, to introduce new researchers to open problems in Friendly AI, and to identify candidates for MIRI to hire.</li>\n<li>Describing more <strong>open problems in Friendly AI</strong>. Our current strategy is for Yudkowsky to explain them as quickly as possible via Facebook discussion, followed by more structured explanations written by others in collaboration with Yudkowsky.</li>\n<li>Improving humanity's <strong>strategic understanding</strong> of what to do about superintelligence. In the coming months this will include (1) additional interviews and analyses on our blog, (2) a reader's guide for Nick Bostrom's forthcoming&nbsp;<a href=\"http://ukcatalogue.oup.com/product/9780199678112.do\"><em>Superintelligence</em></a><a href=\"http://ukcatalogue.oup.com/product/9780199678112.do\"> book</a>, and (3) an introductory ebook currently titled&nbsp;<em>Smarter Than Us.</em></li>\n</ul>\n<p>Other projects are still being surveyed for likely cost and impact.</p>\n<p><small>We appreciate your support for our work! <a href=\"https://intelligence.org/donate/#donation-methods\">Donate now</a>, and seize a better than usual chance to move our work forward. If you have questions about donating, please contact Louie Helm at (510) 717-1477 or louie@intelligence.org. <a href=\"https://linkpeek.com\">Screenshot Service</a> provided by LinkPeek.com.</small></p>", "sections": [{"title": "Accomplishments Since Our July 2013 Fundraiser Launched:", "anchor": "Accomplishments_Since_Our_July_2013_Fundraiser_Launched_", "level": 1}, {"title": "How Will Marginal Funds Be Used?", "anchor": "How_Will_Marginal_Funds_Be_Used_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "37 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ueBMpvDsDEZgKiESt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-17T20:43:49.506Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes December 2013", "slug": "rationality-quotes-december-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:04.257Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wz752MMvLT9cBMNYy/rationality-quotes-december-2013", "pageUrlRelative": "/posts/wz752MMvLT9cBMNYy/rationality-quotes-december-2013", "linkUrl": "https://www.lesswrong.com/posts/wz752MMvLT9cBMNYy/rationality-quotes-december-2013", "postedAtFormatted": "Tuesday, December 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20December%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20December%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwz752MMvLT9cBMNYy%2Frationality-quotes-december-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20December%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwz752MMvLT9cBMNYy%2Frationality-quotes-december-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwz752MMvLT9cBMNYy%2Frationality-quotes-december-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<p>Rationality quotes time!&nbsp;</p>\n<p>The usual rules:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">\n<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson. If you'd like to revive an old quote from one of those sources, please do so&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wz752MMvLT9cBMNYy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 1.4707701126853483e-06, "legacy": true, "legacyId": "24936", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 464, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-17T20:45:00.004Z", "modifiedAt": null, "url": null, "title": "Open thread for December 17-23, 2013", "slug": "open-thread-for-december-17-23-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:34.484Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hvy79twuX4QGZ3MTZ/open-thread-for-december-17-23-2013", "pageUrlRelative": "/posts/Hvy79twuX4QGZ3MTZ/open-thread-for-december-17-23-2013", "linkUrl": "https://www.lesswrong.com/posts/Hvy79twuX4QGZ3MTZ/open-thread-for-december-17-23-2013", "postedAtFormatted": "Tuesday, December 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%20for%20December%2017-23%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%20for%20December%2017-23%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHvy79twuX4QGZ3MTZ%2Fopen-thread-for-december-17-23-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%20for%20December%2017-23%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHvy79twuX4QGZ3MTZ%2Fopen-thread-for-december-17-23-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHvy79twuX4QGZ3MTZ%2Fopen-thread-for-december-17-23-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hvy79twuX4QGZ3MTZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 1.470771334053072e-06, "legacy": true, "legacyId": "25097", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 306, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-18T01:28:30.265Z", "modifiedAt": null, "url": null, "title": "Meetup : Rationality Meetup Vienna", "slug": "meetup-rationality-meetup-vienna", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:33.214Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ratcourse", "createdAt": "2012-11-03T10:26:05.641Z", "isAdmin": false, "displayName": "Ratcourse"}, "userId": "qwnfbBpAcbxLT4JBr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7vvRz4e3iz3HXzpbE/meetup-rationality-meetup-vienna", "pageUrlRelative": "/posts/7vvRz4e3iz3HXzpbE/meetup-rationality-meetup-vienna", "linkUrl": "https://www.lesswrong.com/posts/7vvRz4e3iz3HXzpbE/meetup-rationality-meetup-vienna", "postedAtFormatted": "Wednesday, December 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Rationality%20Meetup%20Vienna&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Rationality%20Meetup%20Vienna%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7vvRz4e3iz3HXzpbE%2Fmeetup-rationality-meetup-vienna%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Rationality%20Meetup%20Vienna%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7vvRz4e3iz3HXzpbE%2Fmeetup-rationality-meetup-vienna", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7vvRz4e3iz3HXzpbE%2Fmeetup-rationality-meetup-vienna", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/uv'>Rationality Meetup Vienna</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 January 2014 03:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Vienna, Reichsratstra\u00dfe 17 Cafe Votiv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Quantum Physics and its implications for a rational world view</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/uv'>Rationality Meetup Vienna</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7vvRz4e3iz3HXzpbE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.4710660951548428e-06, "legacy": true, "legacyId": "25099", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Rationality_Meetup_Vienna\">Discussion article for the meetup : <a href=\"/meetups/uv\">Rationality Meetup Vienna</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 January 2014 03:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Vienna, Reichsratstra\u00dfe 17 Cafe Votiv</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Quantum Physics and its implications for a rational world view</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Rationality_Meetup_Vienna1\">Discussion article for the meetup : <a href=\"/meetups/uv\">Rationality Meetup Vienna</a></h2>", "sections": [{"title": "Discussion article for the meetup : Rationality Meetup Vienna", "anchor": "Discussion_article_for_the_meetup___Rationality_Meetup_Vienna", "level": 1}, {"title": "Discussion article for the meetup : Rationality Meetup Vienna", "anchor": "Discussion_article_for_the_meetup___Rationality_Meetup_Vienna1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-18T03:02:17.243Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham: New Years' Resolutions etc.", "slug": "meetup-durham-new-years-resolutions-etc", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WJnjofrhtAFB29R5f/meetup-durham-new-years-resolutions-etc", "pageUrlRelative": "/posts/WJnjofrhtAFB29R5f/meetup-durham-new-years-resolutions-etc", "linkUrl": "https://www.lesswrong.com/posts/WJnjofrhtAFB29R5f/meetup-durham-new-years-resolutions-etc", "postedAtFormatted": "Wednesday, December 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%3A%20New%20Years'%20Resolutions%20etc.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%3A%20New%20Years'%20Resolutions%20etc.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWJnjofrhtAFB29R5f%2Fmeetup-durham-new-years-resolutions-etc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%3A%20New%20Years'%20Resolutions%20etc.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWJnjofrhtAFB29R5f%2Fmeetup-durham-new-years-resolutions-etc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWJnjofrhtAFB29R5f%2Fmeetup-durham-new-years-resolutions-etc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/uw'>Durham: New Years' Resolutions etc.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 December 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2411 N Roxboro St, Durham NC 27704</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As your intrepid meetup organizers are still a bit worn out from their Solstice adventuring, this will be a somewhat low-key meetup. Our proposed discussion topic is New Years' resolutions, what makes for good ones, and whether or not you should make them.</p>\n\n<p>We'll be at the Roxboro St house with the red door at 7 for general introductions, planning to start actual meetup discussions around 7:30. Meetup will run until 9:30 officially, but we're unlikely to kick people out / go to bed for a while after that.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/uw'>Durham: New Years' Resolutions etc.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WJnjofrhtAFB29R5f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.471163624850207e-06, "legacy": true, "legacyId": "25100", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham__New_Years__Resolutions_etc_\">Discussion article for the meetup : <a href=\"/meetups/uw\">Durham: New Years' Resolutions etc.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 December 2013 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2411 N Roxboro St, Durham NC 27704</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As your intrepid meetup organizers are still a bit worn out from their Solstice adventuring, this will be a somewhat low-key meetup. Our proposed discussion topic is New Years' resolutions, what makes for good ones, and whether or not you should make them.</p>\n\n<p>We'll be at the Roxboro St house with the red door at 7 for general introductions, planning to start actual meetup discussions around 7:30. Meetup will run until 9:30 officially, but we're unlikely to kick people out / go to bed for a while after that.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham__New_Years__Resolutions_etc_1\">Discussion article for the meetup : <a href=\"/meetups/uw\">Durham: New Years' Resolutions etc.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham: New Years' Resolutions etc.", "anchor": "Discussion_article_for_the_meetup___Durham__New_Years__Resolutions_etc_", "level": 1}, {"title": "Discussion article for the meetup : Durham: New Years' Resolutions etc.", "anchor": "Discussion_article_for_the_meetup___Durham__New_Years__Resolutions_etc_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-19T06:22:40.082Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow: Art Therapy for Emotions", "slug": "meetup-moscow-art-therapy-for-emotions", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J96h94baMPv5qaHji/meetup-moscow-art-therapy-for-emotions", "pageUrlRelative": "/posts/J96h94baMPv5qaHji/meetup-moscow-art-therapy-for-emotions", "linkUrl": "https://www.lesswrong.com/posts/J96h94baMPv5qaHji/meetup-moscow-art-therapy-for-emotions", "postedAtFormatted": "Thursday, December 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%3A%20Art%20Therapy%20for%20Emotions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%3A%20Art%20Therapy%20for%20Emotions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ96h94baMPv5qaHji%2Fmeetup-moscow-art-therapy-for-emotions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%3A%20Art%20Therapy%20for%20Emotions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ96h94baMPv5qaHji%2Fmeetup-moscow-art-therapy-for-emotions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ96h94baMPv5qaHji%2Fmeetup-moscow-art-therapy-for-emotions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ux'>Moscow: Art Therapy for Emotions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 December 2013 06:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, Losinoostrovskaya ulitsa 24\u04411</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at a new location this time to participate in an art therapy workshop. You can learn how to work with your emotions with this method, there will be practical demonstration and discussion about it.</p>\n\n<p>We will start at 18:00 and finish about 20.</p>\n\n<p><strong>How to get there</strong></p>\n\n<p>Please read a detailed <a href=\"http://umneem.org/%D0%BD%D0%BE%D0%B2%D0%BE%D1%81%D1%82%D0%B8/69?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_meet_up_notice+20131224_meet_up&amp;utm_content=20131224_meet_up&amp;utm_campaign=moscow_meetups\">description in Russian here</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ux'>Moscow: Art Therapy for Emotions</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J96h94baMPv5qaHji", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4728713925311112e-06, "legacy": true, "legacyId": "25104", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Art_Therapy_for_Emotions\">Discussion article for the meetup : <a href=\"/meetups/ux\">Moscow: Art Therapy for Emotions</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 December 2013 06:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, Losinoostrovskaya ulitsa 24\u04411</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at a new location this time to participate in an art therapy workshop. You can learn how to work with your emotions with this method, there will be practical demonstration and discussion about it.</p>\n\n<p>We will start at 18:00 and finish about 20.</p>\n\n<p><strong id=\"How_to_get_there\">How to get there</strong></p>\n\n<p>Please read a detailed <a href=\"http://umneem.org/%D0%BD%D0%BE%D0%B2%D0%BE%D1%81%D1%82%D0%B8/69?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_meet_up_notice+20131224_meet_up&amp;utm_content=20131224_meet_up&amp;utm_campaign=moscow_meetups\">description in Russian here</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Art_Therapy_for_Emotions1\">Discussion article for the meetup : <a href=\"/meetups/ux\">Moscow: Art Therapy for Emotions</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow: Art Therapy for Emotions", "anchor": "Discussion_article_for_the_meetup___Moscow__Art_Therapy_for_Emotions", "level": 1}, {"title": "How to get there", "anchor": "How_to_get_there", "level": 2}, {"title": "Discussion article for the meetup : Moscow: Art Therapy for Emotions", "anchor": "Discussion_article_for_the_meetup___Moscow__Art_Therapy_for_Emotions1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-20T15:28:26.843Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Rituals", "slug": "meetup-urbana-champaign-rituals", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ipPzQGY4zarijFSn7/meetup-urbana-champaign-rituals", "pageUrlRelative": "/posts/ipPzQGY4zarijFSn7/meetup-urbana-champaign-rituals", "linkUrl": "https://www.lesswrong.com/posts/ipPzQGY4zarijFSn7/meetup-urbana-champaign-rituals", "postedAtFormatted": "Friday, December 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Rituals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Rituals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipPzQGY4zarijFSn7%2Fmeetup-urbana-champaign-rituals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Rituals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipPzQGY4zarijFSn7%2Fmeetup-urbana-champaign-rituals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FipPzQGY4zarijFSn7%2Fmeetup-urbana-champaign-rituals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/uy\">Urbana-Champaign: Rituals</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">21 December 2013 02:00:00PM (-0600)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">300 S Goodwin Ave Apt 102, Urbana Illinois</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>What are rituals good for? What makes a good ritual? What are the dangers of rituals? Are they worth it?</p>\n<p>This being the solstice, conditioning on a \"yes\" to the last question, we may end up doing some of these ones: <a rel=\"nofollow\" href=\"https://dl.dropboxusercontent.com/u/2000477/SolsticeEve_2012.pdf\">https://dl.dropboxusercontent.com/u/2000477/SolsticeEve_2012.pdf</a></p>\n<p>The location is 300 S Goodwin Ave Apt 102, Urbana Illinois. The main entrance to the building requires card access, but there is a door directly to my apartment from the outside at the Northwest corner of the building. If you have any trouble getting in, call my phone, REDACTED</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/uy\">Urbana-Champaign: Rituals</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ipPzQGY4zarijFSn7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.474943446013531e-06, "legacy": true, "legacyId": "25105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Rituals\">Discussion article for the meetup : <a href=\"/meetups/uy\">Urbana-Champaign: Rituals</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">21 December 2013 02:00:00PM (-0600)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">300 S Goodwin Ave Apt 102, Urbana Illinois</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>What are rituals good for? What makes a good ritual? What are the dangers of rituals? Are they worth it?</p>\n<p>This being the solstice, conditioning on a \"yes\" to the last question, we may end up doing some of these ones: <a rel=\"nofollow\" href=\"https://dl.dropboxusercontent.com/u/2000477/SolsticeEve_2012.pdf\">https://dl.dropboxusercontent.com/u/2000477/SolsticeEve_2012.pdf</a></p>\n<p>The location is 300 S Goodwin Ave Apt 102, Urbana Illinois. The main entrance to the building requires card access, but there is a door directly to my apartment from the outside at the Northwest corner of the building. If you have any trouble getting in, call my phone, REDACTED</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Rituals1\">Discussion article for the meetup : <a href=\"/meetups/uy\">Urbana-Champaign: Rituals</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Rituals", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Rituals", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Rituals", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Rituals1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-20T17:02:58.839Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-18", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AtNk66CwotT6AKWN5/weekly-lw-meetups-18", "pageUrlRelative": "/posts/AtNk66CwotT6AKWN5/weekly-lw-meetups-18", "linkUrl": "https://www.lesswrong.com/posts/AtNk66CwotT6AKWN5/weekly-lw-meetups-18", "postedAtFormatted": "Friday, December 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAtNk66CwotT6AKWN5%2Fweekly-lw-meetups-18%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAtNk66CwotT6AKWN5%2Fweekly-lw-meetups-18", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAtNk66CwotT6AKWN5%2Fweekly-lw-meetups-18", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 531, "htmlBody": "<p><strong>This summary was posted to LW main on December 13th. The following week's summary is <a href=\"/lw/jdf/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/u6\">[Leipzig] Secular Solstice Celebration! (And the Inauguration of the LW Leipzig Community):&nbsp;<span class=\"date\">21 December 2013 05:05PM</span></a></li>\n<li><a href=\"/meetups/tv\">Mumbai Meetup:&nbsp;<span class=\"date\">15 December 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/u4\">Utrecht:&nbsp;<span class=\"date\">14 December 2013 02:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/tf\"></a><a href=\"/meetups/ui\">[Atlanta] December Meetup :&nbsp;<span class=\"date\">14 December 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/ty\">Helsinki Meetup:&nbsp;<span class=\"date\">15 December 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/ul\">[Jacksonville FL] Book Mini-Review: Doug Hubbard's How to Measure Anything:&nbsp;<span class=\"date\">15 December 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/ue\">[Vienna] The Return of the Rationalists!:&nbsp;<span class=\"date\">14 December 2013 03:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">14 December 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/tf\">Berlin:&nbsp;<span class=\"date\">01 January 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/ti\">Brussels monthly meetup: time!:&nbsp;<span class=\"date\">14 December 2013 01:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AtNk66CwotT6AKWN5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.4750422149323263e-06, "legacy": true, "legacyId": "25074", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jqS9JQwpv6ZfpFWXG", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-20T17:04:09.426Z", "modifiedAt": null, "url": null, "title": "Local truth", "slug": "local-truth", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:29.106Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j3g32sLzCYGxFev4p/local-truth", "pageUrlRelative": "/posts/j3g32sLzCYGxFev4p/local-truth", "linkUrl": "https://www.lesswrong.com/posts/j3g32sLzCYGxFev4p/local-truth", "postedAtFormatted": "Friday, December 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Local%20truth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALocal%20truth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj3g32sLzCYGxFev4p%2Flocal-truth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Local%20truth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj3g32sLzCYGxFev4p%2Flocal-truth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj3g32sLzCYGxFev4p%2Flocal-truth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 450, "htmlBody": "<p><a href=\"http://www.sciencedaily.com/releases/2013/12/131219142138.htm\">New Salt Compounds Challenge the Foundation of Chemistry</a></p>\n<p>The title is overblown (it depends on what you think the foundation is), but get a load of this:</p>\n<blockquote>\"I think this work is the beginning of a revolution in chemistry,\" Oganov says. \"We found, at low pressures achievable in the lab, perfectly stable compounds that contradict the classical rules of chemistry. If you apply the rather modest pressure of 200,000 atmospheres -- for comparison purposes, the pressure at the center of the Earth is 3.6 million atmospheres -- everything we know from chemistry textbooks falls apart.\"</blockquote>\n<blockquote>Standard chemistry textbooks say that sodium and chlorine have very different electronegativities, and thus must form an ionic compound with a well-defined composition. Sodium's charge is +1, chlorine's charge is -1; sodium will give away an electron, chlorine wants to take an electron. According to chemistry texts and common sense, the only possible combination of these atoms in a compound is 1:1 -- rock salt, or NaCl. \"We found crazy compounds that violate textbook rules -- NaCl3, NaCl7, Na3Cl2, Na2Cl, and Na3Cl,\" says Weiwei Zhang, the lead author and visiting scholar at the Oganov lab and Stony Brook's Center for Materials by Design, directed by Oganov.</blockquote>\n<blockquote>\"These compounds are thermodynamically stable and, once made, remain indefinitely; nothing will make them fall apart. Classical chemistry forbids their very existence. Classical chemistry also says atoms try to fulfill the octet rule -- elements gain or lose electrons to attain an electron configuration of the nearest noble gas, with complete outer electron shells that make them very stable. Well, here that rule is not satisfied.\"</blockquote>\n<p>And here's the philosophical bit:</p>\n<blockquote>\"For a long time, this idea was haunting me -- when a chemistry textbook says that a certain compound is impossible, what does it really mean, impossible? Because I can, on the computer, place atoms in certain positions and in certain proportions. Then I can compute the energy. 'Impossible' really means that the energy is going to be high. So how high is it going to be? And is there any way to bring that energy down, and make these compounds stable?\"</blockquote>\n<blockquote>To Oganov, impossible didn't mean something absolute. \"The rules of chemistry are not like mathematical theorems, which cannot be broken,\" he says. \"The rules of chemistry can be broken, because impossible only means 'softly' impossible! You just need to find conditions where these rules no longer hold.\"</blockquote>\n<p>The obvious example of local truth is relativistic effects being pretty much invisible over the durations and distances that are normal for people, but there's also that the surface of the earth is near enough to flat for many human purposes.</p>\n<p>Any suggestions for other truths which could turn out to be local?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j3g32sLzCYGxFev4p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 20, "extendedScore": null, "score": 1.475043444116186e-06, "legacy": true, "legacyId": "25108", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-20T17:10:12.021Z", "modifiedAt": null, "url": null, "title": "[LINK] Meditation Can Debias The Mind", "slug": "link-meditation-can-debias-the-mind", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.377Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JQuinton", "createdAt": "2011-04-29T16:29:16.319Z", "isAdmin": false, "displayName": "JQuinton"}, "userId": "edHNo35soNo2w6ZwN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oa2bjCfdR5vbfGDXn/link-meditation-can-debias-the-mind", "pageUrlRelative": "/posts/oa2bjCfdR5vbfGDXn/link-meditation-can-debias-the-mind", "linkUrl": "https://www.lesswrong.com/posts/oa2bjCfdR5vbfGDXn/link-meditation-can-debias-the-mind", "postedAtFormatted": "Friday, December 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Meditation%20Can%20Debias%20The%20Mind&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Meditation%20Can%20Debias%20The%20Mind%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foa2bjCfdR5vbfGDXn%2Flink-meditation-can-debias-the-mind%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Meditation%20Can%20Debias%20The%20Mind%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foa2bjCfdR5vbfGDXn%2Flink-meditation-can-debias-the-mind", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Foa2bjCfdR5vbfGDXn%2Flink-meditation-can-debias-the-mind", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 379, "htmlBody": "<p>This is interesting. Apparently, <a href=\"http://www.spring.org.uk/2013/12/meditation-can-debias-the-mind-in-only-15-minutes.php\">meditating for 15 minutes can reduce susceptibility to the sunk cost bias</a>.</p>\n<blockquote>Across two separate experiments, the researchers tested this by giving one group of participants a 15-minute mindfulness meditation induction.\n<p>Then they were given a business scenario which was designed to test the sunk cost bias.</p>\n<p>In comparison to a control condition, thinking mindfully doubled the number of people who could avoid the sunk cost bias.</p>\n<p>In the control condition just over 40% of people were able to resist the bias. This shot up to almost 80% among those who were thinking mindfully.</p>\n<p>The researchers achieved similar results in another experiment and then went on to examine exactly how mindfulness is helpful.</p>\n<p>In a third experiment they found that mindfulness increases the focus on the present moment, as it should.</p>\n<p>A focus on the present in turn reduced negative feelings participants had about the &lsquo;sunk cost&rsquo;&ndash;the time, money and effort that had gone to waste.</p>\n<p>This reduction in negative emotion meant participants were much better equipped to resist the bias.</p>\n</blockquote>\n<p>Ironically, I did a search on Less Wrong to see if something like this had been posted before and came across <a href=\"/lw/2rd/understanding_vipassana_meditation/2s8a\">this comment</a>:</p>\n<blockquote>\n<p>Good points. The lack of scientific research discussed is certainly an issue. I did a quick literature sweep before writing this post, but decided not to include that information here.</p>\n<blockquote>One is a sunk cost fallacy. If you have sunk ten days into it you are less willing to ditch it because fallible humans are often unable to act like good economists and recognize that sunk costs are irrelevant.</blockquote>\n<p>At the dhamma.org courses I haven't found that to be the case. The management at the Massachusetts center informed me that a large majority of students never return to take a second course. Perhaps the cost needs to be larger; people may find it difficult to give up the practice (when they have good reason to) if they have done it daily for some length of time.</p>\n</blockquote>\n<p>According to that anecdote, a large majority of students never take a second course in meditation. It might be due to the study above, where meditating itself makes people less likely to engage in sunk cost thinking.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oa2bjCfdR5vbfGDXn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 1.4750497585143457e-06, "legacy": true, "legacyId": "25106", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-20T21:07:04.671Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston - Greens Versus Blues", "slug": "meetup-boston-greens-versus-blues", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oMCynqEo8Kjb8FkLz/meetup-boston-greens-versus-blues", "pageUrlRelative": "/posts/oMCynqEo8Kjb8FkLz/meetup-boston-greens-versus-blues", "linkUrl": "https://www.lesswrong.com/posts/oMCynqEo8Kjb8FkLz/meetup-boston-greens-versus-blues", "postedAtFormatted": "Friday, December 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20-%20Greens%20Versus%20Blues&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20-%20Greens%20Versus%20Blues%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoMCynqEo8Kjb8FkLz%2Fmeetup-boston-greens-versus-blues%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20-%20Greens%20Versus%20Blues%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoMCynqEo8Kjb8FkLz%2Fmeetup-boston-greens-versus-blues", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoMCynqEo8Kjb8FkLz%2Fmeetup-boston-greens-versus-blues", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/uz'>Boston - Greens Versus Blues</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 December 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Citadel, 98 Elm St Apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Eloise Rosen is giving a talk, \"Greens Versus Blues: Why Politics Make People Stupid, And How to Protect Yourself\".</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square).</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation, and maybe even improv games!</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 3pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/uz'>Boston - Greens Versus Blues</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oMCynqEo8Kjb8FkLz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 0, "extendedScore": null, "score": 1.475297303230828e-06, "legacy": true, "legacyId": "25109", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston___Greens_Versus_Blues\">Discussion article for the meetup : <a href=\"/meetups/uz\">Boston - Greens Versus Blues</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 December 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Citadel, 98 Elm St Apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Eloise Rosen is giving a talk, \"Greens Versus Blues: Why Politics Make People Stupid, And How to Protect Yourself\".</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square).</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation, and maybe even improv games!</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 3pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston___Greens_Versus_Blues1\">Discussion article for the meetup : <a href=\"/meetups/uz\">Boston - Greens Versus Blues</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston - Greens Versus Blues", "anchor": "Discussion_article_for_the_meetup___Boston___Greens_Versus_Blues", "level": 1}, {"title": "Discussion article for the meetup : Boston - Greens Versus Blues", "anchor": "Discussion_article_for_the_meetup___Boston___Greens_Versus_Blues1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-21T03:04:32.109Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington Goals/Projects (try again!)", "slug": "meetup-washington-goals-projects-try-again", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.710Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m4GD64T2zNv8ThcJm/meetup-washington-goals-projects-try-again", "pageUrlRelative": "/posts/m4GD64T2zNv8ThcJm/meetup-washington-goals-projects-try-again", "linkUrl": "https://www.lesswrong.com/posts/m4GD64T2zNv8ThcJm/meetup-washington-goals-projects-try-again", "postedAtFormatted": "Saturday, December 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20Goals%2FProjects%20(try%20again!)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20Goals%2FProjects%20(try%20again!)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm4GD64T2zNv8ThcJm%2Fmeetup-washington-goals-projects-try-again%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20Goals%2FProjects%20(try%20again!)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm4GD64T2zNv8ThcJm%2Fmeetup-washington-goals-projects-try-again", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm4GD64T2zNv8ThcJm%2Fmeetup-washington-goals-projects-try-again", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/v0'>Washington Goals/Projects (try again!)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 December 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to try and get the goals meetup working again, or at least figure out why it didn't in the past. Also (very relatedly) we can talk about what we're working on now.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/v0'>Washington Goals/Projects (try again!)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m4GD64T2zNv8ThcJm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.4756709969270508e-06, "legacy": true, "legacyId": "25110", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_Goals_Projects__try_again__\">Discussion article for the meetup : <a href=\"/meetups/v0\">Washington Goals/Projects (try again!)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 December 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to try and get the goals meetup working again, or at least figure out why it didn't in the past. Also (very relatedly) we can talk about what we're working on now.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_Goals_Projects__try_again__1\">Discussion article for the meetup : <a href=\"/meetups/v0\">Washington Goals/Projects (try again!)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington Goals/Projects (try again!)", "anchor": "Discussion_article_for_the_meetup___Washington_Goals_Projects__try_again__", "level": 1}, {"title": "Discussion article for the meetup : Washington Goals/Projects (try again!)", "anchor": "Discussion_article_for_the_meetup___Washington_Goals_Projects__try_again__1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-21T04:11:05.529Z", "modifiedAt": null, "url": null, "title": "[Meta] Post-meetup reports and discussion", "slug": "meta-post-meetup-reports-and-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:38.004Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TylerJay", "createdAt": "2010-08-16T22:37:13.189Z", "isAdmin": false, "displayName": "TylerJay"}, "userId": "rR64xYGdnRFZ5MPQc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SNtt8MwCrpPmC4Wei/meta-post-meetup-reports-and-discussion", "pageUrlRelative": "/posts/SNtt8MwCrpPmC4Wei/meta-post-meetup-reports-and-discussion", "linkUrl": "https://www.lesswrong.com/posts/SNtt8MwCrpPmC4Wei/meta-post-meetup-reports-and-discussion", "postedAtFormatted": "Saturday, December 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMeta%5D%20Post-meetup%20reports%20and%20discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMeta%5D%20Post-meetup%20reports%20and%20discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSNtt8MwCrpPmC4Wei%2Fmeta-post-meetup-reports-and-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMeta%5D%20Post-meetup%20reports%20and%20discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSNtt8MwCrpPmC4Wei%2Fmeta-post-meetup-reports-and-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSNtt8MwCrpPmC4Wei%2Fmeta-post-meetup-reports-and-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 436, "htmlBody": "<p>Looking at the discussion section recently, it seems like over half of the posts are meetups. &nbsp;I think it's really great that so many LessWrongers are able to get together and do interesting stuff. &nbsp;Looking at a lot of the topics, I often find myself thinking \"I wonder what they ended up talking about.\" &nbsp;I looked at the&nbsp;<a href=\"/meetups/\">meetups page</a>&nbsp;and it looks like many give a description of the topic, but there is rarely any public followup. &nbsp;I also did a search which turned up surprisingly few post-meetup posts.</p>\n<p>For example, <a title=\"this Los Angeles meetup\" href=\"/r/discussion/lw/jcu/meetup_west_los_angeles_resolutions/\" target=\"_blank\">this Los Angeles meetup</a> from a few days ago about resolutions looked really interesting to me and I'm curious to hear what kinds of strategies were proposed and if there were any insights or anecdotes that came up that would be useful to share with those of us that couldn't attend.</p>\n<p>I remember reading a <a href=\"/lw/j3n/london_lw_coze_exercise_report/\">meetup report</a> back in November that told the story of the exercises they went through and it seemed to spark some good discussion. &nbsp;It even forced me to make a note to try some things on my own. &nbsp;This one was atypical in that it was very detailed and was a crosspost from a personal blog, but I feel like even short reports would give a chance for the rest of the community to chime in and give praise, suggestions, and feedback. &nbsp;</p>\n<p>When I tried to think of reasons not to share what happened in meetups, I came up with a few potential factors:</p>\n<ol>\n<li>It's extra work</li>\n<li>Keeping it private increases the feeling of community within the group</li>\n<li>Meetups are supposed to be a safe place where your actions or comments won't be broadcast to the world</li>\n<li>Nothing really post-worthy happened</li>\n</ol>\n<div>Some potential arguments in favor of having a post-meetup discussion post:</div>\n<div><ol>\n<li>It would allow LessWrongers who weren't in attendance to get involved in the discussion</li>\n<li>Insights would be shared with the whole community</li>\n<li>Meetup organizers and attendees could get suggestions for ways to improve future meetups</li>\n<li>Non-attendees could use these ideas to host their own meetups</li>\n<li>Summarizing key points of a discussion is helpful for those involved to retain the information they discussed</li>\n</ol>\n<div>Does anyone else feel like this would be useful? &nbsp;What is the real reason there are not very many reports?</div>\n</div>\n<div><br /></div>\n<div>In the spirit of putting my money where my mouth is, I pledge to attend a LessWrong meetup sometime in the next two months and do a post-meetup report to see if it is useful to anyone (provided the meetup group is okay with me sharing it.)&nbsp;</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SNtt8MwCrpPmC4Wei", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 23, "extendedScore": null, "score": 1.4757405955636518e-06, "legacy": true, "legacyId": "25111", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kmTtNqNwQrPktmcoj", "ze5Y5dfh5vmRfMknm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-22T02:50:55.898Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Practical Rationality", "slug": "meetup-melbourne-practical-rationality-12", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "CeTijoorgtNBeSrZG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XhtftNxpfse7swrxQ/meetup-melbourne-practical-rationality-12", "pageUrlRelative": "/posts/XhtftNxpfse7swrxQ/meetup-melbourne-practical-rationality-12", "linkUrl": "https://www.lesswrong.com/posts/XhtftNxpfse7swrxQ/meetup-melbourne-practical-rationality-12", "postedAtFormatted": "Sunday, December 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Practical%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Practical%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXhtftNxpfse7swrxQ%2Fmeetup-melbourne-practical-rationality-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Practical%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXhtftNxpfse7swrxQ%2Fmeetup-melbourne-practical-rationality-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXhtftNxpfse7swrxQ%2Fmeetup-melbourne-practical-rationality-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/v1'>Melbourne Practical Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 January 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Level 2, 491 King St West Melbourne 3003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical Rationality. This meetup repeats on the first Friday of each month and is distinct from our monthly Social Meetup. We aim to improve our thinking and decision making techniques.</p>\n\n<p>The topic of this month is communication. We'll be exploring topics such as:</p>\n\n<p>Storytelling - how to build an engaging narrative;</p>\n\n<p>Wait vs Interrupt Culture - optimising conversations</p>\n\n<p>Radical Honesty - going too far, or the ideal way to communicate?</p>\n\n<p>Given and receiving feedback</p>\n\n<p>Discussion can be found on our mailing list: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All are welcome from 6:30pm. If the door downstairs is locked, call the phone number on the door and we'll let you in. We aim to start structured activities at 7:30pm and continue until 9pm. Afterwards informal discussion will continue late into the night.</p>\n\n<p>Please RSVP at our Meetup.com page if you are coming.\n<a href=\"http://www.meetup.com/Melbourne-Less-Wrong/events/143167062/\" rel=\"nofollow\">http://www.meetup.com/Melbourne-Less-Wrong/events/143167062/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/v1'>Melbourne Practical Rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XhtftNxpfse7swrxQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [], "voteCount": 0, "baseScore": 0, "extendedScore": null, "score": 1.4771638518849905e-06, "legacy": true, "legacyId": "25112", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Practical_Rationality\">Discussion article for the meetup : <a href=\"/meetups/v1\">Melbourne Practical Rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 January 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Level 2, 491 King St West Melbourne 3003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical Rationality. This meetup repeats on the first Friday of each month and is distinct from our monthly Social Meetup. We aim to improve our thinking and decision making techniques.</p>\n\n<p>The topic of this month is communication. We'll be exploring topics such as:</p>\n\n<p>Storytelling - how to build an engaging narrative;</p>\n\n<p>Wait vs Interrupt Culture - optimising conversations</p>\n\n<p>Radical Honesty - going too far, or the ideal way to communicate?</p>\n\n<p>Given and receiving feedback</p>\n\n<p>Discussion can be found on our mailing list: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All are welcome from 6:30pm. If the door downstairs is locked, call the phone number on the door and we'll let you in. We aim to start structured activities at 7:30pm and continue until 9pm. Afterwards informal discussion will continue late into the night.</p>\n\n<p>Please RSVP at our Meetup.com page if you are coming.\n<a href=\"http://www.meetup.com/Melbourne-Less-Wrong/events/143167062/\" rel=\"nofollow\">http://www.meetup.com/Melbourne-Less-Wrong/events/143167062/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Practical_Rationality1\">Discussion article for the meetup : <a href=\"/meetups/v1\">Melbourne Practical Rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Practical Rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne_Practical_Rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Practical Rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne_Practical_Rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": null, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-22T20:51:25.123Z", "modifiedAt": null, "url": null, "title": "[Link] Anti-ageing compound set for human trials", "slug": "link-anti-ageing-compound-set-for-human-trials", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:22.513Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gavin", "createdAt": "2009-02-27T05:00:57.191Z", "isAdmin": false, "displayName": "Gavin"}, "userId": "9gMQSKRMgpPFYTNFY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xuoq2kXzDGdgxWgs7/link-anti-ageing-compound-set-for-human-trials", "pageUrlRelative": "/posts/Xuoq2kXzDGdgxWgs7/link-anti-ageing-compound-set-for-human-trials", "linkUrl": "https://www.lesswrong.com/posts/Xuoq2kXzDGdgxWgs7/link-anti-ageing-compound-set-for-human-trials", "postedAtFormatted": "Sunday, December 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Anti-ageing%20compound%20set%20for%20human%20trials&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Anti-ageing%20compound%20set%20for%20human%20trials%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuoq2kXzDGdgxWgs7%2Flink-anti-ageing-compound-set-for-human-trials%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Anti-ageing%20compound%20set%20for%20human%20trials%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuoq2kXzDGdgxWgs7%2Flink-anti-ageing-compound-set-for-human-trials", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuoq2kXzDGdgxWgs7%2Flink-anti-ageing-compound-set-for-human-trials", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p>This seems like an advance in understanding, even if it doesn't lead directly to a treatment.</p>\n<p>News stories:</p>\n<p>http://www.theguardian.com/science/2013/dec/20/anti-ageing-human-trials?CMP=EMCNEWEML6619I2</p>\n<p>http://www.bbc.co.uk/news/health-25445748</p>\n<p>&nbsp;</p>\n<p>Abstract of the paper, actual paper behind a paywall:&nbsp;</p>\n<p>http://www.cell.com/retrieve/pii/S0092867413015213?cc=y</p>\n<p>&nbsp;</p>\n<p>Relatively solid stories like this help raise my estimate that significant life extension is possible in our lifetimes. The likelihood seems to be that it won't be a \"magic pill\" but a combination of therapies.</p>\n<p>If nothing else, it's another reason to eat healthy and stay in shape.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xuoq2kXzDGdgxWgs7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 14, "extendedScore": null, "score": 1.4782964606029775e-06, "legacy": true, "legacyId": "25116", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-23T15:33:00.840Z", "modifiedAt": null, "url": null, "title": "[LINK] Up Vs Down is the new Left vs Right", "slug": "link-up-vs-down-is-the-new-left-vs-right", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:31.442Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stabilizer", "createdAt": "2011-12-02T09:36:56.841Z", "isAdmin": false, "displayName": "Stabilizer"}, "userId": "Qa3pLZx3o2TApyfgq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yETbMuRfJW2R3qAGw/link-up-vs-down-is-the-new-left-vs-right", "pageUrlRelative": "/posts/yETbMuRfJW2R3qAGw/link-up-vs-down-is-the-new-left-vs-right", "linkUrl": "https://www.lesswrong.com/posts/yETbMuRfJW2R3qAGw/link-up-vs-down-is-the-new-left-vs-right", "postedAtFormatted": "Monday, December 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Up%20Vs%20Down%20is%20the%20new%20Left%20vs%20Right&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Up%20Vs%20Down%20is%20the%20new%20Left%20vs%20Right%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyETbMuRfJW2R3qAGw%2Flink-up-vs-down-is-the-new-left-vs-right%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Up%20Vs%20Down%20is%20the%20new%20Left%20vs%20Right%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyETbMuRfJW2R3qAGw%2Flink-up-vs-down-is-the-new-left-vs-right", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyETbMuRfJW2R3qAGw%2Flink-up-vs-down-is-the-new-left-vs-right", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<p><a href=\"http://aeon.co/magazine/living-together/right-and-left-are-fading-the-future-is-black-and-green/\">This</a> is a great article at Aeon magazine. The author argues that the new ideological dichotomy is going to be between people who have great faith in technology and human innovation (Up) and the people who believe that humans are much more tied to their biology and the Earth (Down).</p>\n<p>LW of course is a very Up community.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yETbMuRfJW2R3qAGw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -8, "extendedScore": null, "score": -1e-05, "legacy": true, "legacyId": "25118", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-23T19:57:22.555Z", "modifiedAt": null, "url": null, "title": "Building Phenomenological Bridges", "slug": "building-phenomenological-bridges", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:03.530Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobbBB", "createdAt": "2012-08-10T00:50:11.669Z", "isAdmin": true, "displayName": "Rob Bensinger"}, "userId": "2aoRX3ookcCozcb3m", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ethRJh2E7mSSjzCay/building-phenomenological-bridges", "pageUrlRelative": "/posts/ethRJh2E7mSSjzCay/building-phenomenological-bridges", "linkUrl": "https://www.lesswrong.com/posts/ethRJh2E7mSSjzCay/building-phenomenological-bridges", "postedAtFormatted": "Monday, December 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Building%20Phenomenological%20Bridges&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABuilding%20Phenomenological%20Bridges%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FethRJh2E7mSSjzCay%2Fbuilding-phenomenological-bridges%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Building%20Phenomenological%20Bridges%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FethRJh2E7mSSjzCay%2Fbuilding-phenomenological-bridges", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FethRJh2E7mSSjzCay%2Fbuilding-phenomenological-bridges", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3294, "htmlBody": "<p><strong><a href=\"http://wiki.lesswrong.com/wiki/Naturalized_induction\">Naturalized induction</a></strong>&nbsp;is an open problem in <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">Friendly Artificial Intelligence</a>&nbsp;(OPFAI). The problem, in brief: Our current leading models of induction do not allow reasoners to treat their own computations as processes in the world.&nbsp;</p>\n<p>The problem's roots lie in algorithmic information theory and formal epistemology, but finding answers will require us to wade into debates on everything from theoretical physics to anthropic reasoning and self-reference. This post will lay the groundwork for a sequence of posts (titled '<strong>Artificial Naturalism</strong>') introducing different aspects of this OPFAI.</p>\n<p>&nbsp;</p>\n<h2>AI perception and belief: A toy model</h2>\n<p>A more concrete problem: Construct an algorithm that, given a sequence of the colors cyan, magenta, and yellow, predicts the next colored field.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 1px solid black; vertical-align: middle; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_0.png\" alt=\"\" width=\"574\" height=\"388\" /></p>\n<p align=\"center\"><em>Colors: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; CYYM &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;CYYY &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;CYCM &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;CYYY &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;????</em></p>\n<p>&nbsp;</p>\n<p>This is an instance of the general problem 'From an incomplete data series, how can a reasoner best make predictions about future data?'. In practice, any agent that acquires information from its environment and makes predictions about what's coming next will need to have two map-like<a name=\"footnote1back\"></a><a href=\"#footnote1\"><sup>1</sup></a> subprocesses:</p>\n<p style=\"padding-left: 30px;\">1. Something that generates the agent's predictions, its expectations. By analogy with human scientists, we can call this prediction-generator the agent's <strong>hypotheses </strong>or <strong>beliefs</strong>.</p>\n<p style=\"padding-left: 30px;\">2. Something that transmits new information to the agent's prediction-generator so that its hypotheses can be <a href=\"http://yudkowsky.net/rational/bayes\">updated</a>. Employing another anthropomorphic analogy, we can call this process the agent's <strong>data</strong>&nbsp;or <strong>perceptions</strong>.</p>\n<p><a id=\"more\"></a></p>\n<p>Here's an example of a hypothesis an agent could use to try to predict the next color field. I'll call the imaginary agent '<strong>Cai</strong>'. Any reasoner will need to begin with some (perhaps provisional) assumptions about the world.<a name=\"footnote2back\"></a><sup><a href=\"#footnote2\">2</a></sup>&nbsp;Cai begins with the belief<a name=\"footnote3back\"></a><sup><a href=\"#footnote3\">3</a></sup> that its environment behaves like a <a href=\"/lw/fok/causal_universes/\">cellular automaton</a>: the world is a grid whose tiles change over time based on a set of stable laws. The laws are local in time and space, meaning that you can perfectly predict a tile's state based on the states of the tiles next to it a moment prior&nbsp;&mdash; if you know which laws are in force.</p>\n<p>Cai believes that it lives in a closed 3x3 grid where tiles have no diagonal effects. Each tile can occupy one of three states. We might call the states '0', '1', and '2', or, to make visualization easier, 'white', 'black', and 'gray'. So, on Cai's view, the world as it changes looks something like this:</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 1px solid black; vertical-align: middle; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_1.png?v=1f67eceff1678f2d2493f13ff464731e\" alt=\"\" width=\"304\" height=\"127\" /></p>\n<p align=\"center\"><em>An example of the world's state at one moment, and its state a moment later.</em></p>\n<p>&nbsp;</p>\n<p>Cai also has beliefs about its own location in the cellular automaton. Cai believes that it is a black tile at the center of the grid. Since there are no diagonal laws of physics in this world, Cai can only directly interact with the four tiles directly above, below, to the left, and to the right. As such, any perceptual data Cai acquires will need to come from those four tiles; anything else about Cai's universe will be known <a href=\"/lw/pb/belief_in_the_implied_invisible\">only by inference</a>.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jd9_2.png\" alt=\"\" width=\"314\" height=\"340\" /></p>\n<p align=\"center\"><em>Cai perceives stimuli in four directions. Unobservable tiles fall outside the cross.</em></p>\n<p>&nbsp;</p>\n<p>How does all this bear on the color-predicting problem? Cai hypothesizes that the sequence of colors is sensory &mdash; it's an experience within Cai, triggered by environmental changes. Cai conjectures that since its visual field comes in at most four colors, its visual field's quadrants probably represent its four adjacent tiles. The leftmost color comes from a southern stimulus, the next one to the right from a western stimulus, then a northern one, then an eastern one. And the south, west, north, east cycle repeats again and again.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"vertical-align: middle; border: 1px solid black; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_3.png?v=943de6003ceac30216fcd64e3634d655\" alt=\"\" width=\"274\" height=\"327\" /></p>\n<p align=\"center\"><em>Cai&rsquo;s visual experiences break down into quadrants, corresponding to four directions.</em></p>\n<p>&nbsp;</p>\n<p>On this model, the way Cai&rsquo;s senses organize the data isn't wholly veridical; the four patches of color aren&rsquo;t perfectly shaped like Cai&rsquo;s environment. But the organization of Cai's sensory apparatus and the organization of the world around Cai are similar enough that Cai can reconstruct many features of its world.</p>\n<p>By linking its visual patterns to patterns of changing tiles, Cai can hypothesize laws that guide the world's changes and explain Cai's sensory experiences. Here's one possibility, Hypothesis A:</p>\n<ul>\n<li>Black corresponds to cyan, white to yellow, and gray to magenta.</li>\n<li>At present, the top two rows are white and the bottom row is black, except for the upper-right tile (which is gray) and Cai itself, a black middle tile.</li>\n<li>Adjacent gray and white tiles exchange shades. Exception: When a white tile is pinned by a white and gray tile on either side, it turns black.</li>\n<li>Black tiles pinned by white ones on either side turn white. Exception: When the black tile is adjacent to a third white tile, it remains black.</li>\n</ul>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 1px solid black; vertical-align: middle; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_4.png\" alt=\"\" width=\"490\" height=\"234\" /></p>\n<p align=\"center\"><em>Hypothesis A's physical content. On the left: Cai's belief about the world's present state.&nbsp;</em><em>On the right: Cai's belief about the rules by which the world changes over time. The rules are symmetric under rotation and reflection.</em></p>\n<p>&nbsp;</p>\n<h2>Bridging stimulus and experience</h2>\n<p>So that's one way of modeling Cai's world; and it will yield a prediction about the cellular automaton's next state, and therefore about Cai's next visual experience. It will also yield retrodictions of the cellular automaton's state during Cai's three past sensory experiences.</p>\n<p align=\"center\"><img style=\"vertical-align: middle; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_9.png?v=f3e4246e40ca30caef91ef966818947f\" alt=\"\" width=\"578\" height=\"741\" /></p>\n<p align=\"center\"><em>Hypothesis A asserts that tiles below Cai, to Cai's left, above, and to Cai's right relate to Cai's color experiences via the rule&nbsp;{black &harr; cyan, white &harr; yellow, gray&nbsp;&harr; magenta}. Corner tiles, and future world-states and experiences, can be inferred from Hypothesis A's cell transition rules.</em></p>\n<p>&nbsp;</p>\n<p>Are there other, similar hypotheses that can explain the same data? Here's one, Hypothesis B:</p>\n<ul>\n<li>Normally, the correspondences between experienced colors and neighboring tile states are {black &harr; cyan, white &harr; yellow, gray&nbsp;&harr; magenta}, as in Hypothesis A. But northern grays are perceived as though they were black, helping explain irregularities in the distribution of cyan.</li>\n<li>Hypothesis B's cellular automaton presently looks similar to Hypothesis A's, but with a gray tile in the upper-left corner.</li>\n<li>Adjacent gray and white tiles exchange shades. Nothing else changes.</li>\n</ul>\n<p>The added complexity in the perception-to-environment link allows Hypothesis B to do away with most of the complexity in Hypothesis A's physical laws. Breaking down Hypotheses A and B into their respective physical and perception-to-environment components makes it more obvious how the two differ:</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 0; vertical-align: middle; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_6.png\" alt=\"\" width=\"751\" height=\"420\" /></p>\n<p align=\"center\"><em>A has the simpler bridge hypothesis, while B has the simpler physical hypothesis.</em></p>\n<p>&nbsp;</p>\n<p>Though they share a lot in common, and both account for Cai's experiences to date, these two hypotheses diverge substantially in the cellular automaton states and future experiences they predict:</p>\n<p align=\"center\"><img style=\"border: 0; vertical-align: middle; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_7.png?v=c90b983339fe44caf48a51878ca21e0f\" alt=\"\" width=\"752\" height=\"804\" /></p>\n<p align=\"center\"><em>The two hypotheses infer different distributions and dynamical rules for the tile shades from the same perceptual data. These worldly differences then diverge in the future experiences they predict.</em></p>\n<p>&nbsp;</p>\n<p>Hypotheses linking observations to theorized entities appear to be quite different from hypothesis that just describe the theorized entities in their own right. In Cai's case, the latter hypotheses look like pictures of physical worlds, while the former are ties between different kinds of representation. But in both cases it's useful to treat these processes in humans or machines as beliefs, since they can be assigned weights of expectation and updated.</p>\n<p>'Phenomenology' is a general term for an agent's models of its own introspected experiences. As such, we can call these hypotheses linking experienced data to theorized processes <strong>phenomenological bridge hypotheses</strong>. Or just 'bridge hypotheses', for short.</p>\n<p>If we want to build an agent that tries to evaluate the accuracy of a model based on the accuracy of its predictions, we need some scheme to compare thingies in the model (like tiles) and thingies in the sensory stream (like colors). Thus a <strong>bridge rule</strong> appears to be necessary to talk about induction over models of the world. And bridge hypotheses are just bridge rules treated as probabilistic, updatable beliefs.</p>\n<p>As the last figure above illustrates, bridge hypotheses can make a big difference for one's scientific beliefs and <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">expectations</a>. And bridge hypotheses aren't a free lunch; it would be a mistake to shunt all complexity onto them in order to simplify your physical hypotheses. Allow your bridge hypotheses to get too complicated, and you'll be able to justify mad world-models, e.g., ones where the universe consists of a single apricot whose individual atoms each get a separate bridge to some complex experience. At the same time, if you demand <em>too much</em>&nbsp;simplicity from your bridge hypotheses, you'll end up concluding that the physical world consists of a series of objects shaped just like your mental states. That way you can get away with a comically <a href=\"http://plato.stanford.edu/entries/berkeley/#3\">simple</a> bridge rule like {exists(x) &harr; experiences(y,x)}.</p>\n<p>In the absence of further information, it may not be possible to rule out Hypothesis A or Hypothesis B. The takeaway is that tradeoffs between the complexity of bridging hypotheses and the complexity of physical hypotheses do occur, and do matter.&nbsp;Any artificial agent needs some way of formulating good hypotheses of this type in order to be able to understand the universe at all, whether or not it finds itself in doubt after it has done so.</p>\n<p>&nbsp;</p>\n<p><a name=\"generalizing\"></a></p>\n<h2>Generalizing bridge rules and data</h2>\n<p><span style=\"font-size: small;\">Reasoners &mdash; both human and artificial &mdash; don't begin with perfect knowledge of their own design. When they have working self-models at all, these self-models are fallible. Aristotle thought the brain was an organ for cooling the blood. We had to find out about neurons by opening up the heads of people who looked like us, putting the big corrugated gray organ under a microscope, seeing (with our eyes, our visual cortex, our senses) that the microscope (which we'd previously generalized shows us tiny things as if they were large) showed this incredibly fine mesh of connected blobs, and realizing, \"Hey, I bet this does information processing and </span><em style=\"font-size: small;\">that's</em><span style=\"font-size: small;\"> what I am! The big gray corrugated organ that's inside my own head is </span><em style=\"font-size: small;\">me</em><span style=\"font-size: small;\">!\"</span></p>\n<p>The bridge hypotheses in Hypotheses A and B are about linking an agent's environment-triggered experiences to environmental causes. But in fact bridge hypotheses are more general than that.</p>\n<p style=\"padding-left: 30px;\">1. An agent's experiences needn't all have&nbsp;<em>environmental&nbsp;</em>causes. They can be caused by something inside the agent.</p>\n<p style=\"padding-left: 30px;\">2. The cause-effect relation we're bridging can go the other way. E.g., a bridge hypothesis can link an experienced&nbsp;<em>decision&nbsp;</em>to a behavioral consequence, or to an expected outcome of the behavior.</p>\n<p style=\"padding-left: 30px;\">3. The bridge hypothesis needn't link causes to effects at all. E.g., it can assert that the agent's experienced sensations or decisions just&nbsp;<em>are&nbsp;</em>a certain physical state. Or it can assert neutral correlations.</p>\n<p>Phenomenological bridge hypotheses, then, can relate theoretical posits to any sort of experiential data. Experiential data are internally evident facts that get compared to hypotheses and cause updates &mdash; the kind of data of direct epistemic relevance to individual scientists updating their personal beliefs. Light shines on your retina, gets transduced to neural firings, gets reconstructed in your visual cortex and then &mdash; this is the key part &mdash; that internal fact gets used to decide what sort of universe you're probably in.</p>\n<p>The data from an AI&rsquo;s environment is just one of many kinds of information it can use to update its probability distributions. In addition to ordinary sensory content such as vision and smell, update-triggering data could include things like how much RAM is being used. This is because an inner RAM sense can tell you that the universe is such as to include a copy of you with at least that much RAM.</p>\n<p>We normally think of science as reliant mainly on sensory faculties, not&nbsp;<a href=\"http://ase.tufts.edu/cogstud/dennett/papers/chalmersdeb3dft.htm\">introspective</a>&nbsp;ones. Arriving at conclusions just by&nbsp;<a href=\"http://rationallyspeaking.blogspot.com/2012/05/using-thought-experiments-to.html\">examining your own intuitions and imaginings</a>&nbsp;sounds more like math or philosophy. But for present purposes the distinction isn't important. What matters is just whether the AGI forms accurate beliefs and makes good decisions. Prototypical scientists may shun&nbsp;<a href=\"http://books.google.com/books?id=pOn6YSRQzKUC&amp;pg=PA55&amp;dq=introspectionist&amp;hl=en&amp;sa=X&amp;ei=BPOmUr2zDvC1sASko4CACA&amp;ved=0CHgQ6AEwCQ#v=onepage&amp;q=introspectionist&amp;f=false\">introspectionism</a>&nbsp;because humans do a better job of directly apprehending and communicating facts about their environments than facts about their own inner lives, but AGIs can have a very different set of strengths and weaknesses. Although introspection, like sensation, is fallible, introspective self-representations sometimes empirically&nbsp;<a href=\"/lw/jl/what_is_evidence/\">correlate</a>&nbsp;with world-states.<a name=\"footnote4back\"></a><span style=\"font-size: 11px;\"><sup><a href=\"#footnote4\">4</a></sup></span>&nbsp;And that&rsquo;s all it takes for them to constitute&nbsp;<a href=\"/lw/in/scientific_evidence_legal_evidence_rational/\">Bayesian evidence</a>.</p>\n<p>&nbsp;</p>\n<h2>Bridging hardware and experience</h2>\n<p>In my above discussion, all of Cai's world-models included representations of Cai itself. However, these representations were very simple &mdash; no more than a black tile in a specific environment. Since Cai's own computations are complex, it must be the case that either they are occurring outside the universe depicted (as though Cai is plugged into a cellular automaton&nbsp;<a href=\"http://consc.net/papers/matrix.html\">Matrix</a>), or the universe depicted is much more complex than Cai thinks.<a name=\"footnote5back\"></a><span style=\"font-size: 11px;\"><sup><a href=\"#footnote5\">5</a></sup></span>&nbsp;Perhaps its model is wildly mistaken, or perhaps the high-level cellular patterns it's hypothesized arise from other, smaller-scale regularities.</p>\n<p>Regardless, Cai&rsquo;s computations must be embodied in some causal pattern. Cai will eventually need to construct bridge hypotheses between its experiences and their physical substrate if it is to make reliable predictions about its own behavior and about its relationship with its surroundings.</p>\n<p>Visualize the epistemic problem that an agent needs to solve. Cai has access to a series of sensory impressions. In principle we could also add introspective data to that. But you'll still get a series of (presumably time-indexed) facts in some native format of that mind. Those facts very likely won't be structured exactly like any ontologically basic feature of the universe in which the mind lives. They won't be a precise position of a Newtonian particle, for example. And even if we were dealing with sense data shaped just like ontologically basic facts, a rational agent could never <a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">know for certain</a> that they were ontologically basic, so it would still have to consider hypotheses about even more basic particles.</p>\n<p>When humans or AGIs try to match up hypotheses about universes to sensory experiences, there will be a type error. Our representation of the universe will be in hypothetical atoms or quantum fields, while our representation of sensory experiences will be in a native format like 'red-green'.<a name=\"footnote6back\"></a><sup><a href=\"#footnote6\">6</a></sup>&nbsp;This is where bridge rules like Cai's color conversions come in &mdash; bridges that relate our experiences to environmental stimuli, as well as ones that relate our experiences to the hardware that runs us.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"vertical-align: middle; border: 1px solid black; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_8.png\" alt=\"\" width=\"338\" height=\"304\" /></p>\n<p align=\"center\"><em>Cai can form physical hypotheses about its own internal state, in addition to ones about its environment. This means it can form bridge hypotheses between its experiences and its own hardware, in addition to ones between its experiences and environment.</em></p>\n<p>&nbsp;</p>\n<p>If you were an AI, you might be able to decode your red-green visual field into binary data &mdash; on-vs.-off &mdash; and make very simple hypotheses about how that corresponded to transistors making you up. Once you used a microscope on yourself to see the transistors, you'd see that they had binary states of positive and negative voltage, and all that would be left would be a hypothesis about whether the positive (or negative) voltage corresponded to an introspected 1 (or 0).</p>\n<p>But even then, I don't quite see how you could do without the bridge rules &mdash; there has to be some way to go from internal sensory types to the types featured in your hypotheses about physical laws.</p>\n<p>Our sensory experience of red, green, blue&nbsp;<em>is</em>&nbsp;certain neurons firing in the visual cortex, and these neurons are in turn made from atoms. But internally, so far as information processing goes, we just know about the red, the green, the blue. This is what you'd expect an agent made of atoms to feel like&nbsp;<a href=\"/lw/of/dissolving_the_question/\">from the inside</a>. Our native representation of a pixel field won't come with a little tag telling us with infallible transparency about the underlying quantum mechanics.</p>\n<p>But this means that when we're done positing a physical universe in all its detail, we also need one last (hopefully simple!) step that connects hypotheses about 'a brain that processes visual information' to 'I see blue'.</p>\n<p>One way to avoid worrying about bridge hypotheses would be to instead code the AI to accept <strong>bridge axioms</strong>, bridge rules with no degrees of freedom and no uncertainty. But the AI&rsquo;s designers are not in fact <a href=\"/lw/mp/0_and_1_are_not_probabilities/\">infinitely confident</a> about how the AI&rsquo;s perceptual states emerge from the physical world &mdash; that, say, quantum field theory is the One True Answer, and shall be so from now until the end of time. Nor can they transmit infinite rational confidence to the AI merely by making it more stubbornly convinced of the view. If you pretend to know more than you do, <a href=\"/lw/js/the_bottom_line\">the world will still bite back</a>. As an agent in the world, you really do have to think about and test a variety of different uncertain hypotheses about what hardware you&rsquo;re running on, what kinds of environmental triggers produce such-and-such experiences, and so on. This is particularly true if your hardware is likely to undergo substantial changes over time.</p>\n<p>If you don&rsquo;t allow the AI to form probabilistic, updatable hypotheses about the relation between its phenomenology and the physical world, the AI will either be unable to reason at all, or it will reason its way off a cliff. In my next post, <strong><a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">Bridge Collapse</a></strong>,&nbsp;I'll begin discussing how the latter problem sinks an otherwise extremely promising approach to formalizing ideal AGI reasoning: Solomonoff induction.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p><sup><a name=\"footnote1\"></a>1</sup>&nbsp;By 'map-like', I mean that the processes look similar to the representational processes in human thought. They systematically correlate with external events, within a pattern-tracking system that can readily propagate and exploit the correlation.&nbsp;<a href=\"#footnote1back\">\u21a9</a></p>\n<p><sup><a name=\"footnote2\"></a>2</sup>&nbsp;Agents need <a href=\"/lw/s0/where_recursive_justification_hits_bottom/\">initial assumptions</a>, built-in <a href=\"/lw/hk/priors_as_mathematical_objects/\">prior information</a>. The prior is defined by whatever algorithm the reasoner follows in making its very first updates.</p>\n<p>If I leave an agent's priors undefined, no <a href=\"/lw/rf/ghosts_in_the_machine/\">ghost</a> of <a href=\"/lw/k2/a_priori/\">reasonableness</a> will intervene to give the agent a '<a href=\"/lw/rn/no_universally_compelling_arguments/\">default</a>' prior. For example, it won't default to a uniform prior over possible coinflip outcomes in the absence of relevant evidence. Rather, without something that acts like a prior, the agent just won't work &mdash;&nbsp;in the same way that a calculator won't work if you grant it the freedom to do math however it wishes. A <a href=\"/lw/1gc/frequentist_statistics_are_frequently_subjective/\">frequentist</a> AI might refuse to <em>talk </em>about priors, but it would still need to <em>act</em>&nbsp;like it has priors, else break.&nbsp;<a href=\"#footnote2back\">\u21a9</a></p>\n<p><span style=\"font-size: 11px;\"><a name=\"footnote3\"></a>3</span> This talk of 'belief' and 'assumption' and 'perception'&nbsp;<em>is&nbsp;</em>anthropomorphizing, and the analogies to human psychology won't be perfect.&nbsp;This is important to keep in view, though there's only so much we can do to avoid vagueness and analogical reasoning when the architecture of AGIs remains unknown. In particular, I'm not assuming that every artificial scientist is particularly intelligent. Or particularly conscious.</p>\n<p>What I mean with all this 'Cai believes...' talk is that Cai weights predictions and selects actions&nbsp;<em>just as though</em>&nbsp;it believed itself to be in a cellular automaton world. One can treat Cai's automaton-theoretic model as just a bookkeeping device for assigning&nbsp;<a href=\"http://ksvanhorn.com/bayes/Papers/rcox.pdf\">Cox's-theorem-following</a>&nbsp;real numbers to encoded images of color fields. But one can also treat Cai's model as a psychological expectation, to the extent it functionally resembles the corresponding human mental states. Words like 'assumption' and 'thinks' here needn't mean that the agent thinks in the same fashion humans think; what we're interested in are the broad class of information-processing algorithms that yield similar&nbsp;<a href=\"/lw/v8/belief_in_intelligence/\">behaviors</a>.&nbsp;<a href=\"#footnote3back\">\u21a9</a></p>\n<p><span style=\"font-size: 11px;\"><a name=\"footnote4\"></a>4</span> To illustrate: In principle, even a human pining to become a parent could, by introspection alone, infer that they might be an evolved mind (since they are experiencing a desire to self-replicate) and embedded in a universe which had evolved minds with evolutionary histories. An AGI with more reliable internal monitors could learn a great deal about the rest of the universe just by investigating itself.&nbsp;<a href=\"#footnote4back\">\u21a9</a></p>\n<p><span style=\"font-size: 11px;\"><a name=\"footnote5\"></a>5</span> In either case, we shouldn't be surprised to see Cai failing to fully represent its own inner workings. An agent cannot explicitly represent itself in its totality, since it would then need to represent itself representing itself representing itself ... ad infinitum. Environmental phenomena, too, must usually be <a href=\"/lw/nw/fallacies_of_compression/tk_53emgtCkOP0_qu45FrTq8A\">compressed</a>.&nbsp;<a href=\"#footnote5back\">\u21a9</a></p>\n<p><span style=\"font-size: 11px;\"><a name=\"footnote6\"></a>6</span> One response would be to place the blame on Cai's positing white, gray, and black for its world-models, rather than sticking with cyan, yellow, and magenta. But there will still be a type error when one tries to compare perceived cyan/yellow/magenta with hypothesized (but perceptually invisible) cyan/yellow/magenta. Explicitly introducing separate words for hypothesized v. perceived colors doesn't produce the distinction; it just makes it easier to keep track of a distinction that was already present.&nbsp;<a href=\"#footnote6back\">\u21a9</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XSryTypw5Hszpa4TS": 2, "uLqT8mmFiA8NeytTi": 2, "LnEEs8xGooYmQ8iLA": 2, "wMPYFGmhcFg4bSb4Z": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ethRJh2E7mSSjzCay", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 93, "extendedScore": null, "score": 0.000251, "legacy": true, "legacyId": "25101", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 93, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong><a href=\"http://wiki.lesswrong.com/wiki/Naturalized_induction\">Naturalized induction</a></strong>&nbsp;is an open problem in <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">Friendly Artificial Intelligence</a>&nbsp;(OPFAI). The problem, in brief: Our current leading models of induction do not allow reasoners to treat their own computations as processes in the world.&nbsp;</p>\n<p>The problem's roots lie in algorithmic information theory and formal epistemology, but finding answers will require us to wade into debates on everything from theoretical physics to anthropic reasoning and self-reference. This post will lay the groundwork for a sequence of posts (titled '<strong>Artificial Naturalism</strong>') introducing different aspects of this OPFAI.</p>\n<p>&nbsp;</p>\n<h2 id=\"AI_perception_and_belief__A_toy_model\">AI perception and belief: A toy model</h2>\n<p>A more concrete problem: Construct an algorithm that, given a sequence of the colors cyan, magenta, and yellow, predicts the next colored field.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 1px solid black; vertical-align: middle; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_0.png\" alt=\"\" width=\"574\" height=\"388\"></p>\n<p align=\"center\"><em>Colors: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; CYYM &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;CYYY &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;CYCM &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;CYYY &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;????</em></p>\n<p>&nbsp;</p>\n<p>This is an instance of the general problem 'From an incomplete data series, how can a reasoner best make predictions about future data?'. In practice, any agent that acquires information from its environment and makes predictions about what's coming next will need to have two map-like<a name=\"footnote1back\"></a><a href=\"#footnote1\"><sup>1</sup></a> subprocesses:</p>\n<p style=\"padding-left: 30px;\">1. Something that generates the agent's predictions, its expectations. By analogy with human scientists, we can call this prediction-generator the agent's <strong>hypotheses </strong>or <strong>beliefs</strong>.</p>\n<p style=\"padding-left: 30px;\">2. Something that transmits new information to the agent's prediction-generator so that its hypotheses can be <a href=\"http://yudkowsky.net/rational/bayes\">updated</a>. Employing another anthropomorphic analogy, we can call this process the agent's <strong>data</strong>&nbsp;or <strong>perceptions</strong>.</p>\n<p><a id=\"more\"></a></p>\n<p>Here's an example of a hypothesis an agent could use to try to predict the next color field. I'll call the imaginary agent '<strong>Cai</strong>'. Any reasoner will need to begin with some (perhaps provisional) assumptions about the world.<a name=\"footnote2back\"></a><sup><a href=\"#footnote2\">2</a></sup>&nbsp;Cai begins with the belief<a name=\"footnote3back\"></a><sup><a href=\"#footnote3\">3</a></sup> that its environment behaves like a <a href=\"/lw/fok/causal_universes/\">cellular automaton</a>: the world is a grid whose tiles change over time based on a set of stable laws. The laws are local in time and space, meaning that you can perfectly predict a tile's state based on the states of the tiles next to it a moment prior&nbsp;\u2014 if you know which laws are in force.</p>\n<p>Cai believes that it lives in a closed 3x3 grid where tiles have no diagonal effects. Each tile can occupy one of three states. We might call the states '0', '1', and '2', or, to make visualization easier, 'white', 'black', and 'gray'. So, on Cai's view, the world as it changes looks something like this:</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 1px solid black; vertical-align: middle; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_1.png?v=1f67eceff1678f2d2493f13ff464731e\" alt=\"\" width=\"304\" height=\"127\"></p>\n<p align=\"center\"><em>An example of the world's state at one moment, and its state a moment later.</em></p>\n<p>&nbsp;</p>\n<p>Cai also has beliefs about its own location in the cellular automaton. Cai believes that it is a black tile at the center of the grid. Since there are no diagonal laws of physics in this world, Cai can only directly interact with the four tiles directly above, below, to the left, and to the right. As such, any perceptual data Cai acquires will need to come from those four tiles; anything else about Cai's universe will be known <a href=\"/lw/pb/belief_in_the_implied_invisible\">only by inference</a>.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_jd9_2.png\" alt=\"\" width=\"314\" height=\"340\"></p>\n<p align=\"center\"><em>Cai perceives stimuli in four directions. Unobservable tiles fall outside the cross.</em></p>\n<p>&nbsp;</p>\n<p>How does all this bear on the color-predicting problem? Cai hypothesizes that the sequence of colors is sensory \u2014 it's an experience within Cai, triggered by environmental changes. Cai conjectures that since its visual field comes in at most four colors, its visual field's quadrants probably represent its four adjacent tiles. The leftmost color comes from a southern stimulus, the next one to the right from a western stimulus, then a northern one, then an eastern one. And the south, west, north, east cycle repeats again and again.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"vertical-align: middle; border: 1px solid black; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_3.png?v=943de6003ceac30216fcd64e3634d655\" alt=\"\" width=\"274\" height=\"327\"></p>\n<p align=\"center\"><em>Cai\u2019s visual experiences break down into quadrants, corresponding to four directions.</em></p>\n<p>&nbsp;</p>\n<p>On this model, the way Cai\u2019s senses organize the data isn't wholly veridical; the four patches of color aren\u2019t perfectly shaped like Cai\u2019s environment. But the organization of Cai's sensory apparatus and the organization of the world around Cai are similar enough that Cai can reconstruct many features of its world.</p>\n<p>By linking its visual patterns to patterns of changing tiles, Cai can hypothesize laws that guide the world's changes and explain Cai's sensory experiences. Here's one possibility, Hypothesis A:</p>\n<ul>\n<li>Black corresponds to cyan, white to yellow, and gray to magenta.</li>\n<li>At present, the top two rows are white and the bottom row is black, except for the upper-right tile (which is gray) and Cai itself, a black middle tile.</li>\n<li>Adjacent gray and white tiles exchange shades. Exception: When a white tile is pinned by a white and gray tile on either side, it turns black.</li>\n<li>Black tiles pinned by white ones on either side turn white. Exception: When the black tile is adjacent to a third white tile, it remains black.</li>\n</ul>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 1px solid black; vertical-align: middle; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_4.png\" alt=\"\" width=\"490\" height=\"234\"></p>\n<p align=\"center\"><em>Hypothesis A's physical content. On the left: Cai's belief about the world's present state.&nbsp;</em><em>On the right: Cai's belief about the rules by which the world changes over time. The rules are symmetric under rotation and reflection.</em></p>\n<p>&nbsp;</p>\n<h2 id=\"Bridging_stimulus_and_experience\">Bridging stimulus and experience</h2>\n<p>So that's one way of modeling Cai's world; and it will yield a prediction about the cellular automaton's next state, and therefore about Cai's next visual experience. It will also yield retrodictions of the cellular automaton's state during Cai's three past sensory experiences.</p>\n<p align=\"center\"><img style=\"vertical-align: middle; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_9.png?v=f3e4246e40ca30caef91ef966818947f\" alt=\"\" width=\"578\" height=\"741\"></p>\n<p align=\"center\"><em>Hypothesis A asserts that tiles below Cai, to Cai's left, above, and to Cai's right relate to Cai's color experiences via the rule&nbsp;{black \u2194 cyan, white \u2194 yellow, gray&nbsp;\u2194 magenta}. Corner tiles, and future world-states and experiences, can be inferred from Hypothesis A's cell transition rules.</em></p>\n<p>&nbsp;</p>\n<p>Are there other, similar hypotheses that can explain the same data? Here's one, Hypothesis B:</p>\n<ul>\n<li>Normally, the correspondences between experienced colors and neighboring tile states are {black \u2194 cyan, white \u2194 yellow, gray&nbsp;\u2194 magenta}, as in Hypothesis A. But northern grays are perceived as though they were black, helping explain irregularities in the distribution of cyan.</li>\n<li>Hypothesis B's cellular automaton presently looks similar to Hypothesis A's, but with a gray tile in the upper-left corner.</li>\n<li>Adjacent gray and white tiles exchange shades. Nothing else changes.</li>\n</ul>\n<p>The added complexity in the perception-to-environment link allows Hypothesis B to do away with most of the complexity in Hypothesis A's physical laws. Breaking down Hypotheses A and B into their respective physical and perception-to-environment components makes it more obvious how the two differ:</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"border: 0; vertical-align: middle; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_6.png\" alt=\"\" width=\"751\" height=\"420\"></p>\n<p align=\"center\"><em>A has the simpler bridge hypothesis, while B has the simpler physical hypothesis.</em></p>\n<p>&nbsp;</p>\n<p>Though they share a lot in common, and both account for Cai's experiences to date, these two hypotheses diverge substantially in the cellular automaton states and future experiences they predict:</p>\n<p align=\"center\"><img style=\"border: 0; vertical-align: middle; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_7.png?v=c90b983339fe44caf48a51878ca21e0f\" alt=\"\" width=\"752\" height=\"804\"></p>\n<p align=\"center\"><em>The two hypotheses infer different distributions and dynamical rules for the tile shades from the same perceptual data. These worldly differences then diverge in the future experiences they predict.</em></p>\n<p>&nbsp;</p>\n<p>Hypotheses linking observations to theorized entities appear to be quite different from hypothesis that just describe the theorized entities in their own right. In Cai's case, the latter hypotheses look like pictures of physical worlds, while the former are ties between different kinds of representation. But in both cases it's useful to treat these processes in humans or machines as beliefs, since they can be assigned weights of expectation and updated.</p>\n<p>'Phenomenology' is a general term for an agent's models of its own introspected experiences. As such, we can call these hypotheses linking experienced data to theorized processes <strong>phenomenological bridge hypotheses</strong>. Or just 'bridge hypotheses', for short.</p>\n<p>If we want to build an agent that tries to evaluate the accuracy of a model based on the accuracy of its predictions, we need some scheme to compare thingies in the model (like tiles) and thingies in the sensory stream (like colors). Thus a <strong>bridge rule</strong> appears to be necessary to talk about induction over models of the world. And bridge hypotheses are just bridge rules treated as probabilistic, updatable beliefs.</p>\n<p>As the last figure above illustrates, bridge hypotheses can make a big difference for one's scientific beliefs and <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">expectations</a>. And bridge hypotheses aren't a free lunch; it would be a mistake to shunt all complexity onto them in order to simplify your physical hypotheses. Allow your bridge hypotheses to get too complicated, and you'll be able to justify mad world-models, e.g., ones where the universe consists of a single apricot whose individual atoms each get a separate bridge to some complex experience. At the same time, if you demand <em>too much</em>&nbsp;simplicity from your bridge hypotheses, you'll end up concluding that the physical world consists of a series of objects shaped just like your mental states. That way you can get away with a comically <a href=\"http://plato.stanford.edu/entries/berkeley/#3\">simple</a> bridge rule like {exists(x) \u2194 experiences(y,x)}.</p>\n<p>In the absence of further information, it may not be possible to rule out Hypothesis A or Hypothesis B. The takeaway is that tradeoffs between the complexity of bridging hypotheses and the complexity of physical hypotheses do occur, and do matter.&nbsp;Any artificial agent needs some way of formulating good hypotheses of this type in order to be able to understand the universe at all, whether or not it finds itself in doubt after it has done so.</p>\n<p>&nbsp;</p>\n<p><a name=\"generalizing\"></a></p>\n<h2 id=\"Generalizing_bridge_rules_and_data\">Generalizing bridge rules and data</h2>\n<p><span style=\"font-size: small;\">Reasoners \u2014 both human and artificial \u2014 don't begin with perfect knowledge of their own design. When they have working self-models at all, these self-models are fallible. Aristotle thought the brain was an organ for cooling the blood. We had to find out about neurons by opening up the heads of people who looked like us, putting the big corrugated gray organ under a microscope, seeing (with our eyes, our visual cortex, our senses) that the microscope (which we'd previously generalized shows us tiny things as if they were large) showed this incredibly fine mesh of connected blobs, and realizing, \"Hey, I bet this does information processing and </span><em style=\"font-size: small;\">that's</em><span style=\"font-size: small;\"> what I am! The big gray corrugated organ that's inside my own head is </span><em style=\"font-size: small;\">me</em><span style=\"font-size: small;\">!\"</span></p>\n<p>The bridge hypotheses in Hypotheses A and B are about linking an agent's environment-triggered experiences to environmental causes. But in fact bridge hypotheses are more general than that.</p>\n<p style=\"padding-left: 30px;\">1. An agent's experiences needn't all have&nbsp;<em>environmental&nbsp;</em>causes. They can be caused by something inside the agent.</p>\n<p style=\"padding-left: 30px;\">2. The cause-effect relation we're bridging can go the other way. E.g., a bridge hypothesis can link an experienced&nbsp;<em>decision&nbsp;</em>to a behavioral consequence, or to an expected outcome of the behavior.</p>\n<p style=\"padding-left: 30px;\">3. The bridge hypothesis needn't link causes to effects at all. E.g., it can assert that the agent's experienced sensations or decisions just&nbsp;<em>are&nbsp;</em>a certain physical state. Or it can assert neutral correlations.</p>\n<p>Phenomenological bridge hypotheses, then, can relate theoretical posits to any sort of experiential data. Experiential data are internally evident facts that get compared to hypotheses and cause updates \u2014 the kind of data of direct epistemic relevance to individual scientists updating their personal beliefs. Light shines on your retina, gets transduced to neural firings, gets reconstructed in your visual cortex and then \u2014 this is the key part \u2014 that internal fact gets used to decide what sort of universe you're probably in.</p>\n<p>The data from an AI\u2019s environment is just one of many kinds of information it can use to update its probability distributions. In addition to ordinary sensory content such as vision and smell, update-triggering data could include things like how much RAM is being used. This is because an inner RAM sense can tell you that the universe is such as to include a copy of you with at least that much RAM.</p>\n<p>We normally think of science as reliant mainly on sensory faculties, not&nbsp;<a href=\"http://ase.tufts.edu/cogstud/dennett/papers/chalmersdeb3dft.htm\">introspective</a>&nbsp;ones. Arriving at conclusions just by&nbsp;<a href=\"http://rationallyspeaking.blogspot.com/2012/05/using-thought-experiments-to.html\">examining your own intuitions and imaginings</a>&nbsp;sounds more like math or philosophy. But for present purposes the distinction isn't important. What matters is just whether the AGI forms accurate beliefs and makes good decisions. Prototypical scientists may shun&nbsp;<a href=\"http://books.google.com/books?id=pOn6YSRQzKUC&amp;pg=PA55&amp;dq=introspectionist&amp;hl=en&amp;sa=X&amp;ei=BPOmUr2zDvC1sASko4CACA&amp;ved=0CHgQ6AEwCQ#v=onepage&amp;q=introspectionist&amp;f=false\">introspectionism</a>&nbsp;because humans do a better job of directly apprehending and communicating facts about their environments than facts about their own inner lives, but AGIs can have a very different set of strengths and weaknesses. Although introspection, like sensation, is fallible, introspective self-representations sometimes empirically&nbsp;<a href=\"/lw/jl/what_is_evidence/\">correlate</a>&nbsp;with world-states.<a name=\"footnote4back\"></a><span style=\"font-size: 11px;\"><sup><a href=\"#footnote4\">4</a></sup></span>&nbsp;And that\u2019s all it takes for them to constitute&nbsp;<a href=\"/lw/in/scientific_evidence_legal_evidence_rational/\">Bayesian evidence</a>.</p>\n<p>&nbsp;</p>\n<h2 id=\"Bridging_hardware_and_experience\">Bridging hardware and experience</h2>\n<p>In my above discussion, all of Cai's world-models included representations of Cai itself. However, these representations were very simple \u2014 no more than a black tile in a specific environment. Since Cai's own computations are complex, it must be the case that either they are occurring outside the universe depicted (as though Cai is plugged into a cellular automaton&nbsp;<a href=\"http://consc.net/papers/matrix.html\">Matrix</a>), or the universe depicted is much more complex than Cai thinks.<a name=\"footnote5back\"></a><span style=\"font-size: 11px;\"><sup><a href=\"#footnote5\">5</a></sup></span>&nbsp;Perhaps its model is wildly mistaken, or perhaps the high-level cellular patterns it's hypothesized arise from other, smaller-scale regularities.</p>\n<p>Regardless, Cai\u2019s computations must be embodied in some causal pattern. Cai will eventually need to construct bridge hypotheses between its experiences and their physical substrate if it is to make reliable predictions about its own behavior and about its relationship with its surroundings.</p>\n<p>Visualize the epistemic problem that an agent needs to solve. Cai has access to a series of sensory impressions. In principle we could also add introspective data to that. But you'll still get a series of (presumably time-indexed) facts in some native format of that mind. Those facts very likely won't be structured exactly like any ontologically basic feature of the universe in which the mind lives. They won't be a precise position of a Newtonian particle, for example. And even if we were dealing with sense data shaped just like ontologically basic facts, a rational agent could never <a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">know for certain</a> that they were ontologically basic, so it would still have to consider hypotheses about even more basic particles.</p>\n<p>When humans or AGIs try to match up hypotheses about universes to sensory experiences, there will be a type error. Our representation of the universe will be in hypothetical atoms or quantum fields, while our representation of sensory experiences will be in a native format like 'red-green'.<a name=\"footnote6back\"></a><sup><a href=\"#footnote6\">6</a></sup>&nbsp;This is where bridge rules like Cai's color conversions come in \u2014 bridges that relate our experiences to environmental stimuli, as well as ones that relate our experiences to the hardware that runs us.</p>\n<p>&nbsp;</p>\n<p align=\"center\"><img style=\"vertical-align: middle; border: 1px solid black; margin: 3px;\" src=\"http://images.lesswrong.com/t3_jd9_8.png\" alt=\"\" width=\"338\" height=\"304\"></p>\n<p align=\"center\"><em>Cai can form physical hypotheses about its own internal state, in addition to ones about its environment. This means it can form bridge hypotheses between its experiences and its own hardware, in addition to ones between its experiences and environment.</em></p>\n<p>&nbsp;</p>\n<p>If you were an AI, you might be able to decode your red-green visual field into binary data \u2014 on-vs.-off \u2014 and make very simple hypotheses about how that corresponded to transistors making you up. Once you used a microscope on yourself to see the transistors, you'd see that they had binary states of positive and negative voltage, and all that would be left would be a hypothesis about whether the positive (or negative) voltage corresponded to an introspected 1 (or 0).</p>\n<p>But even then, I don't quite see how you could do without the bridge rules \u2014 there has to be some way to go from internal sensory types to the types featured in your hypotheses about physical laws.</p>\n<p>Our sensory experience of red, green, blue&nbsp;<em>is</em>&nbsp;certain neurons firing in the visual cortex, and these neurons are in turn made from atoms. But internally, so far as information processing goes, we just know about the red, the green, the blue. This is what you'd expect an agent made of atoms to feel like&nbsp;<a href=\"/lw/of/dissolving_the_question/\">from the inside</a>. Our native representation of a pixel field won't come with a little tag telling us with infallible transparency about the underlying quantum mechanics.</p>\n<p>But this means that when we're done positing a physical universe in all its detail, we also need one last (hopefully simple!) step that connects hypotheses about 'a brain that processes visual information' to 'I see blue'.</p>\n<p>One way to avoid worrying about bridge hypotheses would be to instead code the AI to accept <strong>bridge axioms</strong>, bridge rules with no degrees of freedom and no uncertainty. But the AI\u2019s designers are not in fact <a href=\"/lw/mp/0_and_1_are_not_probabilities/\">infinitely confident</a> about how the AI\u2019s perceptual states emerge from the physical world \u2014 that, say, quantum field theory is the One True Answer, and shall be so from now until the end of time. Nor can they transmit infinite rational confidence to the AI merely by making it more stubbornly convinced of the view. If you pretend to know more than you do, <a href=\"/lw/js/the_bottom_line\">the world will still bite back</a>. As an agent in the world, you really do have to think about and test a variety of different uncertain hypotheses about what hardware you\u2019re running on, what kinds of environmental triggers produce such-and-such experiences, and so on. This is particularly true if your hardware is likely to undergo substantial changes over time.</p>\n<p>If you don\u2019t allow the AI to form probabilistic, updatable hypotheses about the relation between its phenomenology and the physical world, the AI will either be unable to reason at all, or it will reason its way off a cliff. In my next post, <strong><a href=\"/lw/jlg/bridge_collapse_reductionism_as_engineering/\">Bridge Collapse</a></strong>,&nbsp;I'll begin discussing how the latter problem sinks an otherwise extremely promising approach to formalizing ideal AGI reasoning: Solomonoff induction.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr>\n<p><sup><a name=\"footnote1\"></a>1</sup>&nbsp;By 'map-like', I mean that the processes look similar to the representational processes in human thought. They systematically correlate with external events, within a pattern-tracking system that can readily propagate and exploit the correlation.&nbsp;<a href=\"#footnote1back\">\u21a9</a></p>\n<p><sup><a name=\"footnote2\"></a>2</sup>&nbsp;Agents need <a href=\"/lw/s0/where_recursive_justification_hits_bottom/\">initial assumptions</a>, built-in <a href=\"/lw/hk/priors_as_mathematical_objects/\">prior information</a>. The prior is defined by whatever algorithm the reasoner follows in making its very first updates.</p>\n<p>If I leave an agent's priors undefined, no <a href=\"/lw/rf/ghosts_in_the_machine/\">ghost</a> of <a href=\"/lw/k2/a_priori/\">reasonableness</a> will intervene to give the agent a '<a href=\"/lw/rn/no_universally_compelling_arguments/\">default</a>' prior. For example, it won't default to a uniform prior over possible coinflip outcomes in the absence of relevant evidence. Rather, without something that acts like a prior, the agent just won't work \u2014&nbsp;in the same way that a calculator won't work if you grant it the freedom to do math however it wishes. A <a href=\"/lw/1gc/frequentist_statistics_are_frequently_subjective/\">frequentist</a> AI might refuse to <em>talk </em>about priors, but it would still need to <em>act</em>&nbsp;like it has priors, else break.&nbsp;<a href=\"#footnote2back\">\u21a9</a></p>\n<p><span style=\"font-size: 11px;\"><a name=\"footnote3\"></a>3</span> This talk of 'belief' and 'assumption' and 'perception'&nbsp;<em>is&nbsp;</em>anthropomorphizing, and the analogies to human psychology won't be perfect.&nbsp;This is important to keep in view, though there's only so much we can do to avoid vagueness and analogical reasoning when the architecture of AGIs remains unknown. In particular, I'm not assuming that every artificial scientist is particularly intelligent. Or particularly conscious.</p>\n<p>What I mean with all this 'Cai believes...' talk is that Cai weights predictions and selects actions&nbsp;<em>just as though</em>&nbsp;it believed itself to be in a cellular automaton world. One can treat Cai's automaton-theoretic model as just a bookkeeping device for assigning&nbsp;<a href=\"http://ksvanhorn.com/bayes/Papers/rcox.pdf\">Cox's-theorem-following</a>&nbsp;real numbers to encoded images of color fields. But one can also treat Cai's model as a psychological expectation, to the extent it functionally resembles the corresponding human mental states. Words like 'assumption' and 'thinks' here needn't mean that the agent thinks in the same fashion humans think; what we're interested in are the broad class of information-processing algorithms that yield similar&nbsp;<a href=\"/lw/v8/belief_in_intelligence/\">behaviors</a>.&nbsp;<a href=\"#footnote3back\">\u21a9</a></p>\n<p><span style=\"font-size: 11px;\"><a name=\"footnote4\"></a>4</span> To illustrate: In principle, even a human pining to become a parent could, by introspection alone, infer that they might be an evolved mind (since they are experiencing a desire to self-replicate) and embedded in a universe which had evolved minds with evolutionary histories. An AGI with more reliable internal monitors could learn a great deal about the rest of the universe just by investigating itself.&nbsp;<a href=\"#footnote4back\">\u21a9</a></p>\n<p><span style=\"font-size: 11px;\"><a name=\"footnote5\"></a>5</span> In either case, we shouldn't be surprised to see Cai failing to fully represent its own inner workings. An agent cannot explicitly represent itself in its totality, since it would then need to represent itself representing itself representing itself ... ad infinitum. Environmental phenomena, too, must usually be <a href=\"/lw/nw/fallacies_of_compression/tk_53emgtCkOP0_qu45FrTq8A\">compressed</a>.&nbsp;<a href=\"#footnote5back\">\u21a9</a></p>\n<p><span style=\"font-size: 11px;\"><a name=\"footnote6\"></a>6</span> One response would be to place the blame on Cai's positing white, gray, and black for its world-models, rather than sticking with cyan, yellow, and magenta. But there will still be a type error when one tries to compare perceived cyan/yellow/magenta with hypothesized (but perceptually invisible) cyan/yellow/magenta. Explicitly introducing separate words for hypothesized v. perceived colors doesn't produce the distinction; it just makes it easier to keep track of a distinction that was already present.&nbsp;<a href=\"#footnote6back\">\u21a9</a></p>", "sections": [{"title": "AI perception and belief: A toy model", "anchor": "AI_perception_and_belief__A_toy_model", "level": 1}, {"title": "Bridging stimulus and experience", "anchor": "Bridging_stimulus_and_experience", "level": 1}, {"title": "Generalizing bridge rules and data", "anchor": "Generalizing_bridge_rules_and_data", "level": 1}, {"title": "Bridging hardware and experience", "anchor": "Bridging_hardware_and_experience", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "115 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 116, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["o5F2p3krzT4JgzqQc", "3XMwPNMSbaPm2suGz", "a7n8GdKiAZRX86T5A", "6s3xABaXKPdFwA3FS", "fhojYBGGiYAFcryHZ", "6FmqiAgS8h4EJm86s", "Mc6QcrsbH5NRXbCRX", "QGkYCwyC7wTDyt3yT", "34XxbRFe54FycoCDw", "3WuAjWMtxQwTxr2Qn", "C8nEXTcjZb9oauTCW", "jzf4Rcienrm6btRyt", "cnYHFNBF3kZEyx24v", "qmqLxvtsPzZ2s6mpY", "PtoQdG7E8MxYJrigu", "9qCN6tRBtksSyXfHu", "HktFCy6dgsqJ9WPpX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-23T20:48:12.469Z", "modifiedAt": null, "url": null, "title": "Review of Scott Adams\u2019 \u201cHow to Fail at Almost Everything and Still Win Big\u201d", "slug": "review-of-scott-adams-how-to-fail-at-almost-everything-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:36.535Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QTkij5fmPXPd7GD4Z/review-of-scott-adams-how-to-fail-at-almost-everything-and", "pageUrlRelative": "/posts/QTkij5fmPXPd7GD4Z/review-of-scott-adams-how-to-fail-at-almost-everything-and", "linkUrl": "https://www.lesswrong.com/posts/QTkij5fmPXPd7GD4Z/review-of-scott-adams-how-to-fail-at-almost-everything-and", "postedAtFormatted": "Monday, December 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Review%20of%20Scott%20Adams%E2%80%99%20%E2%80%9CHow%20to%20Fail%20at%20Almost%20Everything%20and%20Still%20Win%20Big%E2%80%9D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReview%20of%20Scott%20Adams%E2%80%99%20%E2%80%9CHow%20to%20Fail%20at%20Almost%20Everything%20and%20Still%20Win%20Big%E2%80%9D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQTkij5fmPXPd7GD4Z%2Freview-of-scott-adams-how-to-fail-at-almost-everything-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Review%20of%20Scott%20Adams%E2%80%99%20%E2%80%9CHow%20to%20Fail%20at%20Almost%20Everything%20and%20Still%20Win%20Big%E2%80%9D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQTkij5fmPXPd7GD4Z%2Freview-of-scott-adams-how-to-fail-at-almost-everything-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQTkij5fmPXPd7GD4Z%2Freview-of-scott-adams-how-to-fail-at-almost-everything-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1665, "htmlBody": "<p>Dilbert creator and bestselling author Scott Adams recently wrote a LessWrong compatible advice book that even contains a long list of cognitive biases. &nbsp; Adams told me in a phone interview that he is a lifelong consumer of academic studies, which perhaps accounts for why his book jibes so well with LessWrong teachings. &nbsp;Along with <a href=\"http://hpmor.com/\">HPMOR</a>, <a href=\"http://smile.amazon.com/How-Fail-Almost-Everything-Still/dp/0241003709/ref=tmm_pap_swatch_0?_encoding=UTF8&amp;sr=8-1&amp;qid=1387580413\">How to Fail at Almost Everything and Still Win Big</a> should be among your first choices when recommending books to novice rationalists. &nbsp;Below are some of the main lessons from the book, followed by a summary of my conversation with Adams about issues of particular concern to LessWrong readers.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p>My favorite passage describes when Adams gave a talk to a fifth-grade class and asked everyone to finish the sentence &ldquo;If you play a slot machine long enough, eventually you will&hellip;&rdquo; &nbsp;The students all shouted &ldquo;WIN!&rdquo; because, Adams suspects, they had the value of persistence drilled into them and confused it with success. &nbsp;</p>\n<p>&ldquo;WIN!&rdquo; would have been the right answer if you didn&rsquo;t have to pay to play but the machine still periodically gave out jackpots. &nbsp;Adams thinks you can develop a system to turn your life into a winning slot machine that doesn&rsquo;t require money but does require &ldquo;time, focus, and energy&rdquo; to repeatedly pull the lever. &nbsp;</p>\n<p>Adams argues that maximizing your energy level through proper diet, exercise, and sleep should take priority over everything else. &nbsp;Even if your only goal is to help others, be selfish with respect your energy level because it will determine your capacity for doing good. &nbsp;Adams has convinced me that appropriate diet, exercise, and sleep should be the starting point for effective altruists. &nbsp;Adams believes we have limited willpower and argues that if you make being active every single day a habit, you won&rsquo;t have to consume any precious willpower to motivate yourself to exercise.</p>\n<p>Since most pulls of the life slot machine will win you nothing, Adams argues that lack of fear of embarrassment is a key ingredient for success. &nbsp;Adams would undoubtedly approve of <a href=\"http://rationality.org/\">CFAR&rsquo;s</a> <a href=\"/lw/izq/meetup_comfort_zone_expansion_at_citadel_boston/\">comfort zone expansion exercises</a>.</p>\n<p>Adams lists skills that increase your chances of success. &nbsp;These include knowledge of public speaking, psychology, business writing, accounting, design, conversation, overcoming shyness, second language, golf, proper grammar, persuasion, technology (hobby level), and proper voice technique. &nbsp;He gives a bit of actionable advice on each, basically ideas for <a href=\"/lw/5b8/insufficiently_awesome/3z5r\">becoming more awesome</a>. &nbsp;I wish my teenage self had been told of Adams&rsquo; theory that a shy person can frequently achieve better social outcomes by pretending that he is an actor playing the part of an extrovert. &nbsp;</p>\n<p>Adams believes we should rely on systems rather than goals, and indeed he thinks that &ldquo;goals are for losers.&rdquo; &nbsp;If, when playing the slot machine of life, your goal is winning a jackpot, then you will feel like a loser each time you don&rsquo;t win. But if, instead, you are systems oriented then you can be in the constant state of success, something that will probably make you happier.</p>\n<p>Adams claims that happiness &ldquo;isn&rsquo;t as dependent on your circumstances as you might think,&rdquo; and &ldquo;anyone who has experienced happiness probably has the capacity to spend more time at the top of his or her personal range and less time near the bottom.&rdquo; &nbsp;His suggestions for becoming happier include improving your exercise, diet, and sleep; having a flexible work schedule, being able to imagine a better future, and being headed towards a brighter future.</p>\n<p>The part of the book most likely to trouble LessWrong readers is when Adams recommends engaging in self-delusion. &nbsp;He writes:&nbsp;</p>\n<blockquote>\n<p>&ldquo;Athletes are known to stop shaving for the duration of a tournament or to wear socks they deem lucky. &nbsp;These superstitions probably help in some small way to bolster their confidence, which in turn can influence success. &nbsp;It&rsquo;s irrelevant that lucky socks aren&rsquo;t a real thing&hellip;Most times our misconceptions about reality are benign and sometimes, even helpful. &nbsp;Other times, not so much.&rdquo;</p>\n</blockquote>\n<p>For me, being rational means having accurate beliefs and useful emotions. &nbsp;But what if these two goals conflict? &nbsp;For example, a college friend of mine who used to be a book editor <a href=\"http://sarahwynde.com/2011/12/following-the-rules/\">wrote</a> &ldquo;Most [authors] would do better by getting a minimum wage job and spending their entire paycheck on lottery tickets.&rdquo; &nbsp;I know this is true for me, yet I motivated myself to write <a href=\"http://www.amazon.com/Singularity-Rising-Surviving-Thriving-Dangerous/dp/1936661659\">my last book</a> in part by repeatedly dreaming of how it would be a best seller, and such willful delusions did, at the very least, make writing the book more enjoyable.</p>\n<p>To successfully distort reality you probably need to keep two separate mental books: the false one designed to motivate yourself, and the accurate one to keep you out of trouble. &nbsp;If you forget that you don&rsquo;t really have a &ldquo;<a href=\"http://dilbert.com/blog/entry/reality_distortion_field/\">reality distortion field</a>&rdquo;, that you can&rsquo;t change the territory by falsifying your map, you might make a <a href=\"/lw/81x/link_why_did_steve_jobs_choose_not_to_effectively/\">Steve Jobs level error</a> by, say, voluntarily forgoing lifesaving medical care because you think you can wish your problem away.&nbsp;</p>\n<p>The strangest part of the book concerns affirmations, which Adams defines as the &ldquo;practice of repeating to yourself what you want to achieve while imagining the outcome you want.&rdquo; &nbsp;Adams describes his fantastic success with achieving his affirmations, which included his becoming a famous cartoonist, having a seemingly hopeless medical problem fixed, and scoring at exactly the 94th percentile on the GMATs. &nbsp;Adams writes that the success of affirmations for him and others seems to go beyond what could be achieved by positive thinking. &nbsp;Thankfully he rules out magic as a possible solution and suggests that the success of affirmations might be due to selective memories, false memories, optimists tending to notice opportunities, selection effect of people who make affirmations, and mysterious science we don&rsquo;t understand. &nbsp;His support of affirmations seems to contradict his dislike of goals.</p>\n<p>&nbsp;</p>\n<p><strong>Our Phone Conversation</strong></p>\n<p>I took advantage of <a href=\"http://dilbert.com/blog/entry/open_interview_with_scott/\">this offer</a> to get a 15-minute phone interview with Adams. &nbsp;</p>\n<p>He has heard of LessWrong, but doesn&rsquo;t have any specific knowledge of us. &nbsp;He thinks the Singularity is a very probable outcome for mankind. &nbsp;He believes it will likely turn out all right due to what he calls &ldquo;<a href=\"http://dilbert.com/blog/entry/obamacare__side_bets/\">Adams&rsquo; Law of Slow-Moving Disasters</a>&rdquo; which says that &ldquo;any disaster we see coming with plenty of advance notice gets fixed.&rdquo; &nbsp;To the extent that Adams is correct, we will owe much to <a href=\"http://intelligence.org/\">Eliezer and associates</a> for providing us with loud early warnings of how the Singularity might go wrong.</p>\n<p>I failed in my brief attempt to get Adams interested in cryonics. &nbsp;He likes the idea of brain uploading and thinks it will be unnecessary to save the biological part of us. &nbsp;I was unable to convince him that cryonics is a good intermediate step until someone develops the technology for uploading. &nbsp;He mentioned that so long as the Internet survives a huge amount of him basically will as well. &nbsp;&nbsp;</p>\n<p>Recalling Adams&rsquo; claim that having a high tolerance for embarrassment is a key attribute for success, &nbsp;I asked him about my theory of how American dating culture, in which it&rsquo;s usually the man who risks rejection by asking the woman to go on the first date, gives men an entrepreneurial advantage because it eventually makes men more tolerant of rejection. &nbsp;Adams didn&rsquo;t directly respond to my theory, but brought up evolutionary psychology by saying that men would encounter much rejection because of their preference for variety and role as hunters. &nbsp;Adams stressed that this was just speculation and not back up by evidence.</p>\n<p>Adams has heard of <a href=\"https://www.metamed.com/\">MetaMed</a>. &nbsp;He is very optimistic about the ability of medicine to become more rational and to handle big data. &nbsp;When I pointed out that doctors often <a href=\"http://slatestarcodex.com/2013/12/17/statistical-literacy-among-doctors-now-lower-than-chance/\">don&rsquo;t know basic statistics</a>, he said that this probably doesn&rsquo;t impede their ability to treat patients.</p>\n<p>After I explained the concept of akrasis to Adams, he mentioned the book &ldquo;The Power of Habit&rdquo; and told me that we can use science to develop better habits. &nbsp;(Kaj_Sotala&nbsp;recently wrote a highly upvoted LessWrong <a href=\"/lw/ita/how_habits_work_and_how_you_may_control_them/\">post</a> on the &ldquo;The Power of Habit.&rdquo;)&nbsp;</p>\n<p>Adams suggested that if you have trouble accomplishing a certain task, just focus on the part that you can do: &nbsp;for example if spending an hour at the gym seems too difficult, then just think about putting on your sneakers. &nbsp;</p>\n<p>Although he didn&rsquo;t use these exact words, Adams basically defended himself against the charge of &ldquo;<a href=\"/lw/9v/beware_of_otheroptimizing/\">other-optimizing</a>.&rdquo; &nbsp;He explained that it would be very difficult to describe to an alien what a horse is. &nbsp;But once you succeeded describing the horse, it would be much easier to describe a zebra because you could do so in part by making references to the horse. &nbsp;Adams said he knows his advice isn&rsquo;t ideal for everyone, but it provides a useful template you can use to develop a plan better optimized for yourself. &nbsp;</p>\n<p>At the end of the interview Adams said he was surprised I had not brought up assisted suicide, given his recent blog post on the topic. &nbsp;In the <a href=\"http://www.dilbert.com/blog/entry/i_hope_my_father_dies_soon/\">post</a> Adams wrote:</p>\n<blockquote>\n<p>&ldquo;My father, age 86, is on the final approach to the long dirt nap (to use his own phrase). His mind is 98% gone, and all he has left is hours or possibly months of hideous unpleasantness in a hospital bed. I'll spare you the details, but it's as close to a living Hell as you can get...&rdquo;</p>\n</blockquote>\n<blockquote>\n<p>&ldquo;If you're a politician who has ever voted against doctor-assisted suicide, or you would vote against it in the future, I hate your fucking guts and I would like you to die a long, horrible death. I would be happy to kill you personally and watch you bleed out. I won't do that, because I fear the consequences. But I'd enjoy it, because you motherfuckers are responsible for torturing my father. Now it's personal.&rdquo;</p>\n</blockquote>\n<p>Based on <a href=\"http://slatestarcodex.com/2013/07/17/who-by-very-slow-decay/\">this blog post</a> I suspect that Yvain would agree with Adams about the magnitude and intensity of the evil of outlawing assisted suicide for the terminally ill.</p>\n<p>If you are unwilling to buy Adams&rsquo; book, I strongly recommend you at least <a href=\"http://www.dilbert.com/blog/\">read his blog</a>, which normally has a much softer and less angry tone than the passage I just cited.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QTkij5fmPXPd7GD4Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 73, "extendedScore": null, "score": 0.000215, "legacy": true, "legacyId": "25119", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 73, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ARmEm3aHv5DKSko2f", "3aN627urvdTTkEwhc", "5wMTZLZZmZEbXdoMD", "6NvbSwuSAooQxxf7f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-24T08:58:55.253Z", "modifiedAt": null, "url": null, "title": "Open thread for December 24-31, 2013 ", "slug": "open-thread-for-december-24-31-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:39.809Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Z43isMka9gvpZCnhF/open-thread-for-december-24-31-2013", "pageUrlRelative": "/posts/Z43isMka9gvpZCnhF/open-thread-for-december-24-31-2013", "linkUrl": "https://www.lesswrong.com/posts/Z43isMka9gvpZCnhF/open-thread-for-december-24-31-2013", "postedAtFormatted": "Tuesday, December 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%20for%20December%2024-31%2C%202013%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%20for%20December%2024-31%2C%202013%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ43isMka9gvpZCnhF%2Fopen-thread-for-december-24-31-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%20for%20December%2024-31%2C%202013%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ43isMka9gvpZCnhF%2Fopen-thread-for-december-24-31-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ43isMka9gvpZCnhF%2Fopen-thread-for-december-24-31-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Z43isMka9gvpZCnhF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.4805731592443673e-06, "legacy": true, "legacyId": "25121", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 211, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-24T19:53:38.907Z", "modifiedAt": null, "url": null, "title": "Thinking Fast and Slow for Kindle $3 at Amazon", "slug": "thinking-fast-and-slow-for-kindle-usd3-at-amazon", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:10.570Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "buybuydandavis", "createdAt": "2011-09-04T00:02:35.971Z", "isAdmin": false, "displayName": "buybuydandavis"}, "userId": "sQqaueY24d7xa3PXD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5ns5FtknGRMs9ytYz/thinking-fast-and-slow-for-kindle-usd3-at-amazon", "pageUrlRelative": "/posts/5ns5FtknGRMs9ytYz/thinking-fast-and-slow-for-kindle-usd3-at-amazon", "linkUrl": "https://www.lesswrong.com/posts/5ns5FtknGRMs9ytYz/thinking-fast-and-slow-for-kindle-usd3-at-amazon", "postedAtFormatted": "Tuesday, December 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thinking%20Fast%20and%20Slow%20for%20Kindle%20%243%20at%20Amazon&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThinking%20Fast%20and%20Slow%20for%20Kindle%20%243%20at%20Amazon%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ns5FtknGRMs9ytYz%2Fthinking-fast-and-slow-for-kindle-usd3-at-amazon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thinking%20Fast%20and%20Slow%20for%20Kindle%20%243%20at%20Amazon%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ns5FtknGRMs9ytYz%2Fthinking-fast-and-slow-for-kindle-usd3-at-amazon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ns5FtknGRMs9ytYz%2Fthinking-fast-and-slow-for-kindle-usd3-at-amazon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 8, "htmlBody": "<p>Santa delivers for LessWrong! Electronically, of course.</p>\n<p>Merry Christmas.</p>\n<p><a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman-ebook/dp/B00555X8OA/ref=la_B001ILFNQG_1_1?s=books&amp;ie=UTF8&amp;qid=1387914703&amp;sr=1-1\">http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman-ebook/dp/B00555X8OA/ref=la_B001ILFNQG_1_1?s=books&amp;ie=UTF8&amp;qid=1387914703&amp;sr=1-1</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5ns5FtknGRMs9ytYz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 1.4812620915970226e-06, "legacy": true, "legacyId": "25122", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-25T18:27:32.219Z", "modifiedAt": null, "url": null, "title": "Critiquing Gary Taubes, Part 1: Mainstream Nutrition Science on Obesity", "slug": "critiquing-gary-taubes-part-1-mainstream-nutrition-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:29.069Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JGBxoM4poNEzvmFBJ/critiquing-gary-taubes-part-1-mainstream-nutrition-science", "pageUrlRelative": "/posts/JGBxoM4poNEzvmFBJ/critiquing-gary-taubes-part-1-mainstream-nutrition-science", "linkUrl": "https://www.lesswrong.com/posts/JGBxoM4poNEzvmFBJ/critiquing-gary-taubes-part-1-mainstream-nutrition-science", "postedAtFormatted": "Wednesday, December 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Critiquing%20Gary%20Taubes%2C%20Part%201%3A%20Mainstream%20Nutrition%20Science%20on%20Obesity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACritiquing%20Gary%20Taubes%2C%20Part%201%3A%20Mainstream%20Nutrition%20Science%20on%20Obesity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJGBxoM4poNEzvmFBJ%2Fcritiquing-gary-taubes-part-1-mainstream-nutrition-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Critiquing%20Gary%20Taubes%2C%20Part%201%3A%20Mainstream%20Nutrition%20Science%20on%20Obesity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJGBxoM4poNEzvmFBJ%2Fcritiquing-gary-taubes-part-1-mainstream-nutrition-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJGBxoM4poNEzvmFBJ%2Fcritiquing-gary-taubes-part-1-mainstream-nutrition-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 748, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/iu0/trusting_expert_consensus/\">Trusting Expert Consensus</a></p>\n<p>Lately, I've been thinking a lot about whether we can find any clear exceptions to the general \"trust the experts (when they agree)\" heuristic. One example that keeps coming up&mdash;at least on LessWrong and related blogs&mdash;is Gary Taubes' claims about mainstream nutrition experts allegedly getting obesity horribly wrong.</p>\n<p>Taubes is probably best-known for his book <em><a href=\"http://www.amazon.com/Good-Calories-Bad-Controversial-Science/dp/1400033462\">Good Calories, Bad Calories</a>.</em>&nbsp;I'd previously had a mildly negative impression of him from discussion of him on Yvain's old blog, particularly some of other posts Yvain and other people linked from there, such as <a href=\"http://wholehealthsource.blogspot.com/2011/08/carbohydrate-hypothesis-of-obesity.html\">this discussion of Taubes' \"carbohydrate hypothesis\"</a> and especially this <a href=\"http://www.chrisstucchio.com/blog/2011/inanity_of_overeating.html\">discussion of Taubes' attempt to refute the standard calories-in/calories-out model of weight</a>.</p>\n<p>But I figured maybe the criticism of Taubes I'd read hadn't been fair to him, so I decided to read him for myself... and <em>holy crap, </em>Taubes turned out to be <em>far worse than I expected. </em>I decided to write a post explaining why, and then realized that, even if I were somewhat selective about the issues I focused on, I had enough material for a whole series of posts, which I'll be posting over the course of the next week.</p>\n<p>The problem with Taubes is not that everything he says is wrong. Much of it <em>is </em>ludicrously wrong, but that's only one half of the problem. The other half is that he says a fair number of things mainstream nutrition science would agree with, but then hides this fact, and instead pretends those things are a&nbsp;<em>refutation </em>of mainstream nutrition science. So it's worth starting with a brief in-a-nutshell version of what mainstream nutrition science <em>actually </em>says about obesity.</p>\n<p>(The following summary is drawn from a number of sources, including <a href=\"http://profiles.nlm.nih.gov/NN/B/C/Q/G/\">this</a>, <a href=\"http://www.nap.edu/openbook.php?record_id=1222&amp;page=R1\">this</a>, and <a href=\"https://www.google.com/search?q=joslin's+diabetes+mellitus+obesity&amp;btnG=Search+Books&amp;tbm=bks&amp;tbo=1\">this</a>. Everything I'm about to say will be discussed in much greater detail in subsequent posts.)</p>\n<p>Here it goes: people gain weight when they consume more calories than they burn. But both calorie intake and calorie expenditure are regulated by complicated mechanisms we don't fully understand yet. This means the causes of overweight and obesity* are also complicated and not fully understood. It is, however, worth watching out for foods with lots of added fat and sugar, if only because they're an easy way to consume way too many calories.</p>\n<p>We currently don't have any great solutions to the problem of overweight and obesity. If you consume fewer calories than you burn, you will lose weight, but sticking to a diet is hard. It's relatively easy to lose weight in the short run, and it's possible to do so on a wide variety of diets, but only a small percentage of people keep the weight off over the long run.</p>\n<p>As for low-carb diets, people <em>do </em>lose weight on them, but they do so because low-carb diets generally lead people to restrict their calorie intake even when they aren't actively counting calories. For one thing, it's hard to consume as many calories when you drastically restrict the range of foods you can eat. There's also&nbsp;<em>some </em>evidence that low-carb diets may have some advantages. in terms of, say, warding off hunger, but the evidence is mixed. There's certainly no basis for claiming low-carb diets as a magic bullet for the problems of overweight and obesity.</p>\n<p>The above points are not the only issues at stake in Taubes' writings on nutrition. Admittedly, he covers a huge amount of ground, from the relationship between sugar and diabetes to the relationship between fat intake and heart disease to the alleged dangers of extremely-low carbohydrate diets. However, I'll be focusing on his claims about the causes of and solutions to the problems of overweight and obesity, because that seems to be the main thing people talk about when they talk about Taubes supposedly showing how wrong mainstream experts can be.</p>\n<p>I'll also focus heavily on how Taubes misrepresents the views of mainstream experts on obesity. In the next post, though, I'll be temporarily setting that issue aside in order to look at what Taubes is proposing as an alternative. This will involve examining some claims made by Dr. Robert Atkins, whose ideas' Taubes champions.</p>\n<p><em>*Note: if the use of \"overweight\" as a noun sounds weird to you, it does to me too, but I discovered as I researched this article that it's standard&nbsp;usage in the literature on the subject. I came to realize there's a good reason for this&nbsp;usage: it's inaccurate to talk about the problem solely in terms of \"obesity,\" but constantly saying \"the problem of people being overweight and obese\" gets really wordy.</em></p>\n<p><strong>Next: </strong><a href=\"/lw/je4/critiquing_gary_taubes_part_2_atkins_redux/\">Atkins Redux</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"92SxJsDZ78ApAGq72": 1, "yt9Z7xdQrofW7fCN8": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JGBxoM4poNEzvmFBJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 24, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "25124", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["R8YpYTq8LoD3k948L", "eSfPGQjArWXWXu99Z"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-25T20:00:51.640Z", "modifiedAt": null, "url": null, "title": "How the Grinch Ought to Have Stolen Christmas", "slug": "how-the-grinch-ought-to-have-stolen-christmas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:29.507Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Quirinus_Quirrell", "createdAt": "2011-01-01T22:51:11.686Z", "isAdmin": false, "displayName": "Quirinus_Quirrell"}, "userId": "NYTkawCcDR3cMs6W5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ighcwDMvQcRGAajtt/how-the-grinch-ought-to-have-stolen-christmas", "pageUrlRelative": "/posts/ighcwDMvQcRGAajtt/how-the-grinch-ought-to-have-stolen-christmas", "linkUrl": "https://www.lesswrong.com/posts/ighcwDMvQcRGAajtt/how-the-grinch-ought-to-have-stolen-christmas", "postedAtFormatted": "Wednesday, December 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20the%20Grinch%20Ought%20to%20Have%20Stolen%20Christmas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20the%20Grinch%20Ought%20to%20Have%20Stolen%20Christmas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FighcwDMvQcRGAajtt%2Fhow-the-grinch-ought-to-have-stolen-christmas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20the%20Grinch%20Ought%20to%20Have%20Stolen%20Christmas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FighcwDMvQcRGAajtt%2Fhow-the-grinch-ought-to-have-stolen-christmas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FighcwDMvQcRGAajtt%2Fhow-the-grinch-ought-to-have-stolen-christmas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 570, "htmlBody": "<p>On Dec. 24, 1957, a Mr. T. Grinch attempted to disrupt Christmas by stealing associated gifts and decorations. His plan failed, the occupants of Dr. Suess' narrative remained festive, and Mr. Grinch himself succumbed to cardiac hypertrophy. To help others avoid repeating his mistakes, I've written a brief guide to properly disrupting holidays. Holiday-positive readers should read this with the orthogonality thesis in mind. Fighting Christmas is tricky, because the obvious strategy - making a big demoralizing catastrophe - doesn't work. No matter what happens, the media will put the word <em>Christmas</em> in front of it and convert your scheme into even more free advertising for the holiday. It'll be a <em>Christmas</em> tragedy, a <em>Christmas</em> earthquake, a <em>Christmas</em> wave of foreclosures. That's no good; attacking Christmas takes more finesse.</p>\n<p>The first thing to remember is that, whether you're stealing a holiday or a magical artifact of immense power, it's almost always a good idea to leave a decoy in its place. When people notice that something important is missing, they'll go looking to find or replace it. This rule can be generalized from physical objects to abstractions like <em>sense of community</em>. T. Grinch tried to prevent community gatherings by vandalizing the spaces where they would've taken place. A better strategy would've been to promise to organize a Christmas party, then skip the actual organizing and leave people to sit at home by themselves. Unfortunately, this solution is not scalable, but someone came up with a very clever solution: encourage people to watch Christmas-themed films instead of talking to each other, achieving almost as much erosion of community without the backlash.</p>\n<p>I'd like to particularly applaud Raymond Arnold, for inventing a vaguely-Christmas-like holiday in December, with no gifts, and death (rather than cheer) as its central theme [1]. I really wish it didn't involve so much singing and community, though. I recommend raising the musical standards; people who can't sing at studio-recording quality should not be allowed to sing at all.</p>\n<p>Gift-giving traditions are particularly important to stamp out, but stealing gifts is ineffective because they're usually cheap and replaceable. A better approach would've been to promote giving undesirable gifts, such as religious sculptures and fruitcake. Even better would be to convince the Mayor of Whoville to enact bad economic policies, and grind the Whos into a poverty that would make gift-giving difficult to sustain. Had Mr. Grinch pursued this strategy effectively, he could've stolen Christmas <em>and</em> Birthdays <em>and</em> gotten himself a Nobel Prize in Economics [2].</p>\n<p>Finally, it's important to avoid rhyming. This is one of those things that should be completely obvious in hindsight, with a little bit of genre savvy; villains like us win much more often in prose and in life than we do in verse.</p>\n<p>And with that, I'll leave you with a few closing thoughts. If you gave presents, your friends are disappointed with them. Any friends who didn't give you presents, it's because they don't care, and any fiends who did give you presents, they're cheap and lame presents for the same reason. If you have a Christmas tree, it's ugly, and if it's snowing, the universe is trying to freeze you to death.</p>\n<p>Merry Christmas!</p>\n<p>&nbsp;</p>\n<p>[1] I was initially concerned that the Solstice would pattern-match and mutate into a less materialistic version of Christmas, but running a Kickstarter campaign seems to have addressed that problem.</p>\n<p>[2] This is approximately the reason why Alfred Nobel specifically opposed the existence of that prize.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hNFdS3rRiYgqqD8aM": 10, "hXTqT62YDTTiqJfxG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ighcwDMvQcRGAajtt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 63, "baseScore": 72, "extendedScore": null, "score": 0.000184, "legacy": true, "legacyId": "25126", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 76, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-25T20:48:58.648Z", "modifiedAt": null, "url": null, "title": "One Sided Policy Debate - The Science of Literature", "slug": "one-sided-policy-debate-the-science-of-literature", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:29.561Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kremlin", "createdAt": "2011-11-20T13:42:17.401Z", "isAdmin": false, "displayName": "kremlin"}, "userId": "hs8N5xYvR4GGkDg8f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RjGHgszcrMwZcZ9n9/one-sided-policy-debate-the-science-of-literature", "pageUrlRelative": "/posts/RjGHgszcrMwZcZ9n9/one-sided-policy-debate-the-science-of-literature", "linkUrl": "https://www.lesswrong.com/posts/RjGHgszcrMwZcZ9n9/one-sided-policy-debate-the-science-of-literature", "postedAtFormatted": "Wednesday, December 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20One%20Sided%20Policy%20Debate%20-%20The%20Science%20of%20Literature&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOne%20Sided%20Policy%20Debate%20-%20The%20Science%20of%20Literature%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRjGHgszcrMwZcZ9n9%2Fone-sided-policy-debate-the-science-of-literature%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=One%20Sided%20Policy%20Debate%20-%20The%20Science%20of%20Literature%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRjGHgszcrMwZcZ9n9%2Fone-sided-policy-debate-the-science-of-literature", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRjGHgszcrMwZcZ9n9%2Fone-sided-policy-debate-the-science-of-literature", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 214, "htmlBody": "<p>On HackerNews, <a href=\"http://www.nytimes.com/2013/12/25/technology/as-new-services-track-habits-the-e-books-are-reading-you.html?pagewanted=1&amp;_r=2&amp;hp\">this article</a> was linked. The general idea is that companies are studying what people like to read, to help authors produce books that people like to read.</p>\n<p>Now, for me, when I look at this idea, I see some down sides, but I certainly see some benefits as well.</p>\n<p>Almost none of the commenters on NYTimes seemed to see any benefit whatsoever to studying reader behaviour. There were a few who saw the downsides as more <em>mild</em> than the other commenters. But most of the commenters basically saw this technology as some sort of 1984-esque idea that will turn all books into uninteresting, unimaginative pieces of paper that would better serve as a door stopper than as something for literary consumption. Out of 50 comments that I've read, only one person has said something along the lines of, 'This technology can possibly offer something to help authors improve their books'.</p>\n<p>Is this just technophobia? Or am I missing something, and this really is a horrible, evil technology that should be avoided at all costs? [That's a rhetorical question -- I'd be surprised if even one LWian held that position]</p>\n<p>I guess what I'm asking is, what are the psychological roots for the almost-unanimous aversion to this attempt at gathering and using information about what people want?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RjGHgszcrMwZcZ9n9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 4, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "25127", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-26T14:38:39.685Z", "modifiedAt": null, "url": null, "title": "CFAR has a matched-donations fundraiser going on [LINK]", "slug": "cfar-has-a-matched-donations-fundraiser-going-on-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:19.351Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benquo", "createdAt": "2009-03-06T00:17:35.184Z", "isAdmin": false, "displayName": "Benquo"}, "userId": "nt2XsHkdksqZ3snNr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xmiECrgwXmHmkfYEt/cfar-has-a-matched-donations-fundraiser-going-on-link", "pageUrlRelative": "/posts/xmiECrgwXmHmkfYEt/cfar-has-a-matched-donations-fundraiser-going-on-link", "linkUrl": "https://www.lesswrong.com/posts/xmiECrgwXmHmkfYEt/cfar-has-a-matched-donations-fundraiser-going-on-link", "postedAtFormatted": "Thursday, December 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CFAR%20has%20a%20matched-donations%20fundraiser%20going%20on%20%5BLINK%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACFAR%20has%20a%20matched-donations%20fundraiser%20going%20on%20%5BLINK%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxmiECrgwXmHmkfYEt%2Fcfar-has-a-matched-donations-fundraiser-going-on-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CFAR%20has%20a%20matched-donations%20fundraiser%20going%20on%20%5BLINK%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxmiECrgwXmHmkfYEt%2Fcfar-has-a-matched-donations-fundraiser-going-on-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxmiECrgwXmHmkfYEt%2Fcfar-has-a-matched-donations-fundraiser-going-on-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 139, "htmlBody": "<p><a href=\"http://rationality.org/fundraiser2013/\">http://rationality.org/fundraiser2013/</a></p>\n<p>The folks at CFAR are working on a post talking about all the good work they've done and plan to do, but in the meantime I'm putting this up as a placeholder in case anyone finishing up their 2013 donations literally doesn't know about it. Someone please let me know if the official announcement's already up and I've missed it.</p>\n<p>UPDATE: Lukeprog pointed out that there's a stub in Main too. Also I put up my own bleg on behalf of CFAR here: <a rel=\"nofollow nofollow\" href=\"http://benjaminrosshoffman.com/cfar-second-thoughts-and-a-bleg/\" target=\"_blank\">http://benjaminrosshoffman.com/cfar-second-thoughts-and-a-bleg/</a>&nbsp;It was an interesting exercise in trying to write glowing praise while experiencing extreme annoyance.&nbsp;I hope&nbsp;I succeeded.</p>\n<p>I also hope&nbsp;the official version will be up very soon!<br /><br />UPDATE2: CFAR now has a post up explaining what they are working on and why you should give. You should probably trust the official version more than mine.</p>\n<p><a href=\"/lw/jej/why_cfar/\">http://lesswrong.com/lw/jej/why_cfar/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xmiECrgwXmHmkfYEt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 14, "extendedScore": null, "score": 1.483966579458828e-06, "legacy": true, "legacyId": "25131", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JjGs6mDZxeCWkg3ii"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-27T03:43:04.752Z", "modifiedAt": null, "url": null, "title": "Donating to MIRI vs. FHI vs. CEA vs. CFAR", "slug": "donating-to-miri-vs-fhi-vs-cea-vs-cfar", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:33.202Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/na5sjAsgtNJzDEwSv/donating-to-miri-vs-fhi-vs-cea-vs-cfar", "pageUrlRelative": "/posts/na5sjAsgtNJzDEwSv/donating-to-miri-vs-fhi-vs-cea-vs-cfar", "linkUrl": "https://www.lesswrong.com/posts/na5sjAsgtNJzDEwSv/donating-to-miri-vs-fhi-vs-cea-vs-cfar", "postedAtFormatted": "Friday, December 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Donating%20to%20MIRI%20vs.%20FHI%20vs.%20CEA%20vs.%20CFAR&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADonating%20to%20MIRI%20vs.%20FHI%20vs.%20CEA%20vs.%20CFAR%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fna5sjAsgtNJzDEwSv%2Fdonating-to-miri-vs-fhi-vs-cea-vs-cfar%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Donating%20to%20MIRI%20vs.%20FHI%20vs.%20CEA%20vs.%20CFAR%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fna5sjAsgtNJzDEwSv%2Fdonating-to-miri-vs-fhi-vs-cea-vs-cfar", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fna5sjAsgtNJzDEwSv%2Fdonating-to-miri-vs-fhi-vs-cea-vs-cfar", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 277, "htmlBody": "<p>In a discussion a couple months ago, Luke <a href=\"/lw/ixt/lone_genius_bias_and_returns_on_additional/9zm1\">said</a>, \"I think it's hard to tell whether donations do more good at MIRI, FHI, CEA, or CFAR.\" So I want to have a thread to discuss that.</p>\n<p>My own very rudimentary thoughts: I think the research MIRI does is probably valuable, but I don't think it's likely to lead to MIRI itself building FAI. I'm convinced AGI is much more likely to be built by a government or major corporation, which makes me more inclined to think movement-building activities are likely to be valuable, to increase the odds of the people at that government or corporation being conscious of AI safety issues, which MIRI isn't doing.</p>\n<p>It seems like FHI is the obvious organization to donate to for that purpose, but Luke seems to think CEA (the Centre for Effective Altruism) and CFAR could also be good for that, and I'm not entirely clear on why. I sometimes get the impression that some of CFAR's work ends up being covert movement-building for AI-risk issues, but I'm not sure to what extent that's true. I know very little about CEA, and a brief check of their website leaves me a little unclear on why Luke recommends them, aside from the fact that they apparently work closely with FHI.</p>\n<p>This has some immediate real-world relevance to me: I'm currently in the middle of a coding bootcamp and not making any money, but today my mom offered to make a donation to a charity of my choice for Christmas. So any input on what to tell her would be greatly appreciated, as would more information on CFAR and CEA, which I'm sorely lacking in.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"K6oowPZC6kds6LDTg": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "na5sjAsgtNJzDEwSv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 31, "extendedScore": null, "score": 0.000101, "legacy": true, "legacyId": "25137", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-27T03:48:56.031Z", "modifiedAt": null, "url": null, "title": "A proposed inefficiency in the Bitcoin markets", "slug": "a-proposed-inefficiency-in-the-bitcoin-markets", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:09.457Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Liron", "createdAt": "2009-02-27T04:43:11.294Z", "isAdmin": false, "displayName": "Liron"}, "userId": "AyzRrs8hNm54QptLi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WajrBMnoh4WpuvGTH/a-proposed-inefficiency-in-the-bitcoin-markets", "pageUrlRelative": "/posts/WajrBMnoh4WpuvGTH/a-proposed-inefficiency-in-the-bitcoin-markets", "linkUrl": "https://www.lesswrong.com/posts/WajrBMnoh4WpuvGTH/a-proposed-inefficiency-in-the-bitcoin-markets", "postedAtFormatted": "Friday, December 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20proposed%20inefficiency%20in%20the%20Bitcoin%20markets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20proposed%20inefficiency%20in%20the%20Bitcoin%20markets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWajrBMnoh4WpuvGTH%2Fa-proposed-inefficiency-in-the-bitcoin-markets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20proposed%20inefficiency%20in%20the%20Bitcoin%20markets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWajrBMnoh4WpuvGTH%2Fa-proposed-inefficiency-in-the-bitcoin-markets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWajrBMnoh4WpuvGTH%2Fa-proposed-inefficiency-in-the-bitcoin-markets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1544, "htmlBody": "<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Simplicio, do you think the Bitcoin markets are efficient?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: If you'd asked me two years ago, I would have said yes. I know hindsight is 20/20, but even at the time, I think the fact that relatively few people were trading it would have risen to prominence in my analysis.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: And what about today?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Today, it seems like there's no shortage of trading volume. The hedge funds of the world have heard of Bitcoin, and had their quants do their fancy analyses on it, and they actively trade it.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Well, I'm certainly not a quant, but I think I've spotted a systematic market inefficiency. Would you like to hear it?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Nah, I'm good.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Did you hear what I said? I think I've spotted an exploitable pattern of price movements in a $10 Billion market. If I'm right, it could make us a lot of money.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Sure, but you won't convince me that whatever pattern you're thinking of is a \"reliable\" one.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Come on, you don't even know what my argument is.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: But I know how your argument is going to be structured. First you're going to identify some property of Bitcoin prices in past data. Then you'll explain some causal model you have which supposedly accounts for why prices have had that property in the past. Then you'll say that your model will continue to account for that same property in future Bitcoin prices.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Yeah, so? What's wrong with that?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: The problem is that you are not a trained quant, and therefore, your brain is not capable of bringing a worthwhile property of Bitcoin prices to your attention.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Dude, I just want to let you know because this happens often and no one else is ever going to say anything: you're being a dick.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Look, quants are good at their job. To a first approximation, quants are like perfect Bayesian reasoners who maintain a probability distribution over the \"reliability\" of <em>every single property</em> of Bitcoin prices that you and I are capable of formulating. So this argument you're going to make to me, a quant has already made to another quant, and the other quant has incorporated it into his hedge fund's trading algorithms.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Fine, but so what if quants have already figured out my argument for themselves? We can make money on it too.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: No, we can't. I told you I'm pretty confident that the market is <a href=\"http://en.wikipedia.org/wiki/Efficient-market_hypothesis\">efficient</a>, i.e.&nbsp;<a href=\"/lw/yv/markets_are_antiinductive/\">anti-inductive</a>, meaning the quants of the world haven't left behind any reliable patterns that an armchair investor like you can detect and profit from.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Would you just shut up and let me say my argument?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Whatever, knock yourself out.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Ok, here goes. Everyone knows Bitcoin prices are volatile, right?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Yeah, highly volatile. But at any given moment, you don't know if the volatility is going to move the price up or down next. From your state of knowledge,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Random_walk_hypothesis\">it looks like a random walk</a>. If today's Bitcoin price is $1000, then tomorrow's price is as likely to be $900 as it is to be $1100.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: I agree that the Random Walk Hypothesis provides a good model of prices in efficient markets, and that the size of a each step in a random walk provides a good model of price volatility in efficient markets.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: See, I told you you wouldn't convince me.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Ah, but my empirical observation of Bitcoin prices is inconsistent with the Random Walk hypothesis. So the only thing I'm led to conclude is that the Bitcoin market is not efficient.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: What do you mean \"inconsistent\"?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: I mean Bitcoin's past prices don't look much like a random walk. They look more like a random walk&nbsp;<em>on a log scale</em>. If today's price is $1000, then tomorrow's price is equally likely to be $900 or $1111. So if I buy $1000 of Bitcoin today, I expect to have 0.5($900) + 0.5($1111) = $1005.50 tomorrow.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: How do you know that? Did you write a script to loop through Bitcoin's daily closing price on Mt. Gox and simulate the behavior of a Bayesian reasoner with a variable-step-size random-walk prior and a second Bayesian reasoner with a variable-step-size log-random-walk prior, and thus calculate a much higher&nbsp;<a href=\"http://yudkowsky.net/rational/technical/\">Bayesian Score</a>&nbsp;for the log-random-walk model?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Yeah, I did.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: That's very <a href=\"http://yudkowsky.net/rational/virtues/\">virtuous</a> of you.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">[This is a fictional dialogue. The truth is, I was too lazy to do that. Can someone please do that? I would much appreciate it. --Liron.]</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: So, have I convinced you that the market is anti-inductive now?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Well, you've empirically demonstrated that the log Random Walk Hypothesis was a good model for predicting Bitcoin prices in the past. But that's just a historical pattern. My original point was that you're not qualified to evaluate which historical patterns are *reliable* patterns. The Bitcoin markets are full of pattern-annihilating forces, and you're not qualified to evaluate which past-data-fitting models are eligible for future-data-fitting.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Ok, I'm not saying you have to believe that the future accuracy of log-Random-Walk will probably be higher than the future accuracy of linear Random Walk. I'm just saying you should perform a Bayesian update in the direction of that conclusion.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Ok, but the only reason the update has nonzero strength is because I assigned an a-priori chance of 10% to the set of possible worlds wherein Bitcoin markets were inefficient, and that set of possible worlds gives a higher probability that a model like your log-Random-Walk model would fit the price data well. So I update my beliefs to promote the hypothesis that Bitcoin is inefficient, and in particular that it is inefficient in a log-Random-Walk way.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Thanks. And hey, guess what: I think I've traced the source of the log-Random-Walk regularity.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: I'm surprised you waited this long to mention that.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: I figured that if I mentioned it earlier, you'd snap back about how efficient markets sever the causal connection between would-be price-regularity-causing dynamics, and actual prices.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Fair enough.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Anyway, the reason Bitcoin prices follow a log-Random-Walk is because they reflect the long-term Expected Value of Bitcoin's actual utility.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Bitcoin has no real utility.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: It does. It's liquid in novel, qualitatively different ways. It's kind of anonymous. It's a more stable unit of account than the official currencies of some countries.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Come on, how much utility is all that really worth in expectation?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: I don't know. The Bitcoin economy could be anywhere from hundreds of millions of dollars, to trillions of dollars. Our belief about the long-term future value of a single BTC is spread out across a range whose 90% confidence interval is something like [$10, $100,000] for 1BTC.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Are you saying it's spread out over the interval [$10, $100,000] in a uniform distribution?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Nope, it's closer to a bell curve centered at $1000 on a log scale. It gives equal probability of ~10% both to the $10-100 range and to the $10,000-100,000 range.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: How do you know that everyone's beliefs are shaped like that?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Because everyone has a causal model in their head with a node for \"order of magnitude of Bitcoin's value\", and that node varies in the characteristically linear fashion of a Bayes net.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: I don't feel confident in that explanation.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Then take whatever explanation you give yourself to explain the effectiveness of <a href=\"/lw/h5e/fermi_estimates/\">Fermi estimates</a>. Those output a bell curve on a log scale too, and seems like estimating Bitcoin's future value should have a lot of methodology in common with doing back-of-the-envelope calculations about the blast radius of a nuclear bomb.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Alright.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: So the causality of Bitcoin prices roughly looks like this:</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">\n<blockquote style=\"margin: 0px 0px 0px 40px; border: none; padding: 0px;\">[Beliefs about order of magnitude of Bitcoin's future value] --&gt; [Beliefs about Bitcoin's future price] --&gt; [Trading decisions]</blockquote>\n</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: Okay, I see how the first node can fluctuate a lot in reaction to daily news events, and that would have a disproportionately high effect on the last node. But how can an efficient market avoid that kind of log-scale fluctuation? Efficient markets always reflect a consensus estimate of an asset's price, and it's rational to arrive at an estimate that fluctuates on a log scale!</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Actually, I think a truly efficient market shouldn't just skip around across orders of magnitudes, just because expectations of future prices do. I think truly efficient markets show some degree of \"drag\", which should be invisible in typical cases like publicly-traded stocks, but become noticeable in cases of order-of-magnitude value-uncertainty like Bitcoin.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Simplicio: So you think you're the only one smart enough to notice that it's worth trading Bitcoin so as to create drag on Bitcoin's log-scale random walk?</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Salviati: Yeah, I think maybe I am.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\"><span style=\"color: #000000; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 19px; text-align: justify; background-color: #f7f7f8;\"><strong>Salviati is claiming that his empirical observations show a lack of drag on Bitcoin price shifts, which would be actionable evidence of inefficiency. Discuss.</strong></span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WajrBMnoh4WpuvGTH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 1, "extendedScore": null, "score": 1.4848015846605294e-06, "legacy": true, "legacyId": "25135", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 140, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["h24JGbmweNpWZfBkM", "PsEppdvgRisz5xAHG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-27T14:23:10.317Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jqS9JQwpv6ZfpFWXG/weekly-lw-meetups-3", "pageUrlRelative": "/posts/jqS9JQwpv6ZfpFWXG/weekly-lw-meetups-3", "linkUrl": "https://www.lesswrong.com/posts/jqS9JQwpv6ZfpFWXG/weekly-lw-meetups-3", "postedAtFormatted": "Friday, December 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqS9JQwpv6ZfpFWXG%2Fweekly-lw-meetups-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqS9JQwpv6ZfpFWXG%2Fweekly-lw-meetups-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqS9JQwpv6ZfpFWXG%2Fweekly-lw-meetups-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 540, "htmlBody": "<p><strong>This summary was posted to LW main on December 27th. The following week's summary is <a href=\"/lw/jea/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/u6\">[Leipzig] Secular Solstice Celebration! (And the Inauguration of the LW Leipzig Community):&nbsp;<span class=\"date\">21 December 2013 05:05PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/up\">Israel Meetup (Tel Aviv) with special guest Tomer Kagan:&nbsp;<span class=\"date\">26 December 2013 08:00PM</span></a></li>\n<li><a href=\"/meetups/uu\">Montreal - How to Actually Change your Mind:&nbsp;<span class=\"date\">07 January 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/uq\">Moscow, How Can Visual Image Be Rational:&nbsp;<span class=\"date\">22 December 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/ux\">Moscow: Art Therapy for Emotions:&nbsp;<span class=\"date\">24 December 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/uy\">Urbana-Champaign: Rituals:&nbsp;<span class=\"date\">21 December 2013 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/tf\">Berlin:&nbsp;<span class=\"date\">01 January 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/us\">Boston Winter Solstice:&nbsp;<span class=\"date\">21 December 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/uo\">Brussels monthly meetup: [topic TBD]:&nbsp;<span class=\"date\">11 January 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/uv\">Vienna:&nbsp;<span class=\"date\">18 January 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jqS9JQwpv6ZfpFWXG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.4854723248775975e-06, "legacy": true, "legacyId": "25107", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TisRMAYLy2ZLPjhF7", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-27T15:28:34.052Z", "modifiedAt": null, "url": null, "title": "Ritual Report: Boston Solstice Celebration", "slug": "ritual-report-boston-solstice-celebration", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:28.702Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r8zyX8zfQ2i4DAy66/ritual-report-boston-solstice-celebration", "pageUrlRelative": "/posts/r8zyX8zfQ2i4DAy66/ritual-report-boston-solstice-celebration", "linkUrl": "https://www.lesswrong.com/posts/r8zyX8zfQ2i4DAy66/ritual-report-boston-solstice-celebration", "postedAtFormatted": "Friday, December 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ritual%20Report%3A%20Boston%20Solstice%20Celebration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARitual%20Report%3A%20Boston%20Solstice%20Celebration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr8zyX8zfQ2i4DAy66%2Fritual-report-boston-solstice-celebration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ritual%20Report%3A%20Boston%20Solstice%20Celebration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr8zyX8zfQ2i4DAy66%2Fritual-report-boston-solstice-celebration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr8zyX8zfQ2i4DAy66%2Fritual-report-boston-solstice-celebration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 653, "htmlBody": "<p>A week after the large-scale Solstice celebration in NYC, we held a smaller one in Boston at Citadel house, with around 20 people attending. The content was essentially a subset (given below) of the 2012 NYC solstice <a href=\"https://dl.dropboxusercontent.com/u/2000477/SolsticeEve_2012.pdf\">set list</a> - a mix of silly and serious songs and readings, and following a thematic progression from light to darkness and back to light. We had several people leading songs - <a href=\"/user/juliawise/overview/\">Julia</a>, <a href=\"/user/beth/overview/\">Beth</a>, <a href=\"/user/janos/overview/\">Janos</a> and me, with <a href=\"/user/jkaufman/overview/\">Jeff</a> accompanying on the piano, and everyone else singing along using the slides. Here is a <a href=\"http://www.youtube.com/watch?v=BkkV8-opcbE\">video</a> of our rendition of Mindspace is Deep and Wide, courtesy of Julia.</p>\n<p>A number of things went well about our celebration. The number of people was just right for keeping it cozy and personal, and fitting into the Citadel living room. It was great to have a variety of lead singers, and the others sang along readily. The piano accompaniment was especially awesome, and created a more solemn atmosphere. A highlight of the evening was <a href=\"/user/jimrandomh/overview/\">Jim</a>'s inspirational and touching speech during the Moment of Darkness. The light-dark-light progression worked quite well with this subset of the original set list. The ceremony lasted for 90 minutes, which was a good length for immersing the audience without dragging on for too long. There was a decent amount of free-form discussion before and after the event, with some nice potluck food.</p>\n<p>The parts that didn't go as well mostly had to do with last-minute preparation on our part. The event was planned during the preceding week, so we could have used a few more rehearsals of the songs and even more instrumental accompaniment (this time about half were acapella). We also went through the songs a little quickly, and many of them could have used more introduction or explanation. Next year, we will probably involve even more people in leading the songs, and it would be great to vary the set list and add more songs that have personal significance to the people involved. (This year, we added <a href=\"http://www.youtube.com/watch?v=YufsbE4-jmY\">Jewel in the Night</a>, composed on the International Space Station, which worked really well with the event theme.)</p>\n<p>Another improvement would be to avoid disruptions caused by latecomers. Both in NYC last year and in Boston this year, there were people who rang the doorbell and came in during intense solemn portions of the event. As Jeff suggested, it would be a good idea to request ahead of time for people to arrive before time X, and to have a sign on the door saying \"if it's after time X please come back after time Y\".</p>\n<p>I am really grateful to everyone who made this event happen, and to <a href=\"/user/raemon/overview/\">Ray</a> for putting together the original materials. Singing rationalist-themed songs with friends felt awesome, and it seems like something we should probably do more than once a year!</p>\n<p>&nbsp;</p>\n<p><strong>Here is the set list we used:</strong></p>\n<p><em>Light</em><br class=\"kix-line-break\" />Introduction: The Story of Winter <br />Why Does the Sun Shine (Part 0)<br />First Litany of Tarski: If the sky is blue...<br />Mindspace is Deep and Wide<br /><a href=\"http://www.youtube.com/watch?v=4-vDhYTlCNw\">God Wrote the Sky</a><br /><a href=\"http://www.youtube.com/watch?v=uLpu2UP3rGI\">Why Does the Sun Shine</a> (Part 1)</p>\n<p><em>Twilight</em><br />One Wish<br /><a href=\"http://www.youtube.com/watch?v=Y6ljFaKRTrI\">Still Alive</a> <br />Ballad of Barry the Em(ulation) <br />Second Litany of Tarski: If I'm going to be outcompeted by simulated brains...<br />The X Days of X-Risk<br />Third Litany of Tarski: If humanity will be wiped out by unfriendly AI...<br /><a href=\"http://glenraphael.bandcamp.com/track/when-i-die\">When I Die</a></p>\n<p><em>Into Darkness</em><br />Beyond the Reach of God <br /><a href=\"http://www.youtube.com/watch?v=LBDV2LTNWLE\">Take my Love, Take my Land</a> (Mal's song) <br /><a href=\"http://www.youtube.com/watch?v=5xaxP_kErTU\">No One is Alone</a> <br />Serenity<br />The Gift We Give Tomorrow</p>\n<p><em>Darkness</em><br />Moment of Darkness</p>\n<p><em>Dawn</em><br /><a href=\"https://www.youtube.com/watch?v=VDCN6I1n6o4\">Brighter Than Today</a><br /><a href=\"http://www.youtube.com/watch?v=YufsbE4-jmY\">Jewel in the Night</a> <br /><a href=\"http://www.youtube.com/watch?v=QPoTGyWT0Cg\">Lean on Me</a></p>\n<p><em>Light</em><br />Move the World<br /><a href=\"http://www.youtube.com/watch?v=sLkGSV9WDMA\">The Sun is a Miasma of Incandescent Plasma</a><br /><a href=\"http://www.youtube.com/watch?v=kViMQhSF-_Y\">Gonna be a Cyborg</a> <br /><a href=\"https://dl.dropboxusercontent.com/u/2000477/Uplift.mp3\">Uplift</a> <br />Final Litany of Tarski: If human values will survive for five thousand years...<br /><a href=\"https://soundcloud.com/raymond-arnold/five-thousand-years-with\">Five Thousand Years</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r8zyX8zfQ2i4DAy66", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "25139", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-27T22:25:58.920Z", "modifiedAt": null, "url": null, "title": "Meetup : Southeast Michigan", "slug": "meetup-southeast-michigan-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:31.690Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hFHDW4s8ZCBvZdc4P/meetup-southeast-michigan-0", "pageUrlRelative": "/posts/hFHDW4s8ZCBvZdc4P/meetup-southeast-michigan-0", "linkUrl": "https://www.lesswrong.com/posts/hFHDW4s8ZCBvZdc4P/meetup-southeast-michigan-0", "postedAtFormatted": "Friday, December 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Southeast%20Michigan&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Southeast%20Michigan%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhFHDW4s8ZCBvZdc4P%2Fmeetup-southeast-michigan-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Southeast%20Michigan%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhFHDW4s8ZCBvZdc4P%2Fmeetup-southeast-michigan-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhFHDW4s8ZCBvZdc4P%2Fmeetup-southeast-michigan-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<h2></h2>\n<div class=\"meetup-meta\"><strong>WHEN:</strong> <span class=\"date\">04 January 2014 02:00:00PM (-0500)</span>\n<p><strong>WHERE:</strong> <span class=\"address\">19334 Angling Street, Livonia, MI</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Just about six months after the last meetup (which was lots of fun!), please take time out to attend another Detroit/Ann Arbor area Less Wrong meetup. With special visiting guests - Alicorn and Mike Blume!</p>\n<p>Meetup will be held at the same house it was in last time. Itinerary is still \"snacks and informal discussion\". RSVPs still appreciated but not required.</p>\n<p>If there are people who can't make it then but can make that Sunday, I will consider changing the date.</p>\n</div>\n</div>\n<!-- .content -->", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hFHDW4s8ZCBvZdc4P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "25140", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-28T04:31:40.829Z", "modifiedAt": null, "url": null, "title": "Online vs. Personal Conversations", "slug": "online-vs-personal-conversations", "viewCount": null, "lastCommentedAt": "2020-12-03T18:14:04.085Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3mufjmbN7YshguRrG/online-vs-personal-conversations", "pageUrlRelative": "/posts/3mufjmbN7YshguRrG/online-vs-personal-conversations", "linkUrl": "https://www.lesswrong.com/posts/3mufjmbN7YshguRrG/online-vs-personal-conversations", "postedAtFormatted": "Saturday, December 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Online%20vs.%20Personal%20Conversations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOnline%20vs.%20Personal%20Conversations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3mufjmbN7YshguRrG%2Fonline-vs-personal-conversations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Online%20vs.%20Personal%20Conversations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3mufjmbN7YshguRrG%2Fonline-vs-personal-conversations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3mufjmbN7YshguRrG%2Fonline-vs-personal-conversations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>When I was younger, I thought that conversations in real life were much more likely to promote true beliefs and meaningful changes than conversations online, because people in real life were only willing/able to cite evidence they were actually confident in, while those online were able to easily search for arguments favoring their position.</p>\n<p>While this is obviously wrong&mdash;the concept that people in real life only cite evidence they are justifiably confident in is comically false&mdash;I do think the dichotomy illustrated there is interesting. One thing I've noticed is that in general the \"rigor\" of discussions online is higher (in terms of citations, links to external content, etc.), but that conversations in real life seem still much more likely to actually change people's minds.</p>\n<p>I have noticed this effect in both myself and others&mdash;what do you think is going on here, and how do you think we might circumvent it? If online discussions could be made more effective at causing people to actually change their minds, this could potentially prove extremely useful.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZXFpyQWPB5ideFbEG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3mufjmbN7YshguRrG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 1.4863705060443064e-06, "legacy": true, "legacyId": "25144", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-28T07:13:54.211Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC: Birthday Party/PD tournament", "slug": "meetup-washington-dc-birthday-party-pd-tournament", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:27.795Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xTNjziyD28wZRkMcd/meetup-washington-dc-birthday-party-pd-tournament", "pageUrlRelative": "/posts/xTNjziyD28wZRkMcd/meetup-washington-dc-birthday-party-pd-tournament", "linkUrl": "https://www.lesswrong.com/posts/xTNjziyD28wZRkMcd/meetup-washington-dc-birthday-party-pd-tournament", "postedAtFormatted": "Saturday, December 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%3A%20Birthday%20Party%2FPD%20tournament&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%3A%20Birthday%20Party%2FPD%20tournament%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxTNjziyD28wZRkMcd%2Fmeetup-washington-dc-birthday-party-pd-tournament%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%3A%20Birthday%20Party%2FPD%20tournament%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxTNjziyD28wZRkMcd%2Fmeetup-washington-dc-birthday-party-pd-tournament", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxTNjziyD28wZRkMcd%2Fmeetup-washington-dc-birthday-party-pd-tournament", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/v3'>Washington DC: Birthday Party/PD tournament</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 December 2013 03:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's Robby's birthday! We'll be meeting to hang out and generally have fun.</p>\n\n<p>There will be delicious baked goods! These delicious baked goods will be at least partially used as prizes in a prisoner's dilemma tournament.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/v3'>Washington DC: Birthday Party/PD tournament</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xTNjziyD28wZRkMcd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.4865423351975624e-06, "legacy": true, "legacyId": "25146", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Birthday_Party_PD_tournament\">Discussion article for the meetup : <a href=\"/meetups/v3\">Washington DC: Birthday Party/PD tournament</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 December 2013 03:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's Robby's birthday! We'll be meeting to hang out and generally have fun.</p>\n\n<p>There will be delicious baked goods! These delicious baked goods will be at least partially used as prizes in a prisoner's dilemma tournament.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Birthday_Party_PD_tournament1\">Discussion article for the meetup : <a href=\"/meetups/v3\">Washington DC: Birthday Party/PD tournament</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC: Birthday Party/PD tournament", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Birthday_Party_PD_tournament", "level": 1}, {"title": "Discussion article for the meetup : Washington DC: Birthday Party/PD tournament", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Birthday_Party_PD_tournament1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-28T15:26:40.958Z", "modifiedAt": null, "url": null, "title": "Doubt, Science, and Magical Creatures - a Child's Perspective", "slug": "doubt-science-and-magical-creatures-a-child-s-perspective", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:33.839Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benquo", "createdAt": "2009-03-06T00:17:35.184Z", "isAdmin": false, "displayName": "Benquo"}, "userId": "nt2XsHkdksqZ3snNr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p6qkjLzkqLFS6QYfh/doubt-science-and-magical-creatures-a-child-s-perspective", "pageUrlRelative": "/posts/p6qkjLzkqLFS6QYfh/doubt-science-and-magical-creatures-a-child-s-perspective", "linkUrl": "https://www.lesswrong.com/posts/p6qkjLzkqLFS6QYfh/doubt-science-and-magical-creatures-a-child-s-perspective", "postedAtFormatted": "Saturday, December 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Doubt%2C%20Science%2C%20and%20Magical%20Creatures%20-%20a%20Child's%20Perspective&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoubt%2C%20Science%2C%20and%20Magical%20Creatures%20-%20a%20Child's%20Perspective%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp6qkjLzkqLFS6QYfh%2Fdoubt-science-and-magical-creatures-a-child-s-perspective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Doubt%2C%20Science%2C%20and%20Magical%20Creatures%20-%20a%20Child's%20Perspective%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp6qkjLzkqLFS6QYfh%2Fdoubt-science-and-magical-creatures-a-child-s-perspective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp6qkjLzkqLFS6QYfh%2Fdoubt-science-and-magical-creatures-a-child-s-perspective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 648, "htmlBody": "<p><strong></strong><span style=\"text-decoration: underline;\"><strong>Doubt</strong></span></p>\n<p>I grew up in a Jewish household, so I didn't have Santa Claus to doubt - but I did have the tooth fairy.</p>\n<p>It was hard for me to believe that a magical being I had never seen somehow knew whenever any child lost their tooth, snuck into their house unobserved without setting off the alarms, for unknown reasons took the tooth, and for even less fathomable reasons left a dollar and a note in my mom's handwriting.</p>\n<p>On the other hand, the alternative hypothesis was no less disturbing: my parents were lying to me.</p>\n<p>Of course I had to know which of these terrible things was true. So one night, when my parents were out (though I was still young enough to have a babysitter), I noticed that my tooth was coming out and decided that this would be...</p>\n<p><span style=\"text-decoration: underline;\"><strong>A&nbsp;Perfect Opportunity for an Experiment.</strong></span></p>\n<p>I reasoned that if my parents didn't know about the tooth, they wouldn't be able to fake a tooth fairy appearance. I would find a dollar and note under my pillow <a href=\"http://en.wikipedia.org/wiki/If_and_only_if\">if, but only if</a>, the tooth fairy were real.</p>\n<p>I solemnly told the babysitter, \"I lost my tooth, but don't tell Mom and Dad. It's important - it's science!\" Then at the end of the night I went to my bedroom, put the tooth under the pillow, and went to sleep. The next morning, I woke up and looked under my pillow. The tooth was gone, and in place there was a dollar and a note from the \"tooth fairy.\"</p>\n<p>This could have been the end of the story. I could have decided that I'd performed an experiment that would come out one way if the tooth fairy were real, and a different way if the tooth fairy were not. But I was more skeptical than that. I thought, \"What's more likely? That a magical creature took my tooth? Or that the babysitter told my parents?\"</p>\n<p>I was furious at the possibility of such an egregious violation of experimental protocol, and never trusted that babysitter in the lab again.</p>\n<p><span style=\"text-decoration: underline;\"><strong>An Improvement in Experimental Design</strong></span></p>\n<p>The next time, I was more careful. I understood that the flaw in the previous experiment had been failure to adequately conceal the information from my parents. So the next time I lost a tooth, I told no one. As soon as I felt it coming loose in my mouth, I ducked into the bathroom, ran it under the tap to clean it, wrapped it in a tissue, stuck it in my pocket, and went about my day as if nothing had happened. That night, when no one was around to see, I put the tooth under my pillow before I went to sleep.</p>\n<p>In the morning, I looked under the pillow. No note. No dollar. Just that tooth. I grabbed the incriminating evidence and burst into my parents bedroom, demanding to know:</p>\n<p>\"If, as you say, there is a tooth fairy, then how do you explain THIS?!\"</p>\n<p><strong><span style=\"text-decoration: underline;\">What can&nbsp;we learn from this?</span></strong></p>\n<p>The basic idea of the experiment was ideal. It was testing a binary hypothesis, and was expected to perfectly distinguish between the two possibilities. However, if I had known then what I know now about rationality, I could have done better.</p>\n<p>As soon as my first experiment produced an unexpected positive result, just by learning that fact,&nbsp;I knew why it had happened, and what I needed to fix in the experiment to produce strong evidence. Prior to the first experiment would have been a perfect opportunity to apply the \"Internal Simulator,\" as CFAR calls it - imagining in advance getting each of the two possible results, and what I think afterwards - do I think the experiment worked? Do I wish I'd done something differently? - in order to give myself the opportunity to correct those errors in advance instead of performing a costly experiment (I had a limited number of baby teeth!) to find them.</p>\n<p>&nbsp;</p>\n<p><a href=\"http://benjaminrosshoffman.com/doubt-science-and-magical-creatures/\"> Cross-posted</a> at my <a href=\"http://benjaminrosshoffman.com\">personal blog</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"32DdRimdM7sB5wmKu": 2, "Ng8Gice9KNkncxqcj": 2, "EdnFte9kTvWnRskrN": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p6qkjLzkqLFS6QYfh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 57, "extendedScore": null, "score": 0.000168, "legacy": true, "legacyId": "25148", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 57, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-28T16:17:23.254Z", "modifiedAt": null, "url": null, "title": "Meetup : London 2014 Protospective", "slug": "meetup-london-2014-protospective", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:27.878Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sixes_and_sevens", "createdAt": "2009-11-11T14:42:23.502Z", "isAdmin": false, "displayName": "sixes_and_sevens"}, "userId": "n83meJ5yG2WQzygvw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BCgujCrt8DuFeaQNX/meetup-london-2014-protospective", "pageUrlRelative": "/posts/BCgujCrt8DuFeaQNX/meetup-london-2014-protospective", "linkUrl": "https://www.lesswrong.com/posts/BCgujCrt8DuFeaQNX/meetup-london-2014-protospective", "postedAtFormatted": "Saturday, December 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%202014%20Protospective&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%202014%20Protospective%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCgujCrt8DuFeaQNX%2Fmeetup-london-2014-protospective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%202014%20Protospective%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCgujCrt8DuFeaQNX%2Fmeetup-london-2014-protospective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCgujCrt8DuFeaQNX%2Fmeetup-london-2014-protospective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/v4'>London 2014 Protospective</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 January 2014 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">The Shakespeare's Head, Holborn, London WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The first London meetup of 2014, and we'll be having the unstructured (read \"probably imaginary\") discussion topic of the year ahead. We can speculate wildly about significant events in 2014, talk about short- to mid-term goals, or propose novel resolution commitment strategies.</p>\n\n<p>Alternatively, we can sit around and pleasantly chew over the usual Less Wrong related subjects, which works pretty well for us. If you're in or around London and haven't come to a meetup before, why not start in 2014? We're a friendly bunch, and this meetup session won't be all that involved or taxing.</p>\n\n<p>The venue is The Shakespeare's Head, which is very easy to get to. Exit Holborn tube station and turn left. It's less than a hundred metres on your left. We'll have a Less Wrong / paperclip sign somewhere on the table. Any problems finding it, or locating the group in the pub, ring 07887 718458 and we'll send someone to find you..</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/v4'>London 2014 Protospective</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BCgujCrt8DuFeaQNX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 1.4871182581794044e-06, "legacy": true, "legacyId": "25149", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_2014_Protospective\">Discussion article for the meetup : <a href=\"/meetups/v4\">London 2014 Protospective</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 January 2014 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">The Shakespeare's Head, Holborn, London WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The first London meetup of 2014, and we'll be having the unstructured (read \"probably imaginary\") discussion topic of the year ahead. We can speculate wildly about significant events in 2014, talk about short- to mid-term goals, or propose novel resolution commitment strategies.</p>\n\n<p>Alternatively, we can sit around and pleasantly chew over the usual Less Wrong related subjects, which works pretty well for us. If you're in or around London and haven't come to a meetup before, why not start in 2014? We're a friendly bunch, and this meetup session won't be all that involved or taxing.</p>\n\n<p>The venue is The Shakespeare's Head, which is very easy to get to. Exit Holborn tube station and turn left. It's less than a hundred metres on your left. We'll have a Less Wrong / paperclip sign somewhere on the table. Any problems finding it, or locating the group in the pub, ring 07887 718458 and we'll send someone to find you..</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_2014_Protospective1\">Discussion article for the meetup : <a href=\"/meetups/v4\">London 2014 Protospective</a></h2>", "sections": [{"title": "Discussion article for the meetup : London 2014 Protospective", "anchor": "Discussion_article_for_the_meetup___London_2014_Protospective", "level": 1}, {"title": "Discussion article for the meetup : London 2014 Protospective", "anchor": "Discussion_article_for_the_meetup___London_2014_Protospective1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-28T23:25:10.296Z", "modifiedAt": null, "url": null, "title": "Why CFAR?", "slug": "why-cfar", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:30.345Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JjGs6mDZxeCWkg3ii/why-cfar", "pageUrlRelative": "/posts/JjGs6mDZxeCWkg3ii/why-cfar", "linkUrl": "https://www.lesswrong.com/posts/JjGs6mDZxeCWkg3ii/why-cfar", "postedAtFormatted": "Saturday, December 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20CFAR%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20CFAR%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJjGs6mDZxeCWkg3ii%2Fwhy-cfar%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20CFAR%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJjGs6mDZxeCWkg3ii%2Fwhy-cfar", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJjGs6mDZxeCWkg3ii%2Fwhy-cfar", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4427, "htmlBody": "<p>Summary: &nbsp;We outline the case for CFAR, including:</p>\n<ul>\n<li><a href=\"/lw/jej/why_cfar/#ourgoal\">Our long-term goal</a>;</li>\n<li><a href=\"/lw/jej/why_cfar/#ourplan\">Our plan, and our progress to date</a>;</li>\n<li><a href=\"/lw/jej/why_cfar/#ourfinancials\">Our financials</a>; and</li>\n<li><a href=\"/lw/jej/why_cfar/#help\">How you can help</a>.</li>\n</ul>\n<p>CFAR is in the middle of our <a href=\"http://rationality.org/fundraiser2013/\">annual matching fundraiser</a> right now. &nbsp;If you've been thinking of donating to CFAR, now is the best time to decide for probably at least half a year. &nbsp;Donations up to $150,000 will be matched until January 31st; and Matt Wage, who is matching the last $50,000 of donations, has vowed <em>not</em> to donate unless matched.<a name=\"footnote1back\"></a><a href=\"/lw/jej/why_cfar/#footnote1\">[1]</a></p>\n<p><a href=\"/lw/jej/why_cfar/#footnote1\"></a>Our workshops are cash-flow positive, and subsidize our basic operations (you are not subsidizing workshop attendees). &nbsp;But we can't yet run workshops often enough to fully cover our core operations. &nbsp;We also need to do more formal experiments, and we want to create free and low-cost curriculum with far broader reach than the current workshops. &nbsp;Donations are needed to keep the lights on at CFAR, fund free programs like the <a href=\"http://sparc2013.org/\">Summer Program on Applied Rationality and Cognition</a>, and let us do new and interesting things in 2014 (see below, at length).<a style=\"background-color: #ffffff;\" name=\"footnote2back\"></a><a href=\"/lw/jej/why_cfar/#footnote2\">[2]</a></p>\n<p><a id=\"more\"></a></p>\n<h1><a name=\"ourgoal\"></a>Our long-term goal</h1>\n<p><span style=\"font-size: small;\">CFAR's long-term goal is to create people who can and will solve important problems -- whatever the important problems turn out to be.</span><a style=\"background-color: #ffffff; color: #000000; font-size: small;\" name=\"footnote3back\"></a><a style=\"font-size: small;\" href=\"/lw/jej/why_cfar/#footnote3\">[3]</a><span style=\"font-size: small;\">&nbsp;&nbsp;</span></p>\n<p>We therefore aim to create a community with three key properties:</p>\n<ol>\n<li><strong>Competence</strong> -- The ability to get things done in the real world. &nbsp;For example, the ability to work hard, follow through on plans, push past your fears, navigate social situations, organize teams of people, start and run successful businesses, <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">etc</a>.</li>\n<li><strong>Epistemic rationality</strong> -- The ability to form relatively accurate beliefs. &nbsp;Especially the ability to form such beliefs in cases where data is limited, motivated cognition is tempting, or the conventional wisdom is incorrect.&nbsp;</li>\n<li><strong>Do-gooding</strong> -- A desire to make the world better for all its people; the tendency to jump in and start/assist projects that might help (whether by labor or by donation); and ambition in keeping an eye out for projects that might help a lot and not just a little. &nbsp;</li>\n</ol>\n<div>Why <strong>competence</strong>, <strong>epistemic rationality</strong>, and <strong>do-gooding</strong>?</div>\n<div>\n<div><br /></div>\n<div>To change the world, we'll need to be able to take effective action (competence). &nbsp;We'll need to be able to form a good implicit and explicit understanding of the human world and how to shift it. We'll need to have the best shot we can get at modeling situations yet unseen. &nbsp;We'll need to solve problems outside the realms where competent business people already find traction (all of which require competence plus epistemic rationality). And we'll need to blend these abilities with a burning ambition to leave the world far better than we found it (competence plus epistemic rationality plus do-gooding).</div>\n<div><br /></div>\n<div>And we'll need a community, not just a set of individuals. &nbsp;It is hard for an isolated individual to figure out what the most important problems are, let alone how to effectively solve them. &nbsp;This is still harder for individuals who have interesting day jobs, and who are busy amassing real-world competence of varied sorts. &nbsp;Communities can assemble a complex world-model piece by piece. &nbsp;Communities can build and sustain motivation, as well, and facilitate the practice and transfer of useful skills. &nbsp;The aim is thus to create a community that, collectively, can figure out what needs doing and can then do it -- even when this requires multiple simultaneous competencies (e.g., locating a particular existential risk, <em>and</em> having good scientific connections, <em>and</em> knowing good folks in policy, <em>and</em> knowing how to do good technical research).</div>\n<div><br /></div>\n<div>We intend to build that sort of community.</div>\n<div><br /></div>\n<h1><a name=\"ourplan\"></a>Our plan, and our progress to date</h1>\n<p><span style=\"font-size: small;\">How can we create a community with high levels of competence, epistemic rationality, and do-gooding? &nbsp;By creating curricula that teach (or enhance) these properties; by seeding the community with diverse competencies and diverse perspectives on how to do good; and by linking people together into the right kind of community.</span></p>\n<div>\n<div><br /></div>\n<div>We've now had two years to execute on this vision.<a name=\"footnote4back\"></a><a href=\"/lw/jej/why_cfar/#footnote4\">[4]</a> &nbsp;It's not a <em>lot</em> of time, but it's enough to get started; and it's enough that folks should already be able to update as to our ability to execute.</div>\n<div><br /></div>\n<div>Here's our current working plan, the progress we've made so far, and the pieces we still need to hit.</div>\n<div><br /></div>\n<h2>Curriculum design</h2>\n<div>In October 2012, we had no money and little visible means of obtaining more.<a name=\"footnote5back\"></a><a href=\"/lw/jej/why_cfar/#footnote5\">[5]</a> We needed runway; and we needed a way to use that runway to rapidly iterate curriculum. &nbsp;</div>\n<div><br /></div>\n<div>We therefore focused our initial efforts into making a workshop that could pay its own bills, and at the same time give us data -- a workshop that would give us the opportunity to run (and learn from) many further workshops. &nbsp;Our <a href=\"http://rationality.org/workshops/\">applied rationality workshops</a> have filled this role.</div>\n<div><br /></div>\n<div>\n<p><strong>Progress to date</strong></p>\n<div style=\"padding-left: 30px;\"><em><strong>Reported benefits</strong></em></div>\n<div style=\"padding-left: 30px;\">After about a dozen workshops (and over 100 classes that we&rsquo;ve designed and tested), we&rsquo;ve settled on a workshop model that runs smoothly, and seems to provide value to our participants, who report a mean of 9.3 out of 10 to the question &ldquo;Are you glad you came?&rdquo;. In the process we&rsquo;ve substantially improved our skill at curriculum design: it used to take us about 40 hours to design a unit we regarded as decent (design; test on volunteers; re-design; test on volunteers; etc). It now takes us about 8 hours to design a unit of the same quality.<a name=\"footnote6back\"></a><a href=\"/lw/jej/why_cfar/#footnote6\">[6]</a></div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div style=\"padding-left: 30px;\">Anecdotally, we have many, many stories from alumni about how our workshop increased their competence (both generally and for altruistic ends). For example, alum Ben Toner, CEO of Draftable, recounts that after the July 2012 workshop, &ldquo;At work, I realized I wasn&rsquo;t doing anywhere near enough planning. My employees were spending time on the wrong things because I hadn&rsquo;t planned things out in enough detail to make it clear what was the most important thing to do next. I fixed this immediately after the camp.&rdquo; Alum Ben Kuhn has described how the CFAR workshop helped his effective altruism group &ldquo;vastly increase our campus presence--everything from making uncomfortable cold calls to powering through bureaucracy, and from running complex events to quickly updating on feedback.&rdquo; (Check out our <a href=\"http://rationality.org/testimonials/\">testimonials</a> page for more examples.) &nbsp;</div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div style=\"padding-left: 30px;\"><em><strong>Measurement</strong></em></div>\n</div>\n<div style=\"padding-left: 30px;\">Anecdata notwithstanding, the jury is still out regarding the workshops' usefulness to those who come. &nbsp;During the very first minicamps (the current workshops are agreed to be better) we randomized admission of 15 applicants, with 17 controls. &nbsp;Our study was low-powered and effects on e.g. income would have needed to be very large for us to expect to detect them. &nbsp;Still, we ended up with non-negligible <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">evidence of absence</a>: income, happiness, and exercise did not visibly trend upward one year later. &nbsp;We detected statistically significant positive impacts on the standard (<a href=\"http://www.westmont.edu/_academics/departments/psychology/documents/Rammstedt_and_John.pdf\">BFI-10</a>) survey pair for emotional stability \"I see myself as someone who is relaxed, handles stress well\" / \"I get nervous easily\" (p=.002). &nbsp;Also significant were effects on an abridged <a href=\"http://userpage.fu-berlin.de/health/selfscal.htm\">General Self-Efficacy Scale</a> (sample item:\"I can solve most problems if I invest the necessary effort\") (p=.007). &nbsp;The details will be available soon on our blog (including a much larger number of negative results). &nbsp;We'll run another RCT soon, funding permitting. &nbsp;</div>\n<div>\n<div><br /></div>\n<div style=\"padding-left: 30px;\">Like many participants, we at CFAR have the subjective impression that the workshops boost strategicness; and, like most who have observed two workshops, we have the impression that today's workshops are much better than those in the initial RCT. &nbsp;We'll need to find ways to actually test those impressions, and to create stronger feedbacks from measurement into curriculum development.&nbsp;</div>\n</div>\n<div><br /></div>\n<div>\n<div style=\"padding-left: 30px;\"><em><strong>Epistemic rationality curricula</strong></em></div>\n<div style=\"padding-left: 30px;\">After a rocky start, our epistemic rationality curriculum has seen a number of recent victories. &nbsp;Our &ldquo;Building Bayesian Habits&rdquo; class began performing much better after we figured out how to help people notice their intuitive, &ldquo;System 1&Prime; expectations of probabilities.<a name=\"footnote7back\"></a><a href=\"/lw/jej/why_cfar/#footnote7\">[7]</a> &nbsp;Our \"inner simulator\" class conveys the distinction between profession and anticipation while aiming at immediate, practical benefits; it isn't about religion and politics, it's about whether your mother will actually enjoy the potted plant you&rsquo;re thinking of giving her. &nbsp;More generally, the epistemic rationality curriculum <a href=\"/lw/il3/three_ways_cfar_has_changed_my_view_of_rationality/\">appears to be</a> integrating deeply with the competence curriculum, and appears to be becoming more appealing to participants as it does so. &nbsp;Strengthening this curriculum, and building in real tests of its efficacy, will be a major focus in 2014.</div>\n<div><br /></div>\n<div style=\"padding-left: 30px;\"><em><strong>Integrating with academic research</strong></em></div>\n<div style=\"padding-left: 30px;\">We made preliminary efforts in this direction - for example by taking standard questionnaires from the academic literature, including <a href=\"http://www.keithstanovich.com/Site/Research_on_Reasoning.html\">Stanovich's</a> indicators of the traits he calls &ldquo;rationality&rdquo;, and administering them to attendees at a Less Wrong meetup. &nbsp;(We found that meetup attendees scored near the ceiling, so we'll probably need new questionnaires with better discrimination.) &nbsp;Our research fellow, Dan Keys (whose masters thesis was on heuristics and biases), spends a majority of his time keeping up with the literature and integrating it with CFAR workshops, as well as designing tests for our ongoing <a href=\"http://rationality.org/2013/09/27/surprise-as-a-cue-to-probability-experiment-1/\">forays</a> into randomized controlled trials. &nbsp;We're particularly excited by Tetlock's <a href=\"http://www.goodjudgmentproject.com\">Good Judgment Project</a>, and we'll be piggybacking on it a bit to see if we can get decent ratings. &nbsp;</div>\n</div>\n<div><br /></div>\n<div>\n<div style=\"padding-left: 30px;\"><em><strong>Accessibility</strong></em></div>\n<div style=\"padding-left: 30px;\">Initial workshops worked only for those who had already read the LW Sequences. Today, workshop participants who are smart and analytical, but with no prior exposure to rationality -- such as a local politician, a police officer, a Spanish teacher, and others -- are by and large quite happy with the workshop and feel it is valuable.</div>\n<div><br /></div>\n<div style=\"padding-left: 30px;\">Nevertheless, the total set of people who can travel to a 4.5-day immersive workshop, and who can spend $3900 to do so, is limited. &nbsp;We want to eventually give a substantial skill-boost in a less expensive, more accessible format; we are slowly bootstrapping toward this. &nbsp;</div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div style=\"padding-left: 30px;\">Specifically:</div>\n<div>\n<ul style=\"padding-left: 30px;\">\n<li><strong>Shorter workshops:</strong> &nbsp;We&rsquo;re working on shorter versions of our workshops (including <a href=\"http://rationality.org/2013/09/20/fight-flight-freeze-or-attend-class-september-28/\">three-hour</a> and <a href=\"http://rationality.org/2013/09/10/were-running-a-specialty-workshop-for-programmers/\">one-day</a> courses) that can be given to larger sets of people at lower cost.&nbsp;</li>\n<li><strong>College courses:</strong> &nbsp;We helped develop a <a href=\"http://sensesensibilityscience.com\">course on rational thinking</a> -- for UC Berkeley undergraduates, in partnership with Nobel Laureate Saul Perlmutter. &nbsp;We also brought several high school and university instructors to our workshop, to help seed early experimentation into their curricula.</li>\n<li><strong>Increasing visibility: </strong>We&rsquo;ve been working on increasing our visibility among the general public, with alumni James Miller and Tim Czech both working on non-fiction books that feature CFAR, and several mainstream media articles about CFAR on their way, including one forthcoming shortly in the Wall Street Journal. \n<ul>\n</ul>\n</li>\n</ul>\n</div>\n<p><strong>Next steps</strong></p>\n<div style=\"padding-left: 30px;\">In 2014, we&rsquo;ll be devoting more resources to epistemic curriculum development; to research measuring the effects of our curriculum on both competence and epistemic rationality; and to more widely accessible curricula. &nbsp;</div>\n<div><br /></div>\n<div>\n<h2>Forging community</h2>\n<div>The most powerful interventions are not one-off experiences; rather, they are the start of an ongoing practice. &nbsp;Changing one's social environment is <a href=\"/lw/52g/the_good_news_of_situationist_psychology/ \">one of the highest impact ways to create personal change</a>. &nbsp;Alum Paul Crowley writes that &ldquo;The most valuable lasting thing I got out of attending, I think, is a renewed determination to continually up my game. A big part of that is that the minicamp creates a lasting community of fellow alumni who are also trying for the biggest bite of increased utility they can get, and that&rsquo;s no accident.&rdquo;&nbsp;</div>\n<div><br /></div>\n<div>The goal is to create a community that is directly helpful for its members, and that simultaneously improves its members' impact on the world.</div>\n<div><br /></div>\n<p><strong>Progress to date</strong></p>\n<div style=\"padding-left: 30px;\"><em><strong>A strong set of seed alumni</strong></em></div>\n<div style=\"padding-left: 30px;\">We have roughly 350 alumni so far, which include scientists from MIT and Berkeley, college students, engineers from Google and Facebook, founders of Y-combinator startups, teachers, professional writers, and the exceptionally gifted high-school students who participated in <a href=\"http://rationality.org/sparc2012/\">SPARC</a> 2013 and 2012. (Not counted in that tally are the 50-some attendees of the <a href=\"http://www.effectivealtruismsummit.com/ \">2013 Effective Altruism Summit</a>, for whom we ran a free, abridged version of our workshop.)&nbsp;</div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div style=\"padding-left: 30px;\"><em><strong>Alumni contact/community</strong></em></div>\n<div style=\"padding-left: 30px;\">There is an active alumni Google group, which gets daily traffic. Alumni use it to share useful life hacks they&rsquo;ve discovered, help each other trouble-shoot, and notify each other of upcoming events and opportunities. We&rsquo;ve also been using our post-workshop parties as reunions for alumni nearby (in the San Francisco Bay area, the New York City area, and -- in two months -- Melbourne, Australia).</div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div style=\"padding-left: 30px;\">In large part thanks to our alumni forum and the post-workshop party networking, there have already been numerous cases of alumni helping each other find jobs and collaborating on startups or other projects. &nbsp;There have also been several alumni recruited to do-gooding projects (e.g., MIRI and Leverage Research have engaged multiple alumni), and of alumni improving their &ldquo;earn to give&rdquo; ability or shifting their own do-gooding strategy.</div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div style=\"padding-left: 30px;\">Many alumni also take CFAR skills back to Less Wrong meet-ups or other local communities (for example, the effective-altruism meetup in Melbourne, a homeless youth shelter in Oregon, and a self-improvement group in NYC; many have also practiced in their start-ups and with co-workers (for example, Beeminder, MetaMed, and Aquahug)).</div>\n</div>\n<div><br /></div>\n<div>\n<div style=\"padding-left: 30px;\"><em><strong>Do-gooding diversity</strong></em></div>\n<div style=\"padding-left: 30px;\">We&rsquo;d like the alumni community to have an accurate picture of how to effectively improve the world. &nbsp;We don&rsquo;t want to try to figure out how to improve the world all from scratch. &nbsp;There are already a number of groups who&rsquo;ve done a lot of good thinking on the subject; including some who call themselves \"effective altruists\", but also people who call themselves \"social entrepreneurs\", \"x-risk minimizers\", and \"philanthropic foundations\". &nbsp;</div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div style=\"padding-left: 30px;\">We aim to bring in the best thinkers and doers from all of these groups to seed the community with diverse good ideas on the subject. &nbsp;<strong>The goal is to create a culture rich enough that the alumni, as a community, can overcome any errors in CFAR&rsquo;s founders&rsquo; perspectives.</strong> &nbsp;The goal is also to create a community that is defined by its pursuit of true beliefs, and that is not defined by any particular preconceptions as to what those beliefs are.</div>\n<div><br /></div>\n<div style=\"padding-left: 30px;\">We use applicants&rsquo; inclination to do good as a major criterion of financial aid. Recipients of our informally-dubbed &ldquo;altruism scholarships&rdquo; have included members of the Future of Humanity Institute, CEA, Giving What We Can, MIRI, and Leverage Research. &nbsp;They also include many college or graduate students who have no official EA affiliation, but who are passionate about their desire to devote their career to world-saving (and who hope the workshops can help them figure out how to do so). &nbsp;And they include folks who are working full-time on varied do-gooding projects of broader origin, such as social entrepreneurs, someone working on community policing, and folks working at a major philanthropic foundation.</div>\n<div><br /></div>\n<div style=\"padding-left: 30px;\"><em><strong>International outreach</strong></em></div>\n<div style=\"padding-left: 30px;\">We'll be running our first international workshop in Australia, in February 2014, thanks to alumni Matt and Andrew Fallshaw. &nbsp;</div>\n<div><br /></div>\n<div style=\"padding-left: 30px;\">Also, starting in 2014, we'll be bringing about 20 Estonian math and science award-winners per year to CFAR workshops, thanks to a 5-year pledge from Jaan Tallinn to sponsor workshop spots for leading students from his home country. &nbsp;Estonia is an EU member country with a population of 1.2 million and a high-technology economy, and going forward this might be the first opportunity to check whether there are network effects in relatively larger fractions of a stratum.</div>\n<div><br /></div>\n<p><strong>Next steps</strong></p>\n<div style=\"padding-left: 30px;\">Over 2014, a major focus will be improving opportunities for ongoing alumni involvement. &nbsp;If funding allows, we&rsquo;ll also try our hand at pilot activities for meet-ups.</div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div style=\"padding-left: 30px;\">Specific plans include:</div>\n<div style=\"padding-left: 30px;\">\n<ul>\n<li>A two-day \"Epistemic Rationality and EA\" mini-workshop in January, targeted at alumni</li>\n<li>An alumni reunion this summer (which will be a multi-day event drawing folks our entire worldwide alumni community, unlike the alumni parties at each workshop);</li>\n<li>An alumni directory, as an attempt to increase business and philanthropic partnerships among alumni.</li>\n</ul>\n</div>\n</div>\n<div>\n<h1><a name=\"ourfinancials\"></a>Financials</h1>\n<h2>Expenses</h2>\n<div>Our fixed expenses come to about <strong>$40k per month</strong>. In some detail:</div>\n<div>\n<ul>\n<li>About $7k for our office space</li>\n<li>About $3k for miscellaneous expenses</li>\n<li>About $30k for salary &amp; wages, going forward \n<ul>\n<li>We have five full-time people on salary, each getting $3.5k per month gross. The employer portion of taxes adds roughly an additional $1k/month per employee.</li>\n<li>The remaining $7k or so goes to hourly employees and contractors. &nbsp;We have two roughly full-time hourly employees, and a few contractors who do website adjustment and maintenance, workbook compilation for a workshop, and similarly targeted tasks.</li>\n</ul>\n</li>\n</ul>\n</div>\n<div><br /></div>\n<div>In addition to our fixed expenses, we chose to run <a href=\"http://rationality.org/sparc/\">SPARC 2013</a>, even though it would cause us to run out of money right around the end-of-year fundraising drive. We did so because we judged SPARC to be potentially very important<a name=\"footnote8back\"></a><a href=\"/lw/jej/why_cfar/#footnote8\">[8]</a>, enough to justify the risk of leaning on this winter fundraiser to continue. All told, SPARC cost approximately $50k in direct costs (not counting staff time).</div>\n<div><br /></div>\n<div>(We also chose to e.g. teach at the EA Summit, do rationality research, put some effort into curricula that can be delivered cheaply to a larger crowd, etc. &nbsp;These did not incur much direct expense, but did require staff time which could otherwise have been directed towards revenue-producing projects.)</div>\n<div><br /></div>\n<h2>Revenue</h2>\n<div>Workshops are our primary source of non-donation income. &nbsp;We ran 7 of them in 2013, and they became increasingly cash-positive through the year. &nbsp;We now expect a full 4-day workshop held in the Bay Area to give us a profit of about $25k (ignoring fixed costs, such as staff time and office rent), which is just under 3 weeks of CFAR runway. &nbsp;Demand isn't yet reliable enough to let us run them at that frequency. We've made significant traction on building interest outside of the Less Wrong community, but there's still work to be done here, and that work will take time. &nbsp;In the meantime, workshops can subsidize some of our non-workshop activities, but not all of them. &nbsp;(Your donations do not go to subsidize workshops!)</div>\n<div><br /></div>\n<div>We're also actively exploring revenue models other than the four-day workshop. Several of them look promising, but need time to come to fruition before the income they offer us is relevant.</div>\n<div><br /></div>\n<h2>Donations</h2>\n<div>CFAR received $166k in our previous fundraising drive at the start of 2013, and a smaller amount of donations spread across the rest of the year. &nbsp;SPARC was partially sponsored with $15k from Dropbox and $5k from Quixey. &nbsp;These donations subsidized SPARC, the rationality workshop at the EA summit, research and development, and core expenses and salary.</div>\n<div><br /></div>\n<h2>Savings and debt</h2>\n<div>Right now CFAR has essentially no savings. The savings we accumulated by the end of 2012 went to (a) feeding the gap between income and expenses and (b) funding SPARC.</div>\n<div><br /></div>\n<div>A $30k loan, which helped us cover core 2013 expenses, comes due in March 2014.</div>\n<div><br /></div>\n<h2>Summary</h2>\n<div>If this winter fundraiser goes well, it will give us time to make some of our current experimental products mature. We think we have an excellent shot at making major strides forward in CFAR's mission as well as becoming much more self-sustaining during 2014.</div>\n<div><br /></div>\n<div>If this winter fundraiser goes poorly, CFAR will not yet have sufficient funding to continue core operations.</div>\n</div>\n<div><br /></div>\n<div>\n<h1><a name=\"help\"></a>How you can help</h1>\n<p><span style=\"font-size: small;\">Our main goals in 2014: &nbsp;</span></p>\n<div><ol>\n<li>Building a <strong>scalable revenue base</strong>, including via ramping up our workshop quality, workshop variety, and our marketing reach.</li>\n<li><strong>Community-building</strong>, including an alumni reunion.&nbsp;</li>\n<li>Creating more connections with the <strong>effective altruism community</strong>, and other opportunities for our alumni to get involved in do-gooding.</li>\n<li><strong>Research </strong>to feed back into our curriculum -- on the effectiveness of particular rationality techniques, as well as the long-term impact of rationality training on meaningful life outcomes.</li>\n<li>Developing more classes on <strong>epistemic rationality</strong>.</li>\n</ol></div>\n<div><br /></div>\n<div><strong>The three most important ways you can help:</strong></div>\n<div><br /></div>\n<div style=\"padding-left: 30px;\"><strong>1. &nbsp;Donations</strong></div>\n<div style=\"padding-left: 30px;\">If you&rsquo;re considering donating but want to learn more about how CFAR uses money, or you have other questions or hesitations, let us know -- we&rsquo;d be more than happy to chat with you via Skype. You can sign up for a one-on-one call with Anna <a href=\"http://rationality.org/talk/anna/\">here</a>.</div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div style=\"padding-left: 30px;\"><strong>2. &nbsp;Talent</strong></div>\n<div style=\"padding-left: 30px;\">We&rsquo;re actively seeking a new <a href=\"http://rationality.org/director-of-operations/\">director of operations</a> to organize our workshops; good operations can be a great multiplier on CFAR&rsquo;s total ability to get things done. &nbsp;We are continuing to try out exceptional candidates for a <a href=\"http://rationality.org/hiring/\">curriculum designer</a>.<a name=\"footnote9back\"></a><a href=\"/lw/jej/why_cfar/#footnote9\">[9]</a>&nbsp; And we always need more <a href=\"http://rationality.org/testsessionsignup\">volunteers</a> to help out with alpha-testing new classes in Berkeley, and to participate in online experiments.</div>\n<div style=\"padding-left: 30px;\"><br /></div>\n<div style=\"padding-left: 30px;\"><strong>3. &nbsp;Participants</strong></div>\n<div style=\"padding-left: 30px;\">We're continually searching for additional awesome people for our workshops. This really is a high-impact way people can help us; and we do have a large amount of data suggesting that (you /your friends) will be glad to have come. &nbsp;You can apply <a href=\"http://rationality.org/apply/\">here</a> -- it takes 1 minute, and leads to a conversation with Anna or Kenzi, which (you'll / they&rsquo;ll) probably find interesting whether or not they choose to come.</div>\n<div style=\"padding-left: 60px;\"><br /></div>\n<div>Like the open-source movement, applied rationality will be the product of thousands of individuals&rsquo; contributions. The ideas we've come up with so far are only a beginning. If you have other suggestions for people we should meet, other workshops we should attend, ways to branch out from our current business model, or anything else -- get in touch, we&rsquo;d love to Skype with you.&nbsp;</div>\n<div><br /></div>\n<div>You can also be a part of open-source applied rationality by creating good content for Less Wrong. Some of our best workshop participants, volunteers, hires, ideas for rationality techniques, use cases, and general inspiration have come from Less Wrong. &nbsp;Help keep the LW community vibrant and growing.</div>\n<div><br /></div>\n<div>And, if you&rsquo;re willing -- do consider <a href=\"http://rationality.org/donate/\">donating now</a>.</div>\n<div><br /></div>\n<div><span id=\"docs-internal-guid-3a78b892-39c2-77a5-c9ec-989794c9ec54\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n</span></div>\n<h2>Footnotes</h2>\n<div><a href=\"/lw/jej/why_cfar/#footnote1back\">[1]</a><a name=\"footnote1\"></a>&nbsp; That is: by giving up a dollar, you can, given some simplifications, cause CFAR to gain two dollars. &nbsp;Much thanks to Matt Wage, Peter McCluskey, Benjamin Hoffman, Janos Kramar &amp; Victoria Krakovna, Liron Shapira, Satvik Beri, Kevin Harrington, Jonathan Weissman, and Ted Suzman for together putting up $150k in matching funds. &nbsp;(Matt Wage, as mentioned, promises not only that he will donate if the pledge is matched, but also that he <em>won't </em>donate the $50k of matching funds to CFAR if the pledge <em>isn't </em>filled -- so your donation probably really does cause matching at the margin.)</div>\n<div><br /></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote2back\">[2]</a> <a name=\"footnote2\"></a>&nbsp;This post was result of a collaborative effort between Anna Salamon, Kenzi Amodei, Julia Galef, and &ldquo;Valentine&rdquo; Michael Smith - like many of our endeavors at CFAR, it went through many iterations, in many hands, to create an overall whole where the credit due is difficult to tease apart. &nbsp;</div>\n<div><br /></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote3back\">[3]</a> <a name=\"footnote3\"></a>&nbsp;In the broadest sense, CFAR can be seen as a cognitive branch of effective altruism - making a marginal improvement to thinking where thinking matters a lot. &nbsp;MIRI did not gain traction until it began to include explicit rationality in its message - maybe because thinking about AI puts heavy loads on particular cognitive skills, though there are other hypotheses. &nbsp;Other branches of effective altruism may encounter their own problems with a heavy cognitive load. &nbsp;Effective altruism is limited in its growth by the supply of competent people who want to quantify the amount of good they do.</div>\n<div><br /></div>\n<div>It has been true over the course of human history that improvements in world welfare have often been tied to improvements in explicit thinking skills, most notably with the invention of science. &nbsp;Even for someone who doesn't think that existential risk is the right place to look, trying to invest more in good reasoning, qua good reasoning - doubling down on the huge benefits which explicit cognitive skills have already brought humanity - is a plausible candidate for the highest-impact marginal altruism.</div>\n<div><br /></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote4back\">[4]</a> <a name=\"footnote4\"></a>&nbsp;That is, we&rsquo;ve had two years since our barest beginnings, when Anna, Julia, and Val began working together under the auspices of MIRI; and just over a year as a financially and legally independent organization.</div>\n<div><br /></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote5back\">[5]</a> <a name=\"footnote5\"></a>&nbsp;Our pilot minicamps, prior to that October, gave us valuable data/iteration; but they did not pay for their own direct (room and board) costs, let alone for the staff time required.&nbsp;</div>\n<div><br /></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote6back\">[6]</a> <a name=\"footnote6\"></a>&nbsp;I&rsquo;m estimating quality by workshop participants&rsquo; feedback, here; it takes many fewer hours now for our instructors to create units that receive the same participant ratings as some older unit that hasn&rsquo;t been revised (we did this accidental experiment several times). &nbsp;Unsurprisingly, large quantities of unit-design practice, with rapid iteration and feedback, were key to improving our curriculum design skills.</div>\n<div><br /></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote7back\">[7]</a> <a name=\"footnote7\"></a>&nbsp;Interestingly, we threw away over a dozen versions of the Bayes class before we developed this one. &nbsp;It has proven somewhat easier to create curricula around strategicness, and around productivity/effectiveness more generally, than around epistemic rationality. &nbsp;The reason for the relative difficulty appears to be two-fold. &nbsp;First, it is somewhat harder to create a felt need for epistemic rationality skills, at least among those who aren&rsquo;t working on gnarly, data-sparse problems such as existential risk. &nbsp;Second, there is more existing material on strategicness than on epistemic rationality; and it is in general harder to create from scratch than to create with borrowing. &nbsp;Nevertheless, we have, via much iteration, had some significant successes, including Bayes, separating professed beliefs from anticipated ones, and with certain subskills of avoiding motivated cognition (e.g. noticing curiosity; noticing and tuning in to mental flinches). &nbsp;Better yet, there seems to be a pattern to these successes which we are gradually getting the hang of.</div>\n<div><br /></div>\n<div>We&rsquo;re excited that Ben Hoffman has pledged $23k of funding specifically to enable us to improve our epistemic rationality curriculum and our research plan.</div>\n<div><br /></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote8back\">[8]</a> <a name=\"footnote8\"></a>&nbsp;From the perspective of long-term, high-impact altruism, highly math-talented people are especially worth impacting for a number of reasons. &nbsp;For one thing, if AI does turn out to pose significant risks over the coming century, there&rsquo;s a significant chance that at least one key figure in the eventual development of AI will have had amazing math tests in high school, judging from the history of past such achievements. &nbsp;An eventual scaled-up SPARC program, including math talent from all over the world, may be able to help that unknown future scientist build the competencies he or she will need to navigate that situation well.&nbsp;</div>\n<div><br /></div>\n<div>More broadly, math talent may be relevant to other technological breakthroughs over the coming century; and tech shifts have historically impacted human well-being quite a lot relative to the political issues of any given day.</div>\n<div><br /></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote9back\">[9]</a> <a name=\"footnote9\"></a>&nbsp;To those who&rsquo;ve already applied: Thanks very much for applying; and our apologies for not getting back to you so far. &nbsp;If the funding drive is filled (so that we can afford to possibly hire someone new), we&rsquo;ll be looking through the applications shortly after the drive completes and will get back to you then.</div>\n</div>\n<div><br /></div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"X7v7Fyp9cgBYaMe2e": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JjGs6mDZxeCWkg3ii", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 75, "baseScore": 110, "extendedScore": null, "score": 0.000306, "legacy": true, "legacyId": "25147", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 110, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Summary: &nbsp;We outline the case for CFAR, including:</p>\n<ul>\n<li><a href=\"/lw/jej/why_cfar/#ourgoal\">Our long-term goal</a>;</li>\n<li><a href=\"/lw/jej/why_cfar/#ourplan\">Our plan, and our progress to date</a>;</li>\n<li><a href=\"/lw/jej/why_cfar/#ourfinancials\">Our financials</a>; and</li>\n<li><a href=\"/lw/jej/why_cfar/#help\">How you can help</a>.</li>\n</ul>\n<p>CFAR is in the middle of our <a href=\"http://rationality.org/fundraiser2013/\">annual matching fundraiser</a> right now. &nbsp;If you've been thinking of donating to CFAR, now is the best time to decide for probably at least half a year. &nbsp;Donations up to $150,000 will be matched until January 31st; and Matt Wage, who is matching the last $50,000 of donations, has vowed <em>not</em> to donate unless matched.<a name=\"footnote1back\"></a><a href=\"/lw/jej/why_cfar/#footnote1\">[1]</a></p>\n<p><a href=\"/lw/jej/why_cfar/#footnote1\"></a>Our workshops are cash-flow positive, and subsidize our basic operations (you are not subsidizing workshop attendees). &nbsp;But we can't yet run workshops often enough to fully cover our core operations. &nbsp;We also need to do more formal experiments, and we want to create free and low-cost curriculum with far broader reach than the current workshops. &nbsp;Donations are needed to keep the lights on at CFAR, fund free programs like the <a href=\"http://sparc2013.org/\">Summer Program on Applied Rationality and Cognition</a>, and let us do new and interesting things in 2014 (see below, at length).<a style=\"background-color: #ffffff;\" name=\"footnote2back\"></a><a href=\"/lw/jej/why_cfar/#footnote2\">[2]</a></p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"Our_long_term_goal\"><a name=\"ourgoal\"></a>Our long-term goal</h1>\n<p><span style=\"font-size: small;\">CFAR's long-term goal is to create people who can and will solve important problems -- whatever the important problems turn out to be.</span><a style=\"background-color: #ffffff; color: #000000; font-size: small;\" name=\"footnote3back\"></a><a style=\"font-size: small;\" href=\"/lw/jej/why_cfar/#footnote3\">[3]</a><span style=\"font-size: small;\">&nbsp;&nbsp;</span></p>\n<p>We therefore aim to create a community with three key properties:</p>\n<ol>\n<li><strong>Competence</strong> -- The ability to get things done in the real world. &nbsp;For example, the ability to work hard, follow through on plans, push past your fears, navigate social situations, organize teams of people, start and run successful businesses, <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">etc</a>.</li>\n<li><strong>Epistemic rationality</strong> -- The ability to form relatively accurate beliefs. &nbsp;Especially the ability to form such beliefs in cases where data is limited, motivated cognition is tempting, or the conventional wisdom is incorrect.&nbsp;</li>\n<li><strong>Do-gooding</strong> -- A desire to make the world better for all its people; the tendency to jump in and start/assist projects that might help (whether by labor or by donation); and ambition in keeping an eye out for projects that might help a lot and not just a little. &nbsp;</li>\n</ol>\n<div>Why <strong>competence</strong>, <strong>epistemic rationality</strong>, and <strong>do-gooding</strong>?</div>\n<div>\n<div><br></div>\n<div>To change the world, we'll need to be able to take effective action (competence). &nbsp;We'll need to be able to form a good implicit and explicit understanding of the human world and how to shift it. We'll need to have the best shot we can get at modeling situations yet unseen. &nbsp;We'll need to solve problems outside the realms where competent business people already find traction (all of which require competence plus epistemic rationality). And we'll need to blend these abilities with a burning ambition to leave the world far better than we found it (competence plus epistemic rationality plus do-gooding).</div>\n<div><br></div>\n<div>And we'll need a community, not just a set of individuals. &nbsp;It is hard for an isolated individual to figure out what the most important problems are, let alone how to effectively solve them. &nbsp;This is still harder for individuals who have interesting day jobs, and who are busy amassing real-world competence of varied sorts. &nbsp;Communities can assemble a complex world-model piece by piece. &nbsp;Communities can build and sustain motivation, as well, and facilitate the practice and transfer of useful skills. &nbsp;The aim is thus to create a community that, collectively, can figure out what needs doing and can then do it -- even when this requires multiple simultaneous competencies (e.g., locating a particular existential risk, <em>and</em> having good scientific connections, <em>and</em> knowing good folks in policy, <em>and</em> knowing how to do good technical research).</div>\n<div><br></div>\n<div>We intend to build that sort of community.</div>\n<div><br></div>\n<h1 id=\"Our_plan__and_our_progress_to_date\"><a name=\"ourplan\"></a>Our plan, and our progress to date</h1>\n<p><span style=\"font-size: small;\">How can we create a community with high levels of competence, epistemic rationality, and do-gooding? &nbsp;By creating curricula that teach (or enhance) these properties; by seeding the community with diverse competencies and diverse perspectives on how to do good; and by linking people together into the right kind of community.</span></p>\n<div>\n<div><br></div>\n<div>We've now had two years to execute on this vision.<a name=\"footnote4back\"></a><a href=\"/lw/jej/why_cfar/#footnote4\">[4]</a> &nbsp;It's not a <em>lot</em> of time, but it's enough to get started; and it's enough that folks should already be able to update as to our ability to execute.</div>\n<div><br></div>\n<div>Here's our current working plan, the progress we've made so far, and the pieces we still need to hit.</div>\n<div><br></div>\n<h2 id=\"Curriculum_design\">Curriculum design</h2>\n<div>In October 2012, we had no money and little visible means of obtaining more.<a name=\"footnote5back\"></a><a href=\"/lw/jej/why_cfar/#footnote5\">[5]</a> We needed runway; and we needed a way to use that runway to rapidly iterate curriculum. &nbsp;</div>\n<div><br></div>\n<div>We therefore focused our initial efforts into making a workshop that could pay its own bills, and at the same time give us data -- a workshop that would give us the opportunity to run (and learn from) many further workshops. &nbsp;Our <a href=\"http://rationality.org/workshops/\">applied rationality workshops</a> have filled this role.</div>\n<div><br></div>\n<div>\n<p><strong id=\"Progress_to_date\">Progress to date</strong></p>\n<div style=\"padding-left: 30px;\"><em><strong>Reported benefits</strong></em></div>\n<div style=\"padding-left: 30px;\">After about a dozen workshops (and over 100 classes that we\u2019ve designed and tested), we\u2019ve settled on a workshop model that runs smoothly, and seems to provide value to our participants, who report a mean of 9.3 out of 10 to the question \u201cAre you glad you came?\u201d. In the process we\u2019ve substantially improved our skill at curriculum design: it used to take us about 40 hours to design a unit we regarded as decent (design; test on volunteers; re-design; test on volunteers; etc). It now takes us about 8 hours to design a unit of the same quality.<a name=\"footnote6back\"></a><a href=\"/lw/jej/why_cfar/#footnote6\">[6]</a></div>\n<div style=\"padding-left: 30px;\"><br></div>\n<div style=\"padding-left: 30px;\">Anecdotally, we have many, many stories from alumni about how our workshop increased their competence (both generally and for altruistic ends). For example, alum Ben Toner, CEO of Draftable, recounts that after the July 2012 workshop, \u201cAt work, I realized I wasn\u2019t doing anywhere near enough planning. My employees were spending time on the wrong things because I hadn\u2019t planned things out in enough detail to make it clear what was the most important thing to do next. I fixed this immediately after the camp.\u201d Alum Ben Kuhn has described how the CFAR workshop helped his effective altruism group \u201cvastly increase our campus presence--everything from making uncomfortable cold calls to powering through bureaucracy, and from running complex events to quickly updating on feedback.\u201d (Check out our <a href=\"http://rationality.org/testimonials/\">testimonials</a> page for more examples.) &nbsp;</div>\n<div style=\"padding-left: 30px;\"><br></div>\n<div style=\"padding-left: 30px;\"><em><strong>Measurement</strong></em></div>\n</div>\n<div style=\"padding-left: 30px;\">Anecdata notwithstanding, the jury is still out regarding the workshops' usefulness to those who come. &nbsp;During the very first minicamps (the current workshops are agreed to be better) we randomized admission of 15 applicants, with 17 controls. &nbsp;Our study was low-powered and effects on e.g. income would have needed to be very large for us to expect to detect them. &nbsp;Still, we ended up with non-negligible <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">evidence of absence</a>: income, happiness, and exercise did not visibly trend upward one year later. &nbsp;We detected statistically significant positive impacts on the standard (<a href=\"http://www.westmont.edu/_academics/departments/psychology/documents/Rammstedt_and_John.pdf\">BFI-10</a>) survey pair for emotional stability \"I see myself as someone who is relaxed, handles stress well\" / \"I get nervous easily\" (p=.002). &nbsp;Also significant were effects on an abridged <a href=\"http://userpage.fu-berlin.de/health/selfscal.htm\">General Self-Efficacy Scale</a> (sample item:\"I can solve most problems if I invest the necessary effort\") (p=.007). &nbsp;The details will be available soon on our blog (including a much larger number of negative results). &nbsp;We'll run another RCT soon, funding permitting. &nbsp;</div>\n<div>\n<div><br></div>\n<div style=\"padding-left: 30px;\">Like many participants, we at CFAR have the subjective impression that the workshops boost strategicness; and, like most who have observed two workshops, we have the impression that today's workshops are much better than those in the initial RCT. &nbsp;We'll need to find ways to actually test those impressions, and to create stronger feedbacks from measurement into curriculum development.&nbsp;</div>\n</div>\n<div><br></div>\n<div>\n<div style=\"padding-left: 30px;\"><em><strong>Epistemic rationality curricula</strong></em></div>\n<div style=\"padding-left: 30px;\">After a rocky start, our epistemic rationality curriculum has seen a number of recent victories. &nbsp;Our \u201cBuilding Bayesian Habits\u201d class began performing much better after we figured out how to help people notice their intuitive, \u201cSystem 1\u2033 expectations of probabilities.<a name=\"footnote7back\"></a><a href=\"/lw/jej/why_cfar/#footnote7\">[7]</a> &nbsp;Our \"inner simulator\" class conveys the distinction between profession and anticipation while aiming at immediate, practical benefits; it isn't about religion and politics, it's about whether your mother will actually enjoy the potted plant you\u2019re thinking of giving her. &nbsp;More generally, the epistemic rationality curriculum <a href=\"/lw/il3/three_ways_cfar_has_changed_my_view_of_rationality/\">appears to be</a> integrating deeply with the competence curriculum, and appears to be becoming more appealing to participants as it does so. &nbsp;Strengthening this curriculum, and building in real tests of its efficacy, will be a major focus in 2014.</div>\n<div><br></div>\n<div style=\"padding-left: 30px;\"><em><strong>Integrating with academic research</strong></em></div>\n<div style=\"padding-left: 30px;\">We made preliminary efforts in this direction - for example by taking standard questionnaires from the academic literature, including <a href=\"http://www.keithstanovich.com/Site/Research_on_Reasoning.html\">Stanovich's</a> indicators of the traits he calls \u201crationality\u201d, and administering them to attendees at a Less Wrong meetup. &nbsp;(We found that meetup attendees scored near the ceiling, so we'll probably need new questionnaires with better discrimination.) &nbsp;Our research fellow, Dan Keys (whose masters thesis was on heuristics and biases), spends a majority of his time keeping up with the literature and integrating it with CFAR workshops, as well as designing tests for our ongoing <a href=\"http://rationality.org/2013/09/27/surprise-as-a-cue-to-probability-experiment-1/\">forays</a> into randomized controlled trials. &nbsp;We're particularly excited by Tetlock's <a href=\"http://www.goodjudgmentproject.com\">Good Judgment Project</a>, and we'll be piggybacking on it a bit to see if we can get decent ratings. &nbsp;</div>\n</div>\n<div><br></div>\n<div>\n<div style=\"padding-left: 30px;\"><em><strong>Accessibility</strong></em></div>\n<div style=\"padding-left: 30px;\">Initial workshops worked only for those who had already read the LW Sequences. Today, workshop participants who are smart and analytical, but with no prior exposure to rationality -- such as a local politician, a police officer, a Spanish teacher, and others -- are by and large quite happy with the workshop and feel it is valuable.</div>\n<div><br></div>\n<div style=\"padding-left: 30px;\">Nevertheless, the total set of people who can travel to a 4.5-day immersive workshop, and who can spend $3900 to do so, is limited. &nbsp;We want to eventually give a substantial skill-boost in a less expensive, more accessible format; we are slowly bootstrapping toward this. &nbsp;</div>\n<div style=\"padding-left: 30px;\"><br></div>\n<div style=\"padding-left: 30px;\">Specifically:</div>\n<div>\n<ul style=\"padding-left: 30px;\">\n<li><strong>Shorter workshops:</strong> &nbsp;We\u2019re working on shorter versions of our workshops (including <a href=\"http://rationality.org/2013/09/20/fight-flight-freeze-or-attend-class-september-28/\">three-hour</a> and <a href=\"http://rationality.org/2013/09/10/were-running-a-specialty-workshop-for-programmers/\">one-day</a> courses) that can be given to larger sets of people at lower cost.&nbsp;</li>\n<li><strong>College courses:</strong> &nbsp;We helped develop a <a href=\"http://sensesensibilityscience.com\">course on rational thinking</a> -- for UC Berkeley undergraduates, in partnership with Nobel Laureate Saul Perlmutter. &nbsp;We also brought several high school and university instructors to our workshop, to help seed early experimentation into their curricula.</li>\n<li><strong>Increasing visibility: </strong>We\u2019ve been working on increasing our visibility among the general public, with alumni James Miller and Tim Czech both working on non-fiction books that feature CFAR, and several mainstream media articles about CFAR on their way, including one forthcoming shortly in the Wall Street Journal. \n<ul>\n</ul>\n</li>\n</ul>\n</div>\n<p><strong id=\"Next_steps\">Next steps</strong></p>\n<div style=\"padding-left: 30px;\">In 2014, we\u2019ll be devoting more resources to epistemic curriculum development; to research measuring the effects of our curriculum on both competence and epistemic rationality; and to more widely accessible curricula. &nbsp;</div>\n<div><br></div>\n<div>\n<h2 id=\"Forging_community\">Forging community</h2>\n<div>The most powerful interventions are not one-off experiences; rather, they are the start of an ongoing practice. &nbsp;Changing one's social environment is <a href=\"/lw/52g/the_good_news_of_situationist_psychology/ \">one of the highest impact ways to create personal change</a>. &nbsp;Alum Paul Crowley writes that \u201cThe most valuable lasting thing I got out of attending, I think, is a renewed determination to continually up my game. A big part of that is that the minicamp creates a lasting community of fellow alumni who are also trying for the biggest bite of increased utility they can get, and that\u2019s no accident.\u201d&nbsp;</div>\n<div><br></div>\n<div>The goal is to create a community that is directly helpful for its members, and that simultaneously improves its members' impact on the world.</div>\n<div><br></div>\n<p><strong id=\"Progress_to_date1\">Progress to date</strong></p>\n<div style=\"padding-left: 30px;\"><em><strong>A strong set of seed alumni</strong></em></div>\n<div style=\"padding-left: 30px;\">We have roughly 350 alumni so far, which include scientists from MIT and Berkeley, college students, engineers from Google and Facebook, founders of Y-combinator startups, teachers, professional writers, and the exceptionally gifted high-school students who participated in <a href=\"http://rationality.org/sparc2012/\">SPARC</a> 2013 and 2012. (Not counted in that tally are the 50-some attendees of the <a href=\"http://www.effectivealtruismsummit.com/ \">2013 Effective Altruism Summit</a>, for whom we ran a free, abridged version of our workshop.)&nbsp;</div>\n<div style=\"padding-left: 30px;\"><br></div>\n<div style=\"padding-left: 30px;\"><em><strong>Alumni contact/community</strong></em></div>\n<div style=\"padding-left: 30px;\">There is an active alumni Google group, which gets daily traffic. Alumni use it to share useful life hacks they\u2019ve discovered, help each other trouble-shoot, and notify each other of upcoming events and opportunities. We\u2019ve also been using our post-workshop parties as reunions for alumni nearby (in the San Francisco Bay area, the New York City area, and -- in two months -- Melbourne, Australia).</div>\n<div style=\"padding-left: 30px;\"><br></div>\n<div style=\"padding-left: 30px;\">In large part thanks to our alumni forum and the post-workshop party networking, there have already been numerous cases of alumni helping each other find jobs and collaborating on startups or other projects. &nbsp;There have also been several alumni recruited to do-gooding projects (e.g., MIRI and Leverage Research have engaged multiple alumni), and of alumni improving their \u201cearn to give\u201d ability or shifting their own do-gooding strategy.</div>\n<div style=\"padding-left: 30px;\"><br></div>\n<div style=\"padding-left: 30px;\">Many alumni also take CFAR skills back to Less Wrong meet-ups or other local communities (for example, the effective-altruism meetup in Melbourne, a homeless youth shelter in Oregon, and a self-improvement group in NYC; many have also practiced in their start-ups and with co-workers (for example, Beeminder, MetaMed, and Aquahug)).</div>\n</div>\n<div><br></div>\n<div>\n<div style=\"padding-left: 30px;\"><em><strong>Do-gooding diversity</strong></em></div>\n<div style=\"padding-left: 30px;\">We\u2019d like the alumni community to have an accurate picture of how to effectively improve the world. &nbsp;We don\u2019t want to try to figure out how to improve the world all from scratch. &nbsp;There are already a number of groups who\u2019ve done a lot of good thinking on the subject; including some who call themselves \"effective altruists\", but also people who call themselves \"social entrepreneurs\", \"x-risk minimizers\", and \"philanthropic foundations\". &nbsp;</div>\n<div style=\"padding-left: 30px;\"><br></div>\n<div style=\"padding-left: 30px;\">We aim to bring in the best thinkers and doers from all of these groups to seed the community with diverse good ideas on the subject. &nbsp;<strong>The goal is to create a culture rich enough that the alumni, as a community, can overcome any errors in CFAR\u2019s founders\u2019 perspectives.</strong> &nbsp;The goal is also to create a community that is defined by its pursuit of true beliefs, and that is not defined by any particular preconceptions as to what those beliefs are.</div>\n<div><br></div>\n<div style=\"padding-left: 30px;\">We use applicants\u2019 inclination to do good as a major criterion of financial aid. Recipients of our informally-dubbed \u201caltruism scholarships\u201d have included members of the Future of Humanity Institute, CEA, Giving What We Can, MIRI, and Leverage Research. &nbsp;They also include many college or graduate students who have no official EA affiliation, but who are passionate about their desire to devote their career to world-saving (and who hope the workshops can help them figure out how to do so). &nbsp;And they include folks who are working full-time on varied do-gooding projects of broader origin, such as social entrepreneurs, someone working on community policing, and folks working at a major philanthropic foundation.</div>\n<div><br></div>\n<div style=\"padding-left: 30px;\"><em><strong>International outreach</strong></em></div>\n<div style=\"padding-left: 30px;\">We'll be running our first international workshop in Australia, in February 2014, thanks to alumni Matt and Andrew Fallshaw. &nbsp;</div>\n<div><br></div>\n<div style=\"padding-left: 30px;\">Also, starting in 2014, we'll be bringing about 20 Estonian math and science award-winners per year to CFAR workshops, thanks to a 5-year pledge from Jaan Tallinn to sponsor workshop spots for leading students from his home country. &nbsp;Estonia is an EU member country with a population of 1.2 million and a high-technology economy, and going forward this might be the first opportunity to check whether there are network effects in relatively larger fractions of a stratum.</div>\n<div><br></div>\n<p><strong id=\"Next_steps1\">Next steps</strong></p>\n<div style=\"padding-left: 30px;\">Over 2014, a major focus will be improving opportunities for ongoing alumni involvement. &nbsp;If funding allows, we\u2019ll also try our hand at pilot activities for meet-ups.</div>\n<div style=\"padding-left: 30px;\"><br></div>\n<div style=\"padding-left: 30px;\">Specific plans include:</div>\n<div style=\"padding-left: 30px;\">\n<ul>\n<li>A two-day \"Epistemic Rationality and EA\" mini-workshop in January, targeted at alumni</li>\n<li>An alumni reunion this summer (which will be a multi-day event drawing folks our entire worldwide alumni community, unlike the alumni parties at each workshop);</li>\n<li>An alumni directory, as an attempt to increase business and philanthropic partnerships among alumni.</li>\n</ul>\n</div>\n</div>\n<div>\n<h1 id=\"Financials\"><a name=\"ourfinancials\"></a>Financials</h1>\n<h2 id=\"Expenses\">Expenses</h2>\n<div>Our fixed expenses come to about <strong>$40k per month</strong>. In some detail:</div>\n<div>\n<ul>\n<li>About $7k for our office space</li>\n<li>About $3k for miscellaneous expenses</li>\n<li>About $30k for salary &amp; wages, going forward \n<ul>\n<li>We have five full-time people on salary, each getting $3.5k per month gross. The employer portion of taxes adds roughly an additional $1k/month per employee.</li>\n<li>The remaining $7k or so goes to hourly employees and contractors. &nbsp;We have two roughly full-time hourly employees, and a few contractors who do website adjustment and maintenance, workbook compilation for a workshop, and similarly targeted tasks.</li>\n</ul>\n</li>\n</ul>\n</div>\n<div><br></div>\n<div>In addition to our fixed expenses, we chose to run <a href=\"http://rationality.org/sparc/\">SPARC 2013</a>, even though it would cause us to run out of money right around the end-of-year fundraising drive. We did so because we judged SPARC to be potentially very important<a name=\"footnote8back\"></a><a href=\"/lw/jej/why_cfar/#footnote8\">[8]</a>, enough to justify the risk of leaning on this winter fundraiser to continue. All told, SPARC cost approximately $50k in direct costs (not counting staff time).</div>\n<div><br></div>\n<div>(We also chose to e.g. teach at the EA Summit, do rationality research, put some effort into curricula that can be delivered cheaply to a larger crowd, etc. &nbsp;These did not incur much direct expense, but did require staff time which could otherwise have been directed towards revenue-producing projects.)</div>\n<div><br></div>\n<h2 id=\"Revenue\">Revenue</h2>\n<div>Workshops are our primary source of non-donation income. &nbsp;We ran 7 of them in 2013, and they became increasingly cash-positive through the year. &nbsp;We now expect a full 4-day workshop held in the Bay Area to give us a profit of about $25k (ignoring fixed costs, such as staff time and office rent), which is just under 3 weeks of CFAR runway. &nbsp;Demand isn't yet reliable enough to let us run them at that frequency. We've made significant traction on building interest outside of the Less Wrong community, but there's still work to be done here, and that work will take time. &nbsp;In the meantime, workshops can subsidize some of our non-workshop activities, but not all of them. &nbsp;(Your donations do not go to subsidize workshops!)</div>\n<div><br></div>\n<div>We're also actively exploring revenue models other than the four-day workshop. Several of them look promising, but need time to come to fruition before the income they offer us is relevant.</div>\n<div><br></div>\n<h2 id=\"Donations\">Donations</h2>\n<div>CFAR received $166k in our previous fundraising drive at the start of 2013, and a smaller amount of donations spread across the rest of the year. &nbsp;SPARC was partially sponsored with $15k from Dropbox and $5k from Quixey. &nbsp;These donations subsidized SPARC, the rationality workshop at the EA summit, research and development, and core expenses and salary.</div>\n<div><br></div>\n<h2 id=\"Savings_and_debt\">Savings and debt</h2>\n<div>Right now CFAR has essentially no savings. The savings we accumulated by the end of 2012 went to (a) feeding the gap between income and expenses and (b) funding SPARC.</div>\n<div><br></div>\n<div>A $30k loan, which helped us cover core 2013 expenses, comes due in March 2014.</div>\n<div><br></div>\n<h2 id=\"Summary\">Summary</h2>\n<div>If this winter fundraiser goes well, it will give us time to make some of our current experimental products mature. We think we have an excellent shot at making major strides forward in CFAR's mission as well as becoming much more self-sustaining during 2014.</div>\n<div><br></div>\n<div>If this winter fundraiser goes poorly, CFAR will not yet have sufficient funding to continue core operations.</div>\n</div>\n<div><br></div>\n<div>\n<h1 id=\"How_you_can_help\"><a name=\"help\"></a>How you can help</h1>\n<p><span style=\"font-size: small;\">Our main goals in 2014: &nbsp;</span></p>\n<div><ol>\n<li>Building a <strong>scalable revenue base</strong>, including via ramping up our workshop quality, workshop variety, and our marketing reach.</li>\n<li><strong>Community-building</strong>, including an alumni reunion.&nbsp;</li>\n<li>Creating more connections with the <strong>effective altruism community</strong>, and other opportunities for our alumni to get involved in do-gooding.</li>\n<li><strong>Research </strong>to feed back into our curriculum -- on the effectiveness of particular rationality techniques, as well as the long-term impact of rationality training on meaningful life outcomes.</li>\n<li>Developing more classes on <strong>epistemic rationality</strong>.</li>\n</ol></div>\n<div><br></div>\n<div><strong>The three most important ways you can help:</strong></div>\n<div><br></div>\n<div style=\"padding-left: 30px;\"><strong>1. &nbsp;Donations</strong></div>\n<div style=\"padding-left: 30px;\">If you\u2019re considering donating but want to learn more about how CFAR uses money, or you have other questions or hesitations, let us know -- we\u2019d be more than happy to chat with you via Skype. You can sign up for a one-on-one call with Anna <a href=\"http://rationality.org/talk/anna/\">here</a>.</div>\n<div style=\"padding-left: 30px;\"><br></div>\n<div style=\"padding-left: 30px;\"><strong>2. &nbsp;Talent</strong></div>\n<div style=\"padding-left: 30px;\">We\u2019re actively seeking a new <a href=\"http://rationality.org/director-of-operations/\">director of operations</a> to organize our workshops; good operations can be a great multiplier on CFAR\u2019s total ability to get things done. &nbsp;We are continuing to try out exceptional candidates for a <a href=\"http://rationality.org/hiring/\">curriculum designer</a>.<a name=\"footnote9back\"></a><a href=\"/lw/jej/why_cfar/#footnote9\">[9]</a>&nbsp; And we always need more <a href=\"http://rationality.org/testsessionsignup\">volunteers</a> to help out with alpha-testing new classes in Berkeley, and to participate in online experiments.</div>\n<div style=\"padding-left: 30px;\"><br></div>\n<div style=\"padding-left: 30px;\"><strong>3. &nbsp;Participants</strong></div>\n<div style=\"padding-left: 30px;\">We're continually searching for additional awesome people for our workshops. This really is a high-impact way people can help us; and we do have a large amount of data suggesting that (you /your friends) will be glad to have come. &nbsp;You can apply <a href=\"http://rationality.org/apply/\">here</a> -- it takes 1 minute, and leads to a conversation with Anna or Kenzi, which (you'll / they\u2019ll) probably find interesting whether or not they choose to come.</div>\n<div style=\"padding-left: 60px;\"><br></div>\n<div>Like the open-source movement, applied rationality will be the product of thousands of individuals\u2019 contributions. The ideas we've come up with so far are only a beginning. If you have other suggestions for people we should meet, other workshops we should attend, ways to branch out from our current business model, or anything else -- get in touch, we\u2019d love to Skype with you.&nbsp;</div>\n<div><br></div>\n<div>You can also be a part of open-source applied rationality by creating good content for Less Wrong. Some of our best workshop participants, volunteers, hires, ideas for rationality techniques, use cases, and general inspiration have come from Less Wrong. &nbsp;Help keep the LW community vibrant and growing.</div>\n<div><br></div>\n<div>And, if you\u2019re willing -- do consider <a href=\"http://rationality.org/donate/\">donating now</a>.</div>\n<div><br></div>\n<div><span id=\"docs-internal-guid-3a78b892-39c2-77a5-c9ec-989794c9ec54\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n</span></div>\n<h2 id=\"Footnotes\">Footnotes</h2>\n<div><a href=\"/lw/jej/why_cfar/#footnote1back\">[1]</a><a name=\"footnote1\"></a>&nbsp; That is: by giving up a dollar, you can, given some simplifications, cause CFAR to gain two dollars. &nbsp;Much thanks to Matt Wage, Peter McCluskey, Benjamin Hoffman, Janos Kramar &amp; Victoria Krakovna, Liron Shapira, Satvik Beri, Kevin Harrington, Jonathan Weissman, and Ted Suzman for together putting up $150k in matching funds. &nbsp;(Matt Wage, as mentioned, promises not only that he will donate if the pledge is matched, but also that he <em>won't </em>donate the $50k of matching funds to CFAR if the pledge <em>isn't </em>filled -- so your donation probably really does cause matching at the margin.)</div>\n<div><br></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote2back\">[2]</a> <a name=\"footnote2\"></a>&nbsp;This post was result of a collaborative effort between Anna Salamon, Kenzi Amodei, Julia Galef, and \u201cValentine\u201d Michael Smith - like many of our endeavors at CFAR, it went through many iterations, in many hands, to create an overall whole where the credit due is difficult to tease apart. &nbsp;</div>\n<div><br></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote3back\">[3]</a> <a name=\"footnote3\"></a>&nbsp;In the broadest sense, CFAR can be seen as a cognitive branch of effective altruism - making a marginal improvement to thinking where thinking matters a lot. &nbsp;MIRI did not gain traction until it began to include explicit rationality in its message - maybe because thinking about AI puts heavy loads on particular cognitive skills, though there are other hypotheses. &nbsp;Other branches of effective altruism may encounter their own problems with a heavy cognitive load. &nbsp;Effective altruism is limited in its growth by the supply of competent people who want to quantify the amount of good they do.</div>\n<div><br></div>\n<div>It has been true over the course of human history that improvements in world welfare have often been tied to improvements in explicit thinking skills, most notably with the invention of science. &nbsp;Even for someone who doesn't think that existential risk is the right place to look, trying to invest more in good reasoning, qua good reasoning - doubling down on the huge benefits which explicit cognitive skills have already brought humanity - is a plausible candidate for the highest-impact marginal altruism.</div>\n<div><br></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote4back\">[4]</a> <a name=\"footnote4\"></a>&nbsp;That is, we\u2019ve had two years since our barest beginnings, when Anna, Julia, and Val began working together under the auspices of MIRI; and just over a year as a financially and legally independent organization.</div>\n<div><br></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote5back\">[5]</a> <a name=\"footnote5\"></a>&nbsp;Our pilot minicamps, prior to that October, gave us valuable data/iteration; but they did not pay for their own direct (room and board) costs, let alone for the staff time required.&nbsp;</div>\n<div><br></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote6back\">[6]</a> <a name=\"footnote6\"></a>&nbsp;I\u2019m estimating quality by workshop participants\u2019 feedback, here; it takes many fewer hours now for our instructors to create units that receive the same participant ratings as some older unit that hasn\u2019t been revised (we did this accidental experiment several times). &nbsp;Unsurprisingly, large quantities of unit-design practice, with rapid iteration and feedback, were key to improving our curriculum design skills.</div>\n<div><br></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote7back\">[7]</a> <a name=\"footnote7\"></a>&nbsp;Interestingly, we threw away over a dozen versions of the Bayes class before we developed this one. &nbsp;It has proven somewhat easier to create curricula around strategicness, and around productivity/effectiveness more generally, than around epistemic rationality. &nbsp;The reason for the relative difficulty appears to be two-fold. &nbsp;First, it is somewhat harder to create a felt need for epistemic rationality skills, at least among those who aren\u2019t working on gnarly, data-sparse problems such as existential risk. &nbsp;Second, there is more existing material on strategicness than on epistemic rationality; and it is in general harder to create from scratch than to create with borrowing. &nbsp;Nevertheless, we have, via much iteration, had some significant successes, including Bayes, separating professed beliefs from anticipated ones, and with certain subskills of avoiding motivated cognition (e.g. noticing curiosity; noticing and tuning in to mental flinches). &nbsp;Better yet, there seems to be a pattern to these successes which we are gradually getting the hang of.</div>\n<div><br></div>\n<div>We\u2019re excited that Ben Hoffman has pledged $23k of funding specifically to enable us to improve our epistemic rationality curriculum and our research plan.</div>\n<div><br></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote8back\">[8]</a> <a name=\"footnote8\"></a>&nbsp;From the perspective of long-term, high-impact altruism, highly math-talented people are especially worth impacting for a number of reasons. &nbsp;For one thing, if AI does turn out to pose significant risks over the coming century, there\u2019s a significant chance that at least one key figure in the eventual development of AI will have had amazing math tests in high school, judging from the history of past such achievements. &nbsp;An eventual scaled-up SPARC program, including math talent from all over the world, may be able to help that unknown future scientist build the competencies he or she will need to navigate that situation well.&nbsp;</div>\n<div><br></div>\n<div>More broadly, math talent may be relevant to other technological breakthroughs over the coming century; and tech shifts have historically impacted human well-being quite a lot relative to the political issues of any given day.</div>\n<div><br></div>\n<div><a href=\"/lw/jej/why_cfar/#footnote9back\">[9]</a> <a name=\"footnote9\"></a>&nbsp;To those who\u2019ve already applied: Thanks very much for applying; and our apologies for not getting back to you so far. &nbsp;If the funding drive is filled (so that we can afford to possibly hire someone new), we\u2019ll be looking through the applications shortly after the drive completes and will get back to you then.</div>\n</div>\n<div><br></div>\n</div>\n</div>\n</div>", "sections": [{"title": "Our long-term goal", "anchor": "Our_long_term_goal", "level": 1}, {"title": "Our plan, and our progress to date", "anchor": "Our_plan__and_our_progress_to_date", "level": 1}, {"title": "Curriculum design", "anchor": "Curriculum_design", "level": 2}, {"title": "Progress to date", "anchor": "Progress_to_date", "level": 3}, {"title": "Next steps", "anchor": "Next_steps", "level": 3}, {"title": "Forging community", "anchor": "Forging_community", "level": 2}, {"title": "Progress to date", "anchor": "Progress_to_date1", "level": 3}, {"title": "Next steps", "anchor": "Next_steps1", "level": 3}, {"title": "Financials", "anchor": "Financials", "level": 1}, {"title": "Expenses", "anchor": "Expenses", "level": 2}, {"title": "Revenue", "anchor": "Revenue", "level": 2}, {"title": "Donations", "anchor": "Donations", "level": 2}, {"title": "Savings and debt", "anchor": "Savings_and_debt", "level": 2}, {"title": "Summary", "anchor": "Summary", "level": 2}, {"title": "How you can help", "anchor": "How_you_can_help", "level": 1}, {"title": "Footnotes", "anchor": "Footnotes", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "117 comments"}], "headingsCount": 18}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 117, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PBRWb2Em5SNeWYwwB", "mnS2WYLCGJP2kQkRn", "68dHanLWsS6SEyZp9", "Q5CjE8pRiACqTvhRM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-30T00:56:06.517Z", "modifiedAt": null, "url": null, "title": "Meditation: a self-experiment", "slug": "meditation-a-self-experiment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:30.117Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DdhnNsLosXdjxjeR4/meditation-a-self-experiment", "pageUrlRelative": "/posts/DdhnNsLosXdjxjeR4/meditation-a-self-experiment", "linkUrl": "https://www.lesswrong.com/posts/DdhnNsLosXdjxjeR4/meditation-a-self-experiment", "postedAtFormatted": "Monday, December 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meditation%3A%20a%20self-experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeditation%3A%20a%20self-experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDdhnNsLosXdjxjeR4%2Fmeditation-a-self-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meditation%3A%20a%20self-experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDdhnNsLosXdjxjeR4%2Fmeditation-a-self-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDdhnNsLosXdjxjeR4%2Fmeditation-a-self-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2422, "htmlBody": "<h2><strong>Introduction</strong></h2>\n<p>The LW/CFAR community has a fair amount of interest in meditation. This isn't surprising; many of the people who practiced and wrote about meditation in the past were trying to train a skill similar to rationality. Schools of meditation seem to be the closest already-existing thing to rationality dojos&ndash;this doesn't mean that they're <em>very </em>similar, only that I can't think of anything else that's more similar.</p>\n<p class=\"MsoNormal\">People are Doing Science on meditation; there are studies on the effects of meditation on <a href=\"http://www.tandfonline.com/doi/abs/10.1080/13674679908406332#preview\">attention</a>, <a href=\"http://www.psychologytoday.com/blog/mindfulness-in-frantic-world/201110/curing-depression-mindfulness-meditation\">depression</a>, <a href=\"http://www.wakehealth.edu/News-Releases/2013/Anxious_Activate_Your_Anterior_Cingulate_Cortex_With_a_Little_Meditation.htm\">anxiety</a>,&nbsp;<a href=\"http://www.jpsychores.com/article/S0022-3999(03)00573-7/abstract\">stress and pain reduction</a>. [Insert usual disclaimer that many of these studies either won't be replicated or aren't measuring what they think they're measuring]. Meditation is apparently considered a <a href=\"http://nccam.nih.gov/health/meditation/overview.htm\">form of alternative medicine</a>; this is quite annoying, actually, since it's a thing that might help a lot of people being lumped in with other things that almost certainly don't work.&nbsp;</p>\n<p class=\"MsoNormal\">[There's the spiritual enlightenment element of meditation, too. I won't touch on that, since my own experience isn't related to that aspect.]</p>\n<p class=\"MsoNormal\">Brienne Strohl has posted about <a href=\"/lw/iuh/meditation_trains_metacognition/\">meditation and metacognition</a>; DavidM has posted on <a href=\"/lw/5h9/meditation_insight_and_rationality_part_1_of_3/\">meditation and insight</a>. <a href=\"/user/Valentine/overview/\">Valentine</a>, of CFAR, talked about mindfulness meditation helping to dispel the illusion of being hurried and never having enough time.&nbsp;</p>\n<p class=\"MsoNormal\">In short, lots of hype&ndash;enough that I found it worthwhile to give it a try myself. The main benefit I hoped to attain from practicing meditation was better control of attention&ndash;to be able to aim my attention more reliably at a particular target, and notice more quickly when it drifted. The secondary benefit would be better understanding and control of emotions, which I had already tried to accomplish through techniques other than meditation.&nbsp;<span lang=\"EN-GB\">However, I&rsquo;d had the experience for several years of thinking that meditation was a valuable thing to try, and not trying it&ndash;evidence that I needed more than good intentions.</span>&nbsp;</p>\n<h2><span lang=\"EN-GB\">The experiment</span></h2>\n<p>Sometime in early September, I saw a poster on the wall at the hospital where I work, advertising a study on mindfulness meditation for people with social anxiety. I called the number on the poster and got myself enrolled because it was a good pre-commitment strategy. The benefits were deadlines, social pressure, and structure, with a steady supply of exercises, audio recordings, and readings. This came at the cost of two hours a week for twelve weeks, not all of which was spent on the specific skills that I wanted to learn. Another possible cost could be thinking of myself more as someone who has social anxiety, which might become a self-fulfilling prophecy, but I don&rsquo;t think this actually happened. If anything, sitting down in a group once a week with people whose anxiety significantly affected their functioning had the effect of making my own anxiety seem pretty insignificant. (I was able to convincingly make the case that I suffer from social anxiety during my interview; I've cried in front of my teachers a lot, including during my last year of nursing school, which caused some adults to think that I wasn't cut out for nursing).&nbsp;<a id=\"more\"></a></p>\n<p>I didn't quite do <em>all </em>of the homework for the study, which would have amounted to almost an hour a day. The social pressure of having to hand in a sheet had me doing most of it, though. I Beemindered twenty minutes a day of meditation; according to Beeminder, this has amounted to about 25 hours since mid-September despite the several occasions on which I derailed.&nbsp;</p>\n<p>The Dropbox folder with the audio files for most of the meditation exercises I've done regularly is <a href=\"https://www.dropbox.com/sh/gpglptgcxayrmqc/cVUSRu0LY1\">here</a>.&nbsp;</p>\n<h2>Breakdown of different meditation exercises</h2>\n<p><em>Compassionate Body Scan</em>: a 25-minute tape with a man talking soporifically about exploring your feet like you would the feet of a beloved child and being curious about the experience of your ankles&ndash;and, eventually, the rest of your body. I've often done this one in bed when I hadn't gotten around to meditating earlier that day, and I often fall asleep around the pelvis area. I wish I could do this on demand; without the tape, it generally takes me 45 minutes to an hour to fall asleep.&nbsp;</p>\n<p><em>10 and 20 minute sitting meditation</em>: A walkthrough of focusing on the breath in various places; the nostrils, the throat, the chest and abdomen; and later focusing on the whole body. I like this one because sometimes near the end I feel like I'm floating in a void. I also do it in a particular position&ndash;kneeling and supporting my bum on a low stool&ndash;which I've conditioned myself to associate with meditation to the point that the posture is calming in and of itself.&nbsp;</p>\n<p><em>Loving-Kindness meditation:&nbsp;</em>Walks through feeling kindness towards someone you love, someone you're indifferent to, and someone you dislike. I don't usually feel very different after this one, but this is partly because I've been training myself to like people in general for years, and because nursing school in and of itself is an exercise in empathy-building. I have noticed that I can't do it as effectively anymore because there's no one I experience a strong emotion of dislike towards&ndash;I used a particular nurse on my unit for a month or so, and now, although I have the same thoughts about her, I don't have the actual emotional experience of dislike. So I guess it worked.&nbsp;</p>\n<p><em>Mountain Meditation:&nbsp;</em>a complex visualization/metaphor of yourself as a mountain. I would say that your mileage may vary the most on this one, because of variations in mental imagery. I have very vivid mental imagery, so I like it a lot. I've got some salient mental images cached now to draw on the metaphor of \"I am a mountain and I will endure seasons, storms, winter, every single alarm in my patient's room going off at once, and WHATEVER ELSE THE UNIVERSE WANTS TO THROW AT ME!\"&nbsp;</p>\n<p><strong style=\"font-size: 16px;\">The Results</strong></p>\n<p>Initial positive; I like meditation enough to want to keep doing it. It feels good, overall. That doesn't mean that I want to do it all the time, or that I effortlessly accomplish my Beeminder goal; it does require willpower to put away the computer/book/food/music and focus on doing nothing for 20 minutes, and I do moan and groan and put it off. But it's generally pleasant while I'm actually doing it, and there are times when I feel a <em>lot </em>better afterwards.</p>\n<p>There are several reasons why, a priori, I would expect meditation to be especially helpful for me. My natural state is to daydream. I'm good at remembering complex sequences of things, but not at noticing details, because I'm too busy thinking about all those complex interesting things that happened earlier. This is all very well, but as a nurse, it's important for me to be paying attention to what I'm doing.&nbsp;</p>\n<p>The single most helpful time for me to meditate is when I'm feeling very frazzled. This usually happens when I've had an extremely busy day at work, running around all day trying to save someone's life, and I feel very motivated, but at some point I get overwhelmed by all the object-level tasks that keep flying at my face, and I lose track of the \"big picture\" and with it the ability to prioritize or plan anything, and end up dealing with tasks in the order of which thing beeps the loudest. Even once I get home after a day like this, the frazzled state persists and I can't actually settle onto any tasks that need to get done at home. A friend of mine once described his experience of having tried cocaine as \"I felt very alert, but it was an illusion because I was also really scattered.\" I haven't tried cocaine, but those words describe the me-after-a-busy-shift very accurately.&nbsp;</p>\n<p>I don't always get myself to meditate as soon as I get home (or on break during the busy shift); it takes willpower, and this is a willpower-reduced state. It would be an excellent habit to train, though. To clarify: I find meditation <em>really difficult </em>in this state. My thoughts are racing and the last thing I want to focus on is my breath, because I did exciting things today and I should think about all of them really fast. But at least meditation forces me to focus on the fact that my thoughts are racing, and notice that from a calm perspective, instead of completely identifying with and being caught up in the flow. Twenty minutes later, I'm generally reset and able to do something else, although that thing is most often sleep.&nbsp;</p>\n<p>The biggest overall change I've noticed after I started meditating regularly is more awareness of my physical and emotional states; physical especially. It's easy for me not to get around to drinking any water at work, for example, and then ten hours later noticing that I \"don't feel well\" in some vague way, but experiencing this mostly as the phrase \"I don't feel well\" in my head, as opposed to focusing on any physical sensation that might clue me in to the source of my discomfort. (Of course, outside view puts drinking water on the list of things I should try if I mysteriously don't feel well, but it's nice to have an actual physical sensation, too). Several of the meditation exercises had aspects of focusing on the body, focusing on sensations of discomfort without trying to apply words to them or interpret them, etc. This is a <a href=\"/lw/5kz/the_5second_level/\">5-second-level</a> skill that I've improved hugely at.</p>\n<p>A specific instance is when I suddenly clue in that something very urgent and serious is happening, and I go from my general at-work state of mild anxiety to a full-blown SNS fight-or-flight adrenaline response. If I pay attention to my thoughts, they generally aren't going anywhere useful. But meanwhile, my body is doing all these interesting things; racing heart, shaky hands, that weird sinking crampy feeling in my stomach, etc. Meditation has trained me to automatically notice and pay attention to the physical sensations, which gives me a couple of seconds to get my thoughts calmed down. I also have a much easier time getting rid of the annoying physical effects like shaking hands, which make it hard to do fiddly things like draw medications up into syringes&ndash;and heaven forbid I ever have to try to put an IV in on a patient in cardiac arrest.</p>\n<p>I identify less with my moods. I was already generally good at recognizing that moods were temporary coloured glasses on the world and not just the way the world actually was, but I'm better now. I can sometimes notice a negative mood and also notice the thing I actually have to do to fix it; this might be as simple as eating something, or might involve observing my thoughts and realizing that the bad mood started unnoticed because of an interaction with someone else which I interpreted negatively. This is a skill that was discussed a great deal in my meditation class, and it's actually not where I've improved the most; the biggest change has been in the level of physical awareness.&nbsp;</p>\n<p>The skills of body scanning and focusing on breathing have been helpful for forms of exercise that I find aversive, such as running. I can notice the cramps in my shins and explore them, rather than getting caught up in the mental verbal loop of \"I have cramps in my shins and this is awful and I want it to stop!\" Surprisingly, this helps. By focusing on my breathing in a meditation-y way while running; by this, I mean literally focusing on the cold feeling of the air going into my nostrils and throat and the feel of my clothes stretching over my stomach as it expands; I somehow clued in that my diaphragm actually could move independently of my legs and I could breathe at a normal rhythm and depth.&nbsp;</p>\n<p>One of the homework exercises was executing certain activities \"mindfully\". I quickly learned which things I could do mindfully without changing my schedule much, and I noticed that various things just felt better. Swimming, for example, is <em>really </em>sensual when you are actually paying attention to the sensation of water flowing over your skin and the sound of it in your ears through a swim cap and the patterns of reflected light through foggy goggles. I have memories of my 11-year-old self experiencing this; at some point, I stopped. I spend a lot of my life on automatic. This isn't always a problem, but when I notice a negative mood, I can turn on more sensory experience and often get rid of it that way.&nbsp;</p>\n<p>I've gotten somewhat better at actually experiencing myself as a modular mind, with various voices that want different things for different reasons. I don't have to endorse or identify with or experience myself as one of the voices; they're all me, and none of them are the \"real\" me. This helps with mental clarity, and being able to think about difficult decisions without actually experiencing the agonizing and aversiveness and confusion; yes, the different parts of me disagree with each other, that's just a fact about the state of the world and it's okay and doesn't mean I have to be angsty.&nbsp;</p>\n<h2>Conclusion</h2>\n<p>The science on it predicts that meditation has positive expected value as a thing to try. My personal experience showed it to be an overall positive for me in particular; not life-changing, but worth spending 20 minutes a day on. Your mileage may vary, and anecdotally there's a lot of variation in how much people value get from it, but it doesn't take a lot of effort to try. I pre-committed to a twelve-week experiment, but it's likely possible to get an idea of whether meditation helps or not in a much shorter time span.&nbsp;</p>\n<p>Speaking from my personal experience, meditation is likely to be helpful if you tend to daydream and are in a position where you need to daydream less. Science also says that meditation will probably help if you suffer a lot of distress from rumination or mulling over unpleasant thoughts.&nbsp;</p>\n<p>If I were to repeat the experiment, I would do it with actual mood tracking, so that the data I got was more accurate than \"In hindsight and upon reflection, I think I feel this way.\" I still haven't found a good method of mood tracking that does all the things I want it to.</p>\n<p>Meditation is a clearly defined activity; there are groups of people who do it together, books about how to do it, and guided-meditation recordings that you can download from the Internet. If having structure helps you to actually do things, there are plenty of ways to obtain structure.&nbsp;</p>\n<p>I am happy to discuss meditation further with anyone who is interested.&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AiNyf5iwbpc7mehiX": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DdhnNsLosXdjxjeR4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": 85, "extendedScore": null, "score": 0.000232, "legacy": true, "legacyId": "25145", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 85, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Introduction\"><strong>Introduction</strong></h2>\n<p>The LW/CFAR community has a fair amount of interest in meditation. This isn't surprising; many of the people who practiced and wrote about meditation in the past were trying to train a skill similar to rationality. Schools of meditation seem to be the closest already-existing thing to rationality dojos\u2013this doesn't mean that they're <em>very </em>similar, only that I can't think of anything else that's more similar.</p>\n<p class=\"MsoNormal\">People are Doing Science on meditation; there are studies on the effects of meditation on <a href=\"http://www.tandfonline.com/doi/abs/10.1080/13674679908406332#preview\">attention</a>, <a href=\"http://www.psychologytoday.com/blog/mindfulness-in-frantic-world/201110/curing-depression-mindfulness-meditation\">depression</a>, <a href=\"http://www.wakehealth.edu/News-Releases/2013/Anxious_Activate_Your_Anterior_Cingulate_Cortex_With_a_Little_Meditation.htm\">anxiety</a>,&nbsp;<a href=\"http://www.jpsychores.com/article/S0022-3999(03)00573-7/abstract\">stress and pain reduction</a>. [Insert usual disclaimer that many of these studies either won't be replicated or aren't measuring what they think they're measuring]. Meditation is apparently considered a <a href=\"http://nccam.nih.gov/health/meditation/overview.htm\">form of alternative medicine</a>; this is quite annoying, actually, since it's a thing that might help a lot of people being lumped in with other things that almost certainly don't work.&nbsp;</p>\n<p class=\"MsoNormal\">[There's the spiritual enlightenment element of meditation, too. I won't touch on that, since my own experience isn't related to that aspect.]</p>\n<p class=\"MsoNormal\">Brienne Strohl has posted about <a href=\"/lw/iuh/meditation_trains_metacognition/\">meditation and metacognition</a>; DavidM has posted on <a href=\"/lw/5h9/meditation_insight_and_rationality_part_1_of_3/\">meditation and insight</a>. <a href=\"/user/Valentine/overview/\">Valentine</a>, of CFAR, talked about mindfulness meditation helping to dispel the illusion of being hurried and never having enough time.&nbsp;</p>\n<p class=\"MsoNormal\">In short, lots of hype\u2013enough that I found it worthwhile to give it a try myself. The main benefit I hoped to attain from practicing meditation was better control of attention\u2013to be able to aim my attention more reliably at a particular target, and notice more quickly when it drifted. The secondary benefit would be better understanding and control of emotions, which I had already tried to accomplish through techniques other than meditation.&nbsp;<span lang=\"EN-GB\">However, I\u2019d had the experience for several years of thinking that meditation was a valuable thing to try, and not trying it\u2013evidence that I needed more than good intentions.</span>&nbsp;</p>\n<h2 id=\"The_experiment\"><span lang=\"EN-GB\">The experiment</span></h2>\n<p>Sometime in early September, I saw a poster on the wall at the hospital where I work, advertising a study on mindfulness meditation for people with social anxiety. I called the number on the poster and got myself enrolled because it was a good pre-commitment strategy. The benefits were deadlines, social pressure, and structure, with a steady supply of exercises, audio recordings, and readings. This came at the cost of two hours a week for twelve weeks, not all of which was spent on the specific skills that I wanted to learn. Another possible cost could be thinking of myself more as someone who has social anxiety, which might become a self-fulfilling prophecy, but I don\u2019t think this actually happened. If anything, sitting down in a group once a week with people whose anxiety significantly affected their functioning had the effect of making my own anxiety seem pretty insignificant. (I was able to convincingly make the case that I suffer from social anxiety during my interview; I've cried in front of my teachers a lot, including during my last year of nursing school, which caused some adults to think that I wasn't cut out for nursing).&nbsp;<a id=\"more\"></a></p>\n<p>I didn't quite do <em>all </em>of the homework for the study, which would have amounted to almost an hour a day. The social pressure of having to hand in a sheet had me doing most of it, though. I Beemindered twenty minutes a day of meditation; according to Beeminder, this has amounted to about 25 hours since mid-September despite the several occasions on which I derailed.&nbsp;</p>\n<p>The Dropbox folder with the audio files for most of the meditation exercises I've done regularly is <a href=\"https://www.dropbox.com/sh/gpglptgcxayrmqc/cVUSRu0LY1\">here</a>.&nbsp;</p>\n<h2 id=\"Breakdown_of_different_meditation_exercises\">Breakdown of different meditation exercises</h2>\n<p><em>Compassionate Body Scan</em>: a 25-minute tape with a man talking soporifically about exploring your feet like you would the feet of a beloved child and being curious about the experience of your ankles\u2013and, eventually, the rest of your body. I've often done this one in bed when I hadn't gotten around to meditating earlier that day, and I often fall asleep around the pelvis area. I wish I could do this on demand; without the tape, it generally takes me 45 minutes to an hour to fall asleep.&nbsp;</p>\n<p><em>10 and 20 minute sitting meditation</em>: A walkthrough of focusing on the breath in various places; the nostrils, the throat, the chest and abdomen; and later focusing on the whole body. I like this one because sometimes near the end I feel like I'm floating in a void. I also do it in a particular position\u2013kneeling and supporting my bum on a low stool\u2013which I've conditioned myself to associate with meditation to the point that the posture is calming in and of itself.&nbsp;</p>\n<p><em>Loving-Kindness meditation:&nbsp;</em>Walks through feeling kindness towards someone you love, someone you're indifferent to, and someone you dislike. I don't usually feel very different after this one, but this is partly because I've been training myself to like people in general for years, and because nursing school in and of itself is an exercise in empathy-building. I have noticed that I can't do it as effectively anymore because there's no one I experience a strong emotion of dislike towards\u2013I used a particular nurse on my unit for a month or so, and now, although I have the same thoughts about her, I don't have the actual emotional experience of dislike. So I guess it worked.&nbsp;</p>\n<p><em>Mountain Meditation:&nbsp;</em>a complex visualization/metaphor of yourself as a mountain. I would say that your mileage may vary the most on this one, because of variations in mental imagery. I have very vivid mental imagery, so I like it a lot. I've got some salient mental images cached now to draw on the metaphor of \"I am a mountain and I will endure seasons, storms, winter, every single alarm in my patient's room going off at once, and WHATEVER ELSE THE UNIVERSE WANTS TO THROW AT ME!\"&nbsp;</p>\n<p><strong style=\"font-size: 16px;\" id=\"The_Results\">The Results</strong></p>\n<p>Initial positive; I like meditation enough to want to keep doing it. It feels good, overall. That doesn't mean that I want to do it all the time, or that I effortlessly accomplish my Beeminder goal; it does require willpower to put away the computer/book/food/music and focus on doing nothing for 20 minutes, and I do moan and groan and put it off. But it's generally pleasant while I'm actually doing it, and there are times when I feel a <em>lot </em>better afterwards.</p>\n<p>There are several reasons why, a priori, I would expect meditation to be especially helpful for me. My natural state is to daydream. I'm good at remembering complex sequences of things, but not at noticing details, because I'm too busy thinking about all those complex interesting things that happened earlier. This is all very well, but as a nurse, it's important for me to be paying attention to what I'm doing.&nbsp;</p>\n<p>The single most helpful time for me to meditate is when I'm feeling very frazzled. This usually happens when I've had an extremely busy day at work, running around all day trying to save someone's life, and I feel very motivated, but at some point I get overwhelmed by all the object-level tasks that keep flying at my face, and I lose track of the \"big picture\" and with it the ability to prioritize or plan anything, and end up dealing with tasks in the order of which thing beeps the loudest. Even once I get home after a day like this, the frazzled state persists and I can't actually settle onto any tasks that need to get done at home. A friend of mine once described his experience of having tried cocaine as \"I felt very alert, but it was an illusion because I was also really scattered.\" I haven't tried cocaine, but those words describe the me-after-a-busy-shift very accurately.&nbsp;</p>\n<p>I don't always get myself to meditate as soon as I get home (or on break during the busy shift); it takes willpower, and this is a willpower-reduced state. It would be an excellent habit to train, though. To clarify: I find meditation <em>really difficult </em>in this state. My thoughts are racing and the last thing I want to focus on is my breath, because I did exciting things today and I should think about all of them really fast. But at least meditation forces me to focus on the fact that my thoughts are racing, and notice that from a calm perspective, instead of completely identifying with and being caught up in the flow. Twenty minutes later, I'm generally reset and able to do something else, although that thing is most often sleep.&nbsp;</p>\n<p>The biggest overall change I've noticed after I started meditating regularly is more awareness of my physical and emotional states; physical especially. It's easy for me not to get around to drinking any water at work, for example, and then ten hours later noticing that I \"don't feel well\" in some vague way, but experiencing this mostly as the phrase \"I don't feel well\" in my head, as opposed to focusing on any physical sensation that might clue me in to the source of my discomfort. (Of course, outside view puts drinking water on the list of things I should try if I mysteriously don't feel well, but it's nice to have an actual physical sensation, too). Several of the meditation exercises had aspects of focusing on the body, focusing on sensations of discomfort without trying to apply words to them or interpret them, etc. This is a <a href=\"/lw/5kz/the_5second_level/\">5-second-level</a> skill that I've improved hugely at.</p>\n<p>A specific instance is when I suddenly clue in that something very urgent and serious is happening, and I go from my general at-work state of mild anxiety to a full-blown SNS fight-or-flight adrenaline response. If I pay attention to my thoughts, they generally aren't going anywhere useful. But meanwhile, my body is doing all these interesting things; racing heart, shaky hands, that weird sinking crampy feeling in my stomach, etc. Meditation has trained me to automatically notice and pay attention to the physical sensations, which gives me a couple of seconds to get my thoughts calmed down. I also have a much easier time getting rid of the annoying physical effects like shaking hands, which make it hard to do fiddly things like draw medications up into syringes\u2013and heaven forbid I ever have to try to put an IV in on a patient in cardiac arrest.</p>\n<p>I identify less with my moods. I was already generally good at recognizing that moods were temporary coloured glasses on the world and not just the way the world actually was, but I'm better now. I can sometimes notice a negative mood and also notice the thing I actually have to do to fix it; this might be as simple as eating something, or might involve observing my thoughts and realizing that the bad mood started unnoticed because of an interaction with someone else which I interpreted negatively. This is a skill that was discussed a great deal in my meditation class, and it's actually not where I've improved the most; the biggest change has been in the level of physical awareness.&nbsp;</p>\n<p>The skills of body scanning and focusing on breathing have been helpful for forms of exercise that I find aversive, such as running. I can notice the cramps in my shins and explore them, rather than getting caught up in the mental verbal loop of \"I have cramps in my shins and this is awful and I want it to stop!\" Surprisingly, this helps. By focusing on my breathing in a meditation-y way while running; by this, I mean literally focusing on the cold feeling of the air going into my nostrils and throat and the feel of my clothes stretching over my stomach as it expands; I somehow clued in that my diaphragm actually could move independently of my legs and I could breathe at a normal rhythm and depth.&nbsp;</p>\n<p>One of the homework exercises was executing certain activities \"mindfully\". I quickly learned which things I could do mindfully without changing my schedule much, and I noticed that various things just felt better. Swimming, for example, is <em>really </em>sensual when you are actually paying attention to the sensation of water flowing over your skin and the sound of it in your ears through a swim cap and the patterns of reflected light through foggy goggles. I have memories of my 11-year-old self experiencing this; at some point, I stopped. I spend a lot of my life on automatic. This isn't always a problem, but when I notice a negative mood, I can turn on more sensory experience and often get rid of it that way.&nbsp;</p>\n<p>I've gotten somewhat better at actually experiencing myself as a modular mind, with various voices that want different things for different reasons. I don't have to endorse or identify with or experience myself as one of the voices; they're all me, and none of them are the \"real\" me. This helps with mental clarity, and being able to think about difficult decisions without actually experiencing the agonizing and aversiveness and confusion; yes, the different parts of me disagree with each other, that's just a fact about the state of the world and it's okay and doesn't mean I have to be angsty.&nbsp;</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>The science on it predicts that meditation has positive expected value as a thing to try. My personal experience showed it to be an overall positive for me in particular; not life-changing, but worth spending 20 minutes a day on. Your mileage may vary, and anecdotally there's a lot of variation in how much people value get from it, but it doesn't take a lot of effort to try. I pre-committed to a twelve-week experiment, but it's likely possible to get an idea of whether meditation helps or not in a much shorter time span.&nbsp;</p>\n<p>Speaking from my personal experience, meditation is likely to be helpful if you tend to daydream and are in a position where you need to daydream less. Science also says that meditation will probably help if you suffer a lot of distress from rumination or mulling over unpleasant thoughts.&nbsp;</p>\n<p>If I were to repeat the experiment, I would do it with actual mood tracking, so that the data I got was more accurate than \"In hindsight and upon reflection, I think I feel this way.\" I still haven't found a good method of mood tracking that does all the things I want it to.</p>\n<p>Meditation is a clearly defined activity; there are groups of people who do it together, books about how to do it, and guided-meditation recordings that you can download from the Internet. If having structure helps you to actually do things, there are plenty of ways to obtain structure.&nbsp;</p>\n<p>I am happy to discuss meditation further with anyone who is interested.&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "The experiment", "anchor": "The_experiment", "level": 1}, {"title": "Breakdown of different meditation exercises", "anchor": "Breakdown_of_different_meditation_exercises", "level": 1}, {"title": "The Results", "anchor": "The_Results", "level": 2}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "30 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JMgffu9AzhYpTpHFJ", "QqSNFcGSZdnARx56E", "JcpzFpPBSmzuksmWM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-30T00:58:23.884Z", "modifiedAt": null, "url": null, "title": "Critiquing Gary Taubes, Part 2: Atkins Redux", "slug": "critiquing-gary-taubes-part-2-atkins-redux", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:35.182Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eSfPGQjArWXWXu99Z/critiquing-gary-taubes-part-2-atkins-redux", "pageUrlRelative": "/posts/eSfPGQjArWXWXu99Z/critiquing-gary-taubes-part-2-atkins-redux", "linkUrl": "https://www.lesswrong.com/posts/eSfPGQjArWXWXu99Z/critiquing-gary-taubes-part-2-atkins-redux", "postedAtFormatted": "Monday, December 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Critiquing%20Gary%20Taubes%2C%20Part%202%3A%20Atkins%20Redux&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACritiquing%20Gary%20Taubes%2C%20Part%202%3A%20Atkins%20Redux%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeSfPGQjArWXWXu99Z%2Fcritiquing-gary-taubes-part-2-atkins-redux%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Critiquing%20Gary%20Taubes%2C%20Part%202%3A%20Atkins%20Redux%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeSfPGQjArWXWXu99Z%2Fcritiquing-gary-taubes-part-2-atkins-redux", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeSfPGQjArWXWXu99Z%2Fcritiquing-gary-taubes-part-2-atkins-redux", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1272, "htmlBody": "<p><strong>Previously: </strong><a href=\"/lw/jdw/critiquing_gary_taubes_part_1_mainstream/\">Mainstream Nutrition Science on Obesity</a></p>\n<p><em>Edit: In retrospect, I think it maybe should have combined this post with <a href=\"/lw/je5/critiquing_gary_taubes_part_3_did_the_us/\">part 3</a>. Unfortunately, the problem of what to do with existing comments makes that hard to fix now.</em></p>\n<p>Taubes first made a name for himself as a low-carb advocate in 2002 with a <em>New York Times </em>article titled <a href=\"http://www.nytimes.com/2002/07/07/magazine/what-if-it-s-all-been-a-big-fat-lie.html?pagewanted=all&amp;src=pm\">\"What if It's All Been a Big Fat Lie?\"</a>&nbsp;When I first read this article, I was getting extremely suspicious by the second paragraph (emphasis added):</p>\n<blockquote>\n<p>If the members of the American medical establishment were to have a collective find-yourself-standing-naked-in-Times-Square-type nightmare, this might be it. <strong>They spend 30 years ridiculing Robert Atkins, author of the phenomenally-best-selling ''Dr. Atkins' Diet Revolution'' and ''Dr. Atkins' New Diet Revolution,'' accusing the Manhattan doctor of quackery and fraud, only to discover that the unrepentant Atkins was right all along.</strong> Or maybe it's this: they find that their very own dietary recommendations -- eat less fat and more carbohydrates -- are the cause of the rampaging epidemic of obesity in America. Or, just possibly this: they find out both of the above are true.</p>\n<p>When Atkins first published his ''Diet Revolution'' in 1972, Americans were just coming to terms with the proposition that fat -- particularly the saturated fat of meat and dairy products -- was the primary nutritional evil in the American diet. Atkins managed to sell millions of copies of a book <strong>promising that we would lose weight eating steak, eggs and butter to our heart's desire</strong>, because it was the carbohydrates, the pasta, rice, bagels and sugar, that caused obesity and even heart disease. <strong>Fat, he said, was harmless.</strong></p>\n<p><strong>Atkins allowed his readers to eat ''truly luxurious foods without limit,'' as he put it, ''lobster with butter sauce, steak with b&eacute;arnaise sauce . . . bacon cheeseburgers,''</strong> but allowed no starches or refined carbohydrates, which means no sugars or anything made from flour. Atkins banned even fruit juices, and permitted only a modicum of vegetables, although the latter were negotiable as the diet progressed.</p>\n</blockquote>\n<p>It's one thing to claim that, all else equal, low-carb diets have advantages over low-fat diets. It's another thing to claim you can eat unlimited amounts of fatty foods without gaining weight.</p>\n<p><a id=\"more\"></a>I'd heard of Atkins before but didn't know much about him. I got curious to know more about the man Taubes was casting as the hero who just may have been \"right all along,\" so I popped over to the <a href=\"http://en.wikipedia.org/wiki/Atkins_diet\">Wikipedia article on the diet</a>, which says:</p>\n<blockquote>\n<p>Many people believe that the Atkins Diet promotes eating unlimited amounts of fatty meats and cheeses. This was allowed and promoted in early editions of the book. In the newest revision, not written by the now deceased Dr. Atkins, this is not promoted. The Atkins Diet does not impose caloric restriction, or definite limits on proteins, with Atkins saying in his book that this plan is \"not a license to gorge,\" but rather that eating protein until satiated is promoted. The director of research and education for Atkins Nutritionals, Collette Heimowitz, has said, \"The media and opponents of Atkins often sensationalise and simplify the diet as the all-the-steak-you-can-eat diet. This has never been true\". However, this new approach by Atkins Nutritionals is often at odds with the earlier writings of Dr. Atkins.</p>\n</blockquote>\n<p>The last sentence of this paragraph is helpfully marked \"citation needed,\" leaving an unresolved conflict between whatever Wikipedia editor wrote the paragraph and what the Atkins folks (at least now) claim. I ordered a used copy of the original 1972 edition of Atkins' book through Amazon, and what I found supports the Wikipedia editor. The folks currently in charge of Atkins Nutritionals are white-washing.</p>\n<p>The sensational \"truly luxurious food without limit\" quote in Taubes' article, for example, can be found on page 15 and comes with no context that would make it more reasonable. In fact, lest anyone misunderstand it, it's followed by a statement that \"As long as you don't take in carbohydrates, you can eat any amount of this 'fattening' food and it won't put a single ounce of fat on you.\" (In the book, this is italicized for emphasis.)</p>\n<p>Atkins acknowledged that most of the people who used his diet ended up eating less overall, but claimed that some of his patients had lost significant amounts of weight eating 3,000 calories per day or more. In one case, Atkins claimed, a man had lost fifty pounds on a diet of 5,000 calories per day. He attempted to explain this by invoking the fact that extremely low-carbohydrate diets will cause people to excrete ketones (which Atkins referred to as \"incompletely burned calories\") in their urine.&nbsp;However, as <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1753-4887.1974.tb05180.x/abstract\">a statement on the Atkins diet put out by the American Medical Association</a> explains:</p>\n<blockquote>\n<p>When ketone excretion incident to such diets has actually been measured, it has been found to range between 0.5 and 10 gm/24 hr. Studies carried out on starving nondieabetic persons indicate that at most about 20 gm of ketones per day may be excreted in the urine. And, as Folin and Denis have show, the total acetone excretion with the breath is quantitatively insignificant; at most, 1 gm/day. Since caloric value of Ketones is about 4.5 kcal/gm, it is clear that, in subjects on ketogenic diets, ketone losses in the urine rarely, if ever, exceed 100 kcal/day, a quantity that could not possibly account for the dramatic results claimed for such diets.</p>\n</blockquote>\n<p>As far as I can tell, nobody today defends Atkins' original \"ketones in the urine\" explanation for how his diet supposedly works. It's not entirely clear to me what was going on with the patients Atkins claimed lost weight on a high-calorie diet, but it wouldn't be surprising if a minority of his patients had simply misjudged their caloric intake. In spite of this, Taubes still appears to want to defend Atkins' most extreme claims about people being able to eat unlimited fat without gaining weight.</p>\n<p>This isn't entirely obvious when you read his books <em><a href=\"http://www.amazon.com/Good-Calories-Bad-Controversial-Science/dp/1400033462/ref=sr_1_1?ie=UTF8&amp;qid=1387755655&amp;sr=8-1&amp;keywords=Good+Calories%2C+Bad+Calories\">Good Calories, Bad Calories</a> </em>or <a style=\"font-style: italic;\" href=\"http://www.amazon.com/Why-We-Get-Fat-About/dp/0307474259/ref=sr_1_1?ie=UTF8&amp;qid=1387755686&amp;sr=8-1&amp;keywords=why+we+get+fat\">Why We Get Fat</a>,<em>&nbsp;</em>which go for a <em>slightly</em> less sensational presentation than the&nbsp;<em>Times </em>article. Nevertheless, in the epilogue to <em>Good Calories, Bad Calories, </em>he claims that&nbsp;\"Dietary fat, whether saturated or not, is not a cause of obesity, heart disease, or any other chronic disease of civilization.\" There's a sense in which that claim <em>might</em> be somewhat plausible, if he meant that it's total calories, not fat <em>per se, </em>that's the main culprit in all those problems. But Taubes also puts a lot of energy (no pun intended) into attacking the mainstream emphasis on calories.&nbsp;</p>\n<p><em>Why We Get Fat, </em>for example, contains claims such as:</p>\n<blockquote>\n<p>[A 1965 <em>New York Times </em>article claimed that] \"It is a medical fact that no dieter can lose weight unless he cuts down on excess calories, either by taking in fewer of them, or by burning them up.\" We now know that this is not a medical fact, but the nutritionists didn't in 1965, and most of them still don't. (p. 161)</p>\n<p>But we know now what happens when we restrict carbohydrates, and why this leads to weight loss and particularly fat loss, independent of the calories we consume from dietary fat and protein... If you restrict only carbohydrates, you can always eat more protein and fat if you feel the urge, since they have no effect on fat accumulation. (p. 174)</p>\n</blockquote>\n<p><em>No </em>effect? That's a strong claim. And as we'll see in the next two posts, Taubes' evidence for this claim ends up consisting largely on a series of misrepresentations of mainstream nutrition science, which allow him to present his views as the only alternative once he's knocked down his straw men.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"92SxJsDZ78ApAGq72": 1, "yt9Z7xdQrofW7fCN8": 1, "3uE2pXvbcnS9nnZRE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eSfPGQjArWXWXu99Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 7, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "25132", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 191, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JGBxoM4poNEzvmFBJ", "LzMMCnmePFKpmD6ki"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-30T00:58:31.461Z", "modifiedAt": null, "url": null, "title": "Critiquing Gary Taubes, Part 3: Did the US Government Give Us Absurd Advice About Sugar?", "slug": "critiquing-gary-taubes-part-3-did-the-us-government-give-us", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:37.895Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LzMMCnmePFKpmD6ki/critiquing-gary-taubes-part-3-did-the-us-government-give-us", "pageUrlRelative": "/posts/LzMMCnmePFKpmD6ki/critiquing-gary-taubes-part-3-did-the-us-government-give-us", "linkUrl": "https://www.lesswrong.com/posts/LzMMCnmePFKpmD6ki/critiquing-gary-taubes-part-3-did-the-us-government-give-us", "postedAtFormatted": "Monday, December 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Critiquing%20Gary%20Taubes%2C%20Part%203%3A%20Did%20the%20US%20Government%20Give%20Us%20Absurd%20Advice%20About%20Sugar%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACritiquing%20Gary%20Taubes%2C%20Part%203%3A%20Did%20the%20US%20Government%20Give%20Us%20Absurd%20Advice%20About%20Sugar%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLzMMCnmePFKpmD6ki%2Fcritiquing-gary-taubes-part-3-did-the-us-government-give-us%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Critiquing%20Gary%20Taubes%2C%20Part%203%3A%20Did%20the%20US%20Government%20Give%20Us%20Absurd%20Advice%20About%20Sugar%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLzMMCnmePFKpmD6ki%2Fcritiquing-gary-taubes-part-3-did-the-us-government-give-us", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLzMMCnmePFKpmD6ki%2Fcritiquing-gary-taubes-part-3-did-the-us-government-give-us", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1789, "htmlBody": "<p><strong>Previously: </strong><a href=\"/lw/jdw/critiquing_gary_taubes_part_1_mainstream/\">Mainstream Nutrition Science on Obesity</a>, <a href=\"/lw/je4/critiquing_gary_taubes_part_2_atkins_redux/\">Atkins Redux</a></p>\n<p>Here's where I start talking about the thing that initially drove me to write this post series: Taubes' repeated misrepresentation of the views of the mainstream nutrition authorities he attacks. I'll start by going back to Taubes' 2002 article. Immediately after the discussion of Atkins, it contains another set of claims that stood out to me as a huge red flag:&nbsp;</p>\n<blockquote>\n<p>Thirty years later, America has become weirdly polarized on the subject of weight. On the one hand, we've been told with almost religious certainty by everyone from the surgeon general on down, and we have come to believe with almost religious certainty, that obesity is caused by the excessive consumption of fat, and that if we eat less fat we will lose weight and live longer. On the other, we have the ever-resilient message of Atkins and decades' worth of best-selling diet books, including ''The Zone,'' ''Sugar Busters'' and ''Protein Power'' to name a few. All push some variation of what scientists would call the alternative hypothesis: it's not the fat that makes us fat, but the carbohydrates, and if we eat less carbohydrates we will lose weight and live longer.</p>\n<p>The perversity of this alternative hypothesis is that it identifies the cause of obesity as precisely those refined carbohydrates at the base of the famous Food Guide Pyramid -- the pasta, rice and bread -- that we are told should be the staple of our healthy low-fat diet, and then on the sugar or corn syrup in the soft drinks, fruit juices and sports drinks that we have taken to consuming in quantity if for no other reason than that they are fat free and so appear intrinsically healthy. While the low-fat-is-good-health dogma represents reality as we have come to know it, and the government has spent hundreds of millions of dollars in research trying to prove its worth, the low-carbohydrate message has been relegated to the realm of unscientific fantasy.</p>\n</blockquote>\n<p>I'll start with the obvious: <em>We </em>thought sugary soft drinks were intrinsically healthy? To quote an old joke, who do you mean we, kemosabe? Given widespread scientific illiteracy, I wouldn't be surprised if <em>some </em>people have believed that low-fat is a sufficient condition for being healthy, but if so, they didn't get this idea from mainstream nutrition science.</p>\n<p><a id=\"more\"></a>Taubes makes it sound the mainstream view and the view Atkins pushed are mirror images of each other: on the one hand, \"everyone from the surgeon general on down\" told us \"if we eat less fat we will lose weight and live longer.\" On the other hand, Atkins et al. told us \"if we eat less carbohydrates we will lose weight and live longer.\"</p>\n<p>This rhetoric ignores a crucial distinction: as I showed in my previous post, Atkins <em>really did</em> claim eating less carbs was a <em>sufficient </em>condition for weight loss, and that no amount of fat could possibly make us fat. Mainstream nutrition experts, on the other hand, did argue low fat diets were better for us, all else being equal. But they never, so far as I've been able to find, claimed a low-fat diet was a sufficient condition for losing weight, or that no amount of sugar could possibly make make us fat.</p>\n<p>Taubes seems unaware of this&mdash;or else he chooses to hide it from his readers. In <em><a href=\"http://www.amazon.com/Good-Calories-Bad-Controversial-Science/dp/1400033462\">Good Calories, Bad Calories</a> </em>(p. 342),<em>&nbsp;</em>for example,<em>&nbsp;</em>he attempts to rebut the suggestion that low-carb diets are really low-calorie diets in disguise that this idea \"seems to contradict the underlying principle of low-fat diets for weight control and the notion that we get obese because we overeat on the dense calories of fat in our diets.\" This response would only make sense if mainstream nutrition scientists were saying eating less fat is the be-all, end-all of weight loss.</p>\n<p>In these initial paragraphs, Taubes only cites one source for what mainstream nutrition experts were supposedly telling us: the USDA's&nbsp;<a href=\"http://en.wikipedia.org/wiki/History_of_USDA_nutrition_guides\">Food Guide Pyramid</a>. This is an unfortunate choice, because as anyone who's actually seen the chart&nbsp;knows, sweets get put right up at the top with fats and oils under the \"use sparingly\" category. At this point in the article, I wonder how anyone reading it could avoid suspecting something was. (The same goes for the references to the Food Pyramid in Taubes' books on nutrition, both of which quote the \"use sparingly\" recommendation in regard to fats and oils, while carefully omitting the fact that it said the same thing about sweets.)</p>\n<p>But then again, the Food Pyramid was first published the year I entered kindergarten, so I'm at exactly the right age to have had it drilled into my head hard in school. And maybe mainstream nutrition messaging was much crazier in the 70's and 80's. So what about the other sources Taubes cites as supposedly showing mainstream nutritionists giving giving us terrible advice about fat vs. sugar?&nbsp;</p>\n<p>Farther down in the article, Taubes talks about how the debate over the 1977 Senate committee report <a href=\"http://zerodisease.com/archive/Dietary_Goals_For_The_United_States.pdf\">\"Dietary Goals for the United States\"</a>&nbsp;supposedly tried to settle the debate over what Taubes calls the \"low-fat-is-good-health dogma\" with politics rather than science.&nbsp;Yet a quick look at the report reveals it's statements to the effect that fat is dangerous and we should eat less of it are consistently paired with parallel statements about sugar, including a recommendation to cut our sugar intake by 40 percent. Instead, the report recommends, we should be eating more fruits, vegetables, and whole grains.</p>\n<p>Okay, what about the sources Taubes cites in his books?&nbsp;Three important ones are summarized in the chapter on sugar in <em>Good Calories, Bad Calories</em>:</p>\n<blockquote>\n<p>In 1986, the FDA exonerated sugar of any nutritional crimes on the basis that \"no conclusive evidence demonstrates a hazard.\" The two-hundred-page report constituted a review of hundreds of articles on the health aspects of sugar, many of which reported that sugar had a range of potentially adverse metabolic effects related to a higher risk of heart disease and diabetes. The FDA interpreted the evidence as inconclusive. Health reporters, the sugar industry, and public-health authorities therefore perceived the FDA report as absolving sugar of having any deleterious effects on our health.</p>\n<p>The identical message was passed along in the 1988 Surgeon General&rsquo;s Report on Nutrition and Health and the 1989 National Academy of Sciences Diet and Health report.</p>\n</blockquote>\n<p>Let's start with the FDA report. If you read the <a href=\"http://ge.tt/7Fh2mJ71/v/0?c\">executive summary</a>&nbsp;(which I obtained thanks to a generous individual on&nbsp;<a href=\"http://www.reddit.com/r/scholar\">r/scholar</a>), a couple things stand out. They continually emphasize that they were interested in whether sugar was harmful at <em>current </em>levels (i.e. in 1986, more than 25 years ago), and whether sugar had a <em>unique </em>role in the cause of obesity. There's nothing to suggest that increasing our sugar consumption (which we in fact did, along with increasing our intake of calories in general) would be harmless.</p>\n<p>The <a href=\"http://profiles.nlm.nih.gov/NN/B/C/Q/G/\">1988 Surgeon General's report</a> is even less impressive as an example of supposed official sanction for sugar consumption. The summary of recommendations (p. 3) recommends that people&nbsp;\"Reduce consumption of fat (especially saturated fat) and cholesterol,\" but recommends replacing high-fat foods not with just any carbohydrates, but with whole grains, fruits, and vegetables. It also recommends, on the subject of weight control, that people \"limit consumption of foods relatively high in calories, fats, and sugars, and to minimize alcohol consumption.\" Similarly, the&nbsp;<a href=\"http://www.nap.edu/openbook.php?record_id=1222&amp;page=R1\">1989 National Academy of Sciences Diet and Health report</a>&nbsp;(p. 18) explicitly recommends replacing fat with whole grains rather than food and drinks containing added sugars.&nbsp;</p>\n<p>One other example bears mentioning. In <em><a href=\"http://www.amazon.com/Why-We-Get-Fat-About/dp/0307474259\">Why We Get Fat</a>, </em>Taubes claims:</p>\n<blockquote>\n<p>[Low-fat] logic may have reached the pinnacle of absurdity in 1995 (at least I hope it did), when the American Heart Association published a pamphlet suggesting that we can eat virtually anything with impunity&mdash;even candy and sugar&mdash;as long as it's low in fat: \"To control the amount and kind of fat, saturated fatty acids and dietary cholesterol you eat,\" the AHA counseled, \"choose snacks from other food groups such as... low-fat cookies, low-fat crackers... unsalted pretzels, hard candy, gum drops, sugar, syrup, honey, jam, jelly, marmalade (as spreads).\" (p. 162)</p>\n</blockquote>\n<p>The pamphlet Taubes is referring to can be found <a href=\"http://acroeng.adobe.com/Test_Files/pdf_versions/PDF%201.1//AHADIET2.PDF\">here</a>. The two halves of the above \"quote\" (\"To control the amount and kind of fat, saturated fatty acids and dietary cholesterol you eat\" and the second part that begins \"chose snacks from other food groups...\") are separated by a dozen pages of boringly mainstream advice which closely resembles that of the Food Guide Pyramid: no more than 6 ounces of lean meat per day; 5 or more servings of fruits and vegetables a day; 2 or more servings of dairy; and 6 or more servings of bread, cereals, pasta, and starchy vegetables.</p>\n<p>The part that Taubes ridicules about low-fat cookies and so on comes from a section on snacks that doesn't come with a recommended number of daily servings. I suppose if you read the AHA pamphlet knowing nothing else about nutrition, you could take that as a sign that the listed snacks are wonderfully healthy and you should eat as much of them as you like. But anyone familiar with the standard nutrition advice of the time would understand that the intended meaning is \"if you snack, choose the low-fat options\"&mdash;not that you should necessarily be snacking much at all. That may or may not have been good advice, but it's not nearly so absurd as Taubes makes it out to be.</p>\n<p>It's possible there's room here to criticize not the underlying science, but the science communication; not what the official reports and said, but how well that information was conveyed to the general public, few of whom are likely to have read the original reports carefully. If a significant number of people really did believe eating less fat was all they needed to do to lose weight and that Coca-Cola is \"intrinsically healthy\" (as Taubes claims \"we\" believed), we'd have an example of a serious failure of science communication.</p>\n<p>Any such criticisms of science communication side of things would need to be tempered with recognition that science communication is <em>really freaking&nbsp;hard. </em>If you haven't seen the amount of hand-wringing that's gone on in the science and skepticism blogosphere over how to do science communication better, trust me: scientists have thought a lot about this stuff, and it's not obvious what the solutions are. Yet if bad science communication were really, say, a major contributing factor in the obesity epidemic, it would underline the need for scientists to do their absolute best in communicating with the general public.</p>\n<p>But a failure of science communication isn't how Taubes frames his attack on mainstream nutrition authorities, nor is it the message most people seem to take away from reading his work. Taubes' irresponsible rhetoric doesn't help the problem of bad science communication&mdash;it adds to it.</p>\n<p><strong>Next: </strong><a href=\"/lw/jdx/critiquing_gary_taubes_part_4_what_causes_obesity/\">What Causes Obesity?</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"92SxJsDZ78ApAGq72": 1, "3uE2pXvbcnS9nnZRE": 1, "yt9Z7xdQrofW7fCN8": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LzMMCnmePFKpmD6ki", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 8, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "25133", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 153, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JGBxoM4poNEzvmFBJ", "eSfPGQjArWXWXu99Z", "pHGP4PS43TbZKNnrj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-30T01:47:09.202Z", "modifiedAt": null, "url": null, "title": "Worse than Worthless", "slug": "worse-than-worthless", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:34.142Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xtP2T2erPv3a6RyND/worse-than-worthless", "pageUrlRelative": "/posts/xtP2T2erPv3a6RyND/worse-than-worthless", "linkUrl": "https://www.lesswrong.com/posts/xtP2T2erPv3a6RyND/worse-than-worthless", "postedAtFormatted": "Monday, December 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Worse%20than%20Worthless&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWorse%20than%20Worthless%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxtP2T2erPv3a6RyND%2Fworse-than-worthless%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Worse%20than%20Worthless%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxtP2T2erPv3a6RyND%2Fworse-than-worthless", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxtP2T2erPv3a6RyND%2Fworse-than-worthless", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 428, "htmlBody": "<p>There are things that are worthless-- that provide no value. There are also things that are worse than worthless-- things that provide <em>negative</em>&nbsp;value. I have found that people sometimes confuse the latter for the former, which can carry potentially dire consequences.</p>\n<p>One simple example of this is in fencing. I once fenced with an opponent who put a bit of an unnecessary twirl on his blade when recovering from each parry. After our bout, one of the spectators pointed out that there wasn't any point to the twirls and that my opponent would improve by simply not doing them anymore. My opponent claimed that, even if the twirls were unnecessary, at worst they were merely an aesthetic preference that was useless but not actually harmful.</p>\n<p>However, the observer explained that <em>any</em> unnecessary movement is harmful in fencing, because it spends time and energy that could be put to better use-- even if that use is just recovering a split second faster! [1]</p>\n<p>During our bout, I indeed scored at least one touch because my opponent's twirling recovery was slower than a less flashy standard movement. That touch could well be the difference between victory and defeat; in a real sword fight, it could be the difference between life and death.</p>\n<p>This isn't, of course, to say that <em>everything</em>&nbsp;unnecessary is damaging. There are many things that we can simply be indifferent towards. If I am about to go and fence a bout, the color of the shirt that I wear under my jacket is of no concern to me-- but if I had spent significant time before the bout debating over what shirt to wear instead of training, it would become a damaging detail rather than a meaningless one.</p>\n<p>In other words, the real damage is dealt when something is not only unnecessary, but&nbsp;<em>consumes resources that could instead be used for productive tasks. </em>We see this relatively easily when it comes to matters of money, but when it comes to wastes of time and effort, many fail to make the inductive leap.</p>\n<p>&nbsp;</p>\n<p>[1] <a href=\"http://en.wikipedia.org/wiki/Miyamoto_Musashi\">Miyamoto Musashi</a> agrees:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The primary thing when you take a sword in your hands is your intention to cut the enemy, whatever the means. Whenever you parry, hit, spring, strike or touch the enemy's cutting sword, you must cut the enemy in the same movement. It is essential to attain this. If you think only of hitting, springing, striking or touching the enemy, you will not be able actually to cut him. More than anything, you must be thinking of carrying your movement through to cutting him. You must thoroughly research this.</span></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xtP2T2erPv3a6RyND", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 21, "extendedScore": null, "score": 1.489251413905381e-06, "legacy": true, "legacyId": "25153", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-30T05:09:54.547Z", "modifiedAt": null, "url": null, "title": "What is the Main/Discussion distinction, and what should it be?", "slug": "what-is-the-main-discussion-distinction-and-what-should-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:32.116Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5nQom4JJJSGESuSMg/what-is-the-main-discussion-distinction-and-what-should-it", "pageUrlRelative": "/posts/5nQom4JJJSGESuSMg/what-is-the-main-discussion-distinction-and-what-should-it", "linkUrl": "https://www.lesswrong.com/posts/5nQom4JJJSGESuSMg/what-is-the-main-discussion-distinction-and-what-should-it", "postedAtFormatted": "Monday, December 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20the%20Main%2FDiscussion%20distinction%2C%20and%20what%20should%20it%20be%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20the%20Main%2FDiscussion%20distinction%2C%20and%20what%20should%20it%20be%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5nQom4JJJSGESuSMg%2Fwhat-is-the-main-discussion-distinction-and-what-should-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20the%20Main%2FDiscussion%20distinction%2C%20and%20what%20should%20it%20be%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5nQom4JJJSGESuSMg%2Fwhat-is-the-main-discussion-distinction-and-what-should-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5nQom4JJJSGESuSMg%2Fwhat-is-the-main-discussion-distinction-and-what-should-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 651, "htmlBody": "<p>Near the beginning of this year Wei Dai <a href=\"/lw/gof/great_rationality_posts_by_lwers_not_posted_to_lw/8h10\">asked</a> why certain people don't post to LessWrong more often, and Yvain&nbsp;<a href=\"/lw/gof/great_rationality_posts_by_lwers_not_posted_to_lw/8h2b\">replied</a>&nbsp;that:</p>\n<blockquote>\n<p>Less Wrong requires no politics / minimal humor / definitely unambiguously rationality-relevant / careful referencing / airtight reasoning (as opposed to a sketch of something which isn't exactly true but points to the truth.) This makes writing for Less Wrong a <em>chore</em> as opposed to an enjoyable pastime.</p>\n</blockquote>\n<p>But Kaj <a href=\"/lw/gof/great_rationality_posts_by_lwers_not_posted_to_lw/8h9w\">disagreed</a> that this was the actual standard:</p>\n<blockquote>\n<p>I agree with the \"no politics\" bit, but I don't think the rest are correct. I've certainly had \"sketch of something that isn't quite true but points in the right direction\" posts with no references and unclear connections to rationality promoted before (<a href=\"/lw/5r9/suffering_as_attentionallocational_conflict/\">example</a>), as well as ones plastered with unnecessary jokes (<a href=\"/lw/akr/i_was_not_almost_wrong_but_i_was_almost_right/\">example</a>).</p>\n</blockquote>\n<p>This raises two questions: what is the real standard, and what should the standard be?</p>\n<p>Because on the one hand, it's not clear Yvain is right, but on the other hand if he is right on the factual question, that standard seems way too high to me. It would suggest that, as John Maxwell <a href=\"/lw/gof/great_rationality_posts_by_lwers_not_posted_to_lw/8q6z\">says</a>&nbsp;in the same thread, \"The overwhelming LW moderation focus seems to be on stifling bad content. There's very little in place to encourage good content.\"</p>\n<p>The wiki sort-of answers the factual question:</p>\n<blockquote>\n<p>These traditionally go in Discussion:</p>\n<ul>\n<li>a link with minimal commentary&nbsp;</li>\n<li>a question or brainstorming opportunity for the Less Wrong community</li>\n</ul>\n<p>Beyond that, here are some factors that suggest you should post in Main:</p>\n<ul>\n<li>Your post discusses core Less Wrong topics.&nbsp;</li>\n<li>The material in your post seems especially important or useful.&nbsp;</li>\n<li>You put a lot of thought or effort into your post. (Citing studies, making diagrams, and agonizing over wording are good indicators of this.)&nbsp;</li>\n<li>Your post is long or deals with difficult concepts. (If a post is in Main, readers know that it may take some effort to understand.)&nbsp;</li>\n<li>You've searched the Less Wrong archives, and you're pretty sure that you're saying something new and non-obvious.</li>\n</ul>\n</blockquote>\n<p>But this isn't an entirely unambiguous answer: <strong>how many of the five \"factors\" does a post need to be in Main?</strong> Furthermore, it often seems that the \"real\" rules are significantly different than what the wiki says. Yvain's perception may be incorrect, but I think there were reasons why he (and presumably the people who upvoted his comment) had that perception. Also, Eliezer recently explained that:</p>\n<blockquote>\n<p>Whenever a non-meta post stays under 5, I always feel free to move it to Discussion, especially if an upvoted comment has also suggested it. I don't always, but often do.</p>\n</blockquote>\n<p>This makes me wonder what other poorly-publicized rules there are in this vicinity.</p>\n<p>As for what the rules <em>should </em>be, I'm going to limit myself to two general suggestions:</p>\n<ul>\n<li>The standard for posting in Main should&nbsp;not be so high that it makes posting at LessWrong feel like a chore, thereby chasing away good contributors like Yvain.</li>\n<li>The standard should not be so high that it would force any significant portion of Eliezer's original sequences off into Discussion.</li>\n</ul>\n<p>Finally, whatever standard we settle on, I think it's really important that we make it clearer to people what it is. Aside from the obvious benefits of doing that, I've found that trying to navigate the unclear Main/Discussion distinction is&nbsp;<em>itself</em>&nbsp;often enough to make blogging at LessWrong feel like a chore.</p>\n<p><em>Edited to add:</em>&nbsp;In terms of karma I'm currently the top contributor for the past 30 days on LessWrong by a wide margin. I managed this in spite of the fact that I'm in the middle of doing App Academy and have no time (this past week has been an exception because vacation). I take this not as evidence of how awesome I am, but as evidence that way too little quality content is being posted in Main.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5nQom4JJJSGESuSMg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 33, "extendedScore": null, "score": 0.000106, "legacy": true, "legacyId": "25155", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 103, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9XBhs3dS4XnBwZRan", "BmGrj9pRkcbJxae3x"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-31T17:06:56.060Z", "modifiedAt": null, "url": null, "title": "LWers living in Boulder/Denver area: any interest in an AI-philosophy reading group?", "slug": "lwers-living-in-boulder-denver-area-any-interest-in-an-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:07.429Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fowlertm", "createdAt": "2012-01-07T20:35:26.490Z", "isAdmin": false, "displayName": "fowlertm"}, "userId": "gDAt4FezxH8dDr5EY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2gersmndGi7totq3H/lwers-living-in-boulder-denver-area-any-interest-in-an-ai", "pageUrlRelative": "/posts/2gersmndGi7totq3H/lwers-living-in-boulder-denver-area-any-interest-in-an-ai", "linkUrl": "https://www.lesswrong.com/posts/2gersmndGi7totq3H/lwers-living-in-boulder-denver-area-any-interest-in-an-ai", "postedAtFormatted": "Tuesday, December 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LWers%20living%20in%20Boulder%2FDenver%20area%3A%20any%20interest%20in%20an%20AI-philosophy%20reading%20group%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALWers%20living%20in%20Boulder%2FDenver%20area%3A%20any%20interest%20in%20an%20AI-philosophy%20reading%20group%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2gersmndGi7totq3H%2Flwers-living-in-boulder-denver-area-any-interest-in-an-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LWers%20living%20in%20Boulder%2FDenver%20area%3A%20any%20interest%20in%20an%20AI-philosophy%20reading%20group%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2gersmndGi7totq3H%2Flwers-living-in-boulder-denver-area-any-interest-in-an-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2gersmndGi7totq3H%2Flwers-living-in-boulder-denver-area-any-interest-in-an-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 100, "htmlBody": "<p><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\">I'd like to put together an AI-philosophy reading group. Ideally we would dig right in to the technical and philosophical topics, not glossing over the details (meaning we might read technical reports, textbook chapters, theses, etc.), and meeting as often as convenient. It'd be best to keep the group as small as possible, and I'm willing to take a leadership role in organizing and presenting (though by no means do I insist on being the main voice).</span></p>\n<p><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\">Potential discussion topics: machine ethics, theory of superintelligence, friendliness in artificial agents, philosophy of logic, really anything related to the building of a mind.&nbsp;</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2gersmndGi7totq3H", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.4917629932332345e-06, "legacy": true, "legacyId": "25157", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-31T22:03:05.365Z", "modifiedAt": null, "url": null, "title": "Cognito Mentoring: An advising service for intellectually curious students", "slug": "cognito-mentoring-an-advising-service-for-intellectually", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.651Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hphGa6xfad3m4imCs/cognito-mentoring-an-advising-service-for-intellectually", "pageUrlRelative": "/posts/hphGa6xfad3m4imCs/cognito-mentoring-an-advising-service-for-intellectually", "linkUrl": "https://www.lesswrong.com/posts/hphGa6xfad3m4imCs/cognito-mentoring-an-advising-service-for-intellectually", "postedAtFormatted": "Tuesday, December 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cognito%20Mentoring%3A%20An%20advising%20service%20for%20intellectually%20curious%20students&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACognito%20Mentoring%3A%20An%20advising%20service%20for%20intellectually%20curious%20students%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhphGa6xfad3m4imCs%2Fcognito-mentoring-an-advising-service-for-intellectually%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cognito%20Mentoring%3A%20An%20advising%20service%20for%20intellectually%20curious%20students%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhphGa6xfad3m4imCs%2Fcognito-mentoring-an-advising-service-for-intellectually", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhphGa6xfad3m4imCs%2Fcognito-mentoring-an-advising-service-for-intellectually", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 266, "htmlBody": "<p>My name is <a href=\"http://jonahsinick.com/\">Jonah Sinick</a>, and I'm posting to announce a new advising service for intellectually curious students: <a href=\"http://cognitomentoring.org/\">Cognito Mentoring</a>. I'm working on this in collaboration with <a href=\"http://vipulnaik.com/\">Vipul Naik</a>.</p>\n<p>We have very broad intellectual interests, cutting across topics such as rationality, economics, pure math, psychology, humanitarian issues and classical music. I have a PhD in pure math, have been an <a href=\"/user/JonahSinick/submitted/\">active participant</a> on Less Wrong, worked at <a href=\"http://www.givewell.org\">GiveWell</a> for a year, and have done research for Machine Intelligence Research Institute (MIRI) on <a href=\"http://intelligence.org/2013/09/04/how-effectively-can-we-plan-for-future-decades/\">how effectively can we plan for future decades</a> and on how<a href=\"http://intelligence.org/2013/09/12/how-well-will-policy-makers-handle-agi-initial-findings/\"> well policy-makers will handle AGI</a>. Vipul has a PhD in pure math, and started <a href=\"http://openborders.info/\">Open Borders</a>, a website devoted to discussing immigration liberalization.</p>\n<p>We both have experience working with intellectually curious young people. I worked for three summers at <a href=\"http://www.mathpath.org/\">MathPath</a> (a summer camp for middle school students who are interested in math), taught at <a href=\"http://en.wikipedia.org/wiki/Thomas_Jefferson_High_School_for_Science_and_Technology\">Thomas Jefferson High School for Science and Technology</a> (an academic magnet high school), and currently teach for <a href=\"http://artofproblemsolving.com/\">Art of Problem Solving</a> (an online school for high performing math students). Vipul has trained students for mathematical olympiads, and taught calculus and linear algebra at University of Chicago for years.</p>\n<p>We spent several months researching the educational resources that are available to high performing students, college selection and college admissions, psychological findings on intellectual giftedness, and the experiences of past and current members of the population that we're serving, and we&rsquo;re ready to help. We're currently offering <strong>free personalized advising</strong> on these things by email, Skype, or phone. You can <a href=\"http://cognitomentoring.org/connect/\">connect with us here</a>. If you're interested, we look forward to hearing from you.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 1, "izp6eeJJEg9v5zcur": 1, "JsJPrdgRGRqnci8cZ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hphGa6xfad3m4imCs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 69, "baseScore": 84, "extendedScore": null, "score": 0.00024626251007829706, "legacy": true, "legacyId": "25142", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-12-31T22:04:47.781Z", "modifiedAt": null, "url": null, "title": "Critiquing Gary Taubes, Part 4: What Causes Obesity?", "slug": "critiquing-gary-taubes-part-4-what-causes-obesity", "viewCount": null, "lastCommentedAt": "2019-05-11T11:07:23.174Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pHGP4PS43TbZKNnrj/critiquing-gary-taubes-part-4-what-causes-obesity", "pageUrlRelative": "/posts/pHGP4PS43TbZKNnrj/critiquing-gary-taubes-part-4-what-causes-obesity", "linkUrl": "https://www.lesswrong.com/posts/pHGP4PS43TbZKNnrj/critiquing-gary-taubes-part-4-what-causes-obesity", "postedAtFormatted": "Tuesday, December 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Critiquing%20Gary%20Taubes%2C%20Part%204%3A%20What%20Causes%20Obesity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACritiquing%20Gary%20Taubes%2C%20Part%204%3A%20What%20Causes%20Obesity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpHGP4PS43TbZKNnrj%2Fcritiquing-gary-taubes-part-4-what-causes-obesity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Critiquing%20Gary%20Taubes%2C%20Part%204%3A%20What%20Causes%20Obesity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpHGP4PS43TbZKNnrj%2Fcritiquing-gary-taubes-part-4-what-causes-obesity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpHGP4PS43TbZKNnrj%2Fcritiquing-gary-taubes-part-4-what-causes-obesity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2632, "htmlBody": "<p><strong>Previously: </strong><a href=\"/lw/jdw/critiquing_gary_taubes_part_1_mainstream/\">Mainstream Nutrition Science on Obesity</a>, <a href=\"/lw/je4/critiquing_gary_taubes_part_2_atkins_redux/\">Atkins Redux</a>, <a href=\"/lw/je5/critiquing_gary_taubes_part_3_did_the_us/\">Did the US Government Give Us Absurd Advice About Sugar?</a></p>\n<p>In this post, I'm going to deal with an issue that's central to Gary Taubes' critique of mainstream nutrition science: what causes obesity?</p>\n<p>This is a post a post I found exceptionally difficult to write. You see, while his <a href=\"http://www.nytimes.com/2002/07/07/magazine/what-if-it-s-all-been-a-big-fat-lie.html?pagewanted=all&amp;src=pm\">2002 </a><em><a href=\"http://www.nytimes.com/2002/07/07/magazine/what-if-it-s-all-been-a-big-fat-lie.html?pagewanted=all&amp;src=pm\">New York Times</a> </em>article portrays mainstream nutrition science as promoting a&nbsp;simplistic mirror-image of the Atkins diet, his books do manage to talk about the mainstream view that if you consume more calories than you burn you'll gain weight... sort of. As I looked closely at the relevant chapters of those books, it became less and less clear what view he's attributing to mainstream experts, or what his alternative is supposed to be.</p>\n<p>Because this discussion may get confusing, I want to start by repeating what I said in my <a href=\"/lw/jdw/critiquing_gary_taubes_part_1_mainstream/\">first post</a>: the mainstream view is that people gain weight when they consume more calories than they burn, but both calorie intake and calorie expenditure are regulated by complicated mechanisms we don't fully understand yet.</p>\n<p>Yet Taubes goes on at great length about how obesity has other causes beyond simple calorie math&nbsp;<em>as if </em>this were somehow a refutation of mainstream nutrition science. So I'm going to provide a series of quotes from relevant sources to show that the experts are perfectly aware of that fact.&nbsp;All of the following sources are ones Taubes cites as examples of how absurd the views of mainstream nutrition experts supposedly are:<a id=\"more\"></a></p>\n<blockquote>\n<p>The etiology of obesity is multifactorial and includes genetic, neurohormonal, endocrine, metabolic and life-style-associated factors. Generally, obesity is a result of excess energy resulting from disturbances in the energy intake/expenditure equilibrium.</p>\n<p style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;\"><a href=\"http://ge.tt/7Fh2mJ71/v/0?c\">Report From FDA's Sugars Task Force.</a>&nbsp;1986. p. S14.</p>\n</blockquote>\n<blockquote>\n<p>The causes of obesity are incompletely understood, so that effective treatment is difficult. Obesity is the net result of an excess of energy consumption over expenditure. Factors that must be considered as contributing to causation are: (1) heredity, (2) primary overeating, (3) altered metabolism of adipose tissue, (4) defective or decreased thermogenesis (the process by which calories are converted into heat), (5) decreased physical activity without an appropriate reduction in food intake, and (6) certain prescribed medications. These potential causes can interact with one another. Of the six factors, individuals may have some control of overeating and underactivity.</p>\n<p><a href=\"http://profiles.nlm.nih.gov/NN/B/C/Q/G/\">The Surgeon General's Report on Nutrition and Health.</a>&nbsp;1988. p. 290.</p>\n</blockquote>\n<blockquote>\n<p>Positive energy balance can result from increased energy intake, reduced energy expenditure, or both, and over the long term, can lead to obesity and its associated complications... Obesity is enhanced not only by this energy imbalance but also by a genetic predisposition to obesity and altered metabolic efficiency... The specific causes of obesity are not well known, although some obese people clearly consume more energy compared to people of normal weight, whereas others are very sedentary or may have increased metabolic efficiency.</p>\n<p><a href=\"http://www.nap.edu/openbook.php?record_id=1222&amp;page=R1\">1989 National Academy of Sciences Diet and Health report</a>. 1989.&nbsp;p. 10.</p>\n</blockquote>\n<blockquote>\n<p>Maintenance of a normal body weight requires a match of food intake to energy expenditure... Both nutrient intake and energy expenditure are regulated by a complex interaction between the periphery and the central nervous system. Although not all aspects of central-peripheral interactions involved in energy balance are understood, key factors have been identified. For example, leptin from the adipocyte, ghrelin from the stomach, peptide YY from the gut, and insulin from the pancreas are all involved in the central regulation of energy balance. In the brain, more than a dozen peptides have been implicated in appetite and satiety.</p>\n<p><a href=\"https://www.google.com/search?q=joslin's+diabetes+mellitus+obesity&amp;btnG=Search+Books&amp;tbm=bks&amp;tbo=1\">Article on obesity on </a><em><a href=\"https://www.google.com/search?q=joslin's+diabetes+mellitus+obesity&amp;btnG=Search+Books&amp;tbm=bks&amp;tbo=1\">Joslin's Diabetes Mellitus</a>. </em>p. 536.</p>\n</blockquote>\n<p>There's also the <em>Handbook of Obesity </em>(whose first edition sadly does not appear to be easily accessible online)<em>, </em>which I attempt won't quote from because it devotes dozens of chapters to the etiology of obesity, including chapters titled \"The Genetics of Human Obesity,\" \"Behavioral Neuroscience of Obesity,\" and \"Endocrine Determinates of Obesity.\"</p>\n<p>Now what exactly is Taubes' objection to the above statements? It's easy to find answers to this question in his books. It's less easy to reconcile all the different answers with each other. At times, he seems to suggest the above statements are self-contradictory, such as when he gives the following example of an \"apparent contradiction\" (in <em><a href=\"http://www.amazon.com/Good-Calories-Bad-Controversial-Science/dp/1400033462\">Good Calories, Bad Calories</a> </em>on p. 271):</p>\n<blockquote>\n<p>It may be true that, &ldquo;for the vast majority of individuals, overweight and obesity result from excess calorie consumption and/or inadequate physical activity,&rdquo; as the Surgeon General&rsquo;s Office says, but it also seems that the accumulation of fat on humans and animals is determined to a large extent by factors that have little to do with how much we eat or exercise, that it has a biologic component.</p>\n</blockquote>\n<p>At times like this, Taubes reminds me of the biologists who Ernst Mayr chided in his paper <a href=\"http://isites.harvard.edu/fs/docs/icb.topic100452.files/Mayr_E._1961._Science_V134.pp1501-1506.pdf\">\"Cause and Effect in Biology</a>\" for failing to realize that biological phenomenon can be cause on multiple levels. Mayr quotes an example:</p>\n<blockquote>\n<p>The earlier writers explained the growth of the legs in the tadpole of the frog or toad as a case of adaptation to life on land. We know through Gundernatsch that the growth of the legs can be produced at any time even in the youngest tadpole, which is unable to live on land, by feeding the animal with the thyroid gland.</p>\n</blockquote>\n<p>Just as there's no contradiction between thinking leg growth in tadpoles is controlled by hormones, and thinking this mechanism is an evolutionary adaptation, there's no contradiction between thinking weight gain is the result of consuming more calories than you burn, and also thinking that there are a lot of different factors that influence calorie intake and expenditure.</p>\n<p>But maybe Taubes doesn't mean to suggest there's a contradiction there. He goes to great lengths to assure his readers he isn't rejecting the laws of thermodynamics. Furthermore, he doesn't seem interested in claiming any loopholes in the basic calories-in, calories-out math along the lines of Atkins' ketones-in-the-urine hypothesis. Instead, the idea often seems to be that the calories-in, calories-out idea is true but trivial. <em><a href=\"http://www.amazon.com/Why-We-Get-Fat-About/dp/0307474259\">Why We Get Fat</a>&nbsp;</em>(p. 74)&nbsp;offers this analogy:</p>\n<blockquote>\n<p>Imagine that, instead of talking about why we get fat, we're talking about why a room gets crowded...</p>\n<p>If you asked me this question and I said, <em>Well, because more people entered the room than left it,</em> you'd probably think I was being a wise guy or an idiot. <em>Of course more people entered than left,</em> you'd say. <em>That's obvious. But why?</em></p>\n</blockquote>\n<p>This is a poor analogy, because the fact the importance of calories in weight gain is far less obvious than the importance of people in a room's getting crowded. Imagine: what if it had turned out that it's the total mass of your food that matters? Or just the total grams of fat? Or just the total grams of carbs? Or the phlogiston content?</p>\n<p>A poorly-chosen analogy, though, is a minor problem compared to the false implication that mainstream that obesity researchers have ignored the factors that influence calorie intake and expenditure. This is a claim that Taubes makes explicit in other cases, for example:</p>\n<p><em>Good Calories, Bad Calories </em>(p. 295)<em>:</em></p>\n<blockquote>\n<p>What may be the single most incomprehensible aspect of the last half-century of obesity research is the failure of those involved to grasp the fact that both hunger and sedentary behavior can be driven by a metabolic-hormonal disposition to grow fat, just as a lack of hunger and the impulse to engage in physical activity can be driven by a metabolic-hormonal disposition to burn calories rather than store them.</p>\n</blockquote>\n<p><em>Why We Get Fat </em>(pp. 80-81):</p>\n<blockquote>\n<p>Of all the dangerous ideas that health officials could have embraced while trying to understand why we get fat, they would have been hard-pressed to find one ultimately more damaging that calories-in/calories-out... it's misleading and misconceived on so many levels that it's hard to imagine how it survived unscathed and virtually unchallenged for the last fifty years...</p>\n<p>There has to be a reason, of course, why anyone would eat more calories than he or she expends, particularly since the penalty for doing so is to suffer the physical and emotional cruelties of obesity. There must be a defect involved somewhere; the question is where.</p>\n<p>The logic of calories-in/calories-out allows only one acceptable answer to this question. The defect cannot lie in the body-perhaps, as the endocrinologist Edwin Astwood suggested half a century ago, in the \"dozens of enzymes\" and the \"variety of hormones\" that control how our bodies \"turn what is eaten into fat\"&mdash;because this would imply that something other than overeating was fundamentally responsible for making us fat. And that's not allowed. So the problem must lie in the brain. And, more precisely, in behavior, which makes it an issue of character.</p>\n</blockquote>\n<p>To which I reply: no, those involved in obesity research did <em>not </em>fail to grasp the factors that drive hunger and sedentary behavior, and there was no unchallenged dogma the causes of obesity can't lie in our bodies. <em>Read your own damn sources, Taubes.</em></p>\n<p>I wish I could end this post there, but there's a complication: what about those statements I talked about in part 2, that it's \"not a medical fact\" that losing weight requires cutting down on excess calories, and that dietary fat has no effect on fat accumulation in the body? Well, there <em>is</em> an explanation for those statements. It's something Taubes goes on at great length about in <em>Good Calories, Bad Calories, </em>but is perhaps most succinctly expressed in <em>Why We Get Fat </em>(p. 99):<em>&nbsp;</em>\"We don't get fat because we overeat; we overeat because we're getting fat.\"</p>\n<p>The part of me that's still trying to figure out how to be charitable to Taubes urges that <em>surely </em>that sentence&nbsp;wasn't meant to be taken too literally. To use Taubes' own analogy of the room getting crowded: it's one thing to say \"the room is getting crowded because more people are entering than leaving\" is too obvious to mention. It's another thing to say that that claim is false, and on the contrary it's the room getting crowded that's causing people to enter. (There's a sense in which that could be true given the phenomenon of <a href=\"http://en.wikipedia.org/wiki/Social_proof\">social proof</a>, but then we're talking about a feedback loop, not one-way causation.)</p>\n<p>So it's natural to assume Taubes is playing with meaning here a bit, using \"getting fat\" to refer not to the weight gain itself but a metabolic tendency to get fat, or something like that. Surely he still recognizes that how much we eat still has an effect on our weight, right? On the one hand it seems that he does: he talks about how calorie intake affects calorie expenditure, but he doesn't claim they march so closely in lockstep that it's&nbsp;<em>literally&nbsp;</em>impossible to lose weight by cutting calorie intake. His discussion of low-calorie diets plays up how unpleasant they are, but he does acknowledge people lose weight on them.&nbsp;</p>\n<p>On the other hand... Taubes seems <em>really </em>serious about this claim,&nbsp;portraying it as one of the fundamental mistakes of mainstream nutrition experts. From <em>Good Calories, Bad Calories </em>(p. 293):</p>\n<blockquote>\n<p>The first law of thermodynamics dictates that weight gain&mdash;the increase in energy stored as fat and lean-tissue mass&mdash;wil be accompanied by or associated with positive energy balance, but it does not say that it is caused by a positive energy balance&mdash;by &ldquo;a plethora of calories,&rdquo; as Russel Cecil and Robert Loeb&rsquo;s 1951 Textbook of Medicine put it. There is no arrow of causality in the equation. It is equally possible, without violating this fundamental truth, for a change in energy stores, the left side of the above equation, to be the driving force in cause and effect; some regulatory phenomenon could drive us to gain weight, which would in turn cause a positive energy balance&mdash;and thus overeating and/or sedentary behavior. Either way, the calories in wil equal the calories out, as they must, but what is cause in one case is effect in the other.</p>\n<p>All those who have insisted (and still do) that overeating and/or sedentary behavior must be the cause of obesity have done so on the basis of this same fundamental error: they will observe correctly that positive caloric balance must be associated with weight gain, but then they will assume without justification that positive caloric balance is the cause of weight gain. This simple misconception has led to a century of misguided obesity research.</p>\n</blockquote>\n<p>He even goes so far as to say (in <em>Why We Get Fat, </em>p. 76):</p>\n<blockquote>\n<p>The experts who say that we get fat <em>because</em> we overeat or we get fat <em>as a result</em> of overeating&mdash;the vast majority&mdash;are making the kind of mistake that would (or at least should) earn a failing grade in a high-school science class.</p>\n</blockquote>\n<p>So what's going on here? I think the answer lies Taubes' eagerness to portray mainstream nutrition experts as big meanies who blame fat people for being fat. I've already quoted him as saying that on the mainstream view, being overweight or obese <em>must </em>result from a defect of character. Just to drive the point home, in the same book he later says (p. 84):</p>\n<blockquote>\n<p>Much of the last half-century of professional discourse on obesity can be perceived as attempts to circumvent what we could call the \"head case\" implications of calories-in/calories-out: how to blame obesity on eating too much without actually blaming the fat person for the human weaknesses of self-indulgence and/or ignorance.</p>\n</blockquote>\n<p>So if you hear an advocate of the mainstream view claiming not to be a big meanie, don't believe them!</p>\n<p>But this puts Taubes in a bind: now if he says how much we eat has an effect on our weight, he's a big meanie too. It doesn't work for him to say fat people can't help overeating because of something wrong with their metabolism, and this in turn causes them to gain weight, because he's committed himself to the principle that blaming behavior equals blaming a character defect. So instead, we get wild rhetoric about how stupid the experts are with no coherent view underneath it.</p>\n<p>A more sensible approach would've been to emphasize that <a href=\"http://wiki.lesswrong.com/wiki/Akrasia\">akrasia</a>&nbsp;is an extremely common problem for humans, and that people who don't suffer from akrasia in regards to diet probably suffer from akrasia about something else. But that wouldn't have made for as an exciting of a book. Robin Hanson <a href=\"http://www.overcomingbias.com/2009/12/meh-transhumanism.html\">once commented</a> that \"few folks actually care much about the future except as a place to tell morality tales about who today is naughty vs. nice.\" I suspect this point generalizes. If you want to sell a book, flatter your audience and give them some villains to hate.</p>\n<p>I have no plans of discussing Taubes claims about carbohydrates having a unique ability to mess up the systems that regulate weight. For one thing, I don't have anything to add to <a href=\"http://wholehealthsource.blogspot.com/2011/08/carbohydrate-hypothesis-of-obesity.html\">what others have already said</a>. For another, one thing Taubes is definitely <em>not </em>claiming is, \"while obesity researchers have spent a great deal of time studying the mechanisms that regulate weight, they've completely failed to realize how badly carbs screw up these mechanisms.\"</p>\n<p>Instead, he accuses them of ignoring the relevant mechanisms entirely. This claim is so wildly untrue as to be grounds to doubt anything you think you learned from Taubes&mdash;indeed, to doubt any ideas you originally got from him <a href=\"/lw/s3/the_genetic_fallacy/\">even if you thought you later got confirmation for them elsewhere</a>.</p>\n<p>Closing thought: it's quite possible for a majority of the experts to be wrong. And I can even imagine finding a case somewhere where a non-expert rationally arrived at the correct answer when 95% of the experts are wrong&mdash;though <a href=\"/lw/iu0/trusting_expert_consensus/\">I've been unable to actually find such a case</a>. But when you see someone claiming that the vast majority of the experts have an <em>obviously </em>stupid view that should have earned them a failing grade in high school science, that is a <em>very </em>strong signal that you are dealing with a crackpot.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pHGP4PS43TbZKNnrj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": null}], "voteCount": 40, "baseScore": 9, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "25125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 122, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JGBxoM4poNEzvmFBJ", "eSfPGQjArWXWXu99Z", "LzMMCnmePFKpmD6ki", "KZLa74SzyKhSJ3M55", "R8YpYTq8LoD3k948L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-01T05:15:46.990Z", "modifiedAt": null, "url": null, "title": "Meetup : Sydney Meetup: January", "slug": "meetup-sydney-meetup-january-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taryneast", "createdAt": "2010-11-29T20:51:06.328Z", "isAdmin": false, "displayName": "taryneast"}, "userId": "xD8wjhiTvwbXdKirW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8StYBBvifKGtMpnwg/meetup-sydney-meetup-january-0", "pageUrlRelative": "/posts/8StYBBvifKGtMpnwg/meetup-sydney-meetup-january-0", "linkUrl": "https://www.lesswrong.com/posts/8StYBBvifKGtMpnwg/meetup-sydney-meetup-january-0", "postedAtFormatted": "Wednesday, January 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sydney%20Meetup%3A%20January&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sydney%20Meetup%3A%20January%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8StYBBvifKGtMpnwg%2Fmeetup-sydney-meetup-january-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sydney%20Meetup%3A%20January%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8StYBBvifKGtMpnwg%2Fmeetup-sydney-meetup-january-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8StYBBvifKGtMpnwg%2Fmeetup-sydney-meetup-january-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/v5'>Sydney Meetup: January</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 January 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Caffe Tiamo, 374 Pitt St, Sydney</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Full details TBD. We're going to try to have some discussion, a rationality exercise and a game. Details of which we're still nutting out on the Sydney-LW facebook group:\n<a href=\"https://www.facebook.com/groups/219526434802422/\" rel=\"nofollow\">https://www.facebook.com/groups/219526434802422/</a></p>\n\n<p>It should run from roughly 6:30pm to 9:30pm. We'll start with some socialising while people are still arriving, then begin at around 7PM</p>\n\n<p>Caffe Tiamo is a new venue for me - so I'm not sure where we''ll be meeting yet.</p>\n\n<p>I suspect I'll put up a sheet of paper with \"LessWrong\" and a picture of a paperclip so you can find us. I'll also tell the staff - so ask them to point out the \"less wrong meetup\" if you can't find me.</p>\n\n<p>You can also message me if you need more info - though I can't guarantee I'll respond in a timely manner. I recommend contacting the facebook group (above) first.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/v5'>Sydney Meetup: January</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8StYBBvifKGtMpnwg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.4925402421262231e-06, "legacy": true, "legacyId": "25159", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sydney_Meetup__January\">Discussion article for the meetup : <a href=\"/meetups/v5\">Sydney Meetup: January</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 January 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Caffe Tiamo, 374 Pitt St, Sydney</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Full details TBD. We're going to try to have some discussion, a rationality exercise and a game. Details of which we're still nutting out on the Sydney-LW facebook group:\n<a href=\"https://www.facebook.com/groups/219526434802422/\" rel=\"nofollow\">https://www.facebook.com/groups/219526434802422/</a></p>\n\n<p>It should run from roughly 6:30pm to 9:30pm. We'll start with some socialising while people are still arriving, then begin at around 7PM</p>\n\n<p>Caffe Tiamo is a new venue for me - so I'm not sure where we''ll be meeting yet.</p>\n\n<p>I suspect I'll put up a sheet of paper with \"LessWrong\" and a picture of a paperclip so you can find us. I'll also tell the staff - so ask them to point out the \"less wrong meetup\" if you can't find me.</p>\n\n<p>You can also message me if you need more info - though I can't guarantee I'll respond in a timely manner. I recommend contacting the facebook group (above) first.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sydney_Meetup__January1\">Discussion article for the meetup : <a href=\"/meetups/v5\">Sydney Meetup: January</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sydney Meetup: January", "anchor": "Discussion_article_for_the_meetup___Sydney_Meetup__January", "level": 1}, {"title": "Discussion article for the meetup : Sydney Meetup: January", "anchor": "Discussion_article_for_the_meetup___Sydney_Meetup__January1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-01T09:38:03.372Z", "modifiedAt": null, "url": null, "title": "New Year's Prediction Thread (2014) ", "slug": "new-year-s-prediction-thread-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:33.855Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ifdmTeZsmTWciNEFM/new-year-s-prediction-thread-2014", "pageUrlRelative": "/posts/ifdmTeZsmTWciNEFM/new-year-s-prediction-thread-2014", "linkUrl": "https://www.lesswrong.com/posts/ifdmTeZsmTWciNEFM/new-year-s-prediction-thread-2014", "postedAtFormatted": "Wednesday, January 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Year's%20Prediction%20Thread%20(2014)%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Year's%20Prediction%20Thread%20(2014)%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FifdmTeZsmTWciNEFM%2Fnew-year-s-prediction-thread-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Year's%20Prediction%20Thread%20(2014)%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FifdmTeZsmTWciNEFM%2Fnew-year-s-prediction-thread-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FifdmTeZsmTWciNEFM%2Fnew-year-s-prediction-thread-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 31, "htmlBody": "<p>It's time to look back to see what was predicted a year ago and how successfully it was.</p>\n<p>But even more, it's time for the fresh predictions for the following year, 2014.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ifdmTeZsmTWciNEFM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 1.4928201069819706e-06, "legacy": true, "legacyId": "25161", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-01T15:19:28.150Z", "modifiedAt": null, "url": null, "title": "January 2014 Media Thread", "slug": "january-2014-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:38.906Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/788LoJvHPW85Eva5n/january-2014-media-thread", "pageUrlRelative": "/posts/788LoJvHPW85Eva5n/january-2014-media-thread", "linkUrl": "https://www.lesswrong.com/posts/788LoJvHPW85Eva5n/january-2014-media-thread", "postedAtFormatted": "Wednesday, January 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20January%202014%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJanuary%202014%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F788LoJvHPW85Eva5n%2Fjanuary-2014-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=January%202014%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F788LoJvHPW85Eva5n%2Fjanuary-2014-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F788LoJvHPW85Eva5n%2Fjanuary-2014-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "788LoJvHPW85Eva5n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "25162", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-01T15:54:33.720Z", "modifiedAt": null, "url": null, "title": "Open thread for January 1-7, 2014", "slug": "open-thread-for-january-1-7-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:35.778Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yFNfY8yjEGg86n2DK/open-thread-for-january-1-7-2014", "pageUrlRelative": "/posts/yFNfY8yjEGg86n2DK/open-thread-for-january-1-7-2014", "linkUrl": "https://www.lesswrong.com/posts/yFNfY8yjEGg86n2DK/open-thread-for-january-1-7-2014", "postedAtFormatted": "Wednesday, January 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%20for%20January%201-7%2C%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%20for%20January%201-7%2C%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyFNfY8yjEGg86n2DK%2Fopen-thread-for-january-1-7-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%20for%20January%201-7%2C%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyFNfY8yjEGg86n2DK%2Fopen-thread-for-january-1-7-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyFNfY8yjEGg86n2DK%2Fopen-thread-for-january-1-7-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yFNfY8yjEGg86n2DK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.493222029079362e-06, "legacy": true, "legacyId": "25163", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 145, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-01T17:51:55.373Z", "modifiedAt": null, "url": null, "title": "What if Strong AI is just not possible?", "slug": "what-if-strong-ai-is-just-not-possible", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:36.128Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "listic", "createdAt": "2009-03-11T10:06:44.719Z", "isAdmin": false, "displayName": "listic"}, "userId": "qTfheLmwMdCtMb5ZT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9Xwj9RrLH7hvZAcJK/what-if-strong-ai-is-just-not-possible", "pageUrlRelative": "/posts/9Xwj9RrLH7hvZAcJK/what-if-strong-ai-is-just-not-possible", "linkUrl": "https://www.lesswrong.com/posts/9Xwj9RrLH7hvZAcJK/what-if-strong-ai-is-just-not-possible", "postedAtFormatted": "Wednesday, January 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20if%20Strong%20AI%20is%20just%20not%20possible%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20if%20Strong%20AI%20is%20just%20not%20possible%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Xwj9RrLH7hvZAcJK%2Fwhat-if-strong-ai-is-just-not-possible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20if%20Strong%20AI%20is%20just%20not%20possible%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Xwj9RrLH7hvZAcJK%2Fwhat-if-strong-ai-is-just-not-possible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Xwj9RrLH7hvZAcJK%2Fwhat-if-strong-ai-is-just-not-possible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<p>If Strong AI turns out to not be possible, what are our best expectations today as to why?</p>\n<p>I'm thinking of trying myself at writing a sci-fi story, do you think exploring this idea has positive utility? I'm not sure myself: it looks like the idea that intelligence explosion is a possibility could use more public exposure, as it is.</p>\n<p><span style=\"color: white;\">I wanted to include a popular <a style=\"color:white;\" href=\"http://i.imgur.com/Oyn4DZu.jpg\">meme image macro</a> here, but decided against it. I can't help it: every time I think \"what if\", I think of this guy.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9Xwj9RrLH7hvZAcJK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 11, "extendedScore": null, "score": 1.493347351622873e-06, "legacy": true, "legacyId": "25164", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 101, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-01T21:27:01.797Z", "modifiedAt": null, "url": null, "title": "One Year of Pomodoros", "slug": "one-year-of-pomodoros", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:44.001Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexvermeer", "createdAt": "2010-08-13T16:28:34.576Z", "isAdmin": false, "displayName": "alexvermeer"}, "userId": "3bK6aDQviGG3ovuDJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vXwjg5PpwSTfuHJ2k/one-year-of-pomodoros", "pageUrlRelative": "/posts/vXwjg5PpwSTfuHJ2k/one-year-of-pomodoros", "linkUrl": "https://www.lesswrong.com/posts/vXwjg5PpwSTfuHJ2k/one-year-of-pomodoros", "postedAtFormatted": "Wednesday, January 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20One%20Year%20of%20Pomodoros&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOne%20Year%20of%20Pomodoros%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvXwjg5PpwSTfuHJ2k%2Fone-year-of-pomodoros%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=One%20Year%20of%20Pomodoros%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvXwjg5PpwSTfuHJ2k%2Fone-year-of-pomodoros", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvXwjg5PpwSTfuHJ2k%2Fone-year-of-pomodoros", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 731, "htmlBody": "<p><small><em>(Pomodoros have been <a href=\"/r/discussion/lw/ha3/unlimited_pomodoro_works_my_scheduling_system/\">talked</a> about a <a href=\"/lw/gp4/the_power_of_pomodoros/\">bunch</a>&nbsp;on LW. I, <a href=\"/lw/gxr/rationality_habits_i_learned_at_the_cfar_workshop/\">like elharo</a>, first started using the technique after attending a CFAR workshop.&nbsp;</em></small><em style=\"font-size: 11.199999809265137px;\">Cross-posted from my&nbsp;<a title=\"One Year of Pomodoros\" href=\"http://alexvermeer.com/one-year-of-pomodoros/\">blog</a>.</em><em>)</em></p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Pomodoro_Technique\" target=\"_blank\">pomodoro technique</a>&nbsp;is, in short, starting a timer and doing 25 minutes of <strong>focused work on a single task without interruption</strong>, followed by a five minute break. Choose a new task, restart the timer, and repeat.</p>\n<p>Throughout 2013 I used pomodoros to execute on pretty much all of my life projects, organized into the following categories:</p>\n<ul>\n<li><strong>work</strong> &ndash; at&nbsp;<a href=\"http://intelligence.org\">MIRI</a></li>\n<li><strong>bizdev</strong> &ndash; other income-generating projects</li>\n<li><strong>growth</strong> &ndash; personal development projects (e.g. reading books, taking notes, making Anki decks; monthly reviews)</li>\n<li><strong>misc</strong> &ndash; miscellaneous life maintenance projects (e.g. banking stuff, knocking off a bunch of small todo&rsquo;s, house cleanup)</li>\n<li><strong>health</strong> &ndash; exercise projects (mostly climbing, some running, some misc other stuff)</li>\n</ul>\n<h2>The Result: 5,008 Pomodoros</h2>\n<p>The end result was 2,504 hours of recorded work&mdash;5,008 pomodoros in total:&nbsp;</p>\n<h3 style=\"text-align: center;\">Stacked Pomodoros by Week in 2013</h3>\n<p style=\"text-align: center;\"><a href=\"http://alexvermeer.com/wp-content/uploads/2013pomodoros.gif\"><img style=\"max-width:100%\" src=\"http://alexvermeer.com/wp-content/uploads/2013pomodoros-600x342.gif\" alt=\"2013pomodoros\" /></a></p>\n<p>A summary, by category (with hours in brackets):</p>\n<ul>\n<li><strong>work</strong> &ndash; 2,457 (1,228.5h) &ndash; 47.3 (23.7h) avg/week</li>\n<li><strong>bizdev</strong> &ndash; 700 (350h) &ndash; 13.5 (6.7h) avg/week</li>\n<li><strong>growth</strong> &ndash; 996 (498h) &ndash; 19.2 (9.6h) avg/week</li>\n<li><strong>misc</strong> &ndash; 448 (224h) &ndash; 8.6 (4.3h) avg/week</li>\n<li><strong>health</strong> &ndash; 407 (203.5h) &ndash; 7.8 (3.9h) avg/week</li>\n</ul>\n<p>Grand Total: <strong>5,008</strong> (2,504h) &ndash;<strong> 96.3</strong> (48.2h) avg/week</p>\n<h2>My version of the pomodoro technique</h2>\n<p>To be clear, I didn&rsquo;t use the pomodoro technique 100% faithfully. Certain things here, such as most Health (exercise) stuff, I never <em>actually</em> ran a pomodoro timer. But since I had a system for tracking where and how I spent my time, and since &ldquo;claiming&rdquo; all that time helped motivate me e.g. to climb regularly, I included them.</p>\n<p>Ways I deviate from the &ldquo;true&rdquo; pomodoro technique:</p>\n<ul>\n<li><em>I don&rsquo;t always take breaks.</em> For example, if I do two pomodoros, get in the zone, and work for another two hours straight, I&rsquo;d still record that as 6 pomodoros (3 hours) total.</li>\n<li><em>I don&rsquo;t always use a timer</em>. Sometimes I just start working, remembering to take small intermittent breaks, and record the total time in pomodoros (4h of work = 8 pomodoros).</li>\n<li><em>I don&rsquo;t record interruptions.</em> You&rsquo;re supposed to track all internal and external interruptions, but I don&rsquo;t bother with that. I merely try remain conscious of interruptions and eliminate/avoid them as much as possible.</li>\n<li><em>I don&rsquo;t let interruptions cancel out pomodoros.</em> Let&rsquo;s say I work for fifteen minutes and someone comes in to chat about something important that&rsquo;s been on their mind. I know that &ldquo;a pomodoro is indivisible&rdquo;, but screw it, I chat, and when the conversation ends I count a pomodoro after ten more minutes of work. Pomodoro blasphemy? Maybe.</li>\n<li><em>I don&rsquo;t always set targets.</em> I don&rsquo;t constantly set detailed pomodoro targets and track how many pomodoros were actually required. I only do this occasionally if I think my estimating ability is getting really off. I&nbsp;<em>do</em> set weekly pomodoro targets by category.</li>\n</ul>\n<h2>How did I track?</h2>\n<p>Near the end of 2012 I whipped up a simple web app that I use for tracking all of my pomodoros. Here&rsquo;s a sample screenshot from a week from earlier this year:</p>\n<p style=\"text-align: center;\"><img style=\"max-width:100%\" src=\"http://alexvermeer.com/wp-content/uploads/pomodoro-tracker-600x612.gif\" alt=\"pomodoro-tracker\" /></p>\n<p>Every pomodoro added is given a description, project, major area, and count. This way I can view all pomodoros by project, area, over a given date range, etc. (I&rsquo;m pretty sure there are other apps out there that let you do basically the same thing, but I haven&rsquo;t taken much time to explore them.)</p>\n<h2>Why I think it&rsquo;s worked really well for <em>me</em></h2>\n<p>Of all the productivity hacks I&rsquo;ve tried over the last decade, the pomodoro technique was,&nbsp;<em>for me</em>, the hands-down most effective technique.&nbsp;My thoughts on why the pomodoro technique has worked so well for me:</p>\n<ul>\n<li><em>It helps you start</em> &ndash; start the timer and then&nbsp;<em>just start working.&nbsp;</em>You&rsquo;ve already decided&nbsp;<em>what</em> to work on, so&nbsp;<em>just start already.</em></li>\n<li><em>It helps you focus on one thing at a time</em> &ndash; work on&nbsp;only one thing and&nbsp;<em>ignore</em> everything else.</li>\n<li><em>It helps you prioritize</em> &ndash; look at your lists/projects/tasks/whatever, pick the most important thing to work on, and then&nbsp;<em>just start already.</em></li>\n<li><em>It helps create <a href=\"/lw/3w3/how_to_beat_procrastination/\">success spirals</a></em> &ndash; when you have 5 successful pomodoros under your belt, it&rsquo;s motivation to keep going.</li>\n</ul>\n<p>In summary, if you haven&rsquo;t yet, I highly recommend giving the pomodoro technique a try.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"udPbn9RthmgTtHMiG": 1, "zcvsZQWJBFK6SxK4K": 1, "Qyeqh8wycbSapBNsp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vXwjg5PpwSTfuHJ2k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 34, "extendedScore": null, "score": 9.3e-05, "legacy": true, "legacyId": "25165", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small><em>(Pomodoros have been <a href=\"/r/discussion/lw/ha3/unlimited_pomodoro_works_my_scheduling_system/\">talked</a> about a <a href=\"/lw/gp4/the_power_of_pomodoros/\">bunch</a>&nbsp;on LW. I, <a href=\"/lw/gxr/rationality_habits_i_learned_at_the_cfar_workshop/\">like elharo</a>, first started using the technique after attending a CFAR workshop.&nbsp;</em></small><em style=\"font-size: 11.199999809265137px;\">Cross-posted from my&nbsp;<a title=\"One Year of Pomodoros\" href=\"http://alexvermeer.com/one-year-of-pomodoros/\">blog</a>.</em><em>)</em></p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Pomodoro_Technique\" target=\"_blank\">pomodoro technique</a>&nbsp;is, in short, starting a timer and doing 25 minutes of <strong>focused work on a single task without interruption</strong>, followed by a five minute break. Choose a new task, restart the timer, and repeat.</p>\n<p>Throughout 2013 I used pomodoros to execute on pretty much all of my life projects, organized into the following categories:</p>\n<ul>\n<li><strong>work</strong> \u2013 at&nbsp;<a href=\"http://intelligence.org\">MIRI</a></li>\n<li><strong>bizdev</strong> \u2013 other income-generating projects</li>\n<li><strong>growth</strong> \u2013 personal development projects (e.g. reading books, taking notes, making Anki decks; monthly reviews)</li>\n<li><strong>misc</strong> \u2013 miscellaneous life maintenance projects (e.g. banking stuff, knocking off a bunch of small todo\u2019s, house cleanup)</li>\n<li><strong>health</strong> \u2013 exercise projects (mostly climbing, some running, some misc other stuff)</li>\n</ul>\n<h2 id=\"The_Result__5_008_Pomodoros\">The Result: 5,008 Pomodoros</h2>\n<p>The end result was 2,504 hours of recorded work\u20145,008 pomodoros in total:&nbsp;</p>\n<h3 style=\"text-align: center;\" id=\"Stacked_Pomodoros_by_Week_in_2013\">Stacked Pomodoros by Week in 2013</h3>\n<p style=\"text-align: center;\"><a href=\"http://alexvermeer.com/wp-content/uploads/2013pomodoros.gif\"><img style=\"max-width:100%\" src=\"http://alexvermeer.com/wp-content/uploads/2013pomodoros-600x342.gif\" alt=\"2013pomodoros\"></a></p>\n<p>A summary, by category (with hours in brackets):</p>\n<ul>\n<li><strong>work</strong> \u2013 2,457 (1,228.5h) \u2013 47.3 (23.7h) avg/week</li>\n<li><strong>bizdev</strong> \u2013 700 (350h) \u2013 13.5 (6.7h) avg/week</li>\n<li><strong>growth</strong> \u2013 996 (498h) \u2013 19.2 (9.6h) avg/week</li>\n<li><strong>misc</strong> \u2013 448 (224h) \u2013 8.6 (4.3h) avg/week</li>\n<li><strong>health</strong> \u2013 407 (203.5h) \u2013 7.8 (3.9h) avg/week</li>\n</ul>\n<p>Grand Total: <strong>5,008</strong> (2,504h) \u2013<strong> 96.3</strong> (48.2h) avg/week</p>\n<h2 id=\"My_version_of_the_pomodoro_technique\">My version of the pomodoro technique</h2>\n<p>To be clear, I didn\u2019t use the pomodoro technique 100% faithfully. Certain things here, such as most Health (exercise) stuff, I never <em>actually</em> ran a pomodoro timer. But since I had a system for tracking where and how I spent my time, and since \u201cclaiming\u201d all that time helped motivate me e.g. to climb regularly, I included them.</p>\n<p>Ways I deviate from the \u201ctrue\u201d pomodoro technique:</p>\n<ul>\n<li><em>I don\u2019t always take breaks.</em> For example, if I do two pomodoros, get in the zone, and work for another two hours straight, I\u2019d still record that as 6 pomodoros (3 hours) total.</li>\n<li><em>I don\u2019t always use a timer</em>. Sometimes I just start working, remembering to take small intermittent breaks, and record the total time in pomodoros (4h of work = 8 pomodoros).</li>\n<li><em>I don\u2019t record interruptions.</em> You\u2019re supposed to track all internal and external interruptions, but I don\u2019t bother with that. I merely try remain conscious of interruptions and eliminate/avoid them as much as possible.</li>\n<li><em>I don\u2019t let interruptions cancel out pomodoros.</em> Let\u2019s say I work for fifteen minutes and someone comes in to chat about something important that\u2019s been on their mind. I know that \u201ca pomodoro is indivisible\u201d, but screw it, I chat, and when the conversation ends I count a pomodoro after ten more minutes of work. Pomodoro blasphemy? Maybe.</li>\n<li><em>I don\u2019t always set targets.</em> I don\u2019t constantly set detailed pomodoro targets and track how many pomodoros were actually required. I only do this occasionally if I think my estimating ability is getting really off. I&nbsp;<em>do</em> set weekly pomodoro targets by category.</li>\n</ul>\n<h2 id=\"How_did_I_track_\">How did I track?</h2>\n<p>Near the end of 2012 I whipped up a simple web app that I use for tracking all of my pomodoros. Here\u2019s a sample screenshot from a week from earlier this year:</p>\n<p style=\"text-align: center;\"><img style=\"max-width:100%\" src=\"http://alexvermeer.com/wp-content/uploads/pomodoro-tracker-600x612.gif\" alt=\"pomodoro-tracker\"></p>\n<p>Every pomodoro added is given a description, project, major area, and count. This way I can view all pomodoros by project, area, over a given date range, etc. (I\u2019m pretty sure there are other apps out there that let you do basically the same thing, but I haven\u2019t taken much time to explore them.)</p>\n<h2 id=\"Why_I_think_it_s_worked_really_well_for_me\">Why I think it\u2019s worked really well for <em>me</em></h2>\n<p>Of all the productivity hacks I\u2019ve tried over the last decade, the pomodoro technique was,&nbsp;<em>for me</em>, the hands-down most effective technique.&nbsp;My thoughts on why the pomodoro technique has worked so well for me:</p>\n<ul>\n<li><em>It helps you start</em> \u2013 start the timer and then&nbsp;<em>just start working.&nbsp;</em>You\u2019ve already decided&nbsp;<em>what</em> to work on, so&nbsp;<em>just start already.</em></li>\n<li><em>It helps you focus on one thing at a time</em> \u2013 work on&nbsp;only one thing and&nbsp;<em>ignore</em> everything else.</li>\n<li><em>It helps you prioritize</em> \u2013 look at your lists/projects/tasks/whatever, pick the most important thing to work on, and then&nbsp;<em>just start already.</em></li>\n<li><em>It helps create <a href=\"/lw/3w3/how_to_beat_procrastination/\">success spirals</a></em> \u2013 when you have 5 successful pomodoros under your belt, it\u2019s motivation to keep going.</li>\n</ul>\n<p>In summary, if you haven\u2019t yet, I highly recommend giving the pomodoro technique a try.</p>", "sections": [{"title": "The Result: 5,008 Pomodoros", "anchor": "The_Result__5_008_Pomodoros", "level": 1}, {"title": "Stacked Pomodoros by Week in 2013", "anchor": "Stacked_Pomodoros_by_Week_in_2013", "level": 2}, {"title": "My version of the pomodoro technique", "anchor": "My_version_of_the_pomodoro_technique", "level": 1}, {"title": "How did I track?", "anchor": "How_did_I_track_", "level": 1}, {"title": "Why I think it\u2019s worked really well for me", "anchor": "Why_I_think_it_s_worked_really_well_for_me", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "20 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Szwofei5zbYAmf2TW", "4iLk2rxTguFqHHs3Y", "ymwyTDc96uaAqZ48e", "RWo4LwFzpHNQCTcYt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-01T22:46:28.842Z", "modifiedAt": null, "url": null, "title": "Chapter 102: Caring", "slug": "chapter-102-caring", "viewCount": null, "lastCommentedAt": "2022-04-01T20:12:52.494Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vsASeE4KDP2nedgGv/chapter-102-caring", "pageUrlRelative": "/posts/vsASeE4KDP2nedgGv/chapter-102-caring", "linkUrl": "https://www.lesswrong.com/posts/vsASeE4KDP2nedgGv/chapter-102-caring", "postedAtFormatted": "Wednesday, January 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Chapter%20102%3A%20Caring&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChapter%20102%3A%20Caring%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsASeE4KDP2nedgGv%2Fchapter-102-caring%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Chapter%20102%3A%20Caring%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsASeE4KDP2nedgGv%2Fchapter-102-caring", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsASeE4KDP2nedgGv%2Fchapter-102-caring", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3885, "htmlBody": "<p><em>June 3rd, 1992.</em></p><p>Professor Quirrell was very sick.</p><p>He&#x27;d seemed better for a while, after drinking his unicorn&#x27;s blood in May, but the air of intense power which had surrounded him afterward hadn&#x27;t lasted even a day. By the Ides of May, Professor Quirrell&#x27;s hands had been trembling again, though subtly. The Defense Professor&#x27;s medical regimen had been interrupted too early, it seemed.</p><p>Six days ago Professor Quirrell had collapsed at dinnertime.</p><p>Madam Pomfrey had tried to forbid Professor Quirrell from teaching classes, and Professor Quirrell had shouted at her in front of everyone. The Defense Professor had shouted that he was dying regardless, and would use his remaining time as he chose.</p><p>So Madam Pomfrey, blinking hard, had forbidden the Defense Professor from doing anything <em>except</em> teaching his classes. She&#x27;d asked for a volunteer to help her take Professor Quirrell to a room in the Hogwarts infirmary. More than a hundred students had risen to their feet, only half wearing green.</p><p>The Defense Professor no longer sat at the Head Table during mealtimes. He didn&#x27;t cast spells during lessons. The oldest students who had the most Quirrell points helped him to teach, the seventh-years who had already sat their Defense N.E.W.T.s in May. They took turns floating him from his room in the infirmary to his classes, and brought him food at mealtimes. Professor Quirrell proctored his Battle Magic classes from a chair, sitting.</p><p>Watching Hermione die had hurt more than this, but that had ended much more quickly.</p><p><em>This is the true Enemy.</em></p><p>Harry had already thought that, after Hermione had died. Being forced to watch Professor Quirrell die, day by day, week by week, had not done much to change his mind.</p><p><em>This is the true Enemy I have to face,</em> Harry thought in Wednesday&#x27;s Defense class, watching Professor Quirrell leaning far to one side of his chair before that day&#x27;s seventh-year assistant caught him.<em> Everything else is just shadows and distraction.</em></p><p>Harry had been turning over Trelawney&#x27;s prophecy in his mind, wondering if maybe the true Dark Lord had nothing to do with Lord Voldemort at all. <em>Born to those who have thrice defied him </em>seemed to strongly invoke the Peverell brothers and the three Deathly Hallows - though Harry didn&#x27;t exactly see how Death could have marked him as an equal, which seemed to imply some sort of deliberate action on Death&#x27;s part.</p><p><em>This alone is the true Enemy,</em> Harry thought. <em>After this will come Professor McGonagall, Mum and Dad, even Neville in his time, unless the wound in the world can be healed before then.</em></p><p>There was nothing Harry could do. Madam Pomfrey was already doing for Professor Quirrell what magic could do, and magic seemed strictly superior to Muggle techniques when it came to healing.</p><p>There was nothing Harry could do.</p><p>Nothing he could do.</p><p>Nothing.</p><p>Nothing at all.</p><p>Harry raised his hand, and knocked upon the door, in case the person there could no longer detect him.</p><p>&quot;What is it?&quot; came a strained voice from the infirmary room.</p><p>&quot;It&#x27;s me.&quot;</p><p>There was a long pause. &quot;Come in,&quot; said that voice.</p><p>Harry slipped inside and closed the door behind him, and cast the Quieting Charm. He stood as far away from Professor Quirrell as he could, just in case his own magic was making the Professor feel uncomfortable.</p><p>Though the sense of doom was fading, fading with each passing day.</p><p>Professor Quirrell was lying back in his infirmary bed, only his head propped up by a pillow. A coverlet of cottony material, red with black stitching, covered him to his chest. A book hovered before his eyes, outlined in a pale glow which also surrounded a black cube lying by the bed. Not the Defense Professor&#x27;s own magic, then, but a device of some kind.</p><p>The book was <em>Thinking Physics</em> by Epstein, the same book Harry had lent to Draco a few months back. Harry had stopped fretting about its possible misuse several weeks earlier.</p><p>&quot;This -&quot; Professor Quirrell said, and coughed, it didn&#x27;t sound quite right. &quot;This is a fascinating book... if I&#x27;d ever realized...&quot; A laugh, mixed with another cough. &quot;Why did I assume the Muggle arts... must not be mine? That they would be... of no use to me? Why did I never bother trying... to test it experimentally... as you would say? In case... my assumption... was wrong? It seems sheerly foolish of me... in retrospect...&quot;</p><p>Harry was having more trouble speaking than Professor Quirrell was. Wordlessly, Harry reached into his pocket, and laid a kerchief on the floor; which he unfolded to reveal a small white pebble, smooth and round.</p><p>&quot;What&#x27;s that?&quot; said the Defense Professor.</p><p>&quot;It&#x27;s a, it&#x27;s a, Transfigured, unicorn.&quot;</p><p>Harry had checked the books, had learned that since he was too young to have sexual thoughts he would be able to approach a unicorn without fear. The same books had said nothing about unicorns being smart. Harry had already noticed that every intelligent magical species was at least partially humanoid, from merfolk to centaurs to giants, from elves to goblins to veela. All had essentially humanlike emotions, many were known to interbreed with humans. Harry had already reasoned out that magic didn&#x27;t create new intelligence but just changed the shape of genetically human beings. Unicorns were equinoid, were not even partially humanoid, didn&#x27;t talk, used no tools, they were almost certainly just magical horses. If it was right to eat a cow to feed yourself for a day, then it <em>had</em> to be right to drink a unicorn&#x27;s blood in order to stave off death for weeks. You couldn&#x27;t have it both ways.</p><p>So Harry had gone into the Forbidden Forest wearing his Cloak. He had searched the Grove of Unicorns until he saw her, a proud creature with a pure white coat and violet hair, with three blue blotches on her flank. Harry had gone over, and the sapphire eyes had stared at him inquisitively. Harry had tapped out the sequence 1-2-3 on the ground several times with his shoes. The unicorn had shown no sign of responding in kind. Harry had reached over, taken her hoof in his hand, and tapped the same sequence with the unicorn&#x27;s hoof. The unicorn had only looked at him curiously.</p><p>And something about feeding the unicorn the sleeping-potion-laced sugar cubes had still felt like murder.</p><p><em>That magic gives their existence a weight of meaning which no mere animal could possess... to slay something innocent to save oneself, that is a very grave sin.</em> Those two phrases, from Professor McGonagall, from the centaur, had both run through Harry&#x27;s mind, over and over as the white unicorn had yawned, laid down on the ground, and closed its eyes for what would be the last time. The Transfiguration had lasted an hour, and Harry&#x27;s eyes had watered repeatedly as he worked. The unicorn&#x27;s death might not have come then, but it would come soon enough, and it was foreign to Harry&#x27;s nature to try to refuse responsibility of any kind. Harry would just have to hope that, if you didn&#x27;t kill the unicorn to save yourself, if you did it to help a friend, it would be acceptable in the end.</p><p>Professor Quirrell&#x27;s eyebrows had climbed toward his hairline. His voice was less soft, had something of his normal sharpness, as he said, &quot;I forbid you from doing that again.&quot;</p><p>&quot;I wondered if you&#x27;d say that,&quot; Harry said. He swallowed again. &quot;But this unicorn is already, already doomed, so you might as well take it, Professor...&quot;</p><p>&quot;Why have you done this?&quot;</p><p>If the Defense Professor really didn&#x27;t understand that, he was slower on the uptake than anyone Harry had ever met. &quot;I kept thinking there was nothing I could do,&quot; Harry said. &quot;I got tired of thinking it.&quot;</p><p>Professor Quirrell closed his eyes. His head leaned back into the pillow. &quot;You were lucky,&quot; the Defense Professor said in a soft voice, &quot;that a unicorn in Transfigured form... did not set off the Hogwarts wards, as a strange creature... I shall have to... take this outside the grounds, to make use of it... but that can be managed. I shall tell them that I wish to look upon the lake... I will ask you to sustain the Transfiguration before you go, and it should last long enough, after that... and with my last strength, dispel whatever death-alarms were placed to watch over the herd... which, the unicorn being not yet dead, but only Transfigured, will not yet have triggered... you were very lucky, Mr. Potter.&quot;</p><p>Harry nodded. He started to speak, then stopped again. Words seemed to stick in his throat once more.</p><p><em>You already calculated the expected utilities, if it works, if it goes wrong. You assigned probabilities, you multiplied, and then you threw out the answer and went with your new gut feeling, which was the same. So say it.</em></p><p>&quot;Do you know,&quot; Harry said unsteadily, &quot;of any way at all, by which your life might be saved?&quot;</p><p>The Defense Professor&#x27;s eyes opened. &quot;Why... do you ask me that, boy?&quot;</p><p>&quot;There&#x27;s... a spell I heard of, a ritual -&quot;</p><p>&quot;Be silent,&quot; said the Defense Professor.</p><p>An instant later a snake lay in the bed.</p><p>Even the snake&#x27;s eyes were dull.</p><p>It did not rise.</p><p>&quot;<em>Sspeak on,&quot;</em> hissed that snake, its flickering tongue its only motion.</p><p>&quot;There is... <em>there iss a ritual, I heard of from the sschoolmasster, by which he thinkss the Dark Lord might have lived on. It iss called -&quot; </em>and Harry stopped, as he realized that he did know how to say the word in Parseltongue. &quot;<em>Horcrux. It requiress a death, I have heard. But if you are dying in any casse, you might try to adapt the ritual, even at great rissk for the new sspell, sso that it can be done with a different ssacrifice. It would change the whole world, if you ssucceed - though I don&#x27;t know anything about the sspell - the sschoolmasster thought it tore off a piece of ssoul, though I don&#x27;t ssee how that could be true -&quot;</em></p><p>The snake was hissing laughter, strange sharp laughter, almost hysterical. &quot;<em>You tell me of that sspell? Me? You musst learn more caution in the future, boy. But it matterss not. I learned of the horcrux sspell ssince long ago. It iss meaninglesss.&quot;</em></p><p>&quot;Meaningless?&quot; Harry said aloud in surprise.</p><p>&quot;<em>Would be pointlesss sspell from beginning, if ssoulss exissted. Tear piece of ssoul? That iss lie. Missdirection to hide true ssecret. Only one who doess not believe in common liess will reasson further, ssee beneath obsscuration, realisse how to casst sspell. Required murder iss not ssacrificial ritual at all. Ssudden death ssometimes makess ghosst, if magic burssts and imprintss on nearby thing. Horcrux sspell channelss death-bursst through casster, createss your own ghosst insstead of victim&#x27;ss, imprintss ghosst in sspecial device. Ssecond victim pickss up horcrux device, device imprintss your memoriess into them. But only memoriess from time horcrux device wass made. You ssee flaw?&quot;</em></p><p>The burning sensation was back in Harry&#x27;s throat. &quot;<em>No continuity of -</em>&quot; there wasn&#x27;t a snake word for consciousness &quot;- <em>sself, you would go on thinking after making the horcrux, then sself with new memoriess diess and iss not resstored -&quot;</em></p><p><em>&quot;Yess, you do ssee. Alsso Merlin&#x27;ss Interdict preventss powerful sspells from passing through ssuch a device, ssince it iss not truly alive. Dark Wizardss who think to return thuss are weaker, eassily disspatched. None have perssissted long by ssuch meanss. Perssonalitiess change, mix with victim&#x27;ss. Death iss not truly gainssaid. Real sself is losst, as you ssay. Not to my pressent tasste. Admit I conssidered it, long ago.&quot;</em></p><p>A man was lying in the infirmary bed once more. The Defense Professor breathed, then made a wretched coughing sound.</p><p>&quot;Can you give me a full recipe for the spell?&quot; Harry said, after a moment&#x27;s deliberation. &quot;There might be some way to improve on the flaws, with enough research. Some way to do it ethically and have it work.&quot; Like doing the transfer into a clone body with a blank brain, instead of an innocent victim, which might also improve the fidelity of the personality transfer... though that still left the other problems.</p><p>Professor Quirrell made a short sound, under his breath, that might have been laughter. &quot;You know, boy,&quot; Professor Quirrell whispered, &quot;I had thought... to teach you everything... the seeds of all the secrets I knew... from one living mind to another... so that later, when you found the right books, you would be able to understand... I would have passed on my knowledge to you, my heir... we would have begun as soon as you asked me... but you never asked.&quot;</p><p>Even the grief surrounding by Harry like thick water gave way to that, to the sheer magnitude of the missed opportunity. &quot;I was supposed to - ? I didn&#x27;t know I was supposed to - !&quot;</p><p>Another coughing chuckle. &quot;Ah yes... the unknowing Muggleborn... in heritage if not in blood... that is you. But I thought... better of it... that you should not walk my path... it was not a good path, in the end.&quot;</p><p>&quot;It&#x27;s not too late, Professor!&quot; Harry said. A part of Harry yelled that he was being selfish, and then another part shouted that down; there would be other people to help.</p><p>&quot;Yes, it is too late... and you shall not... persuade me otherwise... I have... thought better of it... as I said... I am too full... of secrets better left unknown... <em>look at me.</em>&quot;</p><p>Harry looked, almost despite himself.</p><p>He saw a still-unwrinkled face, looking old and pained, beneath a head rapidly losing its hair, even the sides looking wispy now; Harry saw a face he&#x27;d always thought was sharp, now revealed as <em>thin,</em> muscle and fat fading away from the face, as from the arms beneath it, like the skeletal form of Bellatrix Black he&#x27;d seen in Azkaban -</p><p>Harry&#x27;s head wrenched aside, unthinkingly.</p><p>&quot;You see,&quot; whispered the Professor. &quot;I dislike to sound cliched... Mr. Potter... but the truth is... the Arts called Dark... really are not good for a person... in the end.&quot;</p><p>Professor Quirrell breathed in, breathed out. There was quiet for a time in the infirmary, the two of them watched only by the elaborately ornamented stone of the walls.</p><p>&quot;Is there anything left... unsaid between us?&quot; said Professor Quirrell. &quot;I am not dying today... mind you... not right now... but I do not know how long... I shall be able to converse.&quot;</p><p>&quot;There&#x27;s,&quot; Harry said, swallowed again. &quot;There&#x27;s a lot of things, way too many things, but... it might be the wrong thing to ask, but I don&#x27;t want - this one question unanswered - snake?&quot;</p><p>A snake lay on the bed.</p><p>&quot;<em>I learned how the Killing Cursse workss. Requiress true hate to casst, not much hate, but musst want target dead, they ssay. In prisson with life-eaterss, you casst Killing Cursse at guard - ssaid you did not want him dead - wass that lie? Here, now, at thiss disstance - you may sspeak truth - even if you fear it reflectss poorly on you - it sshould not matter now, teacher. I wissh to know. Musst know. Will not abandon you, either way.&quot;</em></p><p>A man lay on the bed.</p><p>&quot;Listen carefully,&quot; Professor Quirrell whispered. &quot;I will tell you a conundrum... a riddle of a dangerous spell... when you know the answer to that puzzle... you will also know... the answer to your question... are you listening?&quot;</p><p>Harry nodded.</p><p>&quot;There is a limitation... to the Killing Curse. To cast it once... in a fight... you must hate enough... to want the other dead. To cast Avada... Kedavra twice... you must hate enough... to kill twice... to cut their throat with your own hands... to watch them die... then do it again. Very few... can hate enough... to kill someone... five times... they would... get bored.&quot; The Defense Professor breathed several times, before continuing. &quot;But if you look at history... you will find some Dark Wizards... who could cast the Killing Curse... over and over. A nineteenth-century witch... who called herself Dark Evangel... the Aurors called her A. K. McDowell. She could cast the Killing Curse... a dozen times... in one fight. Ask yourself... as I asked myself... what is the secret... that she knew? What is deadlier than hate... and flows without limit?&quot;</p><p><em>A second level to the Avada Kedavra spell, just like with the Patronus Charm...</em></p><p>&quot;I don&#x27;t really care,&quot; Harry answered.</p><p>The Defense Professor chuckled wetly. &quot;Good. You are... learning. So you see...&quot; A pause of transformation. &quot;<em>I did not wissh guard dead, after all. Casst Killing Cursse, but not with hate.</em>&quot; And then a man.</p><p>Harry swallowed hard. It was both better, and worse, than what Harry had suspected; and characteristic enough of Professor Quirrell. A cracked soul, for certain; but Professor Quirrell had never claimed to be whole.</p><p>&quot;Any else... to say?&quot; said the man in the bed.</p><p>&quot;Are you absolutely sure,&quot; Harry said, &quot;that there is nothing you&#x27;ve ever heard of that might save you, Professor? In all your lore? Finding and uniting all three Deathly Hallows, an ancient artifact that Merlin sealed behind a riddle nobody&#x27;s ever figured out? You&#x27;ve seen some of what I can do. That I&#x27;m good at solving riddles. You know I can figure things out, sometimes, that other wizards can&#x27;t. I -&quot; Harry&#x27;s voice broke. &quot;I have a strong preference for your life, over your death, Professor Quirrell.&quot;</p><p>There was a long pause.</p><p>&quot;One thing,&quot; whispered Professor Quirrell. &quot;One thing... that might do it... or it might not... but to obtain it... is beyond your power, or mine...&quot;</p><p><em>Oh, it was just the setup for a subquest,</em> said Harry&#x27;s Inner Critic.</p><p>All the other parts screamed for that part to shut up. Life didn&#x27;t work like that. Ancient artifacts could be found, but not in a month, not when you couldn&#x27;t leave Hogwarts and were still in your first year.</p><p>Professor Quirrell took in a deep breath. Exhaled. &quot;I&#x27;m sorry... that came out... too dramatic. Do not... get your hopes up... Mr. Potter. You asked... for anything... no matter how unlikely. There is... a certain object... called...&quot;</p><p>A snake lay on the bed.</p><p><em>&quot;The Philossopher&#x27;ss Sstone,</em>&quot; hissed the snake.</p><p>If there&#x27;d been a mass-manufacturable means of safe immortality this entire time and nobody had bothered, Harry was going to snap and kill everyone.</p><p><em>&quot;I read of it in a book,&quot; </em>Harry hissed. <em>&quot;Concluded it wass obviouss myth. No reason why ssame device would provide immortality and endlesss gold. Not unlesss ssomeone wass jusst inventing happy sstoriess. Not to mention, every ssane persson sshould have been ressearching wayss to make more Sstoness, or kidnapping maker to produce. Thought of you sspecifically, teacher.&quot;</em></p><p>A hissing of cold laughter. <em>&quot;Reassoning iss wisse, but not wisse enough. Like with horcrux sspell, abssurdity hidess true ssecret. True Sstone iss not what that legend ssayss. True power iss not what sstoriess claim. Sstone&#x27;s ssuppossed maker wass not one who made it. One who holdss it now, wass not born to name now ussed. Yet Sstone iss powerful healing device in truth. Have you heard it sspoken of?&quot;</em></p><p><em>&quot;Jusst in the book.</em>&quot;</p><p><em>&quot;One who holdss Sstone iss repossitory of much lore. Taught sschoolmasster many ssecretss. Sschoolmasster hass ssaid nothing of Sstone&#x27;ss holder, nothing of Sstone? No hintss?&quot;</em></p><p><em>&quot;Not that I can eassily recall,</em>&quot; Harry replied honestly.</p><p><em>&quot;Ah,&quot; </em>hissed the snake. <em>&quot;Ah, well.&quot;</em></p><p><em>&quot;Could assk sschoolmasster -&quot;</em></p><p><em>&quot;No! Do not assk him, boy. He would not take quesstion well.&quot;</em></p><p><em>&quot;But if the Sstone only healss -&quot;</em></p><p><em>&quot;Sschoolmasster doess not believe that, would not believe that. Too many have ssought Sstone, or ssought holder&#x27;s lore. Do not assk. Musst not assk. Do not try to obtain Sstone yoursself. I forbid.&quot;</em></p><p>A man lay on the bed once more. &quot;I am at... my limit...&quot; said Professor Quirrell. &quot;I must regain... my strength... before I go... to the forest... with your gift. Leave now... but sustain the Transfiguration... before you go.&quot;</p><p>Harry reached out, touched the white pebble lying within the kerchief, renewing the Transfiguration on it. &quot;It should last for one hour and fifty-three minutes after this,&quot; Harry said.</p><p>&quot;Your studies... do well.&quot;</p><p>It was far longer than Harry&#x27;s Transfigurations had lasted at the start of the school year. Second-year spells came to him easily now, without strain; which wasn&#x27;t surprising, since he would be twelve in less than two months. Harry could even have cast a Memory Charm, if it had been good for someone to forget every memory involving their left arm. He was climbing the power ladder, slowly, from very far down.</p><p>The thought came with a potential for sadness, a thought of one door opening as another closed; which Harry also rejected.</p><p>The door to the infirmary closed behind Harry, as the Boy-Who-Lived walked swiftly and with purpose, shrugging on his Invisibility Cloak as he moved. Soon, presumably, Professor Quirrell would call for assistance; and an older student trio would guide the Defense Professor into some quiet place, maybe the forest, with an excuse of viewing the lake or some such. Someplace the Defense Professor could eat a unicorn undetected, after Harry&#x27;s Transfiguration wore off.</p><p>And then Professor Quirrell would be healthier, for a time. His power would return to him as strong as he&#x27;d ever been, for a much shorter time.</p><p>It wouldn&#x27;t last.</p><p>Harry&#x27;s fists clenched as he strode, the tension radiating up his arm muscles. If the Defense Professor&#x27;s treatment regimen hadn&#x27;t been interrupted, by Harry and the Aurors that <em>he</em> had brought to Hogwarts...</p><p>It was stupid to blame himself, Harry knew it was stupid and somehow his brain was doing it anyway. Like his brain was searching, carefully finding and selecting some way for this to be his fault, no matter how far it had to stretch.</p><p>As if having things be his fault were the only way that his brain knew how to grieve.</p><p>A trio of seventh-year Slytherins passed Harry&#x27;s invisible form in the hallway, heading for the healer&#x27;s offices where the Professor waited, looking deeply serious and concerned. Was that how other people grieved?</p><p>Or did they, on some level, not really <em>care,</em> as Professor Quirrell thought?</p><p><em>There is a second level to the Killing Curse.</em></p><p>Harry&#x27;s brain had solved the riddle instantly, in the moment of first hearing it; as though the knowledge had always been inside him, waiting to make itself known.</p><p>Harry had read once, somewhere, that the opposite of happiness wasn&#x27;t sadness, but boredom; and the author had gone on to say that to find happiness in life you asked yourself not what would make you happy, but what would excite you. And by the same reasoning, hatred wasn&#x27;t the true opposite of love. Even hatred was a kind of respect that you could give to someone&#x27;s existence. If you cared about someone enough to prefer their dying to their living, it meant you were thinking about them.</p><p>It had come up much earlier, before the Trial, in conversation with Hermione; when she&#x27;d said something about magical Britain being Prejudiced, with considerable and recent justification. And Harry had thought - but not said - that at least she&#x27;d been let into Hogwarts to be spat upon.</p><p>Not like certain people living in certain countries, who were, it was <em>said</em>, as human as anyone else; who were <em>said</em> to be sapient beings, worth more than any mere unicorn. But who nonetheless wouldn&#x27;t be allowed to live in Muggle Britain. On that score, at least, no Muggle had the right to look a wizard in the eye. Magical Britain might discriminate against Muggleborns, but at least it allowed them inside so they could be spat upon in person.</p><p><em>What is deadlier than hate, and flows without limit?</em></p><p>&quot;Indifference,&quot; Harry whispered aloud, the secret of a spell he would never be able to cast; and kept striding toward the library to read anything he could find, anything at all, about the Philosopher&#x27;s Stone.</p><p></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vsASeE4KDP2nedgGv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "u7ciTcKteyx2hqdBh", "canonicalCollectionSlug": "hpmor", "canonicalBookId": "zBCyqJgJNJcfzgTZZ", "canonicalNextPostSlug": "chapter-103-tests", "canonicalPrevPostSlug": "chapter-101-precautionary-measures-pt-2", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-01T23:13:21.532Z", "modifiedAt": null, "url": null, "title": "What are you working on? January 2014", "slug": "what-are-you-working-on-january-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:38.844Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R4NXZmozd2sEPYtN2/what-are-you-working-on-january-2014", "pageUrlRelative": "/posts/R4NXZmozd2sEPYtN2/what-are-you-working-on-january-2014", "linkUrl": "https://www.lesswrong.com/posts/R4NXZmozd2sEPYtN2/what-are-you-working-on-january-2014", "postedAtFormatted": "Wednesday, January 1st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20you%20working%20on%3F%20January%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20you%20working%20on%3F%20January%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR4NXZmozd2sEPYtN2%2Fwhat-are-you-working-on-january-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20you%20working%20on%3F%20January%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR4NXZmozd2sEPYtN2%2Fwhat-are-you-working-on-january-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR4NXZmozd2sEPYtN2%2Fwhat-are-you-working-on-january-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<div id=\"entry_t3_irf\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_hvu\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_gkz\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>Happy new year! This is the supposedly-bimonthly-but-we-keep-skipping 'What are you working On?' thread. Previous threads are&nbsp;<a href=\"http://lesswrong.com/r/discussion/tag/waywo\">here</a>. So here's the question:</p>\n<p style=\"padding-left: 60px;\"><em>What are you working on?&nbsp;</em></p>\n<p>Here are some guidelines:</p>\n<ul>\n<li>Focus on projects that you have recently made progress on, not projects that you're thinking about doing but haven't started.</li>\n<li>Why this project and not others? Mention reasons why you're doing  the project and/or why others should contribute to your project (if  applicable).</li>\n<li>Talk about your goals for the project.</li>\n<li>Any kind of project is fair game: personal improvement, research project, art project, whatever.</li>\n<li>Link to your work if it's linkable.</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R4NXZmozd2sEPYtN2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "25166", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-02T01:34:11.518Z", "modifiedAt": null, "url": null, "title": "Tulpa References/Discussion", "slug": "tulpa-references-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:37.418Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vulture", "createdAt": "2012-03-24T19:29:34.072Z", "isAdmin": false, "displayName": "Vulture"}, "userId": "v7KPsDBuK3pLtqTLo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/imxPxMjXE2JT9rLaF/tulpa-references-discussion", "pageUrlRelative": "/posts/imxPxMjXE2JT9rLaF/tulpa-references-discussion", "linkUrl": "https://www.lesswrong.com/posts/imxPxMjXE2JT9rLaF/tulpa-references-discussion", "postedAtFormatted": "Thursday, January 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tulpa%20References%2FDiscussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATulpa%20References%2FDiscussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FimxPxMjXE2JT9rLaF%2Ftulpa-references-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tulpa%20References%2FDiscussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FimxPxMjXE2JT9rLaF%2Ftulpa-references-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FimxPxMjXE2JT9rLaF%2Ftulpa-references-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 593, "htmlBody": "<p>There have been a number of discussions here on LessWrong about \"tulpas\", but it's been scattered about with no central thread for the discussion. So I thought I would put this up here, along with a centralized list of reliable information sources, just so we all stay on the same page.</p>\n<p><em>Tulpas</em> are deliberately created \"imaginary friends\" which in many ways resemble separate, autonomous minds. Often, the creation of a tulpa is coupled with deliberately induced visual, auditory, and/or tactile hallucinations of the being.</p>\n<p>Previous discussions here on LessWrong: <a href=\"/lw/jez/open_thread_for_january_17_2014/a9py\">1</a> <a href=\"/lw/h9b/post_ridiculous_munchkin_ideas/8y79\">2</a> <a href=\"/lw/iyj/open_thread_november_1_7_2013/a0jw\">3</a></p>\n<p>Questions that have been raised:</p>\n<p>1. How do tulpas work?</p>\n<p>2. Are tulpas safe, from a mental health perspective?</p>\n<p>3. Are tulpas conscious? (may be a <a href=\"/lw/x4/nonperson_predicates/\">hard question</a>)</p>\n<p>4. More generally, is making a tulpa a good idea? What are they useful for?</p>\n<p>&nbsp;</p>\n<p><strong> </strong></p>\n<p><strong>Pertinent Links and Publications</strong></p>\n<p>(I will try to keep this updated if/when further sources are found)</p>\n<ul>\n<li>In <a href=\"http://www.nytimes.com/2013/10/15/opinion/luhrmann-conjuring-up-our-own-gods.html?_r=0\">this article</a><sup>1</sup>, the psychological anthropologist Tanya M. Luhrmann connects tulpas to the \"voice of God\" experienced by devout evangelicals - a phenomenon more thoroughly discussed in her book <a href=\"http://www.amazon.com/When-God-Talks-Back-Understanding/dp/0307277275/\">When God Talks Back: Understanding the American Evangelical Relationship with God</a>. Luhrmann has also <a href=\"http://alumni.stanford.edu/get/page/magazine/article/?article_id=54818\">succeeded</a><sup>2</sup> in inducing tulpa-like visions of Leland Stanford, jr. in experimental subjects.</li>\n<li><a href=\"http://loki.klkblake.com/~kyle/IIA.pdf\">This paper</a><sup>3</sup> investigates the phenomenon of authors who experience their characters as \"real\", which may be tulpas by yet another name.</li>\n<li>There is an <a href=\"http://reddit.com/r/tulpas\">active subreddit</a> of people who have or are developing tulpas, with an FAQ, links to creation guides, etc.</li>\n<li><a href=\"http://www.tulpa.info/\">tulpa.info</a> is a valuable resource, particularly the forum. There appears to be a whole \"research\" section for amateur experiments and surveys.</li>\n<li><a href=\"https://docs.google.com/file/d/0B-IrwpisjguaZHd4MnRuSV9SQWM/edit\">This particular experiment</a> suggests that the idea of using tulpas to solve problems faster is a no-go.</li>\n<li>Also, one person helpfully <a href=\"http://community.tulpa.info/thread-eeg-and-tulpa-research\">hooked themselves up to an EEG</a> and then performed various mental activities related to their tulpa.</li>\n<li>Another possibly related phenomenon is the way that actors immerse themselves in their characters. See especially the section on \"Masks\" in Keith Johnstone's book <a href=\"http://www.amazon.com/Impro-Improvisation-Theatre-Keith-Johnstone/dp/0878301178\">Impro: Improvisation and the Theatre</a> (related <a href=\"http://embodied.quora.com/Impro-Masks-and-Trance\">quotations</a> and <a href=\"https://www.youtube.com/watch?v=QGpzYa9d-Hk\">video</a>)<sup>4</sup>.</li>\n<li><a href=\"http://www.meltingasphalt.com/neurons-gone-wild/\">This blogger</a> has some interesting ideas about the neurological basis of tulpas, based on Julian Jaynes's <a href=\"http://www.amazon.com/Origin-Consciousness-Breakdown-Bicameral-Mind/dp/0618057072\">The Origin of Consciousness in the Breakdown of the Bicameral Mind</a>, a book whose scientific validity is not clear to me.</li>\n<li>It is not hard to find new age mystical books about the use of \"thoughtforms\", or the art of \"channeling\" \"spirits\", often clearly talking about the same phenomenon. These books are likely to be low in useful information for our purposes, however. Therefore I'm not going to list the ones I've found here, as they would clutter up the list significantly.</li>\n<li><strong>(Updated 2/9/2015)</strong> The <a href=\"http://kajsotala.fi/Papers/Tulpa.pdf\">abstract</a> of a paper by our very own <a href=\"/user/KajSotala\">Kaj Sotala</a> hypothesizing about the mechanisms behind tulpa creation.<sup>5</sup></li>\n</ul>\n<p>(Bear in mind while perusing these resources that if you have serious qualms about creating a tulpa, it might not be a good idea to read creation guides too carefully; making a tulpa is easy to do and, at least for me, was hard to resist. Proceed at your own risk.)</p>\n<ul>\n</ul>\n<p>&nbsp;</p>\n<p><strong>Footnotes</strong></p>\n<p>1. \"Conjuring Up Our Own Gods\", a 14 October 2013 New York Times Op-Ed</p>\n<p>2. \"Hearing the Voice of God\" by Jill Wolfson in the July/August 2013 Stanford Alumni Magazine</p>\n<p>3. \"The Illusion of Independent Agency: Do Adult Fiction Writers Experience Their Characters as Having Minds of Their Own?\"; Taylor, Hodges &amp; Koh&agrave;nyi in <em>Imagination, Cognition and Personality</em>; 2002/2003; 22, 4</p>\n<p>4. Thanks to pure_awesome</p>\n<p>5. \"Sentient companions predicted and modeled into existence: explaining the tulpa phenomenon\" by Kaj Sotala</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "imxPxMjXE2JT9rLaF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 20, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "25167", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>There have been a number of discussions here on LessWrong about \"tulpas\", but it's been scattered about with no central thread for the discussion. So I thought I would put this up here, along with a centralized list of reliable information sources, just so we all stay on the same page.</p>\n<p><em>Tulpas</em> are deliberately created \"imaginary friends\" which in many ways resemble separate, autonomous minds. Often, the creation of a tulpa is coupled with deliberately induced visual, auditory, and/or tactile hallucinations of the being.</p>\n<p>Previous discussions here on LessWrong: <a href=\"/lw/jez/open_thread_for_january_17_2014/a9py\">1</a> <a href=\"/lw/h9b/post_ridiculous_munchkin_ideas/8y79\">2</a> <a href=\"/lw/iyj/open_thread_november_1_7_2013/a0jw\">3</a></p>\n<p>Questions that have been raised:</p>\n<p>1. How do tulpas work?</p>\n<p>2. Are tulpas safe, from a mental health perspective?</p>\n<p>3. Are tulpas conscious? (may be a <a href=\"/lw/x4/nonperson_predicates/\">hard question</a>)</p>\n<p>4. More generally, is making a tulpa a good idea? What are they useful for?</p>\n<p>&nbsp;</p>\n<p><strong> </strong></p>\n<p><strong id=\"Pertinent_Links_and_Publications\">Pertinent Links and Publications</strong></p>\n<p>(I will try to keep this updated if/when further sources are found)</p>\n<ul>\n<li>In <a href=\"http://www.nytimes.com/2013/10/15/opinion/luhrmann-conjuring-up-our-own-gods.html?_r=0\">this article</a><sup>1</sup>, the psychological anthropologist Tanya M. Luhrmann connects tulpas to the \"voice of God\" experienced by devout evangelicals - a phenomenon more thoroughly discussed in her book <a href=\"http://www.amazon.com/When-God-Talks-Back-Understanding/dp/0307277275/\">When God Talks Back: Understanding the American Evangelical Relationship with God</a>. Luhrmann has also <a href=\"http://alumni.stanford.edu/get/page/magazine/article/?article_id=54818\">succeeded</a><sup>2</sup> in inducing tulpa-like visions of Leland Stanford, jr. in experimental subjects.</li>\n<li><a href=\"http://loki.klkblake.com/~kyle/IIA.pdf\">This paper</a><sup>3</sup> investigates the phenomenon of authors who experience their characters as \"real\", which may be tulpas by yet another name.</li>\n<li>There is an <a href=\"http://reddit.com/r/tulpas\">active subreddit</a> of people who have or are developing tulpas, with an FAQ, links to creation guides, etc.</li>\n<li><a href=\"http://www.tulpa.info/\">tulpa.info</a> is a valuable resource, particularly the forum. There appears to be a whole \"research\" section for amateur experiments and surveys.</li>\n<li><a href=\"https://docs.google.com/file/d/0B-IrwpisjguaZHd4MnRuSV9SQWM/edit\">This particular experiment</a> suggests that the idea of using tulpas to solve problems faster is a no-go.</li>\n<li>Also, one person helpfully <a href=\"http://community.tulpa.info/thread-eeg-and-tulpa-research\">hooked themselves up to an EEG</a> and then performed various mental activities related to their tulpa.</li>\n<li>Another possibly related phenomenon is the way that actors immerse themselves in their characters. See especially the section on \"Masks\" in Keith Johnstone's book <a href=\"http://www.amazon.com/Impro-Improvisation-Theatre-Keith-Johnstone/dp/0878301178\">Impro: Improvisation and the Theatre</a> (related <a href=\"http://embodied.quora.com/Impro-Masks-and-Trance\">quotations</a> and <a href=\"https://www.youtube.com/watch?v=QGpzYa9d-Hk\">video</a>)<sup>4</sup>.</li>\n<li><a href=\"http://www.meltingasphalt.com/neurons-gone-wild/\">This blogger</a> has some interesting ideas about the neurological basis of tulpas, based on Julian Jaynes's <a href=\"http://www.amazon.com/Origin-Consciousness-Breakdown-Bicameral-Mind/dp/0618057072\">The Origin of Consciousness in the Breakdown of the Bicameral Mind</a>, a book whose scientific validity is not clear to me.</li>\n<li>It is not hard to find new age mystical books about the use of \"thoughtforms\", or the art of \"channeling\" \"spirits\", often clearly talking about the same phenomenon. These books are likely to be low in useful information for our purposes, however. Therefore I'm not going to list the ones I've found here, as they would clutter up the list significantly.</li>\n<li><strong>(Updated 2/9/2015)</strong> The <a href=\"http://kajsotala.fi/Papers/Tulpa.pdf\">abstract</a> of a paper by our very own <a href=\"/user/KajSotala\">Kaj Sotala</a> hypothesizing about the mechanisms behind tulpa creation.<sup>5</sup></li>\n</ul>\n<p>(Bear in mind while perusing these resources that if you have serious qualms about creating a tulpa, it might not be a good idea to read creation guides too carefully; making a tulpa is easy to do and, at least for me, was hard to resist. Proceed at your own risk.)</p>\n<ul>\n</ul>\n<p>&nbsp;</p>\n<p><strong id=\"Footnotes\">Footnotes</strong></p>\n<p>1. \"Conjuring Up Our Own Gods\", a 14 October 2013 New York Times Op-Ed</p>\n<p>2. \"Hearing the Voice of God\" by Jill Wolfson in the July/August 2013 Stanford Alumni Magazine</p>\n<p>3. \"The Illusion of Independent Agency: Do Adult Fiction Writers Experience Their Characters as Having Minds of Their Own?\"; Taylor, Hodges &amp; Koh\u00e0nyi in <em>Imagination, Cognition and Personality</em>; 2002/2003; 22, 4</p>\n<p>4. Thanks to pure_awesome</p>\n<p>5. \"Sentient companions predicted and modeled into existence: explaining the tulpa phenomenon\" by Kaj Sotala</p>", "sections": [{"title": "Pertinent Links and Publications", "anchor": "Pertinent_Links_and_Publications", "level": 1}, {"title": "Footnotes", "anchor": "Footnotes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "81 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wqDRRx9RqwKLzWt7R"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-02T01:35:44.290Z", "modifiedAt": null, "url": null, "title": "[LINK] On what ship are we?", "slug": "link-on-what-ship-are-we", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:30.523Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mjjQY5wNY9toeTxFW/link-on-what-ship-are-we", "pageUrlRelative": "/posts/mjjQY5wNY9toeTxFW/link-on-what-ship-are-we", "linkUrl": "https://www.lesswrong.com/posts/mjjQY5wNY9toeTxFW/link-on-what-ship-are-we", "postedAtFormatted": "Thursday, January 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20On%20what%20ship%20are%20we%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20On%20what%20ship%20are%20we%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmjjQY5wNY9toeTxFW%2Flink-on-what-ship-are-we%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20On%20what%20ship%20are%20we%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmjjQY5wNY9toeTxFW%2Flink-on-what-ship-are-we", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmjjQY5wNY9toeTxFW%2Flink-on-what-ship-are-we", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.newyorker.com/talk/comment/2014/01/06/140106taco_talk_gopnik\">http://www.newyorker.com/talk/comment/2014/01/06/140106taco_talk_gopnik</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mjjQY5wNY9toeTxFW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -12, "extendedScore": null, "score": 1.4938428139087604e-06, "legacy": true, "legacyId": "25168", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-02T05:51:30.000Z", "modifiedAt": null, "url": null, "title": "Two Dark Side Statistics Papers", "slug": "two-dark-side-statistics-papers", "viewCount": null, "lastCommentedAt": "2022-05-13T00:08:35.821Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SQAfPKZBAAKYMjx25/two-dark-side-statistics-papers", "pageUrlRelative": "/posts/SQAfPKZBAAKYMjx25/two-dark-side-statistics-papers", "linkUrl": "https://www.lesswrong.com/posts/SQAfPKZBAAKYMjx25/two-dark-side-statistics-papers", "postedAtFormatted": "Thursday, January 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20Dark%20Side%20Statistics%20Papers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20Dark%20Side%20Statistics%20Papers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSQAfPKZBAAKYMjx25%2Ftwo-dark-side-statistics-papers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20Dark%20Side%20Statistics%20Papers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSQAfPKZBAAKYMjx25%2Ftwo-dark-side-statistics-papers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSQAfPKZBAAKYMjx25%2Ftwo-dark-side-statistics-papers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1974, "htmlBody": "<p><b>I.</b></p>\n<p>First we have <A HREF=\"http://www.socio.mta.hu/dynamic/simmons_et_al_2011.pdf\">False Positive Psychology: Undisclosed Flexibility In Data Collection And Analysis Allows Presenting Anything As Significant</A> (h/t Jonas Vollmer).</p>\n<p>The message is hardly unique: there are lots of tricks unscrupulous or desperate scientists can use to artificially nudge results to the 5% significance level. The clarity of the presentation <i>is</i> unique. They start by discussing four particular tricks:</p>\n<p>1. Measure multiple dependent variables, then report the ones that are significant. For example, if you&#8217;re measuring whether treatment for a certain psychiatric disorder improves life outcomes, you can collect five different measures of life outcomes &#8211; let&#8217;s say educational attainment, income, self-reported happiness, whether or not ever arrested, whether or not in romantic relationship &#8211; and have a 25%-ish probability one of them will come out at significance by chance. Then you can publish a paper called &#8220;Psychiatric Treatment Found To Increase Educational Attainment&#8221; without ever mentioning the four negative tests.</p>\n<p>2. Artificially choose when to end your experiment. Suppose you want to prove that yelling at a coin makes it more likely to come up tails. You yell at a coin and flip it. It comes up heads. You try again. It comes up tails. You try again. It comes up heads. You try again. It comes up tails. You try again. It comes up tails again. You try again. It comes up tails again. You note that it came up tails four out of six times &#8211; a 66% success rate compared to expected 50% &#8211; and declare victory. Of course, this result wouldn&#8217;t be significant, and it seems as if this should be a general rule &#8211; that almost by the definition of significance, you shouldn&#8217;t be able to obtain it just be stopping the experiment at the right point. But the authors of the study perform several simulations to prove that this trick is more successful than you&#8217;d think:</p>\n<p><center><IMG SRC=\"http://slatestarcodex.com/blog_images/darkstats0.png\"></center></p>\n<p>3. Control for &#8220;confounders&#8221; (in practice, most often gender). I sometimes call this the &#8220;Elderly Hispanic Woman Effect&#8221; after drug trials that find that their drug doesn&#8217;t have significant effects in the general population, but it <i>does</i> significantly help elderly Hispanic women.  The trick is you split the population into twenty subgroups (young white men, young white women, elderly white men, elderly white women, young black men, etc), in one of those subgroups it will achieve significance by pure chance, and so you declare that your drug must just somehow be a perfect fit for elderly Hispanic women&#8217;s unique body chemistry. This is not <i>always</i> wrong (some antihypertensives have notably different efficacy in white versus black populations) but it is <i>usually</i> suspicious.</p>\n<p>4. Test different conditions and report the ones you like. For example, suppose you are testing whether vegetable consumption affects depression. You conduct the trial with three arms: low veggie diet, medium veggie diet, and high veggie diet. You now have four possible comparisons &#8211; low-medium, low-high, medium-high, low-medium-high trend). One of them will be significant 20% of the time, so you can just report that one: &#8220;People who eat a moderate amount of vegetables are less likely to get depression than people who eat excess vegetables&#8221; sounds like a perfectly reasonable result.</p>\n<p>Then they run simulations to show exactly how much more likely you are to get a significant result in random data by employing each trick:</p>\n<p><center><IMG SRC=\"http://slatestarcodex.com/blog_images/darkstats1.png\"></center></p>\n<p>The image demonstrates that by using all four tricks, you can squeeze random data into a result significant at the p < 0.05 level about 61% of the time.\n\nThe authors then put their money where their mouth is by conducting two studies.\n\nThe first seems like a very very classic social psychology study. Subjects are randomly assigned to listen to one of two songs - either a nondescript control song or a child's nursery song. Then they are asked to rate how old they feel. Sure enough, the subjects who listen to the child's song feel older (p = 0.03).\n\nThe second study is very similar, with one important exception. Once again, subjects are randomly assigned to listen to one of two songs - either a nondescript control song or a song about aging - \"When I'm Sixty-Four\" by The Beatles. Then they are asked to put down their actual age, in years. People who listened to the Beatles song became, on average, a year and a half younger than the control group (p = 0.04).\n\nSo either the experimental intervention changed their subjects' ages, or the researchers were using statistical tricks. Turns out it was the second one. They explain how they used the four statistical tricks they explained above, and that without those tricks there would have been (obviously) no significant difference. They go on to say that their experiment meets the inclusion criteria for every major journal and that under current reporting rules there's no way anyone could have detected their data manipulation.\n\nThey go on to list the changes they think the scientific establishment needs to prevent papers like theirs from reaching print. They're basically \"don't do the things we just talked about\", but as far as I can tell they rely on the honor system. I think a broader meta-point is that on important studies scientists should have to submit their experimental protocol to a journal and get it accepted or rejected in advance so they can't change tactics mid-stream or drop data. This would also force journals to publish more negative results.\n\nSee also their interesting discussion of why they think \"use Bayesian statistics\" is a non-solution to the problem.\n\n<b>II.</b></p>\n<p>Second we have <A HREF=\"http://slatestarcodex.com/Stuff/addiction.pdf\">How To Have A High Success Rate In Treatment: Advice For Evaluators Of Alcoholism Programs</A>.</p>\n<p>This study is very close to my heart, because I&#8217;m working on my hospital&#8217;s Substance Abuse Team this month. Every day we go see patients struggling with alcoholism, heroin abuse, et cetera, and we offer them treatment at our hospital&#8217;s intensive inpatient Chemical Dependency Unit. And every day, our patients say thanks but no thanks, they heard of a program affiliated with their local church that has a 60% success rate, or an 80% success rate, or in one especially rosy-eyed case a frickin&#8217; 97% success rate.</p>\n<p>(meanwhile, <i>real</i> rehab programs still struggle to prove they have a success rate greater than placebo)</p>\n<p>My attending assumes these programs are scum but didn&#8217;t really have a good evidence base for the claim, so I decided to search Google Scholar to find out what was going on. I struck gold in this paper, which is framed as a sarcastic how-to guide for unscrupulous drug treatment program directors who want to inflate their success rates without <i>technically</i> lying.</p>\n<p>By far the best way to do this is to choose your denominator carefully. For example, it seems fair to only include the people who attended your full treatment program, not the people who dropped out on Day One or never showed up at all &#8211; you can hardly be blamed for that, right? So suppose that your treatment program is one month intensive in rehab followed by a series of weekly meetings continuing indefinitely. At the end of one year, you define successful treatment completers as &#8220;the people who are still going to these meetings now, at the end of the year&#8221;. But in general, people who relapse into alcoholism are a whole lot less likely to continue attending their AA meetings than people who stay sober. So all you have to do is go up to people at your AA meeting, ask them if they&#8217;re still on the wagon, and your one-year success rate looks really good.</p>\n<p>Another way to hack your treatment population is to only accept the most promising candidates to begin with (it works for private schools and it can work for you). We know that middle-class, employed people with houses and families have a much better prognosis than lower-class unemployed homeless single people. Although someone would probably notice if you put up a sign saying &#8220;MIDDLE-CLASS EMPLOYED PEOPLE WITH HOUSES AND FAMILIES ONLY&#8221;, a very practical option is to just charge a lot of money and let your client population select themselves. This is why for-profit private rehabs will have a higher success rate than public hospitals and government programs that deal with poor people.</p>\n<p>Still another strategy is to follow the old proverb: &#8220;If at first you don&#8217;t succeed, redefine success&#8221;. &#8220;Abstinence&#8221; is such a harsh word. Why not &#8220;drinking in moderation&#8221;? This is a wonderful phrase, because you can just let the alcoholic involved determine the definition of moderation. A year after the program ends, you can send out little surveys saying &#8220;Remember when we told you God really wants you not to drink? You listened to us and are drinking in moderation now, right? Please check one: Y () N ()&#8221;. Who&#8217;s going to answer &#8216;no&#8217; to that? Heck, some of the alcoholics I talk to say they&#8217;re drinking in moderation <i>while they are in the emergency room for alcohol poisoning</i>. </p>\n<p>If you can&#8217;t handle &#8220;moderation&#8221;, how about &#8220;drinking less than you were before the treatment program&#8221;? This takes advantage of regression to the mean &#8211; you&#8217;re going to enter a rehab program at the worst period of your life, the time when your drinking finally spirals out of control. Just by coincidence, most other parts of your life will include less drinking than when you first came in to rehab, including the date a year after treatment when someone sends you a survey. Clearly rehab was a success!</p>\n<p>And why wait a year? My attending and myself actually looked up what was going on with that one 97% success rate program our patient said he was going to. Here&#8217;s what they do &#8211; it&#8217;s a three month residential program where you live in a building just off the church and you&#8217;re not allowed to go out except on group treatment activities. Obviously there is no alcohol allowed in the building and you are surrounded by very earnest counselors and fellow recovering addicts at all times. Then, <i>at the end of the three months, while you are still in the building</i>, they ask you whether you&#8217;re drinking or not. You say no. Boom &#8211; 97% success rate.</p>\n<p>One other tactic I have actually seen in studies and it <i>breaks my heart</i> is interval subdivision, which reminds me of some of the dirty tricks from the first study above. At five years&#8217; follow-up, you ask people &#8220;Did you drink during Year 1? Did you drink during Year 2? Did you drink during Year 3?&#8230;&#8221; and so on. Now you have five chances to find a significant difference between treatment and control groups. I have literally seen studies that say &#8220;Our rehab didn&#8217;t have an immediate effect, but by Year 4 our patients were doing better than the controls.&#8221; Meanwhile, in years 1, 2, 3, and 5, for all we know the controls were doing better than the patients.</p>\n<p>But if all else fails, there&#8217;s always the old standby of poor researchers everywhere &#8211; just don&#8217;t include a control group at all. This table really speaks to me:</p>\n<p><center><IMG SRC=\"http://slatestarcodex.com/blog_images/darkstats2.png\"></center></p>\n<p>The great thing about this table isn&#8217;t just that it shows that seemingly impressive results are exactly the same as placebo. The great thing it shows is that results in the placebo groups in the four studies could be anywhere from a 22.5% success rate to an 87% success rate. These aren&#8217;t treatment differences &#8211; all four groups are placebo! This is one hundred percent a difference in study populations and in success measures used. In other words, depending on your study protocol, you can prove that there is a 22.5% chance the average untreated alcoholic will achieve remission, or an 87% chance the average untreated alcoholic will achieve remission.</p>\n<p>You can bet that rehabs use the study protocol that finds an 87% chance of remission in the untreated. And then they go on to boast of their 90% success rate. Good job, rehab!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XYHzLjwYiqpeqaf4c": 2, "bh7uxTTqmsQ8jZJdB": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SQAfPKZBAAKYMjx25", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 23, "extendedScore": null, "score": 5.9e-05, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "BQBqPowfxjvoee8jw", "canonicalCollectionSlug": "codex", "canonicalBookId": "YhQ39PPHNrRCgYXcs", "canonicalNextPostSlug": "the-control-group-is-out-of-control", "canonicalPrevPostSlug": "noisy-poll-results-and-reptilian-muslim-climatologists-from", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-02T15:18:59.762Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, January 1-15", "slug": "group-rationality-diary-january-1-15-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:59.029Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RkHKedWX5nJAC3okm/group-rationality-diary-january-1-15-0", "pageUrlRelative": "/posts/RkHKedWX5nJAC3okm/group-rationality-diary-january-1-15-0", "linkUrl": "https://www.lesswrong.com/posts/RkHKedWX5nJAC3okm/group-rationality-diary-january-1-15-0", "postedAtFormatted": "Thursday, January 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20January%201-15&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20January%201-15%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRkHKedWX5nJAC3okm%2Fgroup-rationality-diary-january-1-15-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20January%201-15%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRkHKedWX5nJAC3okm%2Fgroup-rationality-diary-january-1-15-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRkHKedWX5nJAC3okm%2Fgroup-rationality-diary-january-1-15-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 222, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Happy new year to those who observe the Gregorian calendar! &nbsp;T<span style=\"line-height: 19px;\">his is the public group instrumental rationality diary for January 1-15. &nbsp;</span></p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Immediate past diary: &nbsp;<a href=\"/lw/jcv/group_rationality_diary_december_1631/\"><span style=\"color: #8a8a8b;\">December 16-31</span>&nbsp;</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\"><a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RkHKedWX5nJAC3okm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "25169", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WfewSr59eJfwZyYS4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-02T16:47:48.235Z", "modifiedAt": null, "url": null, "title": "TED Prize Nomination", "slug": "ted-prize-nomination", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:32.181Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QipXa23CKkFZPAosn/ted-prize-nomination", "pageUrlRelative": "/posts/QipXa23CKkFZPAosn/ted-prize-nomination", "linkUrl": "https://www.lesswrong.com/posts/QipXa23CKkFZPAosn/ted-prize-nomination", "postedAtFormatted": "Thursday, January 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20TED%20Prize%20Nomination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATED%20Prize%20Nomination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQipXa23CKkFZPAosn%2Fted-prize-nomination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=TED%20Prize%20Nomination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQipXa23CKkFZPAosn%2Fted-prize-nomination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQipXa23CKkFZPAosn%2Fted-prize-nomination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 298, "htmlBody": "<div>I started the process of a <a href=\"http://www.ted.com/pages/prize_nominate\">TED Prize Nomination</a>:</div>\n<div><br /></div>\n<div>&nbsp;Nominate an individual &mdash; or yourself &mdash; to envision and execute a high-impact project that can spur global change. Our TED Prize winner will have an ambitious wish &mdash; and the vision, pragmatism and leadership to turn it into reality. Every self-nomination will include a proposal for a world-changing and achievable wish.&nbsp;</div>\n<div><br /></div>\n<div>Fairly obvious who I'm nominating.</div>\n<div>But then came across a few things that made me suspect I'm not the best person to do this. Such as:</div>\n<div><br /></div>\n<div>* We weigh each single nomination as heavily as multiple nominations and we strongly discourage multiple nominations.&nbsp;</div>\n<div>\n<div>* The heart of the TED Prize is the wish. Though it's small in size, it is the most important element of your nomination. It's worth investing your time refining. At its most basic, a wish = who + what + how = a better world. In other words, who are you going to engage on what issue and in what way for what kind of improvement?</div>\n</div>\n<div>*&nbsp;Imagine your nominee is on the stage at the TED Conference announcing their wish and inviting the wider community--everyone from corporate and nonprofit leaders to TED fellows doing grassroots work in developing countries--to get involved. In a few sentences, what is your nominee's ask?</div>\n<div><br /></div>\n<div>I have absolutely no idea what Eliezer would write as his wish, and I don't think I'm even remotely qualified to take a stab at it. Would someone who knows Eliezer better, or perhaps Eliezer himself, be willing to take this on?</div>\n<div><br /></div>\n<div>To reduce the amount of time needed to complete the application, here are two outside-source articles about Eliezer that I googled up while I was doing this (they ask for links to such articles in Step 3)<br /><a href=\"http://www.cnbc.com/id/48538963\">http://www.cnbc.com/id/48538963</a></div>\n<div><a href=\"http://betabeat.com/2012/07/singularity-institute-less-wrong-peter-thiel-eliezer-yudkowsky-ray-kurzweil-harry-potter-methods-of-rationality/?show=all\">http://betabeat.com/2012/07/singularity-institute-less-wrong-peter-thiel-eliezer-yudkowsky-ray-kurzweil-harry-potter-methods-of-rationality/?show=all</a></div>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QipXa23CKkFZPAosn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "25170", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-02T18:00:49.403Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC fun and games meetup", "slug": "meetup-washington-dc-fun-and-games-meetup-8", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c5s8y2FyWwG3DLaGn/meetup-washington-dc-fun-and-games-meetup-8", "pageUrlRelative": "/posts/c5s8y2FyWwG3DLaGn/meetup-washington-dc-fun-and-games-meetup-8", "linkUrl": "https://www.lesswrong.com/posts/c5s8y2FyWwG3DLaGn/meetup-washington-dc-fun-and-games-meetup-8", "postedAtFormatted": "Thursday, January 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc5s8y2FyWwG3DLaGn%2Fmeetup-washington-dc-fun-and-games-meetup-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc5s8y2FyWwG3DLaGn%2Fmeetup-washington-dc-fun-and-games-meetup-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc5s8y2FyWwG3DLaGn%2Fmeetup-washington-dc-fun-and-games-meetup-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/v6'>Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 January 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/v6'>Washington DC fun and games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c5s8y2FyWwG3DLaGn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4948960788239693e-06, "legacy": true, "legacyId": "25171", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/v6\">Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 January 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/v6\">Washington DC fun and games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-02T20:32:26.931Z", "modifiedAt": null, "url": null, "title": "Rational Resolutions: Special CFAR Mini-workshop SATURDAY", "slug": "rational-resolutions-special-cfar-mini-workshop-saturday", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:33.847Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrienneYudkowsky", "createdAt": "2013-05-13T00:07:08.935Z", "isAdmin": false, "displayName": "LoganStrohl"}, "userId": "uuYBzWLiixkbN3s7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HTTikSEgfRpstMCdh/rational-resolutions-special-cfar-mini-workshop-saturday", "pageUrlRelative": "/posts/HTTikSEgfRpstMCdh/rational-resolutions-special-cfar-mini-workshop-saturday", "linkUrl": "https://www.lesswrong.com/posts/HTTikSEgfRpstMCdh/rational-resolutions-special-cfar-mini-workshop-saturday", "postedAtFormatted": "Thursday, January 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Resolutions%3A%20Special%20CFAR%20Mini-workshop%20SATURDAY&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Resolutions%3A%20Special%20CFAR%20Mini-workshop%20SATURDAY%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHTTikSEgfRpstMCdh%2Frational-resolutions-special-cfar-mini-workshop-saturday%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Resolutions%3A%20Special%20CFAR%20Mini-workshop%20SATURDAY%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHTTikSEgfRpstMCdh%2Frational-resolutions-special-cfar-mini-workshop-saturday", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHTTikSEgfRpstMCdh%2Frational-resolutions-special-cfar-mini-workshop-saturday", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 173, "htmlBody": "<p>This Saturday, Michael Valentine and I are taking the most fun and potent habit-formation material from CFAR's previous four-day workshops, and focusing that material towards making rational New Year's resolutions - and then actually keeping them.</p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.800000190734863px;\">If you're in the Bay Area and interested in resolutions, or in seeing CFAR's habit-formation material distilled, this workshop is for you.</span></p>\n<p>We'll meet at the CFAR office in Berkeley from 11AM to 5PM PST on Saturday, Jan. 4th. <strong>You can register <a href=\"http://rationality.org/rational-resolutions-2014/\">here</a></strong> - as of this posting, there are still several spots left!</p>\n<p>Normally an event like this would cost about $400, but we want to make this workshop more accessible. So, <strong>this time the event costs $195</strong>. And we&rsquo;re confident enough in the value of this material that we offer a no-hassle full money-back guarantee: If, after attending this workshop, you decide you didn&rsquo;t get your money&rsquo;s worth, ask to have it returned and it&rsquo;s yours. (We do recommend you try the techniques out for a week or two first. Some of them are likely to surprise you!)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HTTikSEgfRpstMCdh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 1.4950583152581479e-06, "legacy": true, "legacyId": "25172", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-02T22:08:08.933Z", "modifiedAt": null, "url": null, "title": "Handshakes, Hi, and What's New: What's Going On With Small Talk?", "slug": "handshakes-hi-and-what-s-new-what-s-going-on-with-small-talk", "viewCount": null, "lastCommentedAt": "2019-07-25T19:23:56.067Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benquo", "createdAt": "2009-03-06T00:17:35.184Z", "isAdmin": false, "displayName": "Benquo"}, "userId": "nt2XsHkdksqZ3snNr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/huRxRzwcvwTzvtEPY/handshakes-hi-and-what-s-new-what-s-going-on-with-small-talk", "pageUrlRelative": "/posts/huRxRzwcvwTzvtEPY/handshakes-hi-and-what-s-new-what-s-going-on-with-small-talk", "linkUrl": "https://www.lesswrong.com/posts/huRxRzwcvwTzvtEPY/handshakes-hi-and-what-s-new-what-s-going-on-with-small-talk", "postedAtFormatted": "Thursday, January 2nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Handshakes%2C%20Hi%2C%20and%20What's%20New%3A%20What's%20Going%20On%20With%20Small%20Talk%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHandshakes%2C%20Hi%2C%20and%20What's%20New%3A%20What's%20Going%20On%20With%20Small%20Talk%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhuRxRzwcvwTzvtEPY%2Fhandshakes-hi-and-what-s-new-what-s-going-on-with-small-talk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Handshakes%2C%20Hi%2C%20and%20What's%20New%3A%20What's%20Going%20On%20With%20Small%20Talk%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhuRxRzwcvwTzvtEPY%2Fhandshakes-hi-and-what-s-new-what-s-going-on-with-small-talk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhuRxRzwcvwTzvtEPY%2Fhandshakes-hi-and-what-s-new-what-s-going-on-with-small-talk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1600, "htmlBody": "<p class=\"entry-content\" style=\"border: 0px; margin: 1.286em 0px; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"font-weight: normal;\">This is an attempt to explicitly model what's going on in some small talk conversations. My hope is that at least one of these things will happen:</span></p>\n<ul>\n<li>There is a substantial flaw or missing element to my model that someone will point out.</li>\n<li>Many readers, who are bad at small talk because they don't see the point, will get better at it as a result of acquiring understanding.</li>\n</ul>\n<h2 style=\"border: 0px; font-style: inherit; font-weight: inherit; margin: 0px 0px 0.8125em; outline: 0px; padding: 0px; vertical-align: baseline; clear: both;\"><span style=\"margin: 0px; outline: invert none 0px; vertical-align: baseline; border: 0px; padding: 0px; text-decoration: underline;\"><strong style=\"border: 0px; font-style: inherit; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"font-size: x-small;\">Handshakes</span></strong></span></h2>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">I had some recent conversational failures online, that went roughly like this:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">&ldquo;Hey.&rdquo;<br />&ldquo;Hey.&rdquo;<br />&ldquo;How are you?&rdquo;<br />The end.</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">At first I got upset at the implicit rudeness of my conversation partner walking away and ignoring the question. But then I decided to get curious instead and posted&nbsp;a sample exchange (names omitted)&nbsp;on Facebook with a request for feedback. Unsurprisingly I learned more this way.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0); font-style: inherit;\">Some kind friends helped me troubleshoot the exchange, and in the process of figuring out how online conversation differs from in-person conversation, I realized what these things do in live conversation. They act as a kind of implicit communication protocol by which two parties negotiate how much interaction they&rsquo;re willing to have.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Consider this live conversation:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">&ldquo;Hi.&rdquo;<br />&ldquo;Hi.&rdquo;<br />The end.</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">No mystery here. Two people acknowledged one another&rsquo;s physical presence, and then the interaction ended. This is bare-bones maintenance of your status as persons who can relate to one another socially. There is no intimacy, but at least there is acknowledgement of someone else&rsquo;s existence. A day with &ldquo;Hi&rdquo; alone is less lonely than a day without it.</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">&ldquo;Hi.&rdquo;<br />&ldquo;Hi, how&rsquo;s it going?&rdquo;<br />&ldquo;Can&rsquo;t complain. And you?&rdquo;<br />&ldquo;Life.&rdquo;</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">This exchange establishes the parties as mutually sympathetic &ndash; the kind of people who would ask about each other&rsquo;s emotional state &ndash; but still doesn&rsquo;t get to real intimacy. It is basically just a drawn-out version of the example with just &ldquo;Hi&rdquo;. The exact character of the third and fourth line don&rsquo;t matter much, as there is no real content. For this reason, it isn&rsquo;t particularly rude to leave the question totally unanswered if you&rsquo;re already rounding a corner &ndash; but if you&rsquo;re in each other&rsquo;s company for a longer period of time, you&rsquo;re supposed to give at least a pro forma answer.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">This kind of thing drives crazy the kind of people who actually want to know how someone is, because people often assume that the question is meant insincerely. I&rsquo;m one of the people driven crazy. But this kind of mutual &ldquo;bidding up&rdquo; is important because sometimes people don&rsquo;t want to have a conversation, and if you just launch into your complaint or story or whatever it is you may end up inadvertently cornering someone who doesn&rsquo;t feel like listening to it.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">You could ask them explicitly, but people sometimes feel uncomfortable turning down that kind of request. So the way to open a substantive topic of conversation is to leave a hint and let the other person decide whether to pick it up.&nbsp;So here are some examples of leaving a hint:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">&ldquo;Hi.&rdquo;<br />&ldquo;Hi.&rdquo;<br />&ldquo;Anything interesting this weekend?&rdquo;<br />&ldquo;Oh, did a few errands, caught up on some reading. See you later.&rdquo;</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">This is a way to indicate interest in more than just a &ldquo;Fine, how are you?&rdquo; response. What happened here is that one party asked about the weekend, hoping to elicit specific information to generate a conversation. The other politely technically answered the question without any real information, declining the opportunity to talk about their life.</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">&ldquo;Hi.&rdquo;<br />&ldquo;Hi.&rdquo;<br />&ldquo;Anything interesting happen over the weekend?&rdquo;<br />&ldquo;Oh, did a few errands, caught up on some reading.&rdquo;<br />&ldquo;Ugh, I was going to go to a game, but my basement flooded and I had to take care of that instead.&rdquo;<br />&ldquo;That&rsquo;s tough.&rdquo;<br />&ldquo;Yeah.&rdquo;<br />&ldquo;See you around.&rdquo;</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Here, the person who first asked about the weekend didn&rsquo;t get an engaged response, but got enough of a pro forma response to provide cover for an otherwise out of context complaint and bid for sympathy. The other person offered perfunctory sympathy, and&nbsp;ended&nbsp;the conversation.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Here&rsquo;s a way for the recipient of a &ldquo;How are you?&rdquo; to make a bid for more conversation:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">&ldquo;Hi.&rdquo;<br />&ldquo;Hi.&rdquo;<br />&ldquo;How are you?&rdquo;<br />&ldquo;Oh, my basement flooded over the weekend.&rdquo;<br />&ldquo;That&rsquo;s tough.&rdquo;<br />&ldquo;Yeah.&rdquo;<br />&ldquo;See you around.&rdquo;</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">So the person with the flooded basement provided a socially-appropriate snippet of information &ndash; enough to be a recognizable bid for sympathy, but little enough not to force the other person to choose between listening to a long complaint or rudely cutting off the conversation.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Here&rsquo;s what it looks like if the other person accepts the bid:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">&ldquo;Hi.&rdquo;<br />&ldquo;Hi.&rdquo;<br />&ldquo;How are you?&rdquo;<br />&ldquo;Oh, my basement flooded over the weekend.&rdquo;<br />&ldquo;Wow, that&rsquo;s tough. Is the upstairs okay?&rdquo;<br />&ldquo;Yeah, but it&rsquo;s a finished basement so I&rsquo;m going to have to get a bunch of it redone because of water damage.&rdquo;<br />&ldquo;Ooh, that&rsquo;s tough. Hey, if you need a contractor, I had a good experience with mine when I had my kitchen done.&rdquo;<br />&ldquo;Thanks, that would be a big help, can you email me their contact info?&rdquo;</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">By asking a specific follow-up question the other person indicated that they wanted to hear more about the problem &ndash; which gave the person with the flooded basement permission not just to answer the question directly, but to volunteer additional information / complaints.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">You can do the same thing with happy events, of course:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">&ldquo;Hi.&rdquo;<br />&ldquo;Hi.&rdquo;<br />&ldquo;How are you?&rdquo;<br />&ldquo;I&rsquo;m getting excited for my big California vacation.&rdquo;<br />&ldquo;Oh really, where are you going?&rdquo;<br />&ldquo;We&rsquo;re flying out to Los Angeles, and then we&rsquo;re going to spend a few days there but then drive up to San Francisco, spend a day or two in town, then go hiking in the area.<span style=\"border: 0px; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;\">&ldquo;</span><br />&ldquo;Cool. I used to live in LA, let me know if you need any recommendations.&rdquo;<br />&ldquo;Thanks, I&rsquo;ll come by after lunch?&rdquo;</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">So what went wrong online? Here&rsquo;s the conversation again so you don&rsquo;t have to scroll back up:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">&ldquo;Hey.&rdquo;<br />&ldquo;Hey.&rdquo;<br />&ldquo;How are you?&rdquo;<br />The end.</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Online, there are no external circumstances that demand a &ldquo;Hi,&rdquo; such as passing someone (especially someone you know) in the hallway or getting into an elevator.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">If you import in-person conversational norms, the &ldquo;Hi&rdquo; is redundant &ndash; but instead online it can function as a query as to whether the other person is actually &ldquo;present&rdquo; and available for conversation. (You don&rsquo;t want to start launching into a conversation just because someone&rsquo;s status reads &ldquo;available&rdquo; only to find out they&rsquo;re in the &nbsp;middle of something else and don&rsquo;t &nbsp;have time to read what you wrote.)</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Let&rsquo;s say you&rsquo;ve mutually said &ldquo;Hi.&rdquo; If you were conversing in person, the next thing to do would be to query for a basic status update, asking something like, &ldquo;How are you?&rdquo;. But &ldquo;Hi&rdquo; already did the work of &ldquo;How are you?&rdquo;. Somehow the norm of &ldquo;How are you?&rdquo; being a mostly insincere query doesn&rsquo;t get erased, even though &ldquo;Hi&rdquo; does its work &ndash; so some people think you&rsquo;re being bizarrely redundant. Others might actually tell you how they are.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">To be safe, it&rsquo;s best to open with a short question apropos to what you want to talk about &ndash; or, since it&rsquo;s costless online and serves the same function as &ldquo;Hi&rdquo;, just start with &ldquo;How are you?&rdquo; as your opener.</span></p>\n<h2 style=\"border: 0px; font-style: inherit; font-weight: inherit; margin: 0px 0px 0.8125em; outline: 0px; padding: 0px; vertical-align: baseline; clear: both;\"><strong style=\"border: 0px; font-style: inherit; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"margin: 0px; outline: invert none 0px; vertical-align: baseline; border: 0px; padding: 0px; text-decoration: underline;\"><span style=\"font-size: x-small;\">What&rsquo;s New?</span></span></strong></h2>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">I recently had occasion to explain to someone how to respond when someone asks &ldquo;what&rsquo;s new?&rdquo;, and in the process, ended up explaining some stuff I hadn&rsquo;t realized until the moment I tried to explain it. So I figured this might be a high-value thing to explain to others here on the blog.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Of course, sometimes &ldquo;what&rsquo;s new?&rdquo; is just part of a passing handshake with no content &ndash; I covered that in the first section. But if you&rsquo;re already in a context where you know you&rsquo;re going to be having a conversation, you&rsquo;re supposed to answer the question, otherwise you get conversations like this:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">&ldquo;Hi.&rdquo;<br />&ldquo;Hi.&rdquo;<br />&ldquo;What&rsquo;s new?&rdquo;<br />&ldquo;Not much. How about you?&rdquo;<br />&ldquo;Can&rsquo;t complain.&rdquo;<br />Awkward silence.</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">So I&rsquo;m talking about cases where you actually have to answer the question.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">The problem is that some people, when asked &ldquo;What&rsquo;s New?&rdquo;, will try to think about when they last met the person asking, and all the events in their life since then, sorted from most to least momentous. This is understandably an overwhelming task.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">The trick to responding correctly is to think of your conversational partner&rsquo;s likely motives for asking. They are very unlikely to want a complete list. Nor do they necessarily want to know the thing in your life that happened that&rsquo;s objectively most notable. Think about it &ndash; when&rsquo;s the last time you wanted to know those things?</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Instead, what&rsquo;s most likely the case is that they want to have a conversation about a topic you are comfortable with, are interested in, and have something to say about. &ldquo;What&rsquo;s New?&rdquo; is an offer they are making, to let you pick the life event you most feel like discussing at that time. So for example, if the dog is sick but you&rsquo;d rather talk about a new book you&rsquo;re reading, you get to talk about the book and you can completely fail to mention the dog. You&rsquo;re not lying, you&rsquo;re answering the question as intended.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0); font-style: inherit;\"><a href=\"http://benjaminrosshoffman.com/handshakes-whats-new/\">Cross-posted</a> on my <a href=\"http://www.benjaminrosshoffman.com\">personal blog</a>.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZXFpyQWPB5ideFbEG": 1, "AADZcNS24mmSfPp2w": 1, "Q6P8jLn8hH7kbuXRr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "huRxRzwcvwTzvtEPY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 99, "extendedScore": null, "score": 0.000253, "legacy": true, "legacyId": "25173", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 99, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"entry-content\" style=\"border: 0px; margin: 1.286em 0px; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"font-weight: normal;\">This is an attempt to explicitly model what's going on in some small talk conversations. My hope is that at least one of these things will happen:</span></p>\n<ul>\n<li>There is a substantial flaw or missing element to my model that someone will point out.</li>\n<li>Many readers, who are bad at small talk because they don't see the point, will get better at it as a result of acquiring understanding.</li>\n</ul>\n<h2 style=\"border: 0px; font-style: inherit; font-weight: inherit; margin: 0px 0px 0.8125em; outline: 0px; padding: 0px; vertical-align: baseline; clear: both;\" id=\"Handshakes\"><span style=\"margin: 0px; outline: invert none 0px; vertical-align: baseline; border: 0px; padding: 0px; text-decoration: underline;\"><strong style=\"border: 0px; font-style: inherit; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"font-size: x-small;\">Handshakes</span></strong></span></h2>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">I had some recent conversational failures online, that went roughly like this:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">\u201cHey.\u201d<br>\u201cHey.\u201d<br>\u201cHow are you?\u201d<br>The end.</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">At first I got upset at the implicit rudeness of my conversation partner walking away and ignoring the question. But then I decided to get curious instead and posted&nbsp;a sample exchange (names omitted)&nbsp;on Facebook with a request for feedback. Unsurprisingly I learned more this way.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0); font-style: inherit;\">Some kind friends helped me troubleshoot the exchange, and in the process of figuring out how online conversation differs from in-person conversation, I realized what these things do in live conversation. They act as a kind of implicit communication protocol by which two parties negotiate how much interaction they\u2019re willing to have.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Consider this live conversation:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">\u201cHi.\u201d<br>\u201cHi.\u201d<br>The end.</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">No mystery here. Two people acknowledged one another\u2019s physical presence, and then the interaction ended. This is bare-bones maintenance of your status as persons who can relate to one another socially. There is no intimacy, but at least there is acknowledgement of someone else\u2019s existence. A day with \u201cHi\u201d alone is less lonely than a day without it.</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">\u201cHi.\u201d<br>\u201cHi, how\u2019s it going?\u201d<br>\u201cCan\u2019t complain. And you?\u201d<br>\u201cLife.\u201d</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">This exchange establishes the parties as mutually sympathetic \u2013 the kind of people who would ask about each other\u2019s emotional state \u2013 but still doesn\u2019t get to real intimacy. It is basically just a drawn-out version of the example with just \u201cHi\u201d. The exact character of the third and fourth line don\u2019t matter much, as there is no real content. For this reason, it isn\u2019t particularly rude to leave the question totally unanswered if you\u2019re already rounding a corner \u2013 but if you\u2019re in each other\u2019s company for a longer period of time, you\u2019re supposed to give at least a pro forma answer.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">This kind of thing drives crazy the kind of people who actually want to know how someone is, because people often assume that the question is meant insincerely. I\u2019m one of the people driven crazy. But this kind of mutual \u201cbidding up\u201d is important because sometimes people don\u2019t want to have a conversation, and if you just launch into your complaint or story or whatever it is you may end up inadvertently cornering someone who doesn\u2019t feel like listening to it.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">You could ask them explicitly, but people sometimes feel uncomfortable turning down that kind of request. So the way to open a substantive topic of conversation is to leave a hint and let the other person decide whether to pick it up.&nbsp;So here are some examples of leaving a hint:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">\u201cHi.\u201d<br>\u201cHi.\u201d<br>\u201cAnything interesting this weekend?\u201d<br>\u201cOh, did a few errands, caught up on some reading. See you later.\u201d</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">This is a way to indicate interest in more than just a \u201cFine, how are you?\u201d response. What happened here is that one party asked about the weekend, hoping to elicit specific information to generate a conversation. The other politely technically answered the question without any real information, declining the opportunity to talk about their life.</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">\u201cHi.\u201d<br>\u201cHi.\u201d<br>\u201cAnything interesting happen over the weekend?\u201d<br>\u201cOh, did a few errands, caught up on some reading.\u201d<br>\u201cUgh, I was going to go to a game, but my basement flooded and I had to take care of that instead.\u201d<br>\u201cThat\u2019s tough.\u201d<br>\u201cYeah.\u201d<br>\u201cSee you around.\u201d</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Here, the person who first asked about the weekend didn\u2019t get an engaged response, but got enough of a pro forma response to provide cover for an otherwise out of context complaint and bid for sympathy. The other person offered perfunctory sympathy, and&nbsp;ended&nbsp;the conversation.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Here\u2019s a way for the recipient of a \u201cHow are you?\u201d to make a bid for more conversation:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">\u201cHi.\u201d<br>\u201cHi.\u201d<br>\u201cHow are you?\u201d<br>\u201cOh, my basement flooded over the weekend.\u201d<br>\u201cThat\u2019s tough.\u201d<br>\u201cYeah.\u201d<br>\u201cSee you around.\u201d</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">So the person with the flooded basement provided a socially-appropriate snippet of information \u2013 enough to be a recognizable bid for sympathy, but little enough not to force the other person to choose between listening to a long complaint or rudely cutting off the conversation.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Here\u2019s what it looks like if the other person accepts the bid:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">\u201cHi.\u201d<br>\u201cHi.\u201d<br>\u201cHow are you?\u201d<br>\u201cOh, my basement flooded over the weekend.\u201d<br>\u201cWow, that\u2019s tough. Is the upstairs okay?\u201d<br>\u201cYeah, but it\u2019s a finished basement so I\u2019m going to have to get a bunch of it redone because of water damage.\u201d<br>\u201cOoh, that\u2019s tough. Hey, if you need a contractor, I had a good experience with mine when I had my kitchen done.\u201d<br>\u201cThanks, that would be a big help, can you email me their contact info?\u201d</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">By asking a specific follow-up question the other person indicated that they wanted to hear more about the problem \u2013 which gave the person with the flooded basement permission not just to answer the question directly, but to volunteer additional information / complaints.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">You can do the same thing with happy events, of course:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">\u201cHi.\u201d<br>\u201cHi.\u201d<br>\u201cHow are you?\u201d<br>\u201cI\u2019m getting excited for my big California vacation.\u201d<br>\u201cOh really, where are you going?\u201d<br>\u201cWe\u2019re flying out to Los Angeles, and then we\u2019re going to spend a few days there but then drive up to San Francisco, spend a day or two in town, then go hiking in the area.<span style=\"border: 0px; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;\">\u201c</span><br>\u201cCool. I used to live in LA, let me know if you need any recommendations.\u201d<br>\u201cThanks, I\u2019ll come by after lunch?\u201d</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">So what went wrong online? Here\u2019s the conversation again so you don\u2019t have to scroll back up:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">\u201cHey.\u201d<br>\u201cHey.\u201d<br>\u201cHow are you?\u201d<br>The end.</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Online, there are no external circumstances that demand a \u201cHi,\u201d such as passing someone (especially someone you know) in the hallway or getting into an elevator.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">If you import in-person conversational norms, the \u201cHi\u201d is redundant \u2013 but instead online it can function as a query as to whether the other person is actually \u201cpresent\u201d and available for conversation. (You don\u2019t want to start launching into a conversation just because someone\u2019s status reads \u201cavailable\u201d only to find out they\u2019re in the &nbsp;middle of something else and don\u2019t &nbsp;have time to read what you wrote.)</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Let\u2019s say you\u2019ve mutually said \u201cHi.\u201d If you were conversing in person, the next thing to do would be to query for a basic status update, asking something like, \u201cHow are you?\u201d. But \u201cHi\u201d already did the work of \u201cHow are you?\u201d. Somehow the norm of \u201cHow are you?\u201d being a mostly insincere query doesn\u2019t get erased, even though \u201cHi\u201d does its work \u2013 so some people think you\u2019re being bizarrely redundant. Others might actually tell you how they are.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">To be safe, it\u2019s best to open with a short question apropos to what you want to talk about \u2013 or, since it\u2019s costless online and serves the same function as \u201cHi\u201d, just start with \u201cHow are you?\u201d as your opener.</span></p>\n<h2 style=\"border: 0px; font-style: inherit; font-weight: inherit; margin: 0px 0px 0.8125em; outline: 0px; padding: 0px; vertical-align: baseline; clear: both;\" id=\"What_s_New_\"><strong style=\"border: 0px; font-style: inherit; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"margin: 0px; outline: invert none 0px; vertical-align: baseline; border: 0px; padding: 0px; text-decoration: underline;\"><span style=\"font-size: x-small;\">What\u2019s New?</span></span></strong></h2>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">I recently had occasion to explain to someone how to respond when someone asks \u201cwhat\u2019s new?\u201d, and in the process, ended up explaining some stuff I hadn\u2019t realized until the moment I tried to explain it. So I figured this might be a high-value thing to explain to others here on the blog.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Of course, sometimes \u201cwhat\u2019s new?\u201d is just part of a passing handshake with no content \u2013 I covered that in the first section. But if you\u2019re already in a context where you know you\u2019re going to be having a conversation, you\u2019re supposed to answer the question, otherwise you get conversations like this:</span></p>\n<blockquote>\n<p style=\"border: 0px; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">\u201cHi.\u201d<br>\u201cHi.\u201d<br>\u201cWhat\u2019s new?\u201d<br>\u201cNot much. How about you?\u201d<br>\u201cCan\u2019t complain.\u201d<br>Awkward silence.</span></p>\n</blockquote>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">So I\u2019m talking about cases where you actually have to answer the question.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">The problem is that some people, when asked \u201cWhat\u2019s New?\u201d, will try to think about when they last met the person asking, and all the events in their life since then, sorted from most to least momentous. This is understandably an overwhelming task.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">The trick to responding correctly is to think of your conversational partner\u2019s likely motives for asking. They are very unlikely to want a complete list. Nor do they necessarily want to know the thing in your life that happened that\u2019s objectively most notable. Think about it \u2013 when\u2019s the last time you wanted to know those things?</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0);\">Instead, what\u2019s most likely the case is that they want to have a conversation about a topic you are comfortable with, are interested in, and have something to say about. \u201cWhat\u2019s New?\u201d is an offer they are making, to let you pick the life event you most feel like discussing at that time. So for example, if the dog is sick but you\u2019d rather talk about a new book you\u2019re reading, you get to talk about the book and you can completely fail to mention the dog. You\u2019re not lying, you\u2019re answering the question as intended.</span></p>\n<p style=\"border: 0px; font-style: inherit; margin: 0px 0px 1em; outline: 0px; padding: 0px; vertical-align: baseline;\"><span style=\"background-color: rgba(255, 255, 255, 0); font-style: inherit;\"><a href=\"http://benjaminrosshoffman.com/handshakes-whats-new/\">Cross-posted</a> on my <a href=\"http://www.benjaminrosshoffman.com\">personal blog</a>.</span></p>", "sections": [{"title": "Handshakes", "anchor": "Handshakes", "level": 1}, {"title": "What\u2019s New?", "anchor": "What_s_New_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "79 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-03T09:24:26.686Z", "modifiedAt": null, "url": null, "title": "[Link] Changelings, Infanticide and Nortwest European Guilt Culture", "slug": "link-changelings-infanticide-and-nortwest-european-guilt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:59.704Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NrNkhD7XThbwrYsme/link-changelings-infanticide-and-nortwest-european-guilt", "pageUrlRelative": "/posts/NrNkhD7XThbwrYsme/link-changelings-infanticide-and-nortwest-european-guilt", "linkUrl": "https://www.lesswrong.com/posts/NrNkhD7XThbwrYsme/link-changelings-infanticide-and-nortwest-european-guilt", "postedAtFormatted": "Friday, January 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Changelings%2C%20Infanticide%20and%20Nortwest%20European%20Guilt%20Culture&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Changelings%2C%20Infanticide%20and%20Nortwest%20European%20Guilt%20Culture%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNrNkhD7XThbwrYsme%2Flink-changelings-infanticide-and-nortwest-european-guilt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Changelings%2C%20Infanticide%20and%20Nortwest%20European%20Guilt%20Culture%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNrNkhD7XThbwrYsme%2Flink-changelings-infanticide-and-nortwest-european-guilt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNrNkhD7XThbwrYsme%2Flink-changelings-infanticide-and-nortwest-european-guilt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2791, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/28k/the_psychological_diversity_of_mankind/\">The Psychological Diversity of Mankind</a>, <a href=\"/lw/yl/an_african_folktale/\">An African Folktale</a>, many of the more interesting infanticide &amp; abortion debates on this site</p>\n<p>A fascinating post that however might need some background reading, most relevant material is linked in the article itself. I encourage reading up on the material.</p>\n<p><a href=\"http://staffanspersonalityblog.wordpress.com/2014/01/02/changelings-infanticide-and-nortwest-european-guilt-culture/\">Link to article.</a></p>\n<p>Stories about changelings replacing babies and the recommended course of  action being basically to expose the child is not a human universal,  they are found only in European cultures. These rely more heavily on  guilt and less on shame to regulate behavior than most other human  societies. This may not be a coincidence. The stories look like they work as a  ready made rationalization to reduce guilt from infanticide. Common  problems often acquire common solutions like this.</p>\n<blockquote>\n<p><strong>Guilt and Shame Cultures</strong></p>\n<p>On his blog <a href=\"http://evoandproud.blogspot.se/\">Evo and Proud</a>, anthropologist Peter Frost recently wrote a highly interesting two-part article entitled <a href=\"http://evoandproud.blogspot.se/2013/12/the-origins-of-northwest-european-guilt.html\">The origins of Northwestern European guilt culture</a>. In guilt cultures, social control is regulated more by guilt than by shame, as is the case in shame cultures that exist in most parts of the world. A crucial difference between these types of cultures is that while shame cultures require other people to shame the wrongdoer, guilt cultures do not. Instead, he or she will shame themselves by feeling guilty. This, according to Frost, is also linked to a stronger sense of empathy with others, not just with relatives but people in general.</p>\n<p>The advantages of guilt over shame are many. People can go about their business without being supervised by others, and they can cooperate with people they&rsquo;re not related to as long as both parties have the same view on right and wrong. And with this personal freedom come individualism, innovation and other forms of creativity as well as ideas of universal human rights etc. You could argue, as Frost appears to, that the increased sense of guilt in Northwestern Europe (NWE) is a major factor behind Western Civilization. While this sounds fairly plausible (in my ears at least), a fundamental question is whether there really is more guilt in the NWE sphere than elsewhere.</p>\n<p><strong>How to Measure Guilt</strong></p>\n<p>The idea of NWE countries as guilt cultures may seem obvious to some and dubious to others. The Protestant tradition is surely one indication of this, but some anthropologists argue that other cultures have other forms of guilt, not as easily recognized by Western scholars. For instance, <a href=\"http://www.brunel.ac.uk/sss/anthropology/staff/andrew-beatty\">Andrew Beatty</a> mentions that the Javanese have no word for either shame or guilt but report uneasiness and a sense of haunting regarding certain political murders they&rsquo;ve committed. So maybe they have just as much guilt as NWE Protestants?</p>\n<p>This is one of the problems with soft science &ndash; you can argue about the meaning of terms and concepts back and forth until hell freezes over without coming to any useful conclusion. One way around this is to find some robust metric that most people would agree indicates guilt. One such measure, I believe, would be murder rate. If people in different cultures vary in the guilt they feel for committing murder, then this should hold them back and show up as a variation in the murder rate. I will here take the NWE region to mean the British Isles, the Nordic countries (excluding Finland), Germany, France and Belgium, Netherlands, Luxembourg, Australia, New Zealand and Canada for a total of 14 countries. According to <a href=\"http://en.wikipedia.org/wiki/List_of_countries_by_intentional_homicide_rate\">UNODC/Wikipedia</a>, the average murder rate in the NWE countries is exactly 1.0 murder per 100K inhabitants. To put this in perspective, only 20 other countries (and territories) of 207 listed are below this level and 70 percent of them have twice the murder rate or more.</p>\n<p>Still, criminals are after all not a very representative group having more of the <a href=\"http://en.wikipedia.org/wiki/Dark_triad\">dark traits</a> (psychopathy, narcissism, machiavellism) than the rest of the population. Corruption, on the other hand, as I&rsquo;ve argued in an <a href=\"http://staffanspersonalityblog.wordpress.com/?s=the+corrupt+person\">earlier post</a>, seems relatively unrelated to regular personality traits, so it should tap into the mainstream population. Corruption is often about minor transgressions that many people engage in knowing that they can usually get away with it. They will not be shamed because no one will know about it and many will not care since it&rsquo;s so common, but some will feel guilty and refrain from it. &nbsp;Looking at the <a href=\"http://www.transparency.org/cpi2013/results\">Corruptions Perceptions Index for 2013</a>, the NWE countries are very dominant at the top of the ranking (meaning they lack corruption). There are seven NWEs in the top ten and two additional bordering countries (Finland and Switzerland). &nbsp;The entire NWE region is within the top 24, of a 177 countries and territories.</p>\n<p>But as I&rsquo;ve argued before here, corruption appears to be <a href=\"http://staffanspersonalityblog.wordpress.com/?s=the+corrupt+person\">linked to clannishness</a> and tribalism (traits rarely discussed in psychology) and it&rsquo;s reasonable to assume that it is a casual factor. How does this all add up? Well, the clannish and tribal cultures that I broadly refer to as traditional cultures are all based on the <a href=\"http://hbdchick.wordpress.com/?s=clannishness+defined\">premise</a> that the family, tribe or similar ingroup is that which should be everyone&rsquo;s first concern. So while a member of a traditional culture may have personal feelings of guilt, this means little compared to the collective dislike &ndash; the shame &ndash; from the family or tribe. At the same time traditional cultures are indifferent or hostile towards other groups so if your corruption serves the family or tribe there will be no shame in it, the others will more likely praise you for being clever.</p>\n<p>(In this context it&rsquo;s also interesting to note that people who shame others often do this by expressing <a href=\"http://staffanspersonalityblog.wordpress.com/2013/11/18/a-little-speculation-about-disgust-sensitivity-and-attitudes-towards-homosexuals-and-people-of-other-races/\">disgust</a>, an emotion linked to a traditional dislike for various outgroups, such as homosexuals or people of other races. So disgust, which psychologist Jonathan Haidt connects with the moral foundation of sanctity/degradation, is perhaps equally important to the foundation loyalty/ingroup.)</p>\n<p><strong>When Did Modernity Begin?</strong></p>\n<p>One important question is whether this distinction between modern and traditional is to what extent it&rsquo;s a matter of nature or nurture. There is evidence that it is caused by inbreeding and the accumulation of genes for familial altruism (that&rsquo;s to say a concern for relatives and a corresponding dislike for non-relatives). Since studies on this are non-existent as far as I know &ndash; no doubt for political reasons &ndash; another form of evidence could be found in tracing this distinction back in time. The further we can do this, the more likely it&rsquo;s a matter of genes rather than culture. And the better we can identify populations that are innately modern the better we can understanding the function and origin of this trait. Frost argues that guilt culture can be found as early as the Anglo-Saxon period (550-1066), based thing like the existence of looser family structures with a relatively late age of marriage and the notion of a shame before the spirits or God, which can be construed as guilt. This made me wonder if there is any similar historical evidence for NWE guilt that is old enough to make the case for this to be an inherited behavior (or at least the capacity for guilt-motivated behavior). And that&rsquo;s how I came up with the changeling,</p>\n<p><strong>The Changeling</strong></p>\n<p>As Jung has argued, there is a striking similarity between myths and traditional storytelling over the world. People who have never been in contact with each other have certain recurring structures in their narratives, and, as I&rsquo;ve argued <a href=\"http://staffanspersonalityblog.wordpress.com/2013/12/09/why-a-good-story-must-be-archetypal-and-why-modern-storytellers-must-lie-about-it/\">before here</a>, even modern people adhere to these unspoken rules of storytelling &ndash; the archetypes. The only reasonable explanation for archetypes is that they are a reflection of how humans are wired. But if archetypal stories reveal a universal human nature, what about stories found in some places but not in others? In some cases they may reflect differences in things like climate or geography, but if no such environmental explanation can be found I believe that the variation may be a case of human biodiversity.</p>\n<p>I believe one such variation relevant to guilt culture is the genre of <a href=\"http://en.wikipedia.org/wiki/Changeling\">changeling</a> tales. These folktales are invariably about how otherworldly creatures like fairies abduct newborn children and replace them with something in their likeness, a changeling. The changeling is sometimes a fairy, sometimes just an enchanted piece of wood that has been made to look like a child. It&rsquo;s typically very hungry but sickly and fails to thrive. A woman who suspected that she had a changeling on her hands could find out by beating the changeling, throwing it in the water, leaving it in the woods overnight and so on. According to the folktales, this would prompt the fairies or whoever was responsible for the exchange to come to rescue their child and also return the child they had taken.</p>\n<p><strong>Infanticide Made Easy<br /> </strong></p>\n<p>Most scholars agree that the changeling tales was a way to justify killing sickly and deformed children. According to American folklorist <a href=\"http://www.pitt.edu/%7Edash/changeling.html#protective\">D. L. Ashliman</a> at the University of Pittsburgh, people firmly believed in changelings and did as the tales instructed,</p>\n<p><em>\"\"There is ample evidence that these legendary accounts do not misrepresent or exaggerate the actual abuse of suspected changelings. Court records between about 1850 and 1900 in Germany, Scandinavia, Great Britain, and Ireland reveal numerous proceedings against defendants accused of torturing and murdering suspected changelings.\"\"</em></p>\n<p>This all sounds pretty grisly but before modern medicine and social welfare institutions, a child of this kind was a disaster. Up until the 1900s, children were supposed to be relatively self-sufficient and help out around the house. A child that needed constant supervision without any prospect of ever being able contribute anything to the household was more than a burden; it jeopardized the future of the entire family.</p>\n<p>Still, there is probably no stronger bond between two people than that between a mother and her newborn child. So how could a woman not feel guilty for killing her own child? Because it must be guilt we&rsquo;re talking about here &ndash; you would never be shamed for doing it since it was according to custom. The belief in changelings expressed in the folktales gave the women (and men) a way out of this dilemma. (Ironically, Martin Luther, the icon of guilt culture, dismissed all the popular superstitions of his fellow countrymen with the sole exception of changelings which he firmly believed in.) Thus, the main purpose of these tales seems to have been to alleviate guilt.</p>\n<p><strong>Geography </strong></p>\n<p>If this is true then changeling stories should be more common in the NWE region than elsewhere, which also seems to be the case. There are numerous changeling tales found on the British Isles, in Scandinavia, Germany and France. It can be found elsewhere in Europe as well, in the Basque region and among Slavic people and even as far as North Africa, but at least according to folklorists I&rsquo;ve found discussing these tales, they are <a href=\"http://www.doiserbia.nb.rs/img/doi/0350-7653/2002/0350-76530233143R.pdf\">imported</a> from the NWE region. And if we look beyond regions bordering to Europe changelings seem to be virtually non-existent. Some folklorists have suggested that for instance the Nigerian <a href=\"http://en.wikipedia.org/wiki/Ogbanje\">Ogbanje</a> can be thought of as a changeling, although at a closer inspection the similarity is very superficial. The Ogbanje is reborn into the same family over and over and to break the curse families consult medicine men after the child has died. When they consult a medicine man when the child is still alive it is for the purpose of severing the child&rsquo;s connection to the spirit world and make it normal. So the belief in the Ogbanje never justifies infanticide. Another contender is the Filipino <a href=\"http://en.wikipedia.org/wiki/Aswang\">Aswang</a> which is a creature that will attack children as well as adults and is never takes the place of a child but is more like a vampire. So it&rsquo;s safe to say that the changeling belief is firmly rooted in the NWE region at least back to medieval times and perhaps earlier too.</p>\n<p><strong>Before There Were Changelings, There Was Exposure</strong></p>\n<p>Given how infanticide is such a good candidate for measuring guilt, we could go back further in time, before any evidence of changelings and look at potential differences in attitudes towards this act.</p>\n<p>I doing so I think we can find, if not NWE guilt, so at least Western ditto. According this <a href=\"http://en.wikipedia.org/wiki/Infanticide#Greece_and_Rome\">Wikipedia article</a>, the ancient Greeks and Romans as well as Germanic tribes, killed infants by exposure rather than through a direct act. Here is a quote on the practice in Greece,</p>\n<p><em>\"\"Babies would often be rejected if they were illegitimate, unhealthy or deformed, the wrong sex, or too great a burden on the family. These babies would not be directly killed, but put in a clay pot or jar and deserted outside the front door or on the roadway. In ancient Greek religion, this practice took the responsibility away from the parents because the child would die of natural causes, for example hunger, asphyxiation or exposure to the elements.\"\"</em></p>\n<p>And the Archeology and Classical Research Magazine <a href=\"http://ancientimes.blogspot.se/2011/05/widespread-roman-infanticide-not.html\">Roman Times</a> quotes several classical sources suggesting that exposure was controversial even back then,</p>\n<p><em>\"\"Isocrates (436&ndash;338 BCE) &nbsp;includes the exposure of infants in his catalog of horrendous crimes practiced in some &nbsp;cities (other than Athens) in his work Panathenaicus.\"\"</em></p>\n<p>I also found this <a href=\"http://internetbiblecollege.net/Lessons/Killing%20new-borns%20in%20ancient%20greece%20and%20rome.pdf\">excerpt</a> from the play Ion by Euripides, written at the end of the 400s BC. In it Kreusa talks with an old servant about having exposed an unwanted child,</p>\n<p>Old Servant: Who cast him forth? &ndash; Not thou &ndash; O never thou!</p>\n<p>Kreusa: Even I. My vesture darkling swaddled him.</p>\n<p>Old Servant: Nor any knew the exposing of the child?</p>\n<p>Kreusa: None &ndash; Misery and Secrecy alone.</p>\n<p>Old Servant: How couldst thou leave they babe within the cave?</p>\n<p>Kreusa: Ah how? &ndash; O pitiful farewells I moaned!</p>\n<p>It seems to me that this play, by one of the most prominent playwrights of his time, would not make much sense to the audience unless exposure was something that weighed on many people&rsquo;s hearts.</p>\n<p>Compare this with historical accounts from other cultures, taken from the Wikipedia article mentioned above,</p>\n<p><em>\"\"Some authors believe that there is little evidence that infanticide was prevalent in pre-Islamic Arabia or early Muslim history, except for the case of the Tamim tribe, who practiced it during severe famine. Others state that &ldquo;female infanticide was common all over Arabia during this period of time&rdquo; (pre-Islamic Arabia), especially by burying alive a female newborn.</em></p>\n<p><em>In Kamchatka, babies were killed and thrown to the dogs.</em></p>\n<p><em>The Svans (a Georgian people) killed the newborn females by filling their mouths with hot ashes.</em></p>\n<p><em>A typical method in Japan was smothering through wet paper on the baby&rsquo;s mouth and nose. Mabiki persisted in the 19th century and early 20th century.</em></p>\n<p><em>Female infanticide of newborn girls was systematic in feudatory Rajputs in South Asia for illegitimate female children during the Middle Ages. According to Firishta, as soon as the illegitimate female child was born she was held &ldquo;in one hand, and a knife in the other, that any person who wanted a wife might take her now, otherwise she was immediately put to death&rdquo;</em></p>\n<p><em>Polar Inuit (Inughuit) killed the child by throwing him or her into the sea. There is even a legend in Inuit mythology, &ldquo;The Unwanted Child&rdquo;, where a mother throws her child into the fjord.\"\"</em></p>\n<p>It seems that while people in ancient Greece practiced exposure, something many were troubled by, the active killing was common in the rest of the world and persists to this day in many places. While people in other cultures may or may not feel guilt it doesn&rsquo;t seem to affect them as much, and it&rsquo;s sometimes even trumped by shame as psychiatrist Steven Pitts and clinical psychologist Erin Bale write in an <a href=\"http://www.jaapl.org/content/23/3/375.full.pdf\">article</a> in The Bulletin of the American Academy of Psychiatry and the Law regarding the practice of drowning unwanted girls,</p>\n<p><em>\"\"In China, the birth of a daughter has traditionally been accompanied by disappointment and even shame.\"\"</em></p>\n<p>To summarize, the changeling lore provides evidence of a NWE guilt culture dating back at least to medieval times, and the practice and attitude towards exposure suggests that ancient Greece had an emerging guilt culture as early as the 400s BC which enabled a similar individualism and intellectual development that we&rsquo;ve seen in the NWE in recent centuries. I&rsquo;m not sure exactly how genetically related these populations are, but the geographical proximity makes it hard to ignore the possibility of gene variants for guilt proneness in Europe responsible for guilt cultures both in ancient Greece and the NWE region. Some branch of Indo-Europeans perhaps?</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NrNkhD7XThbwrYsme", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 3, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "25174", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Related:</strong> <a href=\"/lw/28k/the_psychological_diversity_of_mankind/\">The Psychological Diversity of Mankind</a>, <a href=\"/lw/yl/an_african_folktale/\">An African Folktale</a>, many of the more interesting infanticide &amp; abortion debates on this site</p>\n<p>A fascinating post that however might need some background reading, most relevant material is linked in the article itself. I encourage reading up on the material.</p>\n<p><a href=\"http://staffanspersonalityblog.wordpress.com/2014/01/02/changelings-infanticide-and-nortwest-european-guilt-culture/\">Link to article.</a></p>\n<p>Stories about changelings replacing babies and the recommended course of  action being basically to expose the child is not a human universal,  they are found only in European cultures. These rely more heavily on  guilt and less on shame to regulate behavior than most other human  societies. This may not be a coincidence. The stories look like they work as a  ready made rationalization to reduce guilt from infanticide. Common  problems often acquire common solutions like this.</p>\n<blockquote>\n<p><strong id=\"Guilt_and_Shame_Cultures\">Guilt and Shame Cultures</strong></p>\n<p>On his blog <a href=\"http://evoandproud.blogspot.se/\">Evo and Proud</a>, anthropologist Peter Frost recently wrote a highly interesting two-part article entitled <a href=\"http://evoandproud.blogspot.se/2013/12/the-origins-of-northwest-european-guilt.html\">The origins of Northwestern European guilt culture</a>. In guilt cultures, social control is regulated more by guilt than by shame, as is the case in shame cultures that exist in most parts of the world. A crucial difference between these types of cultures is that while shame cultures require other people to shame the wrongdoer, guilt cultures do not. Instead, he or she will shame themselves by feeling guilty. This, according to Frost, is also linked to a stronger sense of empathy with others, not just with relatives but people in general.</p>\n<p>The advantages of guilt over shame are many. People can go about their business without being supervised by others, and they can cooperate with people they\u2019re not related to as long as both parties have the same view on right and wrong. And with this personal freedom come individualism, innovation and other forms of creativity as well as ideas of universal human rights etc. You could argue, as Frost appears to, that the increased sense of guilt in Northwestern Europe (NWE) is a major factor behind Western Civilization. While this sounds fairly plausible (in my ears at least), a fundamental question is whether there really is more guilt in the NWE sphere than elsewhere.</p>\n<p><strong id=\"How_to_Measure_Guilt\">How to Measure Guilt</strong></p>\n<p>The idea of NWE countries as guilt cultures may seem obvious to some and dubious to others. The Protestant tradition is surely one indication of this, but some anthropologists argue that other cultures have other forms of guilt, not as easily recognized by Western scholars. For instance, <a href=\"http://www.brunel.ac.uk/sss/anthropology/staff/andrew-beatty\">Andrew Beatty</a> mentions that the Javanese have no word for either shame or guilt but report uneasiness and a sense of haunting regarding certain political murders they\u2019ve committed. So maybe they have just as much guilt as NWE Protestants?</p>\n<p>This is one of the problems with soft science \u2013 you can argue about the meaning of terms and concepts back and forth until hell freezes over without coming to any useful conclusion. One way around this is to find some robust metric that most people would agree indicates guilt. One such measure, I believe, would be murder rate. If people in different cultures vary in the guilt they feel for committing murder, then this should hold them back and show up as a variation in the murder rate. I will here take the NWE region to mean the British Isles, the Nordic countries (excluding Finland), Germany, France and Belgium, Netherlands, Luxembourg, Australia, New Zealand and Canada for a total of 14 countries. According to <a href=\"http://en.wikipedia.org/wiki/List_of_countries_by_intentional_homicide_rate\">UNODC/Wikipedia</a>, the average murder rate in the NWE countries is exactly 1.0 murder per 100K inhabitants. To put this in perspective, only 20 other countries (and territories) of 207 listed are below this level and 70 percent of them have twice the murder rate or more.</p>\n<p>Still, criminals are after all not a very representative group having more of the <a href=\"http://en.wikipedia.org/wiki/Dark_triad\">dark traits</a> (psychopathy, narcissism, machiavellism) than the rest of the population. Corruption, on the other hand, as I\u2019ve argued in an <a href=\"http://staffanspersonalityblog.wordpress.com/?s=the+corrupt+person\">earlier post</a>, seems relatively unrelated to regular personality traits, so it should tap into the mainstream population. Corruption is often about minor transgressions that many people engage in knowing that they can usually get away with it. They will not be shamed because no one will know about it and many will not care since it\u2019s so common, but some will feel guilty and refrain from it. &nbsp;Looking at the <a href=\"http://www.transparency.org/cpi2013/results\">Corruptions Perceptions Index for 2013</a>, the NWE countries are very dominant at the top of the ranking (meaning they lack corruption). There are seven NWEs in the top ten and two additional bordering countries (Finland and Switzerland). &nbsp;The entire NWE region is within the top 24, of a 177 countries and territories.</p>\n<p>But as I\u2019ve argued before here, corruption appears to be <a href=\"http://staffanspersonalityblog.wordpress.com/?s=the+corrupt+person\">linked to clannishness</a> and tribalism (traits rarely discussed in psychology) and it\u2019s reasonable to assume that it is a casual factor. How does this all add up? Well, the clannish and tribal cultures that I broadly refer to as traditional cultures are all based on the <a href=\"http://hbdchick.wordpress.com/?s=clannishness+defined\">premise</a> that the family, tribe or similar ingroup is that which should be everyone\u2019s first concern. So while a member of a traditional culture may have personal feelings of guilt, this means little compared to the collective dislike \u2013 the shame \u2013 from the family or tribe. At the same time traditional cultures are indifferent or hostile towards other groups so if your corruption serves the family or tribe there will be no shame in it, the others will more likely praise you for being clever.</p>\n<p>(In this context it\u2019s also interesting to note that people who shame others often do this by expressing <a href=\"http://staffanspersonalityblog.wordpress.com/2013/11/18/a-little-speculation-about-disgust-sensitivity-and-attitudes-towards-homosexuals-and-people-of-other-races/\">disgust</a>, an emotion linked to a traditional dislike for various outgroups, such as homosexuals or people of other races. So disgust, which psychologist Jonathan Haidt connects with the moral foundation of sanctity/degradation, is perhaps equally important to the foundation loyalty/ingroup.)</p>\n<p><strong id=\"When_Did_Modernity_Begin_\">When Did Modernity Begin?</strong></p>\n<p>One important question is whether this distinction between modern and traditional is to what extent it\u2019s a matter of nature or nurture. There is evidence that it is caused by inbreeding and the accumulation of genes for familial altruism (that\u2019s to say a concern for relatives and a corresponding dislike for non-relatives). Since studies on this are non-existent as far as I know \u2013 no doubt for political reasons \u2013 another form of evidence could be found in tracing this distinction back in time. The further we can do this, the more likely it\u2019s a matter of genes rather than culture. And the better we can identify populations that are innately modern the better we can understanding the function and origin of this trait. Frost argues that guilt culture can be found as early as the Anglo-Saxon period (550-1066), based thing like the existence of looser family structures with a relatively late age of marriage and the notion of a shame before the spirits or God, which can be construed as guilt. This made me wonder if there is any similar historical evidence for NWE guilt that is old enough to make the case for this to be an inherited behavior (or at least the capacity for guilt-motivated behavior). And that\u2019s how I came up with the changeling,</p>\n<p><strong id=\"The_Changeling\">The Changeling</strong></p>\n<p>As Jung has argued, there is a striking similarity between myths and traditional storytelling over the world. People who have never been in contact with each other have certain recurring structures in their narratives, and, as I\u2019ve argued <a href=\"http://staffanspersonalityblog.wordpress.com/2013/12/09/why-a-good-story-must-be-archetypal-and-why-modern-storytellers-must-lie-about-it/\">before here</a>, even modern people adhere to these unspoken rules of storytelling \u2013 the archetypes. The only reasonable explanation for archetypes is that they are a reflection of how humans are wired. But if archetypal stories reveal a universal human nature, what about stories found in some places but not in others? In some cases they may reflect differences in things like climate or geography, but if no such environmental explanation can be found I believe that the variation may be a case of human biodiversity.</p>\n<p>I believe one such variation relevant to guilt culture is the genre of <a href=\"http://en.wikipedia.org/wiki/Changeling\">changeling</a> tales. These folktales are invariably about how otherworldly creatures like fairies abduct newborn children and replace them with something in their likeness, a changeling. The changeling is sometimes a fairy, sometimes just an enchanted piece of wood that has been made to look like a child. It\u2019s typically very hungry but sickly and fails to thrive. A woman who suspected that she had a changeling on her hands could find out by beating the changeling, throwing it in the water, leaving it in the woods overnight and so on. According to the folktales, this would prompt the fairies or whoever was responsible for the exchange to come to rescue their child and also return the child they had taken.</p>\n<p><strong id=\"Infanticide_Made_Easy_\">Infanticide Made Easy<br> </strong></p>\n<p>Most scholars agree that the changeling tales was a way to justify killing sickly and deformed children. According to American folklorist <a href=\"http://www.pitt.edu/%7Edash/changeling.html#protective\">D. L. Ashliman</a> at the University of Pittsburgh, people firmly believed in changelings and did as the tales instructed,</p>\n<p><em>\"\"There is ample evidence that these legendary accounts do not misrepresent or exaggerate the actual abuse of suspected changelings. Court records between about 1850 and 1900 in Germany, Scandinavia, Great Britain, and Ireland reveal numerous proceedings against defendants accused of torturing and murdering suspected changelings.\"\"</em></p>\n<p>This all sounds pretty grisly but before modern medicine and social welfare institutions, a child of this kind was a disaster. Up until the 1900s, children were supposed to be relatively self-sufficient and help out around the house. A child that needed constant supervision without any prospect of ever being able contribute anything to the household was more than a burden; it jeopardized the future of the entire family.</p>\n<p>Still, there is probably no stronger bond between two people than that between a mother and her newborn child. So how could a woman not feel guilty for killing her own child? Because it must be guilt we\u2019re talking about here \u2013 you would never be shamed for doing it since it was according to custom. The belief in changelings expressed in the folktales gave the women (and men) a way out of this dilemma. (Ironically, Martin Luther, the icon of guilt culture, dismissed all the popular superstitions of his fellow countrymen with the sole exception of changelings which he firmly believed in.) Thus, the main purpose of these tales seems to have been to alleviate guilt.</p>\n<p><strong id=\"Geography_\">Geography </strong></p>\n<p>If this is true then changeling stories should be more common in the NWE region than elsewhere, which also seems to be the case. There are numerous changeling tales found on the British Isles, in Scandinavia, Germany and France. It can be found elsewhere in Europe as well, in the Basque region and among Slavic people and even as far as North Africa, but at least according to folklorists I\u2019ve found discussing these tales, they are <a href=\"http://www.doiserbia.nb.rs/img/doi/0350-7653/2002/0350-76530233143R.pdf\">imported</a> from the NWE region. And if we look beyond regions bordering to Europe changelings seem to be virtually non-existent. Some folklorists have suggested that for instance the Nigerian <a href=\"http://en.wikipedia.org/wiki/Ogbanje\">Ogbanje</a> can be thought of as a changeling, although at a closer inspection the similarity is very superficial. The Ogbanje is reborn into the same family over and over and to break the curse families consult medicine men after the child has died. When they consult a medicine man when the child is still alive it is for the purpose of severing the child\u2019s connection to the spirit world and make it normal. So the belief in the Ogbanje never justifies infanticide. Another contender is the Filipino <a href=\"http://en.wikipedia.org/wiki/Aswang\">Aswang</a> which is a creature that will attack children as well as adults and is never takes the place of a child but is more like a vampire. So it\u2019s safe to say that the changeling belief is firmly rooted in the NWE region at least back to medieval times and perhaps earlier too.</p>\n<p><strong id=\"Before_There_Were_Changelings__There_Was_Exposure\">Before There Were Changelings, There Was Exposure</strong></p>\n<p>Given how infanticide is such a good candidate for measuring guilt, we could go back further in time, before any evidence of changelings and look at potential differences in attitudes towards this act.</p>\n<p>I doing so I think we can find, if not NWE guilt, so at least Western ditto. According this <a href=\"http://en.wikipedia.org/wiki/Infanticide#Greece_and_Rome\">Wikipedia article</a>, the ancient Greeks and Romans as well as Germanic tribes, killed infants by exposure rather than through a direct act. Here is a quote on the practice in Greece,</p>\n<p><em>\"\"Babies would often be rejected if they were illegitimate, unhealthy or deformed, the wrong sex, or too great a burden on the family. These babies would not be directly killed, but put in a clay pot or jar and deserted outside the front door or on the roadway. In ancient Greek religion, this practice took the responsibility away from the parents because the child would die of natural causes, for example hunger, asphyxiation or exposure to the elements.\"\"</em></p>\n<p>And the Archeology and Classical Research Magazine <a href=\"http://ancientimes.blogspot.se/2011/05/widespread-roman-infanticide-not.html\">Roman Times</a> quotes several classical sources suggesting that exposure was controversial even back then,</p>\n<p><em>\"\"Isocrates (436\u2013338 BCE) &nbsp;includes the exposure of infants in his catalog of horrendous crimes practiced in some &nbsp;cities (other than Athens) in his work Panathenaicus.\"\"</em></p>\n<p>I also found this <a href=\"http://internetbiblecollege.net/Lessons/Killing%20new-borns%20in%20ancient%20greece%20and%20rome.pdf\">excerpt</a> from the play Ion by Euripides, written at the end of the 400s BC. In it Kreusa talks with an old servant about having exposed an unwanted child,</p>\n<p>Old Servant: Who cast him forth? \u2013 Not thou \u2013 O never thou!</p>\n<p>Kreusa: Even I. My vesture darkling swaddled him.</p>\n<p>Old Servant: Nor any knew the exposing of the child?</p>\n<p>Kreusa: None \u2013 Misery and Secrecy alone.</p>\n<p>Old Servant: How couldst thou leave they babe within the cave?</p>\n<p>Kreusa: Ah how? \u2013 O pitiful farewells I moaned!</p>\n<p>It seems to me that this play, by one of the most prominent playwrights of his time, would not make much sense to the audience unless exposure was something that weighed on many people\u2019s hearts.</p>\n<p>Compare this with historical accounts from other cultures, taken from the Wikipedia article mentioned above,</p>\n<p><em>\"\"Some authors believe that there is little evidence that infanticide was prevalent in pre-Islamic Arabia or early Muslim history, except for the case of the Tamim tribe, who practiced it during severe famine. Others state that \u201cfemale infanticide was common all over Arabia during this period of time\u201d (pre-Islamic Arabia), especially by burying alive a female newborn.</em></p>\n<p><em>In Kamchatka, babies were killed and thrown to the dogs.</em></p>\n<p><em>The Svans (a Georgian people) killed the newborn females by filling their mouths with hot ashes.</em></p>\n<p><em>A typical method in Japan was smothering through wet paper on the baby\u2019s mouth and nose. Mabiki persisted in the 19th century and early 20th century.</em></p>\n<p><em>Female infanticide of newborn girls was systematic in feudatory Rajputs in South Asia for illegitimate female children during the Middle Ages. According to Firishta, as soon as the illegitimate female child was born she was held \u201cin one hand, and a knife in the other, that any person who wanted a wife might take her now, otherwise she was immediately put to death\u201d</em></p>\n<p><em>Polar Inuit (Inughuit) killed the child by throwing him or her into the sea. There is even a legend in Inuit mythology, \u201cThe Unwanted Child\u201d, where a mother throws her child into the fjord.\"\"</em></p>\n<p>It seems that while people in ancient Greece practiced exposure, something many were troubled by, the active killing was common in the rest of the world and persists to this day in many places. While people in other cultures may or may not feel guilt it doesn\u2019t seem to affect them as much, and it\u2019s sometimes even trumped by shame as psychiatrist Steven Pitts and clinical psychologist Erin Bale write in an <a href=\"http://www.jaapl.org/content/23/3/375.full.pdf\">article</a> in The Bulletin of the American Academy of Psychiatry and the Law regarding the practice of drowning unwanted girls,</p>\n<p><em>\"\"In China, the birth of a daughter has traditionally been accompanied by disappointment and even shame.\"\"</em></p>\n<p>To summarize, the changeling lore provides evidence of a NWE guilt culture dating back at least to medieval times, and the practice and attitude towards exposure suggests that ancient Greece had an emerging guilt culture as early as the 400s BC which enabled a similar individualism and intellectual development that we\u2019ve seen in the NWE in recent centuries. I\u2019m not sure exactly how genetically related these populations are, but the geographical proximity makes it hard to ignore the possibility of gene variants for guilt proneness in Europe responsible for guilt cultures both in ancient Greece and the NWE region. Some branch of Indo-Europeans perhaps?</p>\n</blockquote>", "sections": [{"title": "Guilt and Shame Cultures", "anchor": "Guilt_and_Shame_Cultures", "level": 1}, {"title": "How to Measure Guilt", "anchor": "How_to_Measure_Guilt", "level": 1}, {"title": "When Did Modernity Begin?", "anchor": "When_Did_Modernity_Begin_", "level": 1}, {"title": "The Changeling", "anchor": "The_Changeling", "level": 1}, {"title": "Infanticide Made Easy ", "anchor": "Infanticide_Made_Easy_", "level": 1}, {"title": "Geography ", "anchor": "Geography_", "level": 1}, {"title": "Before There Were Changelings, There Was Exposure", "anchor": "Before_There_Were_Changelings__There_Was_Exposure", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "19 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2oybbEw697CQgcRE5", "yyteh3qwD6kjhLiCb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-03T14:14:40.145Z", "modifiedAt": null, "url": null, "title": "Measuring lethality in reduced expected heartbeats", "slug": "measuring-lethality-in-reduced-expected-heartbeats", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:34.553Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8WfEPwuWp8wGXMoBX/measuring-lethality-in-reduced-expected-heartbeats", "pageUrlRelative": "/posts/8WfEPwuWp8wGXMoBX/measuring-lethality-in-reduced-expected-heartbeats", "linkUrl": "https://www.lesswrong.com/posts/8WfEPwuWp8wGXMoBX/measuring-lethality-in-reduced-expected-heartbeats", "postedAtFormatted": "Friday, January 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Measuring%20lethality%20in%20reduced%20expected%20heartbeats&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeasuring%20lethality%20in%20reduced%20expected%20heartbeats%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8WfEPwuWp8wGXMoBX%2Fmeasuring-lethality-in-reduced-expected-heartbeats%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Measuring%20lethality%20in%20reduced%20expected%20heartbeats%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8WfEPwuWp8wGXMoBX%2Fmeasuring-lethality-in-reduced-expected-heartbeats", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8WfEPwuWp8wGXMoBX%2Fmeasuring-lethality-in-reduced-expected-heartbeats", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 256, "htmlBody": "<p>Some of us here are already familiar with <a href=\"/lw/8ui/measures_risk_death_and_war/\">Micromorts</a> - a unit that stands for a 1 in a million chance of dying. The <a href=\"http://en.wikipedia.org/wiki/Micromort\">wikipedia page</a> lists a number of sample values. One obvious example is that <a href=\"http://www.mckeay.net/2013/10/17/whats-a-micromort/\">smoking 1.4 cigarettes is one micromort</a>. This is a good tool for comparing the relative dangerousness of activites - for example, if you fly in a jet in the US, your micromorts per mile from increased background radiation are twice the micromorts per mile from terrorism. And you can compare activities to baseline average risks of death, given as about 39 per day (averaged over all age groups and sexes).</p>\n<p>However, people suck at imagining small probabilities. So a different unit, which we used in a group exercise at the Secular Solstice in Leipzig, is the number of expected future heartbeats. While Micromorts are a step away from empirical reality, expected heartbeats are another step in the same direction. But the concept got good feedback, it makes people think about life in a new way, so I thought I'd just share it here.</p>\n<p>The average human heart gets to beat about 2.5 billion times - about 100000 times per day. So a micromort is around 2500 expected heartbeats. So you can translate, say, smoking a cigarette into a cost of about 1800 expected heartbeats (or about 80 seconds of life expectancy). And maybe that'll help people optimize their behavior in ways that Micromorts, due to their microprobability nature, aren't very good at doing especially for those who aren't very habitual Bayesians.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8WfEPwuWp8wGXMoBX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 6, "extendedScore": null, "score": 1.4961957453708108e-06, "legacy": true, "legacyId": "25114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KgWticBMH2MxgdmYc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-03T19:53:18.675Z", "modifiedAt": null, "url": null, "title": "Meetup : I moved to Portland! I want to meet you!", "slug": "meetup-i-moved-to-portland-i-want-to-meet-you", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:32.014Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cWbq9PH3Nmkjubeew/meetup-i-moved-to-portland-i-want-to-meet-you", "pageUrlRelative": "/posts/cWbq9PH3Nmkjubeew/meetup-i-moved-to-portland-i-want-to-meet-you", "linkUrl": "https://www.lesswrong.com/posts/cWbq9PH3Nmkjubeew/meetup-i-moved-to-portland-i-want-to-meet-you", "postedAtFormatted": "Friday, January 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20I%20moved%20to%20Portland!%20I%20want%20to%20meet%20you!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20I%20moved%20to%20Portland!%20I%20want%20to%20meet%20you!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcWbq9PH3Nmkjubeew%2Fmeetup-i-moved-to-portland-i-want-to-meet-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20I%20moved%20to%20Portland!%20I%20want%20to%20meet%20you!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcWbq9PH3Nmkjubeew%2Fmeetup-i-moved-to-portland-i-want-to-meet-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcWbq9PH3Nmkjubeew%2Fmeetup-i-moved-to-portland-i-want-to-meet-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/v7'>I moved to Portland! I want to meet you!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 January 2014 11:50:35AM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Portland, OR</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The title is pretty self explanatory, I think. We're organizing a meetup on the Portland email list. Let us know if you want to come!</p>\n\n<p>The location isn't set in stone yet, but will most likely be a coffee shop. Hopefully in NE. Possibly Case Study Coffee.</p>\n\n<p>Hope to meet you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/v7'>I moved to Portland! I want to meet you!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cWbq9PH3Nmkjubeew", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 4.1e-05, "legacy": true, "legacyId": "25175", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___I_moved_to_Portland__I_want_to_meet_you_\">Discussion article for the meetup : <a href=\"/meetups/v7\">I moved to Portland! I want to meet you!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 January 2014 11:50:35AM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Portland, OR</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The title is pretty self explanatory, I think. We're organizing a meetup on the Portland email list. Let us know if you want to come!</p>\n\n<p>The location isn't set in stone yet, but will most likely be a coffee shop. Hopefully in NE. Possibly Case Study Coffee.</p>\n\n<p>Hope to meet you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___I_moved_to_Portland__I_want_to_meet_you_1\">Discussion article for the meetup : <a href=\"/meetups/v7\">I moved to Portland! I want to meet you!</a></h2>", "sections": [{"title": "Discussion article for the meetup : I moved to Portland! I want to meet you!", "anchor": "Discussion_article_for_the_meetup___I_moved_to_Portland__I_want_to_meet_you_", "level": 1}, {"title": "Discussion article for the meetup : I moved to Portland! I want to meet you!", "anchor": "Discussion_article_for_the_meetup___I_moved_to_Portland__I_want_to_meet_you_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-03T20:19:55.135Z", "modifiedAt": null, "url": null, "title": "[link] How do good ideas spread?", "slug": "link-how-do-good-ideas-spread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.906Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MiAnMMC9LAeiRCKyf/link-how-do-good-ideas-spread", "pageUrlRelative": "/posts/MiAnMMC9LAeiRCKyf/link-how-do-good-ideas-spread", "linkUrl": "https://www.lesswrong.com/posts/MiAnMMC9LAeiRCKyf/link-how-do-good-ideas-spread", "postedAtFormatted": "Friday, January 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20How%20do%20good%20ideas%20spread%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20How%20do%20good%20ideas%20spread%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMiAnMMC9LAeiRCKyf%2Flink-how-do-good-ideas-spread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20How%20do%20good%20ideas%20spread%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMiAnMMC9LAeiRCKyf%2Flink-how-do-good-ideas-spread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMiAnMMC9LAeiRCKyf%2Flink-how-do-good-ideas-spread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2598, "htmlBody": "<p><a href=\"http://www.newyorker.com/reporting/2013/07/29/130729fa_fact_gawande?currentPage=all\">http://www.newyorker.com/reporting/2013/07/29/130729fa_fact_gawande?currentPage=all</a></p>\n<p>Seems related to many topics discussed on LW, such as the low adoption of cryonics and the difficulty of getting researchers convinced of AI risk.</p>\n<blockquote>\n<p>Four weeks later, on November 18th, Bigelow published his report on the discovery of &ldquo;insensibility produced by inhalation&rdquo; in the Boston Medical and Surgical Journal. Morton would not divulge the composition of the gas, which he called Letheon, because he had applied for a patent. But Bigelow reported that he smelled ether in it (ether was used as an ingredient in certain medical preparations), and that seems to have been enough. The idea spread like a contagion, travelling through letters, meetings, and periodicals. By mid-December, surgeons were administering ether to patients in Paris and London. By February, anesthesia had been used in almost all the capitals of Europe, and by June in most regions of the world. [...] Within seven years, virtually every hospital in America and Britain had adopted the new discovery. [...]<br /><br />Sepsis&mdash;infection&mdash;was the other great scourge of surgery. It was the single biggest killer of surgical patients, claiming as many as half of those who underwent major operations, such as a repair of an open fracture or the amputation of a limb. [...]<br /><br />During the next few years, he perfected ways to use carbolic acid for cleansing hands and wounds and destroying any germs that might enter the operating field. The result was strikingly lower rates of sepsis and death. You would have thought that, when he published his observations in a groundbreaking series of reports in The Lancet, in 1867, his antiseptic method would have spread as rapidly as anesthesia.<br /><br />Far from it. The surgeon J. M. T. Finney recalled that, when he was a trainee at Massachusetts General Hospital two decades later, hand washing was still perfunctory. Surgeons soaked their instruments in carbolic acid, but they continued to operate in black frock coats stiffened with the blood and viscera of previous operations&mdash;the badge of a busy practice. Instead of using fresh gauze as sponges, they reused sea sponges without sterilizing them. It was a generation before Lister&rsquo;s recommendations became routine and the next steps were taken toward the modern standard of asepsis&mdash;that is, entirely excluding germs from the surgical field, using heat-sterilized instruments and surgical teams clad in sterile gowns and gloves. [...]<br /><br />Did the spread of anesthesia and antisepsis differ for economic reasons? Actually, the incentives for both ran in the right direction. If painless surgery attracted paying patients, so would a noticeably lower death rate. Besides, live patients were more likely to make good on their surgery bill. Maybe ideas that violate prior beliefs are harder to embrace. To nineteenth-century surgeons, germ theory seemed as illogical as, say, Darwin&rsquo;s theory that human beings evolved from primates. Then again, so did the idea that you could inhale a gas and enter a pain-free state of suspended animation. Proponents of anesthesia overcame belief by encouraging surgeons to try ether on a patient and witness the results for themselves&mdash;to take a test drive. When Lister tried this strategy, however, he made little progress. [...]<br /><br />The technical complexity might have been part of the difficulty. [...] But anesthesia was no easier. [...]<br /><br />So what were the key differences? First, one combatted a visible and immediate problem (pain); the other combatted an invisible problem (germs) whose effects wouldn&rsquo;t be manifest until well after the operation. Second, although both made life better for patients, only one made life better for doctors. Anesthesia changed surgery from a brutal, time-pressured assault on a shrieking patient to a quiet, considered procedure. Listerism, by contrast, required the operator to work in a shower of carbolic acid. Even low dilutions burned the surgeons&rsquo; hands. You can imagine why Lister&rsquo;s crusade might have been a tough sell.<br /><br />This has been the pattern of many important but stalled ideas. They attack problems that are big but, to most people, invisible; and making them work can be tedious, if not outright painful. The global destruction wrought by a warming climate, the health damage from our over-sugared modern diet, the economic and social disaster of our trillion dollars in unpaid student debt&mdash;these things worsen imperceptibly every day. Meanwhile, the carbolic-acid remedies to them, all requiring individual sacrifice of one kind or another, struggle to get anywhere. [...]<br /><br />The staff members I met in India had impressive experience. Even the youngest nurses had done more than a thousand child deliveries. [...] But then we hung out in the wards for a while. In the delivery room, a boy had just been born. He and his mother were lying on a cot, bundled under woollen blankets, resting. The room was coffin-cold; I was having trouble feeling my toes. [...] Voluminous evidence shows that it is far better to place the child on the mother&rsquo;s chest or belly, skin to skin, so that the mother&rsquo;s body can regulate the baby&rsquo;s until it is ready to take over. Among small or premature babies, kangaroo care (as it is known) cuts mortality rates by a third.<br /><br />So why hadn&rsquo;t the nurse swaddled the two together? [...]<br /><br />&ldquo;The mother didn&rsquo;t want it,&rdquo; she explained. &ldquo;She said she was too cold.&rdquo;<br /><br />The nurse seemed to think it was strange that I was making such an issue of this. The baby was fine, wasn&rsquo;t he? And he was. He was sleeping sweetly, a tightly wrapped peanut with a scrunched brown face and his mouth in a lowercase &ldquo;o.&rdquo; [...]<br /><br />Everything about the life the nurse leads&mdash;the hours she puts in, the circumstances she endures, the satisfaction she takes in her abilities&mdash;shows that she cares. But hypothermia, like the germs that Lister wanted surgeons to battle, is invisible to her. We picture a blue child, suffering right before our eyes. That is not what hypothermia looks like. It is a child who is just a few degrees too cold, too sluggish, too slow to feed. It will be some time before the baby begins to lose weight, stops making urine, develops pneumonia or a bloodstream infection. Long before that happens&mdash;usually the morning after the delivery, perhaps the same night&mdash;the mother will have hobbled to an auto-rickshaw, propped herself beside her husband, held her new baby tight, and ridden the rutted roads home.<br /><br />From the nurse&rsquo;s point of view, she&rsquo;d helped bring another life into the world. If four per cent of the newborns later died at home, what could that possibly have to do with how she wrapped the mother and child? Or whether she washed her hands before putting on gloves? Or whether the blade with which she cut the umbilical cord was sterilized? [...]<br /><br />A decade after the landmark findings, the idea remained stalled. Nothing much had changed. Diarrheal disease remained the world&rsquo;s biggest killer of children under the age of five.<br /><br />In 1980, however, a Bangladeshi nonprofit organization called brac decided to try to get oral rehydration therapy adopted nationwide. The campaign required reaching a mostly illiterate population. The most recent public-health campaign&mdash;to teach family planning&mdash;had been deeply unpopular. The messages the campaign needed to spread were complicated.<br /><br />Nonetheless, the campaign proved remarkably successful. A gem of a book published in Bangladesh, &ldquo;A Simple Solution,&rdquo; tells the story. The organization didn&rsquo;t launch a mass-media campaign&mdash;only twenty per cent of the population had a radio, after all. It attacked the problem in a way that is routinely dismissed as impractical and inefficient: by going door to door, person by person, and just talking. [...]<br /><br />They recruited teams of fourteen young women, a cook, and a male supervisor, figuring that the supervisor would protect them from others as they travelled, and the women&rsquo;s numbers would protect them from the supervisor. They travelled on foot, pitched camp near each village, fanned out door to door, and stayed until they had talked to women in every hut. They worked long days, six days a week. Each night after dinner, they held a meeting to discuss what went well and what didn&rsquo;t and to share ideas on how to do better. Leaders periodically debriefed them, as well. [...]<br /><br />The program was stunningly successful. Use of oral rehydration therapy skyrocketed. The knowledge became self-propagating. The program had changed the norms. [...]<br /><br />As other countries adopted Bangladesh&rsquo;s approach, global diarrheal deaths dropped from five million a year to two million, despite a fifty-per-cent increase in the world&rsquo;s population during the past three decades. Nonetheless, only a third of children in the developing world receive oral rehydration therapy. Many countries tried to implement at arm&rsquo;s length, going &ldquo;low touch,&rdquo; without sandals on the ground. As a recent study by the Gates Foundation and the University of Washington has documented, those countries have failed almost entirely. People talking to people is still how the world&rsquo;s standards change.<br /><br />Surgeons finally did upgrade their antiseptic standards at the end of the nineteenth century. But, as is often the case with new ideas, the effort required deeper changes than anyone had anticipated. In their blood-slick, viscera-encrusted black coats, surgeons had seen themselves as warriors doing hemorrhagic battle with little more than their bare hands. A few pioneering Germans, however, seized on the idea of the surgeon as scientist. They traded in their black coats for pristine laboratory whites, refashioned their operating rooms to achieve the exacting sterility of a bacteriological lab, and embraced anatomic precision over speed.<br /><br />The key message to teach surgeons, it turned out, was not how to stop germs but how to think like a laboratory scientist. Young physicians from America and elsewhere who went to Germany to study with its surgical luminaries became fervent converts to their thinking and their standards. They returned as apostles not only for the use of antiseptic practice (to kill germs) but also for the much more exacting demands of aseptic practice (to prevent germs), such as wearing sterile gloves, gowns, hats, and masks. Proselytizing through their own students and colleagues, they finally spread the ideas worldwide.<br /><br />In childbirth, we have only begun to accept that the critical practices aren&rsquo;t going to spread themselves. Simple &ldquo;awareness&rdquo; isn&rsquo;t going to solve anything. We need our sales force and our seven easy-to-remember messages. And in many places around the world the concerted, person-by-person effort of changing norms is under way.\"<br /><br />I recently asked BetterBirth workers in India whether they&rsquo;d yet seen a birth attendant change what she does. Yes, they said, but they&rsquo;ve found that it takes a while. They begin by providing a day of classroom training for birth attendants and hospital leaders in the checklist of practices to be followed. Then they visit them on site to observe as they try to apply the lessons. [...]<br /><br />Sister Seema Yadav, a twenty-four-year-old, round-faced nurse three years out of school, was one of the trainers. [...] Her first assignment was to follow a thirty-year-old nurse with vastly more experience than she had. Watching the nurse take a woman through labor and delivery, she saw how little of the training had been absorbed. [...] By the fourth or fifth visit, their conversations had shifted. They shared cups of chai and began talking about why you must wash hands even if you wear gloves (because of holes in the gloves and the tendency to touch equipment without them on), and why checking blood pressure matters (because hypertension is a sign of eclampsia, which, when untreated, is a common cause of death among pregnant women). They learned a bit about each other, too. Both turned out to have one child&mdash;Sister Seema a four-year-old boy, the nurse an eight-year-old girl. [...]<br /><br />Soon, she said, the nurse began to change. After several visits, she was taking temperatures and blood pressures properly, washing her hands, giving the necessary medications&mdash;almost everything. Sister Seema saw it with her own eyes.<br /><br />She&rsquo;d had to move on to another pilot site after that, however. And although the project is tracking the outcomes of mothers and newborns, it will be a while before we have enough numbers to know if a difference has been made. So I got the nurse&rsquo;s phone number and, with a translator to help with the Hindi, I gave her a call.<br /><br />It had been four months since Sister Seema&rsquo;s visit ended. I asked her whether she&rsquo;d made any changes. Lots, she said. [...]<br /><br />She said that she had eventually begun to see the effects. Bleeding after delivery was reduced. She recognized problems earlier. She rescued a baby who wasn&rsquo;t breathing. She diagnosed eclampsia in a mother and treated it. You could hear her pride as she told her stories.<br /><br />Many of the changes took practice for her, she said. She had to learn, for instance, how to have all the critical supplies&mdash;blood-pressure cuff, thermometer, soap, clean gloves, baby respiratory mask, medications&mdash;lined up and ready for when she needed them; how to fit the use of them into her routine; how to convince mothers and their relatives that the best thing for a child was to be bundled against the mother&rsquo;s skin. But, step by step, Sister Seema had helped her to do it. &ldquo;She showed me how to get things done practically,&rdquo; the nurse said.<br /><br />&ldquo;Why did you listen to her?&rdquo; I asked. &ldquo;She had only a fraction of your experience.&rdquo;<br /><br />In the beginning, she didn&rsquo;t, the nurse admitted. &ldquo;The first day she came, I felt the workload on my head was increasing.&rdquo; From the second time, however, the nurse began feeling better about the visits. She even began looking forward to them.<br /><br />&ldquo;Why?&rdquo; I asked.<br /><br />All the nurse could think to say was &ldquo;She was nice.&rdquo;<br /><br />&ldquo;She was nice?&rdquo;<br /><br />&ldquo;She smiled a lot.&rdquo;<br /><br />&ldquo;That was it?&rdquo;<br /><br />&ldquo;It wasn&rsquo;t like talking to someone who was trying to find mistakes,&rdquo; she said. &ldquo;It was like talking to a friend.&rdquo;<br /><br />That, I think, was the answer. Since then, the nurse had developed her own way of explaining why newborns needed to be warmed skin to skin. She said that she now tells families, &ldquo;Inside the uterus, the baby is very warm. So when the baby comes out it should be kept very warm. The mother&rsquo;s skin does this.&rdquo;<br /><br />I hadn&rsquo;t been sure if she was just telling me what I wanted to hear. But when I heard her explain how she&rsquo;d put her own words to what she&rsquo;d learned, I knew that the ideas had spread. &ldquo;Do the families listen?&rdquo; I asked.<br /><br />&ldquo;Sometimes they don&rsquo;t,&rdquo; she said. &ldquo;Usually, they do.&rdquo;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MiAnMMC9LAeiRCKyf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 18, "extendedScore": null, "score": 1.4965872101098837e-06, "legacy": true, "legacyId": "25176", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-03T22:19:48.390Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-116", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TisRMAYLy2ZLPjhF7/weekly-lw-meetups-116", "pageUrlRelative": "/posts/TisRMAYLy2ZLPjhF7/weekly-lw-meetups-116", "linkUrl": "https://www.lesswrong.com/posts/TisRMAYLy2ZLPjhF7/weekly-lw-meetups-116", "postedAtFormatted": "Friday, January 3rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTisRMAYLy2ZLPjhF7%2Fweekly-lw-meetups-116%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTisRMAYLy2ZLPjhF7%2Fweekly-lw-meetups-116", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTisRMAYLy2ZLPjhF7%2Fweekly-lw-meetups-116", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 473, "htmlBody": "<p><strong>This summary was posted to LW Main on December 27th. The following week's summary is <a href=\"/lw/jfd/new_lw_meetups_portland_sydney/\">here</a>.</strong></p>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/uu\">Montreal - How to Actually Change your Mind:&nbsp;<span class=\"date\">07 January 2014 07:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">28 December 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/tf\">Berlin:&nbsp;<span class=\"date\">01 January 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/uo\">Brussels monthly meetup: [topic TBD]:&nbsp;<span class=\"date\">11 January 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/v1\">Melbourne Practical Rationality:&nbsp;<span class=\"date\">03 January 2014 06:30PM</span></a></li>\n<li><a href=\"/meetups/uv\">Vienna:&nbsp;<span class=\"date\">18 January 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TisRMAYLy2ZLPjhF7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4967157418725433e-06, "legacy": true, "legacyId": "25138", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zsEqMikJmTgPWhtHo", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-04T01:33:40.658Z", "modifiedAt": null, "url": null, "title": "[LINK] How Long Does Habit Formation Take?", "slug": "link-how-long-does-habit-formation-take", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:31.420Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Antisuji", "createdAt": "2009-04-15T23:39:08.004Z", "isAdmin": false, "displayName": "Antisuji"}, "userId": "zRsg2BPDaXGTkXFMd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x4zzkbuFWLwuD454n/link-how-long-does-habit-formation-take", "pageUrlRelative": "/posts/x4zzkbuFWLwuD454n/link-how-long-does-habit-formation-take", "linkUrl": "https://www.lesswrong.com/posts/x4zzkbuFWLwuD454n/link-how-long-does-habit-formation-take", "postedAtFormatted": "Saturday, January 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20How%20Long%20Does%20Habit%20Formation%20Take%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20How%20Long%20Does%20Habit%20Formation%20Take%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4zzkbuFWLwuD454n%2Flink-how-long-does-habit-formation-take%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20How%20Long%20Does%20Habit%20Formation%20Take%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4zzkbuFWLwuD454n%2Flink-how-long-does-habit-formation-take", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx4zzkbuFWLwuD454n%2Flink-how-long-does-habit-formation-take", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 549, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/hub/common_failure_modes_in_habit_formation/\">Common failure modes in habit formation</a></p>\n<p>I ran across <a href=\"http://www.brainpickings.org/index.php/2014/01/02/how-long-it-takes-to-form-a-new-habit/\">this bit of pop-sci</a> (a review of Jeremy Dean's <a href=\"http://www.amazon.com/Making-Habits-Breaking-Things-Change/dp/0306822628/\">Making Habits, Breaking Habits</a>), which claims that habits typically take around 66 days to form, not the 21 days that self-help articles tend to cite. The somewhat surprising thing to me, on reflection, was how readily I'd taken the 21-day statistic as fact. From the article:</p>\n<blockquote>\n<p>When he became interested in how long it takes for us to form or change a habit, psychologist Jeremy Dean found himself bombarded with the same magic answer from popular psychology websites and advice columns: 21 days. And yet, strangely &mdash; or perhaps predictably, for the internet &mdash; this one-size-fits-all number was being applied to everything from starting a running regimen to keeping a diary, but wasn&rsquo;t backed by any concrete data.</p>\n</blockquote>\n<p>The original article is <a href=\"http://onlinelibrary.wiley.com/doi/10.1002/ejsp.674/abstract\">here</a>. Abstract:</p>\n<blockquote>\n<p>To investigate the process of habit formation in everyday life, 96 volunteers chose an eating, drinking or activity behaviour to carry out daily in the same context (for example &lsquo;after breakfast&rsquo;) for 12 weeks. They completed the self-report habit index (SRHI) each day and recorded whether they carried out the behaviour. The majority (82) of participants provided sufficient data for analysis, and increases in automaticity (calculated with a sub-set of SRHI items) were examined over the study period. Nonlinear regressions fitted an asymptotic curve to each individual's automaticity scores over the 84 days. The model fitted for 62 individuals, of whom 39 showed a good fit. Performing the behaviour more consistently was associated with better model fit. The time it took participants to reach 95% of their asymptote of automaticity ranged from <strong>18 to 254 days</strong>; indicating considerable variation in how long it takes people to reach their limit of automaticity and highlighting that <strong>it can take a very long time</strong>. Missing one opportunity to perform the behaviour did not materially affect the habit formation process. With repetition of a behaviour in a consistent context, automaticity increases following an asymptotic curve which can be modelled at the individual level. [My emphasis.]</p>\n</blockquote>\n<p>My comments:</p>\n<ul>\n<li>There is an observed &ldquo;automaticity plateau.&rdquo; Can individuals influence the height of the plateau through interventions such as rewards? Would this change the exponential rate constant? Or do we have less control over these things than we think?</li>\n<li>95% of maximum automaticity doesn't quite seem like the right metric to use to describe habit formation, especially if the maximum is on the low side.</li>\n<li>Presumably you'd need familiarity with the SRHI survey to answer this, but it's not clear to me what an automaticity score of 40 really means. (Examples or a baseline might help: what's my automaticity for toothbrushing? checking email?)</li>\n<li>N=96 seems small. It seems slightly problematic that the 14 participants who dropped out were not included in the analysis, and rather problematic that they used a 3-parameter model and only got a &lsquo;good fit&rsquo; for half of the participants. (I'm not an expert in this, so I'd appreciate knowing if my intuitions here are right.)</li>\n<li>It seems that changing habits is harder than I'd previously thought, at least in the absence of <a href=\"http://rationality.org/\">CFAR</a>-like techniques. (Which we still don't know if it works, as far as I know. I'm looking forward to their research.)</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5Whwix4cZ3p5otshm": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x4zzkbuFWLwuD454n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 30, "extendedScore": null, "score": 1.496923632910542e-06, "legacy": true, "legacyId": "25178", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PYgGpmmk3wQhSt6Yv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-04T05:16:23.985Z", "modifiedAt": null, "url": null, "title": "Critiquing Gary Taubes, Final: The Truth About Diets and Weight Loss", "slug": "critiquing-gary-taubes-final-the-truth-about-diets-and", "viewCount": null, "lastCommentedAt": "2019-05-26T02:41:57.661Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jvu2nbW9jLwxH5K5X/critiquing-gary-taubes-final-the-truth-about-diets-and", "pageUrlRelative": "/posts/jvu2nbW9jLwxH5K5X/critiquing-gary-taubes-final-the-truth-about-diets-and", "linkUrl": "https://www.lesswrong.com/posts/jvu2nbW9jLwxH5K5X/critiquing-gary-taubes-final-the-truth-about-diets-and", "postedAtFormatted": "Saturday, January 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Critiquing%20Gary%20Taubes%2C%20Final%3A%20The%20Truth%20About%20Diets%20and%20Weight%20Loss&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACritiquing%20Gary%20Taubes%2C%20Final%3A%20The%20Truth%20About%20Diets%20and%20Weight%20Loss%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjvu2nbW9jLwxH5K5X%2Fcritiquing-gary-taubes-final-the-truth-about-diets-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Critiquing%20Gary%20Taubes%2C%20Final%3A%20The%20Truth%20About%20Diets%20and%20Weight%20Loss%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjvu2nbW9jLwxH5K5X%2Fcritiquing-gary-taubes-final-the-truth-about-diets-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjvu2nbW9jLwxH5K5X%2Fcritiquing-gary-taubes-final-the-truth-about-diets-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2318, "htmlBody": "<p><strong>Previously: </strong><a href=\"/lw/jdw/critiquing_gary_taubes_part_1_mainstream/\">Mainstream Nutrition Science on Obesity</a>, <a href=\"/lw/je4/critiquing_gary_taubes_part_2_atkins_redux/\">Atkins Redux</a>, <a href=\"/lw/je5/critiquing_gary_taubes_part_3_did_the_us/\">Did the US Government Give Us Absurd Advice About Sugar?</a>, <a href=\"/lw/jdx/critiquing_gary_taubes_part_4_what_causes_obesity/\">What Causes Obesity?</a></p>\n<p><em>If you've been wondering what these posts are doing on LessWrong and you haven't read <a href=\"/lw/jdx/critiquing_gary_taubes_part_4_what_causes_obesity/a9i6\">this comment</a> yet, I urge you to do so. Thanks to commenter FiftyTwo for suggesting I say something like this.</em></p>\n<p>To recap: so taking in more calories than you burn will cause you to gain weight, though calorie intake and expenditure are in turn controlled by a number of mechanisms. This suggests a couple of options for losing weight. You <em>can </em>try to intervene directly in the mechanisms controlling food intake, one of the most well-known examples of this being gastric bypass surgery, admittedly a bit of a drastic option. But intervening at the point of calorie intake is also an option.</p>\n<p>Now it turns out that it's relatively easy to lose weight by dieting. That catch is that it's much harder to keep the weight off. A commonly cited rule (for example <a href=\"http://knowledgetranslation.ca/sysrev/articles/project21/Anderson%20JW%20J%20Am%20Coll%20Nutr%201999%2018%206%20620%207-20090717155554.pdf\">here</a>) is that most people who lose weight through dieting will regain it all in five years. However, it's important to emphasize that some people do lose weight through dieting and keep it off long-term. An organization called the <a href=\"http://www.nwcr.ws/\">National Weight Control Registry</a>&nbsp;has made an effort to track those people, and <a href=\"http://www.nwcr.ws/Research/published%20research.htm\">have published quite a few studies based on their work</a>&nbsp;(many of which can be easily found through Google Scholar).</p>\n<p>Unfortunately, the NWCR is working with a self-selected sample and asking them what they did after the fact. They're not randomly assigning people to treatments. So for example, a high percentage of the NWCR group reports successful long-term weight loss following low-fat and/or calorie-restricted diets and exercising a lot. And the percentage following low-carb diets was originally small, but it's <a href=\"http://onlinelibrary.wiley.com/doi/10.1038/oby.2006.81/full\">risen over time</a>. But both of these observations may just reflect the relative popularity of those approaches in the general population.</p>\n<p>We may not be able to conclude anything more from the NWCR data than that a significant minority of dieters do succeed at long-term weight loss, some through calorie-restricted diets, some through low-fat diets, and some through low-carb diets. Remember, though, that as discussed in previous posts there's little reason to think low-fat or low-carb diets could cause weight loss except by indirectly affecting energy balance.</p>\n<p>And now, one last time, I'm going to talk about what Taubes has to say about this issue. I'm going to quote from <em><a href=\"http://www.amazon.com/Why-We-Get-Fat-About/dp/0307474259\">Why We Get Fat</a>&nbsp;</em>(pp. 36-38),&nbsp;though <em><a href=\"http://www.amazon.com/Good-Calories-Bad-Gary-Taubes-ebook/dp/B000UZNSC2/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1388782892&amp;sr=1-1&amp;keywords=good+calories+bad+calories\">Good Calories, Bad Calories</a> </em>contains similar comments, including about the <em>Handbook of Obesity </em>and <em>Joslin's. </em>Taubes begins by citing a review article covering calorie-restricted diets that found that \"Typically, nine or ten pounds are lost in the first six months. After a year, much of what was lost has been regained.\" He also cites a large study that tested a calorie-restricted diet and reached a similar conclusion: participants \"lost on average, only nine pounds. And once again... most of the nine pounds came off in the first six months, and most of the participants were gaining weight back after a year.\"</p>\n<p>Based on this, he concludes that \"Eating less&mdash;that is, undereating&mdash;simply doesn't work for more than a few months, if that.\" Then it's time to really lay in to mainstream nutrition science:</p>\n<blockquote>\n<p>This reality, however, hasn't stopped the authorities from recommending the approach, which makes reading such recommendations an exercise in what psychologists call \"cognitive dissonance,\" the tension that results from trying to hold two incompatible beliefs simultaneously.</p>\n<p>Take, for instance, the <em>Handbook of Obesity,</em> a 1998 textbook edited by three of the most prominent authorities in the field&mdash;George Bray, Claude Bouchard, and W. P. T. James. \"Dietary therapy remains the cornerstone of treatment and the reduction of energy intake continues to be the basis of successful weight reduction programs,\" the book says. But then it states, a few paragraphs later, that the results of such energy-reduced diets \"are known to be poor and not long-lasting.\" So why is such an ineffective therapy the cornerstone of treatment? The <em>Handbook of Obesity</em> neglects to say.</p>\n<p>The latest edition (2005) of <em>Joslin's Diabetes Mellitus,</em> a highly respected textbook for physicians and researchers, is a more recent example of this cognitive dissonance. The chapter on obesity was written by Jeffrey Flier, an obesity researcher who is now dean of Harvard Medical School, and his wife and research colleague, Terry Maratos-Flier. The Fliers also describe \"reduction of caloric intake\" as \"the cornerstone of any therapy for obesity.\" But then they enumerate all the ways in which this cornerstone fails. After examining approaches from the most subtle reductions in calories (eating, say, one hundred calories less each day with the hope of losing a pound every five weeks) to low-calorie diets of eight hundred to one thousand calories a day to very low-calorie diets (two hundred to six hundred calories) and even total starvation, they conclude that \"none of these approaches has any proven merit.\"</p>\n</blockquote>\n<p>But look at the actual sources and it turns out that, surprise surprise, mainstream experts aren't idiots after all. The second quote from the <em>Handbook of Obesity </em>comes from a paragraph explaining that given how hard obesity is to treat, doctors face a \"Shakespearean\" dilemma of whether to attempt to treat it at all. The <a href=\"http://books.google.com/books?id=ohgjG0qAvfgC&amp;pg=PA533&amp;dq=Joslin's+diabetes+mellitus+obesity&amp;hl=en&amp;sa=X&amp;ei=_l_CUufDKMOcyQGgp4HICw&amp;ved=0CEYQ6AEwAA#v=onepage&amp;q=Joslin's%20diabetes%20mellitus%20obesity&amp;f=false\"><em>Joslin's </em>article</a> is even clearer (p. 541, emphasis added):</p>\n<blockquote>\n<p>Successful treatment of obesity, defined as treatment that results in sustained attainment of normal body weight and composition without producing unacceptable treatment induced morbidity, is rarely achievable in clinical practice. Many therapeutic approaches can bring about short-term weight loss, but long-term success is infrequent <strong>regardless of the approach.</strong></p>\n</blockquote>\n<p>Suppose for a moment that this is true, that long-term weight loss is rare <em>regardless of the approach. </em>If it is, no \"cognitive dissonance\" is required to recommend treatments that <em>sometimes </em>work. Furthermore, Taubes commits a serious misrepresentation here. Taubes final quote from the <em>Joslin's </em>article, in context, says that, \"There are also many programs that recommend specific food combinations or unusual sequences for eating, but none of these approaches has any proven merit.\" It's pretty obvious in context that the bit Taubes quotes refers only to the programs that recommend specific food combinations or unusual sequences for eating.\"</p>\n<p>It's also worth mentioning that neither of these sources ignore the debate over low-carb diets. The <em>Handbook of Obesity </em>criticizes Atkins-style low carb diets at some length, but also says that, \"Moderate restriction of carbohydrates may have real calorie-reducing properties.\" And the <em>Joslin's </em>article ends up being fairly positive towards low-carb diets in general (p. 542):</p>\n<blockquote>\n<p>Dietary composition may play a role in long-term success in weight loss and weight maintenance. For example, a study comparing a moderate-fat diet consisting of 35% energy from fat and a low-fat diet in which 20% of energy was derived from fat demonstrated enhanced weight loss assessed by total weight loss, BMI change, and decrease in waist circumference in the group on the moderate-fat diet. Retention in the diet study was greater among those actively participating in the weight loss program in this group compared with 20% in the low-fat diet group.</p>\n<p>Recently, increased interest has focused on the possibility that diet content may affect appetite. For example, diets with a low glycemic index may be useful in preventing the development of obesity; subjects given test meals with different glycemic indexes and then allowed free access to food ate less after eating meals with a low glycemic index. Some data suggest that diets with a high glycemic index predispose to increased postprandial hunger, whereas diets focused on glycemic index and information regarding portion control lead to higher rates of success in weight loss, at least among adolescent populations. Low-carbohydrate diets such as the Atkins diet appear to be associated with significant weight loss. However, this diet has not been systematically studied, nor has long-term maintenance of weight loss.</p>\n</blockquote>\n<p>I assume the author of the <em>Joslin's </em>article would say, however, that low-carb diets haven't been shown to completely solve the problem of long-term weight loss being really hard. But would they be right about that?&nbsp;</p>\n<p>To the best of my knowledge, there have been only two randomized, controlled trials of low-carb diets that have covered a period of two years (and none covering a longer period than that). Taubes has cited both in support of his claims. The first, an <a href=\"http://www.nejm.org/doi/full/10.1056/NEJMoa0708681#t=articleResults\">Israeli study published in 2008</a>, also also included a group assigned to a Mediterranean diet. Here are the results in terms of weight loss:&nbsp;</p>\n<p><a href=\"http://imgur.com/70vujul\"><img title=\"Hosted by imgur.com\" src=\"http://i.imgur.com/70vujul.png?2\" alt=\"\" /></a></p>\n<p>So on the one hand, subjects on the low-carb diet did initially lose more weight, about 6.5 kg (14 lbs.) compared to about 4.5 kg (10 lbs.) for the low-calorie diet. On the other hand, both groups started regaining the weight after six months. If, as Taubes claims, data like this shows that low-calorie diets \"simply doesn't work for more than a few months,\" does this data justify saying the same thing about low-carb diets?</p>\n<p>Furthermore, if you believe the rule about weight lost to dieting coming back in five years, it seems likely that would happen to both groups. Intriguingly, though, while participants on the Mediterranean diet didn't initially lose as much weight as those on the low-carb diet, the weight regain didn't seem to happen as much on the Mediterranean diet. That makes me wonder what a five-year study of the Mediterranean diet would find.</p>\n<p>Note that the Israeli study also found that that participants in all three groups significantly reduced their caloric intake, supporting the hypothesis that even diets that don't explicitly restrict calorie intake work by reducing calorie intake indirectly.</p>\n<p>What about&nbsp;<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2949959/?log%24=activity\">the other study, published in 2010</a>, which Taubes has <a href=\"http://garytaubes.com/wp-content/uploads/2012/02/WWGF-Readers-Digest-feature-Feb-2011.pdf\">hailed</a> as \"the biggest study so far on low-carb diets\"? Here are its results (note that the low-fat diet was also a calorie-restricted diet):</p>\n<p><a href=\"http://imgur.com/TSnoOSL\"><img title=\"Hosted by imgur.com\" src=\"http://i.imgur.com/TSnoOSL.png?1\" alt=\"\" /></a></p>\n<p>That's right, this study found <em>no </em>statistically significant difference between low-fat and low-carb diets in terms of weight loss, and again show the typical pattern of people losing weight in the first six months and then slowly gaining it back. Together, these two studies support the picture painted by <em>Joslin's: </em>low-carb diets may work somewhat better for weight loss, but they don't appear to solve the problem of long-term weight loss being really hard.</p>\n<p>One other relevant detail: the second study found that \"A significantly greater percentage of participants who consumed the low-carbohydrate than the low-fat diet reported bad breath, hair loss, constipation, and dry mouth.\" As Taubes' fellow science writer John Horgan has <a href=\"http://blogs.scientificamerican.com/cross-check/2011/05/16/thin-body-of-evidence-why-i-have-doubts-about-gary-taubess-why-we-get-fat/\">noted</a>, this reveals an apparent inconsistency in how Taubes judges different diets. He goes to great lengths to play up the unpleasantness of calorie-restricted diets, but tells his readers that if they just stick to their low-carb diet theunpleasant side-effects will go away eventually.</p>\n<p>So given all this, what should you do if you want to lose weight? I think depends a lot on who you are. I have ethical qualms about consuming animal products, <a href=\"http://blogs.scientificamerican.com/guest-blog/2011/08/11/want-to-kill-fewer-animals-give-up-eggs-not-meat/\">including and in fact especially eggs</a>, which is one strike against low-carb diets for me.&nbsp;Also, while there's some evidence low-carb diets may be better for hunger, my personal experience is that what foods I find filling is kind of random (lentils, black beans, and baguettes all rate highly on the filling-ness measure for me). So maybe just experiment and try to figure out which foods let you personally eat in moderation and not feel hungry. Keep Eliezer's advice in <a href=\"/lw/9v/beware_of_otheroptimizing/\">Beware of Other Optimizing</a>&nbsp;in mind, and if one thing doesn't work for you, try something else.</p>\n<p>A final point: the truth about weight loss sucks. If your case isn't bad enough to justify something drastic like gastric bypass surgery, your main option is diets which sometimes work but usually don't. Regardless of the approach. Unfortunately, this is not an exciting message to put in a popular book on nutrition. This creates an excellent opportunity for someone like Taubes: imply that if the experts admit they don't have a great solution to the problem, then <em>clearly </em>they don't know what they're talking about, and therefore your solution is sure to work!</p>\n<p>Long-time readers of LessWrong, however, will realize that <a href=\"/lw/uk/beyond_the_reach_of_god/\">the universe is allowed to throw us problems with no good solution</a>. That's something that may be especially worth keeping in mind when evaluating claims in the vicinity of medicine and nutrition. In a way, Taubes' readers are lucky: following his advice won't kill you, and won't lead to you missing out on any <em>wildly </em>more effective solution.&nbsp;It might have some unpleasant side-effects you could've avoided with another approach, but also might have some advantages. However, I've read enough of the literature on medical quackery to know Taubes' rhetorical tactics can be used for much more dangerous ends.</p>\n<p>Just imagine: \"It's doctors and pharmaceutical companies that caused your cancer in the first place. That chemotherapy and radiation therapy stuff they're pushing on you is obviously harmful. Don't you now there are all-natural ways you can cure your cancer?\" If someone says that to you, then knowing that the universe is unfair, and that sometimes the best solution it gives you to a problem will have serious downsides, well knowing that just might save your life. Or not. Because the universe isn't fair.&nbsp;</p>\n<hr />\n<p>Early on in the process of writing this series, I said when it was over with I'd do a post-mortem to look at how I could have broken it up better. However, Vaniver has given me what seems like <a href=\"/lw/je5/critiquing_gary_taubes_part_3_did_the_us/a99v\">good advice</a> on that issue, which I plan to follow in the future. (Unless someone else comes along and persuades me otherwise. You're welcome to try that).</p>\n<p>But there are other issues here, the big meta-issue being that downvotes don't help me distinguish between people thinking the posts were completely off-topic for Lesswrong vs. not liking how finely they were broken up vs. me not realizing what a hot-button issue obesity is for some people vs. other things. So suggestions on how I could best solicit <em>anonymous </em>feedback would be especially appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"92SxJsDZ78ApAGq72": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jvu2nbW9jLwxH5K5X", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 25, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "25156", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JGBxoM4poNEzvmFBJ", "eSfPGQjArWXWXu99Z", "LzMMCnmePFKpmD6ki", "pHGP4PS43TbZKNnrj", "6NvbSwuSAooQxxf7f", "sYgv4eYH82JEsTD34"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-04T15:52:09.980Z", "modifiedAt": null, "url": null, "title": "Soft Paternalism in Parenting", "slug": "soft-paternalism-in-parenting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:39.284Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DEsgKpLJ9LvTRuDh7/soft-paternalism-in-parenting", "pageUrlRelative": "/posts/DEsgKpLJ9LvTRuDh7/soft-paternalism-in-parenting", "linkUrl": "https://www.lesswrong.com/posts/DEsgKpLJ9LvTRuDh7/soft-paternalism-in-parenting", "postedAtFormatted": "Saturday, January 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Soft%20Paternalism%20in%20Parenting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASoft%20Paternalism%20in%20Parenting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDEsgKpLJ9LvTRuDh7%2Fsoft-paternalism-in-parenting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Soft%20Paternalism%20in%20Parenting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDEsgKpLJ9LvTRuDh7%2Fsoft-paternalism-in-parenting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDEsgKpLJ9LvTRuDh7%2Fsoft-paternalism-in-parenting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 586, "htmlBody": "<p>Reading the recently featured <a href=\"/lw/f1/beware_trivial_inconveniences/\">Beware of Trivial Inconveniences</a>&nbsp;I realized that this is the method that makes <a href=\"http://www.babyzone.com/kids/discipline/positive-parenting-power_73633\">Say</a> <a href=\"http://www.webmd.com/parenting/features/say-no-without-saying-no\">Yes</a>&nbsp;really work and thus this is <a href=\"/lw/d4/practical_advice_backed_by_deep_theories/ \">Practical Advice Backed By Deep Theories</a>.</p>\n<p>The trick of saying \"yes\" instead of \"no\" is *not* to say less often \"no\" at the cost at allowing things when you say \"yes\". That just trades the stress of saying \"no\"&nbsp;(staying consequent despite a clash of wills)&nbsp;against the effort to fulfill, monitor, pay or clean up after the \"yes\".</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Soft_paternalism\">Soft paternalism</a>&nbsp;applied to parenting means saying \"Yes, but\" or \"Yes, later\" or \"Yes, if\". This signals to the child that you understand his/her wish but also supplies some context the child may not be aware of. It reduces your cost of saying \"yes\" at the expense of a cost to cash in the \"yes\" for the child.</p>\n<p><a id=\"more\"></a></p>\n<p>Disclaimer: This 'cost reversal' works if</p>\n<ul>\n<li>the condition is no artificial construction to make the \"yes\" into an effective \"no\" (in which case the child will learn this pattern of disguised \"no\" and might e.g. feel cheated. Though this may still be more polite than saying plain \"no\".</li>\n<li>The condition/context for the \"yes\" provides real information for the child.</li>\n<li>The child is old enough to at least grasp the concept of a condition (is in its <a href=\"http://en.wikipedia.org/wiki/Zone_of_proximal_development\">Zone of Proximal Development</a>)</li>\n</ul>\n<p>Examples:</p>\n<p>I use this pattern...</p>\n<p>For my oldest (10) when he wants to do some larger/elaborate projects and e.g. asks \"may I organize event X\" I don't want to stiffle his motivation to show responsibility, learn required tasks and socialize. But I also don't want to do significant parts of this. So I e.g. say: \"Yes, but you have to consult the calendar for a time, write the invitation yourself and clean up afterwards\".</p>\n<p>For my second oldest (7) if he wants some book or other piece of parent stuff like bowls I request: \"yes, but put it back afterwards\".</p>\n<p>This will not work on his younger brother (5) who is not yet disciplined enough to remember to put things back afterwards. For him a limitation like \"yes, but not now; after I have done the dishes\" is more applicable. It a) places a condition he can monitor (or is motivated to monitor) and b) it moves the fulfillment into a position where I am more willing to do/allow/give it and c) when he forgets about it I got rid of it at no cost.</p>\n<p>When applying this to the youngest (2) the conditions need to be most simple as he doesn't understand the condition yet. He hears the \"yes but bla bla\" and recognizes that it somehow means that he doesn't get it (yet). His reaction is mostly the same as to a \"no\" but I can talk some more to him to make clear that it is no \"no\" (and his brothers support this: \"don't cry, you will get it\"). So this montivates him to learn the linguistic concept of a conditional.</p>\n<p>One still has to be consequent in standing behind the context/condition and not be turned around to a plain \"yes\".</p>\n<p>And for older children you still have to be careful what you say. This is already hard for the oldest who is constantly trying to optimize for the conditions or bend the wording of the conditions.</p>\n<p>Such a semi-permissive parenting is also called <a href=\"http://en.wikipedia.org/wiki/Parenting_styles#Authoritative_parenting\">Authoritative Parenting</a>&nbsp;and generally more successful in preparing children for adult life. See e.g.</p>\n<p><a href=\"http://www.parentingscience.com/authoritative-parenting-style.html\">http://www.parentingscience.com/authoritative-parenting-style.html</a>&nbsp;(The authoritative parenting style: Warmth, rationality, and high standards: A guide for the science-minded parent)</p>\n<p><a href=\"http://www.philly.com/philly/blogs/healthy_kids/You-say-yesI-say-no-how-parenting-style-may-affect-teens-behaviors-.html\">http://www.philly.com/philly/blogs/healthy_kids/You-say-yesI-say-no-how-parenting-style-may-affect-teens-behaviors-.html</a></p>\n<p>EDIT: Fixed some typos of this obviously still occassionally read post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DEsgKpLJ9LvTRuDh7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 53, "extendedScore": null, "score": 0.000146, "legacy": true, "legacyId": "25179", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["reitXJgJXFzKpdKyd", "LqjKP255fPRY7aMzw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-04T17:34:24.183Z", "modifiedAt": null, "url": null, "title": "[Link] More ominous than a [Marriage] strike", "slug": "link-more-ominous-than-a-marriage-strike", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.458Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NxDebjR4H2fjw9P3W/link-more-ominous-than-a-marriage-strike", "pageUrlRelative": "/posts/NxDebjR4H2fjw9P3W/link-more-ominous-than-a-marriage-strike", "linkUrl": "https://www.lesswrong.com/posts/NxDebjR4H2fjw9P3W/link-more-ominous-than-a-marriage-strike", "postedAtFormatted": "Saturday, January 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20More%20ominous%20than%20a%20%5BMarriage%5D%20strike&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20More%20ominous%20than%20a%20%5BMarriage%5D%20strike%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNxDebjR4H2fjw9P3W%2Flink-more-ominous-than-a-marriage-strike%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20More%20ominous%20than%20a%20%5BMarriage%5D%20strike%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNxDebjR4H2fjw9P3W%2Flink-more-ominous-than-a-marriage-strike", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNxDebjR4H2fjw9P3W%2Flink-more-ominous-than-a-marriage-strike", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1476, "htmlBody": "<p><a href=\"http://dalrock.wordpress.com/2014/01/03/more-ominous-than-a-strike/\">Dalrock writes</a> an interesting article related to <a href=\"http://en.wikipedia.org/wiki/Helen_Smith_%28psychologist%29\">Dr. Helen Smith</a>'s book the Marriage Strike. I really have to bump it up on my too rapidly growing reading list. (^_^)</p>\n<blockquote>\n<p>Dr. Helen has a <a href=\"http://pjmedia.com/drhelen/2013/12/31/are-men-on-strike-or-have-they-quit/\">thoughtful post</a> up asking if the title of her book is an accurate description of men&rsquo;s response to the changes in the law and culture. &nbsp;While the title of her book is extremely effective in opening the discussion (which is what it needs to do), it isn&rsquo;t an accurate description of problem we face in the West. &nbsp;A strike can be negotiated with; &nbsp;offer them a bit more and they&rsquo;ll get back to work. &nbsp;Better yet, offer a few of them a side deal and break the cohesion. &nbsp;True strikes require moral or legal force to <a href=\"http://dalrock.wordpress.com/2010/09/03/sex-cartel/\">avoid this sort of peeling off</a>. &nbsp;The problem for the modern West is far worse. &nbsp;What we are seeing isn&rsquo;t men throwing a collective temper tantrum, noble or otherwise. &nbsp;What we are seeing is men responding to incentives.<strong> </strong>&nbsp;Even worse, inertia has delayed the response to incentives, which means much more adjustment is likely on the way.</p>\n<p>There was an old joke in the Soviet Union to the effect of:</p>\n<p><em>\"\"We pretend to work. &nbsp;They pretend to pay us.\"\"</em></p>\n<p>The problem for the Soviets was this wasn&rsquo;t a <em>movement</em>. &nbsp;They knew how to handle a movement, and Siberia had plenty of room above ground and below. &nbsp;The Soviets were masters at coercion through fear, but the problem wasn&rsquo;t a rebellion, it was that they had reached the limits of incentive through fear. &nbsp;In the short and even medium term fear is a very effective motivator. &nbsp;But over time if overused it loses some of its power, especially when it comes to the kind of productivity which requires creativity and risk taking. &nbsp;Standing out is risky; &nbsp;you don&rsquo;t want to be the worst worker on the line in a fear based system, but you also have reason to fear being the <em>best</em> worker on the line. &nbsp;This doesn&rsquo;t happen so much by conscious choice, but due to the influence of the incentive structure on the culture over time. &nbsp;Conscious choices can be bargained with, and threats of punishment are still effective. &nbsp;The culture itself is far harder to negotiate with. &nbsp;No one is <em>refusing</em> anything. &nbsp;So the Soviets had no choice but to assign quotas, and severely punish those who failed to meet them. &nbsp;But while the quota/coercion system keeps production running, it works against human nature. &nbsp;If you become the best producer you end up being assigned a larger share of the quota burden; &nbsp;<em>from each according to his abilities</em>. &nbsp;Over time the logic of this works its way into the culture, as everyone gets just a little more inclined to go with the flow and not do more than required. &nbsp;The problem is while momentum causes the response to be slow, it also means it is very difficult to deal with once you have enough of it to recognize.</p>\n<p>The problem we presently face in the West is similar. &nbsp;While we have a small number of men who have decided to slack off as a form of protest, the far more insidious risk to our economy is the across the board weakening of the incentive that a marriage based social structure creates for men to produce at their full potential. &nbsp;We&rsquo;ve moved from a mostly reward based incentive structure to a model the Soviets would have been proud of.</p>\n<p>You can see this at the micro level with a man whose wife goes Jenny Erickson on him. &nbsp;The courts understand that throwing a man out of the home and taking away his children naturally reduces the man&rsquo;s normal incentive to work to support his family. &nbsp;How could it not? &nbsp;It isn&rsquo;t that most men in this situation will stand by and watch their children starve, but they won&rsquo;t be motivated to produce quite as much. &nbsp;You can confiscate a percentage of his income in the form of child support, but he no longer has the incentive to fight his way quite so high up our progressive tax structure. &nbsp;This is why the courts <em>have</em> to assign the man an income quota he has to meet, Soviet style. &nbsp;Imputation of income isn&rsquo;t incidental to the child support family model; &nbsp;it is essential to the function of the model. &nbsp;Note that this doesn&rsquo;t mean the courts have to formally calculate an income quota for each man who ends up in the new child support family structure; &nbsp;in most cases the man has already assigned himself a quota based on past production. &nbsp;All the family courts need to do in most cases is make sure he doesn&rsquo;t fall below this quota.</p>\n<p>As I mentioned above coercion is generally a very effective incentive in the near and medium term. &nbsp;Part of the reason conservatives are so enamored with child support is the <a href=\"http://dalrock.wordpress.com/2012/04/14/threatpoint/\">threatpoint</a> it provides to keep existing husbands working as hard as possible. &nbsp;While in the long run this will ultimately create a culture where husbands are less inclined to become stand out earners, as Keynes famously put it in the long run we are all dead. &nbsp;The other problem is the changes in the culture in response to over use of coercion are by their very nature difficult to identify and quantify. &nbsp;This isn&rsquo;t unlike the <a href=\"http://en.wikipedia.org/wiki/Laffer_curve\">Laffer Curve</a>; &nbsp;while both liberals and conservatives agree regarding the principle of the curve, the <em>shape</em> of the curve is impossible to get agreement on. &nbsp;<em>Eventually</em> you can raise tax rates so high that you end up with lower revenue, but due to the problems of momentum identifying exactly when you have (or will) hit that point can be very difficult.</p>\n<p>The more immediate problem in the West is the reduced incentive young men perceive to compete as breadwinners due to the continuing delay in the age of marriage. &nbsp;Again this isn&rsquo;t a movement, it is a delayed response by the culture to reality. &nbsp;When the average woman marries in her late teens or even her early twenties, the average young man will see himself as competing with his peers for the job of husband. &nbsp;Not only is he competing to not be left out of the game entirely, but he is jockeying for a better choice of wife. &nbsp;But move the age of marriage out far enough, and eventually young men don&rsquo;t see themselves so clearly as competing for the job of husband. &nbsp;Extend the age of marriage far enough and eventually the culture of young men will be less focused on competing to signal provider status, and their priorities will shift (on the margin) toward slacking off. &nbsp;The question isn&rsquo;t <em>if</em> this will happen, but how long you can push the age of marriage out before this starts to happen, how much this will reduce the motivation of young men, and how long between the change in reality and the change in culture. &nbsp;Note also that this doesn&rsquo;t require men to swear off marriage entirely for this to greatly impact our tax base. &nbsp;Changing the culture of men in their formative years will have a lasting impact. &nbsp;You can&rsquo;t rewind time and undo a decade of (relative) slacking. &nbsp;Additionally, momentum tends to start working against you at some point. &nbsp;As the expectations of men as providers declines it eventually creates an <em>expectation of decline</em>. &nbsp;As each generation of new husbands come to the table with less to offer as providers, we eventually will start to expect future generations of husbands to offer even less.</p>\n<p>As I&rsquo;ve said before, all of this places our elites in a very difficult bind. &nbsp;Eventually the momentum which initially masked the problem makes it extremely difficult to address. &nbsp;Denial of the problem is a flawed strategy but it has important advantages. &nbsp;Once you acknowledge that the incentive structure is flawed you tend to accelerate the delayed response to the new structure. &nbsp;At the same time, the changes at the core of the problem are very close to the hearts of both liberals and conservatives. &nbsp;However, ignoring the problem will become more and more difficult because of the impact on the <a href=\"http://dalrock.wordpress.com/2013/12/17/progress/\">bottom line</a>. &nbsp;Because of this, we can expect to see more of what we already see. &nbsp;Feminists will continue their handwringing tentatively asking if <em>perhaps we have gone a bit too far</em>, and conservatives will redouble their efforts to convince men they need to <a href=\"http://dalrock.wordpress.com/2013/10/17/feminism-would-work-if-we-didnt-have-weak-men-screwing-everything-up/\">man up and stop sabotaging the glorious feminist progress</a>. &nbsp;Less conspicuously I also expect we will see some dialing back of the worst excesses of the family courts. &nbsp;However, because of the momentum involved and the reluctance to acknowledge the fundamental problem, these changes will at best only slow the problem, and they will always run the risk of initially accelerating it.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NxDebjR4H2fjw9P3W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 2, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "25180", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 95, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-04T18:05:54.713Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, First New Year", "slug": "meetup-moscow-first-new-year", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vLjCScSiNf895ktgB/meetup-moscow-first-new-year", "pageUrlRelative": "/posts/vLjCScSiNf895ktgB/meetup-moscow-first-new-year", "linkUrl": "https://www.lesswrong.com/posts/vLjCScSiNf895ktgB/meetup-moscow-first-new-year", "postedAtFormatted": "Saturday, January 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20First%20New%20Year&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20First%20New%20Year%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvLjCScSiNf895ktgB%2Fmeetup-moscow-first-new-year%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20First%20New%20Year%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvLjCScSiNf895ktgB%2Fmeetup-moscow-first-new-year", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvLjCScSiNf895ktgB%2Fmeetup-moscow-first-new-year", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 192, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/v8'>Moscow, First New Year</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 January 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It will be our next meet up, so there is no gathering on 5th.</p>\n\n<p>We will gather at the same second entrance, but we will go to the new room at 16:00. So please do not be late.</p>\n\n<p>Main topics and activities:</p>\n\n<ul>\n<li><p>Consequentialism and deontology, short presentation</p></li>\n<li><p>Training game</p></li>\n<li><p>Dutch booking</p></li>\n</ul>\n\n<p><strong>If you are going for the first time:</strong>\nWe gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.</p>\n\n<p>You can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the second entrance and then move to the new room.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/v8'>Moscow, First New Year</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vLjCScSiNf895ktgB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.4979884214357823e-06, "legacy": true, "legacyId": "25181", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__First_New_Year\">Discussion article for the meetup : <a href=\"/meetups/v8\">Moscow, First New Year</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 January 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It will be our next meet up, so there is no gathering on 5th.</p>\n\n<p>We will gather at the same second entrance, but we will go to the new room at 16:00. So please do not be late.</p>\n\n<p>Main topics and activities:</p>\n\n<ul>\n<li><p>Consequentialism and deontology, short presentation</p></li>\n<li><p>Training game</p></li>\n<li><p>Dutch booking</p></li>\n</ul>\n\n<p><strong>If you are going for the first time:</strong>\nWe gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.</p>\n\n<p>You can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the second entrance and then move to the new room.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__First_New_Year1\">Discussion article for the meetup : <a href=\"/meetups/v8\">Moscow, First New Year</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, First New Year", "anchor": "Discussion_article_for_the_meetup___Moscow__First_New_Year", "level": 1}, {"title": "Discussion article for the meetup : Moscow, First New Year", "anchor": "Discussion_article_for_the_meetup___Moscow__First_New_Year1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-04T19:39:54.264Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes January 2014", "slug": "rationality-quotes-january-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:37.008Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dX9JEgNmMKrapwXvk/rationality-quotes-january-2014", "pageUrlRelative": "/posts/dX9JEgNmMKrapwXvk/rationality-quotes-january-2014", "linkUrl": "https://www.lesswrong.com/posts/dX9JEgNmMKrapwXvk/rationality-quotes-january-2014", "postedAtFormatted": "Saturday, January 4th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20January%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20January%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdX9JEgNmMKrapwXvk%2Frationality-quotes-january-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20January%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdX9JEgNmMKrapwXvk%2Frationality-quotes-january-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdX9JEgNmMKrapwXvk%2Frationality-quotes-january-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Rules:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson. If you'd like to revive an old quote from one of those sources, please do so <a title=\"here\" href=\"/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dX9JEgNmMKrapwXvk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 1.498089356359172e-06, "legacy": true, "legacyId": "25182", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 189, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-05T00:41:00.257Z", "modifiedAt": null, "url": null, "title": "Fascists and Rakes", "slug": "fascists-and-rakes", "viewCount": null, "lastCommentedAt": "2021-09-29T16:59:17.890Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EAJTMRr8JpnG7Hw6Q/fascists-and-rakes", "pageUrlRelative": "/posts/EAJTMRr8JpnG7Hw6Q/fascists-and-rakes", "linkUrl": "https://www.lesswrong.com/posts/EAJTMRr8JpnG7Hw6Q/fascists-and-rakes", "postedAtFormatted": "Sunday, January 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fascists%20and%20Rakes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFascists%20and%20Rakes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEAJTMRr8JpnG7Hw6Q%2Ffascists-and-rakes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fascists%20and%20Rakes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEAJTMRr8JpnG7Hw6Q%2Ffascists-and-rakes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEAJTMRr8JpnG7Hw6Q%2Ffascists-and-rakes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 639, "htmlBody": "<p><a href=\"http://reasonableapproximation.net/2014/01/04/facists-and-rakes.html\"><em>Cross-posted from my blog</em></a></p>\n<p>It feels like most people have a moral intuition along the lines of \"you should let people do what they want, unless they're hurting other people\". We follow this guideline, and we expect other people to follow it. I'll call this the permissiveness principle, that behaviour should be permitted by default. When someone violates the permissiveness principle, we might call them a fascist, someone who exercises control for the sake of control.</p>\n<p>And there's another moral intuition, the harm-minimising principle: \"you should not hurt other people unless you have a good reason\". When someone violates harm-minimisation, we might call them a rake, someone who acts purely for their own pleasure without regard for others.</p>\n<p>But sometimes people disagree about what counts as \"hurting other people\". Maybe one group of people believes that tic-tacs are sentient, and that eating them constitutes harm; and another group believes that tic-tacs are not sentient, so eating them does not hurt anyone.</p>\n<p>What should happen here is that people try to work out exactly what it is they disagree about and why. What actually happens is that people appeal to permissiveness.</p>\n<p>Of course, by the permissiveness principle, people should be allowed to believe what they want, because holding a belief is harmless as long as you don't act on it. So we say something like \"I have no problem with people being morally opposed to eating tic-tacs, but they shouldn't impose their beliefs on the rest of us.\"</p>\n<p>Except that by the harm-minimising principle, those people probably <em>should</em> impose their beliefs on the rest of us. Forbidding you to eat tic-tacs doesn't hurt you much, and it saves the tic-tacs a lot of grief.</p>\n<p>It's not that they disagree with the permissiveness principle, they just think it doesn't apply. So appealing to the permissiveness principle isn't going to help much.</p>\n<p>I think the problem (or at least part of it) is, depending how you look at it, either double standards or not-double-enough standards.</p>\n<p>I apply the permissiveness principle \"unless they're hurting other people\", which really means \"unless I think they're hurting other people\". I want you to apply the permissiveness principle \"unless they're hurting other people\", which <em>still</em> means \"unless I think they're hurting other people\".</p>\n<p>Meanwhile, you apply the permissiveness principle unless <em>you</em> think someone is hurting other people; and you want me to apply it unless <em>you</em> think they're hurting other people.</p>\n<p>So when we disagree about whether or not something is hurting other people, I think you're a fascist because you're failing to apply the permissiveness principle; and you think I'm a rake because I'm failing to apply the harm-minimisation principle; or vice-versa. Neither of these things is true, of course.</p>\n<p>It gets worse, because once I've decided that you're a fascist, I think the <em>reason we're arguing</em> is that you're a fascist. If you would only stop being a fascist, we could get along fine. You can go on thinking tic-tacs are sentient, you just need to stop being a fascist.</p>\n<p>But you're not a fascist. The <em>real</em> reason we're arguing is that you think tic-tacs are sentient. You're acting exactly as you should do if tic-tacs were sentient, but they're not. I need to stop treating you like a fascist, and start trying to convince you that tic-tacs are not sentient.</p>\n<p>And, symmetrically, you've decided I'm a rake, which isn't true, and you've decided that that's why we're arguing, which isn't true; we're arguing because I think tic-tacs aren't sentient. You need to stop treating me like a rake, and start trying to convince me that tic-tacs are sentient.</p>\n<p>I don't expect either of us to actually convince the other, very often. If it was that easy, someone would probably have already done it. But at least I'd like us both to acknowledge that our opponent is neither a fascist nor a rake, they just believe something that isn't true.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wzgcQCrwKfETcBpR9": 1, "nSHiKwWyMZFdZg5qt": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EAJTMRr8JpnG7Hw6Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 58, "extendedScore": null, "score": 0.000161, "legacy": true, "legacyId": "25183", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 64, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 71, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-05T07:41:18.034Z", "modifiedAt": null, "url": null, "title": "[Link] Valproic acid, a drug for brain plasticity", "slug": "link-valproic-acid-a-drug-for-brain-plasticity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:34.932Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "vpeitoi89GPGat77P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eoQzegXyhRSniiNdm/link-valproic-acid-a-drug-for-brain-plasticity", "pageUrlRelative": "/posts/eoQzegXyhRSniiNdm/link-valproic-acid-a-drug-for-brain-plasticity", "linkUrl": "https://www.lesswrong.com/posts/eoQzegXyhRSniiNdm/link-valproic-acid-a-drug-for-brain-plasticity", "postedAtFormatted": "Sunday, January 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Valproic%20acid%2C%20a%20drug%20for%20brain%20plasticity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Valproic%20acid%2C%20a%20drug%20for%20brain%20plasticity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeoQzegXyhRSniiNdm%2Flink-valproic-acid-a-drug-for-brain-plasticity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Valproic%20acid%2C%20a%20drug%20for%20brain%20plasticity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeoQzegXyhRSniiNdm%2Flink-valproic-acid-a-drug-for-brain-plasticity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeoQzegXyhRSniiNdm%2Flink-valproic-acid-a-drug-for-brain-plasticity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 141, "htmlBody": "<p>NPR reports on a study <a href=\"http://www.npr.org/2014/01/04/259552442/want-perfect-pitch-you-could-pop-a-pill-for-that?sc=ipad&amp;f=1001\">giving volprioc acid to adults and training them on pitch</a> (singing):</p>\n<blockquote>\n<p>Hensch is studying a drug which might allow adults to learn perfect pitch by recreating this critical period in brain development. Hensch says the drug, valprioc acid, allows the brain to absorb new information as easily as it did before age 7.</p>\n<p>\"It's a mood-stabilizing drug, but we found that it also restores the plasticity of the brain to a juvenile state,\" Hensch tells NPR's Linda Wertheimer.</p>\n</blockquote>\n<p>Brain plasticity is useful for a whole lot more than learning pitch. As the article notes it would be invaluable for training one's ear to pick up sounds of foreign languages, but also it seems reasonable to this commentator that high levels of plasticity during rationality training or other forms of self-development would result in more transformative results.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eoQzegXyhRSniiNdm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 20, "extendedScore": null, "score": 1.4988644367302556e-06, "legacy": true, "legacyId": "25184", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-05T09:51:12.231Z", "modifiedAt": null, "url": null, "title": "Another Critique of Effective Altruism", "slug": "another-critique-of-effective-altruism", "viewCount": null, "lastCommentedAt": "2019-05-14T15:35:42.021Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsteinhardt", "createdAt": "2010-08-05T03:07:27.568Z", "isAdmin": false, "displayName": "jsteinhardt"}, "userId": "EF8W65G6RaXxZjLBX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CZmkPvzkMdQJxXy54/another-critique-of-effective-altruism", "pageUrlRelative": "/posts/CZmkPvzkMdQJxXy54/another-critique-of-effective-altruism", "linkUrl": "https://www.lesswrong.com/posts/CZmkPvzkMdQJxXy54/another-critique-of-effective-altruism", "postedAtFormatted": "Sunday, January 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20Critique%20of%20Effective%20Altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20Critique%20of%20Effective%20Altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCZmkPvzkMdQJxXy54%2Fanother-critique-of-effective-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20Critique%20of%20Effective%20Altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCZmkPvzkMdQJxXy54%2Fanother-critique-of-effective-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCZmkPvzkMdQJxXy54%2Fanother-critique-of-effective-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1659, "htmlBody": "<address><!-- P { margin-bottom: 0.08in; }A:link { } -->Cross-posted from my <a href=\"http://jsteinhardt.wordpress.com/2014/01/05/another-critique-of-effective-altruism/\">blog</a>. It is almost certainly a bad idea to let this post be your first exposure to the effective altruist movement. You should at the very least read <a href=\"http://blog.givewell.org/2013/08/13/effective-altruism/\">these</a> <a href=\"http://blog.givewell.org/2013/08/20/excited-altruism/\">two</a> posts first. </address><address> </address>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\"><br />Recently Ben Kuhn wrote a <a href=\"/lw/j8n/a_critique_of_effective_altruism/\">critique of effective altruism</a>. I'm glad to see such self-examination taking place, but I'm also concerned that the essay did not attack some of the most serious issues I see in the effective altruist movement, so I've decided to write my own critique. Due to time constraints, this critique is short and incomplete. I've tried to bring up arguments that would make people feel uncomfortable and defensive; hopefully I've succeeded.</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">&nbsp;</p>\n<p>Briefly, here are some of the major issues I have with the effective altruism movement as it currently stands:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">Over-focus on &ldquo;tried and true&rdquo; and &ldquo;default&rdquo; options, which may both reduce actual impact and decrease exploration of new potentially high-value opportunities.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">Over-confident claims coupled with insufficient background research.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">Over-reliance on a small set of tools for assessing opportunities, which lead many to underestimate the value of things such as &ldquo;flow-through&rdquo; effects.</p>\n</li>\n</ul>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">The common theme here is a subtle underlying message that simple, shallow analyses can allow one to make high-impact career and giving choices, and divest one of the need to dig further. I doubt that anyone explicitly believes this, but I do believe that this theme comes out implicitly both in arguments people make and in actions people take.</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">Lest this essay give a mistaken impression to the casual reader, I should note that <strong>there are many examplary effective altruists who I feel are mostly immune to the issues above</strong>; for instance, the <a href=\"http://blog.givewell.org/\">GiveWell blog</a> does a very good job of warning against the first and third points above, and I would recommend anyone who isn't already to subscribe to it (and there are other examples that I'm failing to mention). But for the purposes of this essay, I will ignore this fact except for the current caveat.</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">&nbsp;</p>\n<h3 style=\"margin-bottom: 0in\"><strong>Over-focus on \"tried and true\" options</strong></h3>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\"><br />It seems to me that the effective altruist movement over-focuses on &ldquo;tried and true&rdquo; options, both in giving opportunities and in career paths. Perhaps the biggest example of this is the prevalence of &ldquo;earning to give&rdquo;. While this is certainly an admirable option, it should be considered as a baseline to improve upon, not a definitive answer.</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">The biggest issue with the &ldquo;earning to give&rdquo; path is that careers in finance and software (the two most common avenues for this) are incredibly straight-forward and secure. The two things that finance and software have in common is that there is a well-defined application process similar to the one for undergraduate admissions, and given reasonable job performance one will continue to be given promotions and raises (this probably entails working hard, but the end result is still rarely in doubt). One also gets a constant source of extrinsic positive reinforcement from the money they earn. Why do I call these things an &ldquo;issue&rdquo;? Because I think that these attributes encourage people to pursue these paths without looking for less obvious, less certain, but ultimately better paths. <a href=\"http://yaledailynews.com/weekend/2011/09/30/even-artichokes-have-doubts/\">One in six Yale graduates go into finance and consulting</a>, seemingly due to the simplicity of applying and the easy supply of extrinsic motivation. My intuition is that this ratio is higher than an optimal society would have, even if such people commonly gave generously (and it is certainly much higher than the number of people who <em>enter</em><span style=\"font-style: normal\"> college planning to pursue such paths)</span><span style=\"font-style: normal\">.</span></p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\"><span style=\"font-style: normal\"><br /></span></p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">Contrast this with, for instance, working at a start-up. Most start-ups are low-impact, but it is undeniable that at least some have been extraordinarily high-impact, so this seems like an area that effective altruists should be considering strongly. Why aren't there more of us at 23&amp;me, or Coursera, or Quora, or Stripe? I think it is because these opportunities are less obvious and take more work to find, once you start working it often isn't clear whether what you're doing will have a positive impact or not, and your future job security is massively uncertain. There are few sources of extrinsic motivation in such a career: perhaps moreso at one of the companies mentioned above, which are reasonably established and have customers, but what about the 4-person start-up teams working in a warehouse somewhere? Some of them will go on to do great things but right now their lives must be full of anxiousness and uncertainty.</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">I don't mean to fetishize start-ups. They are just one well-known example of a potentially high-value career path that, to me, seems underexplored within the EA movement. I would argue (perhaps self-servingly) that academia is another example of such a path, with similar psychological obstacles: every 5 years or so you have the opportunity to get kicked out (e.g. applying for faculty jobs, and being up for tenure), you need to relocate regularly, few people will read your work and even fewer will praise it, and it won't be clear whether it had a positive impact until many years down the road. And beyond the &ldquo;obvious&rdquo; alternatives of start-ups and academia, what of the paths that haven't been created yet? GiveWell was revolutionary when it came about. Who will be the next GiveWell? And by this I don't mean the next charity evaluator, but the next set of people who fundamentally alter how we view altruism.</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">&nbsp;</p>\n<h3 style=\"margin-bottom: 0in\"><strong>Over-confident claims coupled with insufficient background research</strong></h3>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\"><br />The history of effective altruism is littered with over-confident claims, many of which have later turned out to be false. In 2009, Peter Singer claimed that you could save a life for $200 (and many others repeated his claim). While the number was already questionable at the time, by 2011 we discovered that the number was completely off. Now new numbers were thrown around: from numbers still in the hundreds of dollars (GWWC's estimate for SCI, which was later shown to be flawed) up to $1600 (GiveWell's estimate for AMF, which GiveWell itself expected to go up, and which indeed did go up). These numbers were often cited without caveats, as well as other claims such as that the effectiveness of charities can vary by a factor of 1,000. How many people citing these numbers understood the process that generated them, or the high degree of uncertainty surrounding them, or the inaccuracy of past estimates? How many would have pointed out that saying that charities vary by a factor of 1,000 in effectiveness is by itself not very helpful, and is more a statement about how bad the bottom end is than how good the top end is?</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">More problematic than the careless bandying of numbers is the tendency toward not doing strong background research. A common pattern I see is: an effective altruist makes a bold claim, then when pressed on it offers a heuristic justification together with the claim that &ldquo;<a href=\"http://80000hours.org/blog/4-estimation-is-the-best-we-have\">estimation is </a><a href=\"http://80000hours.org/blog/4-estimation-is-the-best-we-have\">the best</a><a href=\"http://80000hours.org/blog/4-estimation-is-the-best-we-have\"> we have</a>&rdquo;. This sort of argument acts as a conversation-stopper (and can also be quite annoying, which may be part of what drives some people away from effective altruism). In many of these cases, there are relatively easy opportunities to do background reading to further educate oneself about the claim being made. It can appear to an outside observer as though people are opting for the fun, easy activity (speculation) rather than the harder and more worthwhile activity (research). Again, I'm not claiming that this is people's explicit thought process, but it does seem to be what ends up happening.</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\"><span style=\"font-style: normal\">Why haven't more EAs</span> signed up for a course on global security, or tried to understand how DARPA funds projects, or learned about third-world health? I've heard claims that this would be too time-consuming relative to the value it provides, but this seems like a poor excuse if we want to be taken seriously as a movement (or even just want to reach consistently accurate conclusions about the world).</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">&nbsp;</p>\n<h3 style=\"margin-bottom: 0in\"><strong>Over-reliance on a small set of tools</strong></h3>\n<p><br />Effective altruists tend to have a lot of interest in quantitative estimates. We want to know what the best thing to do is, and we want a numerical value. This causes us to rely on scientific studies, economic reports, and Fermi estimates. It can cause us to underweight things like the competence of a particular organization, the strength of the people involved, and other &ldquo;intangibles&rdquo; (which are often not actually intangible but simply difficult to assign a number to). It also can cause us to over-focus on money as a unit of altruism, while often-times &ldquo;it isn't about the money&rdquo;: it's about doing the groundwork that no one is doing, or finding the opportunity that no one has found yet.</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">Quantitative estimates often also tend to ignore <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>: effects which are an indirect, rather than direct, result of an action (such as decreased disease in the third world contributing in the long run to increased global security). These effects are difficult to quantify but human and cultural intuition can do a reasonable job of taking them into account. As such, I often worry that effective altruists may actually be less effective than &ldquo;normal&rdquo; altruists. (One can point to all sorts of examples of farcical charities to claim that regular altruism sucks, but this misses the point that there are also amazing organizations out there, such as the <a href=\"https://www.simonsfoundation.org/\">Simons Foundation</a> or <a href=\"http://www.hhmi.org/\">HHMI</a>, which are doing enormous amounts of good despite not subscribing to the EA philosophy.)</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">What's particularly worrisome is that even if we were less effective than normal altruists, we would probably still end up looking better by our own standards, which explicitly fail to account for the ways in which normal altruists might outperform us (see above). This is a problem with any paradigm, but the fact that the effective altruist community is small and insular and relies heavily on its paradigm makes us far more susceptible to it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CZmkPvzkMdQJxXy54", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 28, "extendedScore": null, "score": 7.9e-05, "legacy": true, "legacyId": "25186", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<address><!-- P { margin-bottom: 0.08in; }A:link { } -->Cross-posted from my <a href=\"http://jsteinhardt.wordpress.com/2014/01/05/another-critique-of-effective-altruism/\">blog</a>. It is almost certainly a bad idea to let this post be your first exposure to the effective altruist movement. You should at the very least read <a href=\"http://blog.givewell.org/2013/08/13/effective-altruism/\">these</a> <a href=\"http://blog.givewell.org/2013/08/20/excited-altruism/\">two</a> posts first. </address><address> </address>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\"><br>Recently Ben Kuhn wrote a <a href=\"/lw/j8n/a_critique_of_effective_altruism/\">critique of effective altruism</a>. I'm glad to see such self-examination taking place, but I'm also concerned that the essay did not attack some of the most serious issues I see in the effective altruist movement, so I've decided to write my own critique. Due to time constraints, this critique is short and incomplete. I've tried to bring up arguments that would make people feel uncomfortable and defensive; hopefully I've succeeded.</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">&nbsp;</p>\n<p>Briefly, here are some of the major issues I have with the effective altruism movement as it currently stands:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">Over-focus on \u201ctried and true\u201d and \u201cdefault\u201d options, which may both reduce actual impact and decrease exploration of new potentially high-value opportunities.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">Over-confident claims coupled with insufficient background research.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">Over-reliance on a small set of tools for assessing opportunities, which lead many to underestimate the value of things such as \u201cflow-through\u201d effects.</p>\n</li>\n</ul>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">The common theme here is a subtle underlying message that simple, shallow analyses can allow one to make high-impact career and giving choices, and divest one of the need to dig further. I doubt that anyone explicitly believes this, but I do believe that this theme comes out implicitly both in arguments people make and in actions people take.</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">Lest this essay give a mistaken impression to the casual reader, I should note that <strong>there are many examplary effective altruists who I feel are mostly immune to the issues above</strong>; for instance, the <a href=\"http://blog.givewell.org/\">GiveWell blog</a> does a very good job of warning against the first and third points above, and I would recommend anyone who isn't already to subscribe to it (and there are other examples that I'm failing to mention). But for the purposes of this essay, I will ignore this fact except for the current caveat.</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">&nbsp;</p>\n<h3 style=\"margin-bottom: 0in\" id=\"Over_focus_on__tried_and_true__options\"><strong>Over-focus on \"tried and true\" options</strong></h3>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\"><br>It seems to me that the effective altruist movement over-focuses on \u201ctried and true\u201d options, both in giving opportunities and in career paths. Perhaps the biggest example of this is the prevalence of \u201cearning to give\u201d. While this is certainly an admirable option, it should be considered as a baseline to improve upon, not a definitive answer.</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">The biggest issue with the \u201cearning to give\u201d path is that careers in finance and software (the two most common avenues for this) are incredibly straight-forward and secure. The two things that finance and software have in common is that there is a well-defined application process similar to the one for undergraduate admissions, and given reasonable job performance one will continue to be given promotions and raises (this probably entails working hard, but the end result is still rarely in doubt). One also gets a constant source of extrinsic positive reinforcement from the money they earn. Why do I call these things an \u201cissue\u201d? Because I think that these attributes encourage people to pursue these paths without looking for less obvious, less certain, but ultimately better paths. <a href=\"http://yaledailynews.com/weekend/2011/09/30/even-artichokes-have-doubts/\">One in six Yale graduates go into finance and consulting</a>, seemingly due to the simplicity of applying and the easy supply of extrinsic motivation. My intuition is that this ratio is higher than an optimal society would have, even if such people commonly gave generously (and it is certainly much higher than the number of people who <em>enter</em><span style=\"font-style: normal\"> college planning to pursue such paths)</span><span style=\"font-style: normal\">.</span></p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\"><span style=\"font-style: normal\"><br></span></p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">Contrast this with, for instance, working at a start-up. Most start-ups are low-impact, but it is undeniable that at least some have been extraordinarily high-impact, so this seems like an area that effective altruists should be considering strongly. Why aren't there more of us at 23&amp;me, or Coursera, or Quora, or Stripe? I think it is because these opportunities are less obvious and take more work to find, once you start working it often isn't clear whether what you're doing will have a positive impact or not, and your future job security is massively uncertain. There are few sources of extrinsic motivation in such a career: perhaps moreso at one of the companies mentioned above, which are reasonably established and have customers, but what about the 4-person start-up teams working in a warehouse somewhere? Some of them will go on to do great things but right now their lives must be full of anxiousness and uncertainty.</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">I don't mean to fetishize start-ups. They are just one well-known example of a potentially high-value career path that, to me, seems underexplored within the EA movement. I would argue (perhaps self-servingly) that academia is another example of such a path, with similar psychological obstacles: every 5 years or so you have the opportunity to get kicked out (e.g. applying for faculty jobs, and being up for tenure), you need to relocate regularly, few people will read your work and even fewer will praise it, and it won't be clear whether it had a positive impact until many years down the road. And beyond the \u201cobvious\u201d alternatives of start-ups and academia, what of the paths that haven't been created yet? GiveWell was revolutionary when it came about. Who will be the next GiveWell? And by this I don't mean the next charity evaluator, but the next set of people who fundamentally alter how we view altruism.</p>\n<p style=\"margin-bottom: 0in\" align=\"LEFT\">&nbsp;</p>\n<h3 style=\"margin-bottom: 0in\" id=\"Over_confident_claims_coupled_with_insufficient_background_research\"><strong>Over-confident claims coupled with insufficient background research</strong></h3>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\"><br>The history of effective altruism is littered with over-confident claims, many of which have later turned out to be false. In 2009, Peter Singer claimed that you could save a life for $200 (and many others repeated his claim). While the number was already questionable at the time, by 2011 we discovered that the number was completely off. Now new numbers were thrown around: from numbers still in the hundreds of dollars (GWWC's estimate for SCI, which was later shown to be flawed) up to $1600 (GiveWell's estimate for AMF, which GiveWell itself expected to go up, and which indeed did go up). These numbers were often cited without caveats, as well as other claims such as that the effectiveness of charities can vary by a factor of 1,000. How many people citing these numbers understood the process that generated them, or the high degree of uncertainty surrounding them, or the inaccuracy of past estimates? How many would have pointed out that saying that charities vary by a factor of 1,000 in effectiveness is by itself not very helpful, and is more a statement about how bad the bottom end is than how good the top end is?</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">More problematic than the careless bandying of numbers is the tendency toward not doing strong background research. A common pattern I see is: an effective altruist makes a bold claim, then when pressed on it offers a heuristic justification together with the claim that \u201c<a href=\"http://80000hours.org/blog/4-estimation-is-the-best-we-have\">estimation is </a><a href=\"http://80000hours.org/blog/4-estimation-is-the-best-we-have\">the best</a><a href=\"http://80000hours.org/blog/4-estimation-is-the-best-we-have\"> we have</a>\u201d. This sort of argument acts as a conversation-stopper (and can also be quite annoying, which may be part of what drives some people away from effective altruism). In many of these cases, there are relatively easy opportunities to do background reading to further educate oneself about the claim being made. It can appear to an outside observer as though people are opting for the fun, easy activity (speculation) rather than the harder and more worthwhile activity (research). Again, I'm not claiming that this is people's explicit thought process, but it does seem to be what ends up happening.</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\"><span style=\"font-style: normal\">Why haven't more EAs</span> signed up for a course on global security, or tried to understand how DARPA funds projects, or learned about third-world health? I've heard claims that this would be too time-consuming relative to the value it provides, but this seems like a poor excuse if we want to be taken seriously as a movement (or even just want to reach consistently accurate conclusions about the world).</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">&nbsp;</p>\n<h3 style=\"margin-bottom: 0in\" id=\"Over_reliance_on_a_small_set_of_tools\"><strong>Over-reliance on a small set of tools</strong></h3>\n<p><br>Effective altruists tend to have a lot of interest in quantitative estimates. We want to know what the best thing to do is, and we want a numerical value. This causes us to rely on scientific studies, economic reports, and Fermi estimates. It can cause us to underweight things like the competence of a particular organization, the strength of the people involved, and other \u201cintangibles\u201d (which are often not actually intangible but simply difficult to assign a number to). It also can cause us to over-focus on money as a unit of altruism, while often-times \u201cit isn't about the money\u201d: it's about doing the groundwork that no one is doing, or finding the opportunity that no one has found yet.</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">Quantitative estimates often also tend to ignore <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>: effects which are an indirect, rather than direct, result of an action (such as decreased disease in the third world contributing in the long run to increased global security). These effects are difficult to quantify but human and cultural intuition can do a reasonable job of taking them into account. As such, I often worry that effective altruists may actually be less effective than \u201cnormal\u201d altruists. (One can point to all sorts of examples of farcical charities to claim that regular altruism sucks, but this misses the point that there are also amazing organizations out there, such as the <a href=\"https://www.simonsfoundation.org/\">Simons Foundation</a> or <a href=\"http://www.hhmi.org/\">HHMI</a>, which are doing enormous amounts of good despite not subscribing to the EA philosophy.)</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">&nbsp;</p>\n<p style=\"margin-bottom: 0in; font-weight: normal\" align=\"LEFT\">What's particularly worrisome is that even if we were less effective than normal altruists, we would probably still end up looking better by our own standards, which explicitly fail to account for the ways in which normal altruists might outperform us (see above). This is a problem with any paradigm, but the fact that the effective altruist community is small and insular and relies heavily on its paradigm makes us far more susceptible to it.</p>", "sections": [{"title": "Over-focus on \"tried and true\" options", "anchor": "Over_focus_on__tried_and_true__options", "level": 1}, {"title": "Over-confident claims coupled with insufficient background research", "anchor": "Over_confident_claims_coupled_with_insufficient_background_research", "level": 1}, {"title": "Over-reliance on a small set of tools", "anchor": "Over_reliance_on_a_small_set_of_tools", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "109 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 109, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["E3beR7bQ723kkNHpA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-05T15:35:38.166Z", "modifiedAt": null, "url": null, "title": "Introducing .impact", "slug": "introducing-impact", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2pKZxbeLhwyqs3upF/introducing-impact", "pageUrlRelative": "/posts/2pKZxbeLhwyqs3upF/introducing-impact", "linkUrl": "https://www.lesswrong.com/posts/2pKZxbeLhwyqs3upF/introducing-impact", "postedAtFormatted": "Sunday, January 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Introducing%20.impact&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntroducing%20.impact%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2pKZxbeLhwyqs3upF%2Fintroducing-impact%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Introducing%20.impact%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2pKZxbeLhwyqs3upF%2Fintroducing-impact", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2pKZxbeLhwyqs3upF%2Fintroducing-impact", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 444, "htmlBody": "<h2><strong>.impact is a new network of volunteers coordinating effective altruist projects.</strong></h2>\n<p>There are many project ideas that could be really useful for the effective altruist community. There are people with the skills and free time to make things happen but who lack guidance or support. .impact aims to provide infrastructure to get people and useful projects together. We hope to help volunteers learn useful skills, meet great people, and create something substantial. &nbsp;</p>\n<p>We're soon launching&nbsp;<a href=\"http://skillshare.im/\">Skillshare.im</a>, a place to share skills and services for free. &nbsp;We've collaborated on several Trello boards to <a href=\"https://trello.com/b/npVl3e9d/effective-altruist-project-board\">organize projects</a>, <a href=\"https://trello.com/b/shY5TwCm/effective-altruist-research-topics\">research topics</a>, and <a href=\"https://trello.com/b/F3O577AH/ea-productivity-resources\">useful resources</a>. &nbsp;We&rsquo;ve brainstormed and started outlining projects like a <a href=\"https://impact.hackpad.com/Studying-Vegetarian-Advocacy-m11aW51ENVW\">vegetarian advocacy study</a>, an <a href=\"https://impact.hackpad.com/EA-Wiki-y8z6wp5yCxD\">EA wiki</a>, and <a href=\"https://impact.hackpad.com/Argument-Mapper-hNKt3IU7gAv\">argument mapping software</a>. &nbsp;We&rsquo;ve had <a href=\"http://dotimpact.im/meetings.html\">several weekly group hangouts and discussions</a> with a variety of individuals. &nbsp;Most of our general discussion holds place in <a href=\"https://www.facebook.com/groups/dotimpact/\">our Facebook group</a>, which now has <a href=\"https://www.facebook.com/groups/dotimpact/members/\">114 members</a> and seems to be growing organically at a rate of 5 per week.</p>\n<p>&nbsp;</p>\n<h2>Our Purpose and Values</h2>\n<p>We&rsquo;re guided first and foremost by a desire to do the most good. This is our purpose. But it&rsquo;s difficult to do this without having some additional values. The following are heuristics we think will best guide future volunteering in order to optimize our purpose. These are will be changed as we gain experience.</p>\n<p><strong>We value action.</strong> &nbsp;\"Help people\" is a good rule, and it&rsquo;s often a more useful one than \"understand how to help people optimally.\" There appears to be a lot of low-hanging fruit&mdash;we can achieve a lot by simply motivating people to <em>do something</em>.</p>\n<p><strong>We value effectiveness.</strong> We encourage and promote projects according to our expectations of their impact and probability of success. We vet and brainstorm ideas before putting them into action. We use <a href=\"http://intelligence.org/2013/04/04/the-lean-nonprofit/\">lean methodology</a> to get things out quickly and then decide whether to expand, pivot, or end a project.</p>\n<p><strong>We value openness and transparency.</strong> Our meetings and projects are <a href=\"https://impact.hackpad.com/Meeting-Notes-ehPmJqlokpH\">documented</a>; published work is <a href=\"https://en.wikipedia.org/wiki/Open_source\">open source</a> or <a href=\"http://creativecommons.org/about/cc0\">creative commons</a>. We&rsquo;ll release information on the success of applications, and we&rsquo;ll publish lessons we&rsquo;ve learned on our wiki and blog.</p>\n<p><strong>We value decentralization.</strong> We believe that volunteers will do best with little outside authority. We will try to limit individual ideologies in favor of collective opinions. Important decisions will be decided through voting whenever possible.&nbsp;</p>\n<p>&nbsp;</p>\n<h2>Get Involved</h2>\n<p>Interested in working on a project? Already working away on something, but want more support? Interested in learning a particular skill, like computer programming or research? We need you!</p>\n<p>If you would like to meet the existing community or would like help finding a project <a href=\"http://dotimpact.im/join.html\">we&rsquo;d be happy to talk to you</a>. You can also <a href=\"https://www.facebook.com/groups/dotimpact/\">join our Facebook group</a> or look through our <a href=\"https://trello.com/b/npVl3e9d/effective-altruist-project-board\">Trello board of projects</a>.</p>\n<p>-</p>\n<address>(Also <a href=\"http://dotimpact.im/2014/01/05/introducing-dotimpact/\">cross-posted on our website</a>.)</address>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2pKZxbeLhwyqs3upF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 27, "extendedScore": null, "score": 1.4993744576118023e-06, "legacy": true, "legacyId": "25187", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"_impact_is_a_new_network_of_volunteers_coordinating_effective_altruist_projects_\"><strong>.impact is a new network of volunteers coordinating effective altruist projects.</strong></h2>\n<p>There are many project ideas that could be really useful for the effective altruist community. There are people with the skills and free time to make things happen but who lack guidance or support. .impact aims to provide infrastructure to get people and useful projects together. We hope to help volunteers learn useful skills, meet great people, and create something substantial. &nbsp;</p>\n<p>We're soon launching&nbsp;<a href=\"http://skillshare.im/\">Skillshare.im</a>, a place to share skills and services for free. &nbsp;We've collaborated on several Trello boards to <a href=\"https://trello.com/b/npVl3e9d/effective-altruist-project-board\">organize projects</a>, <a href=\"https://trello.com/b/shY5TwCm/effective-altruist-research-topics\">research topics</a>, and <a href=\"https://trello.com/b/F3O577AH/ea-productivity-resources\">useful resources</a>. &nbsp;We\u2019ve brainstormed and started outlining projects like a <a href=\"https://impact.hackpad.com/Studying-Vegetarian-Advocacy-m11aW51ENVW\">vegetarian advocacy study</a>, an <a href=\"https://impact.hackpad.com/EA-Wiki-y8z6wp5yCxD\">EA wiki</a>, and <a href=\"https://impact.hackpad.com/Argument-Mapper-hNKt3IU7gAv\">argument mapping software</a>. &nbsp;We\u2019ve had <a href=\"http://dotimpact.im/meetings.html\">several weekly group hangouts and discussions</a> with a variety of individuals. &nbsp;Most of our general discussion holds place in <a href=\"https://www.facebook.com/groups/dotimpact/\">our Facebook group</a>, which now has <a href=\"https://www.facebook.com/groups/dotimpact/members/\">114 members</a> and seems to be growing organically at a rate of 5 per week.</p>\n<p>&nbsp;</p>\n<h2 id=\"Our_Purpose_and_Values\">Our Purpose and Values</h2>\n<p>We\u2019re guided first and foremost by a desire to do the most good. This is our purpose. But it\u2019s difficult to do this without having some additional values. The following are heuristics we think will best guide future volunteering in order to optimize our purpose. These are will be changed as we gain experience.</p>\n<p><strong>We value action.</strong> &nbsp;\"Help people\" is a good rule, and it\u2019s often a more useful one than \"understand how to help people optimally.\" There appears to be a lot of low-hanging fruit\u2014we can achieve a lot by simply motivating people to <em>do something</em>.</p>\n<p><strong>We value effectiveness.</strong> We encourage and promote projects according to our expectations of their impact and probability of success. We vet and brainstorm ideas before putting them into action. We use <a href=\"http://intelligence.org/2013/04/04/the-lean-nonprofit/\">lean methodology</a> to get things out quickly and then decide whether to expand, pivot, or end a project.</p>\n<p><strong>We value openness and transparency.</strong> Our meetings and projects are <a href=\"https://impact.hackpad.com/Meeting-Notes-ehPmJqlokpH\">documented</a>; published work is <a href=\"https://en.wikipedia.org/wiki/Open_source\">open source</a> or <a href=\"http://creativecommons.org/about/cc0\">creative commons</a>. We\u2019ll release information on the success of applications, and we\u2019ll publish lessons we\u2019ve learned on our wiki and blog.</p>\n<p><strong>We value decentralization.</strong> We believe that volunteers will do best with little outside authority. We will try to limit individual ideologies in favor of collective opinions. Important decisions will be decided through voting whenever possible.&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"Get_Involved\">Get Involved</h2>\n<p>Interested in working on a project? Already working away on something, but want more support? Interested in learning a particular skill, like computer programming or research? We need you!</p>\n<p>If you would like to meet the existing community or would like help finding a project <a href=\"http://dotimpact.im/join.html\">we\u2019d be happy to talk to you</a>. You can also <a href=\"https://www.facebook.com/groups/dotimpact/\">join our Facebook group</a> or look through our <a href=\"https://trello.com/b/npVl3e9d/effective-altruist-project-board\">Trello board of projects</a>.</p>\n<p>-</p>\n<address>(Also <a href=\"http://dotimpact.im/2014/01/05/introducing-dotimpact/\">cross-posted on our website</a>.)</address>", "sections": [{"title": ".impact is a new network of volunteers coordinating effective altruist projects.", "anchor": "_impact_is_a_new_network_of_volunteers_coordinating_effective_altruist_projects_", "level": 1}, {"title": "Our Purpose and Values", "anchor": "Our_Purpose_and_Values", "level": 1}, {"title": "Get Involved", "anchor": "Get_Involved", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-05T18:15:11.244Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta, January Meetup & Movie Night!", "slug": "meetup-atlanta-january-meetup-and-movie-night", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:34.222Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AjFsWtHzTe5oDndbT/meetup-atlanta-january-meetup-and-movie-night", "pageUrlRelative": "/posts/AjFsWtHzTe5oDndbT/meetup-atlanta-january-meetup-and-movie-night", "linkUrl": "https://www.lesswrong.com/posts/AjFsWtHzTe5oDndbT/meetup-atlanta-january-meetup-and-movie-night", "postedAtFormatted": "Sunday, January 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%2C%20January%20Meetup%20%26%20Movie%20Night!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%2C%20January%20Meetup%20%26%20Movie%20Night!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAjFsWtHzTe5oDndbT%2Fmeetup-atlanta-january-meetup-and-movie-night%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%2C%20January%20Meetup%20%26%20Movie%20Night!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAjFsWtHzTe5oDndbT%2Fmeetup-atlanta-january-meetup-and-movie-night", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAjFsWtHzTe5oDndbT%2Fmeetup-atlanta-january-meetup-and-movie-night", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/v9'>Atlanta, January Meetup &amp; Movie Night!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 January 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1575 Clairmont Road Decatur, Apartment 718, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A whole new year! Woah.</p>\n\n<p>We'll start January's meetup with an introduction for new members, discussions of New Year's resolutions and how to achieve them (contribution totally voluntary, of course). Followed up with club stuffs, and then a movie night! Vote for your movie choice on our facebook group! <a href=\"https://www.facebook.com/groups/Atlanta.Lesswrong/\" rel=\"nofollow\">https://www.facebook.com/groups/Atlanta.Lesswrong/</a>\nBring your snuggies and popcorn!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/v9'>Atlanta, January Meetup &amp; Movie Night!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AjFsWtHzTe5oDndbT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -3, "extendedScore": null, "score": 1.4995460813419705e-06, "legacy": true, "legacyId": "25188", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta__January_Meetup___Movie_Night_\">Discussion article for the meetup : <a href=\"/meetups/v9\">Atlanta, January Meetup &amp; Movie Night!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 January 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1575 Clairmont Road Decatur, Apartment 718, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A whole new year! Woah.</p>\n\n<p>We'll start January's meetup with an introduction for new members, discussions of New Year's resolutions and how to achieve them (contribution totally voluntary, of course). Followed up with club stuffs, and then a movie night! Vote for your movie choice on our facebook group! <a href=\"https://www.facebook.com/groups/Atlanta.Lesswrong/\" rel=\"nofollow\">https://www.facebook.com/groups/Atlanta.Lesswrong/</a>\nBring your snuggies and popcorn!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta__January_Meetup___Movie_Night_1\">Discussion article for the meetup : <a href=\"/meetups/v9\">Atlanta, January Meetup &amp; Movie Night!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta, January Meetup & Movie Night!", "anchor": "Discussion_article_for_the_meetup___Atlanta__January_Meetup___Movie_Night_", "level": 1}, {"title": "Discussion article for the meetup : Atlanta, January Meetup & Movie Night!", "anchor": "Discussion_article_for_the_meetup___Atlanta__January_Meetup___Movie_Night_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-05T20:34:48.388Z", "modifiedAt": null, "url": null, "title": "[LINK] The Mathematics of Gamification - Application of Bayes Rule to Voting", "slug": "link-the-mathematics-of-gamification-application-of-bayes", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NN3xRof5ZNZFzcvsq/link-the-mathematics-of-gamification-application-of-bayes", "pageUrlRelative": "/posts/NN3xRof5ZNZFzcvsq/link-the-mathematics-of-gamification-application-of-bayes", "linkUrl": "https://www.lesswrong.com/posts/NN3xRof5ZNZFzcvsq/link-the-mathematics-of-gamification-application-of-bayes", "postedAtFormatted": "Sunday, January 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20The%20Mathematics%20of%20Gamification%20-%20Application%20of%20Bayes%20Rule%20to%20Voting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20The%20Mathematics%20of%20Gamification%20-%20Application%20of%20Bayes%20Rule%20to%20Voting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNN3xRof5ZNZFzcvsq%2Flink-the-mathematics-of-gamification-application-of-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20The%20Mathematics%20of%20Gamification%20-%20Application%20of%20Bayes%20Rule%20to%20Voting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNN3xRof5ZNZFzcvsq%2Flink-the-mathematics-of-gamification-application-of-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNN3xRof5ZNZFzcvsq%2Flink-the-mathematics-of-gamification-application-of-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 308, "htmlBody": "<p>Fresh from slashdot: A smart application of Bayes' rule to web-voting.</p>\n<p><a href=\"http://engineering.foursquare.com/2014/01/03/the-mathematics-of-gamification/\">http://engineering.foursquare.com/2014/01/03/the-mathematics-of-gamification/</a></p>\n<blockquote>\n<p style=\"font-size: 14.399999618530273px; margin: 1em 0px; padding: 0px; color: #444444; font-family: 'Helvetica Neue', Arial, Helvetica, sans-serif; line-height: 21.600000381469727px;\">[The results] are exactly the equations for voting you would expect. But now, they&rsquo;re derived from math!</p>\n</blockquote>\n<blockquote>\n<p style=\"font-size: 14.399999618530273px; margin: 1em 0px; padding: 0px; color: #444444; font-family: 'Helvetica Neue', Arial, Helvetica, sans-serif; line-height: 21.600000381469727px;\"><strong style=\"margin: 0px; padding: 0px;\">The Benefits</strong></p>\n<ul style=\"font-size: 14.399999618530273px; margin: 1em 0px 1em 20px; padding: 0px; color: #444444; font-family: 'Helvetica Neue', Arial, Helvetica, sans-serif; line-height: 21.600000381469727px;\">\n<li style=\"margin: 0px; padding: 0px;\"><strong style=\"margin: 0px; padding: 0px;\">Efficient, data-driven guarantees about database accuracy.</strong>&nbsp;By choosing the points based on a user&rsquo;s accuracy, we can intelligently accrue certainty about a proposed update and stop the voting process as soon as the math guarantees the required certainty.</li>\n<li style=\"margin: 0px; padding: 0px;\"><strong style=\"margin: 0px; padding: 0px;\">Still using points, just smart about calculating them.</strong>&nbsp;By relating a user&rsquo;s accuracy and the certainty threshold needed to accept a proposed update to an additive point system&nbsp;<span id=\"MathJax-Element-41-Frame\" class=\"MathJax\" style=\"margin: 0px; padding: 0px; display: inline; line-height: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; border: 0px;\"><span id=\"MathJax-Span-886\" class=\"math\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline-block; position: static; border: 0px; vertical-align: 0px; width: 1.732em;\"><span style=\"font-size: 18.399999618530273px; margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline-block; position: relative; border: 0px; vertical-align: 0px; width: 1.3em; height: 0px;\"><span style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; position: absolute; border: 0px; vertical-align: 0px; clip: rect(1.354em 1000.003em 2.705em -0.484em); top: -2.267em; left: 0.003em;\"><span id=\"MathJax-Span-887\" class=\"mrow\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px;\"><a style=\"margin: 0px; padding: 0px; position: relative; outline: none; color: #2398c9; text-decoration: none; transition: none; -webkit-transition: none; border: 0px; max-width: none; max-height: none; vertical-align: 0px;\" href=\"http://engineering.foursquare.com/2014/01/03/the-mathematics-of-gamification/#mjx-eqn-eqpoints\"><span id=\"MathJax-Span-888\" class=\"mrow MathJax_ref\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px;\"><span id=\"MathJax-Span-889\" class=\"mtext\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px; font-family: MathJax_Main;\">(2)</span></span></a></span></span></span></span></span>, we can still give a user the points that they like. This also makes it easy to take a system of ad-hoc points and convert it over to a smarter system based on empirical evidence.</li>\n<li style=\"margin: 0px; padding: 0px;\"><strong style=\"margin: 0px; padding: 0px;\">Scalable and easily extensible.</strong>&nbsp;The parameters are automatically trained and can adapt to changes in the behavior of the userbase. No more long meetings debating how many points to grant to a narrow use case.<br style=\"margin: 0px; padding: 0px;\" />So far, we&rsquo;ve taken a very user-centric view of&nbsp;<span id=\"MathJax-Element-42-Frame\" class=\"MathJax\" style=\"margin: 0px; padding: 0px; display: inline; line-height: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; border: 0px;\"><span id=\"MathJax-Span-890\" class=\"math\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline-block; position: static; border: 0px; vertical-align: 0px; width: 1.354em;\"><span style=\"font-size: 18.399999618530273px; margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline-block; position: relative; border: 0px; vertical-align: 0px; width: 1.03em; height: 0px;\"><span style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; position: absolute; border: 0px; vertical-align: 0px; clip: rect(1.624em 1000.003em 2.597em -0.538em); top: -2.213em; left: 0.003em;\"><span id=\"MathJax-Span-891\" class=\"mrow\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px;\"><span id=\"MathJax-Span-892\" class=\"msubsup\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px;\"><span style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline-block; position: relative; border: 0px; vertical-align: 0px; width: 0.976em; height: 0px;\"><span style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; position: absolute; border: 0px; vertical-align: 0px; clip: rect(1.678em 1000.003em 2.651em -0.538em); top: -2.267em; left: 0.003em;\"><span id=\"MathJax-Span-893\" class=\"mi\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px; font-family: MathJax_Math; font-style: italic;\">p</span></span><span style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; position: absolute; border: 0px; vertical-align: 0px; top: -2.051em; left: 0.543em;\"><span id=\"MathJax-Span-894\" class=\"mi\" style=\"font-size: 12.800000190734863px; margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px; font-family: MathJax_Math; font-style: italic;\">k</span></span></span></span></span></span></span></span></span>&nbsp;(this is the accuracy of user&nbsp;<span id=\"MathJax-Element-43-Frame\" class=\"MathJax\" style=\"margin: 0px; padding: 0px; display: inline; line-height: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; border: 0px;\"><span id=\"MathJax-Span-895\" class=\"math\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline-block; position: static; border: 0px; vertical-align: 0px; width: 0.651em;\"><span style=\"font-size: 18.399999618530273px; margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline-block; position: relative; border: 0px; vertical-align: 0px; width: 0.489em; height: 0px;\"><span style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; position: absolute; border: 0px; vertical-align: 0px; clip: rect(1.354em 1000.003em 2.381em -0.43em); top: -2.213em; left: 0.003em;\"><span id=\"MathJax-Span-896\" class=\"mrow\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px;\"><span id=\"MathJax-Span-897\" class=\"mi\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px; font-family: MathJax_Math; font-style: italic;\">k</span></span></span></span></span></span>). But we can go well beyond that. For example,&nbsp;<span id=\"MathJax-Element-44-Frame\" class=\"MathJax\" style=\"margin: 0px; padding: 0px; display: inline; line-height: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; border: 0px;\"><span id=\"MathJax-Span-898\" class=\"math\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline-block; position: static; border: 0px; vertical-align: 0px; width: 1.354em;\"><span style=\"font-size: 18.399999618530273px; margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline-block; position: relative; border: 0px; vertical-align: 0px; width: 1.03em; height: 0px;\"><span style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; position: absolute; border: 0px; vertical-align: 0px; clip: rect(1.624em 1000.003em 2.597em -0.538em); top: -2.213em; left: 0.003em;\"><span id=\"MathJax-Span-899\" class=\"mrow\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px;\"><span id=\"MathJax-Span-900\" class=\"msubsup\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px;\"><span style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline-block; position: relative; border: 0px; vertical-align: 0px; width: 0.976em; height: 0px;\"><span style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; position: absolute; border: 0px; vertical-align: 0px; clip: rect(1.678em 1000.003em 2.651em -0.538em); top: -2.267em; left: 0.003em;\"><span id=\"MathJax-Span-901\" class=\"mi\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px; font-family: MathJax_Math; font-style: italic;\">p</span></span><span style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; position: absolute; border: 0px; vertical-align: 0px; top: -2.051em; left: 0.543em;\"><span id=\"MathJax-Span-902\" class=\"mi\" style=\"font-size: 12.800000190734863px; margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px; font-family: MathJax_Math; font-style: italic;\">k</span></span></span></span></span></span></span></span></span>&nbsp;could be &ldquo;the accuracy of user&nbsp;<span id=\"MathJax-Element-45-Frame\" class=\"MathJax\" style=\"margin: 0px; padding: 0px; display: inline; line-height: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; border: 0px;\"><span id=\"MathJax-Span-903\" class=\"math\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline-block; position: static; border: 0px; vertical-align: 0px; width: 0.651em;\"><span style=\"font-size: 18.399999618530273px; margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline-block; position: relative; border: 0px; vertical-align: 0px; width: 0.489em; height: 0px;\"><span style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; position: absolute; border: 0px; vertical-align: 0px; clip: rect(1.354em 1000.003em 2.381em -0.43em); top: -2.213em; left: 0.003em;\"><span id=\"MathJax-Span-904\" class=\"mrow\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px;\"><span id=\"MathJax-Span-905\" class=\"mi\" style=\"margin: 0px; padding: 0px; transition: none; -webkit-transition: none; display: inline; position: static; border: 0px; vertical-align: 0px; font-family: MathJax_Math; font-style: italic;\">k</span></span></span></span></span></span>&rsquo;s vote given that they have been to the venue three times before and work nearby.&rdquo; These clauses can be arbitrarily complicated and estimated from a (logistic) regression of the honeypot performance. The point is that these changes will be based on&nbsp;<em style=\"margin: 0px; padding: 0px;\">data</em>&nbsp;and not subjective judgments of how many &ldquo;points&rdquo; a user or situation should get.</li>\n</ul>\n</blockquote>\n<p>I wonder whether and how this could be applied to voting here as LW posts are not 'correct' per se.</p>\n<p>One rather theoretical possibility would be to assign prior correctness to some posts e.g. the sequences and then use that to determine the 'accuracy' of users based on that.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NN3xRof5ZNZFzcvsq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.4996962931828935e-06, "legacy": true, "legacyId": "25189", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-05T20:41:27.000Z", "modifiedAt": null, "url": null, "title": "Marijuana: Much More Than You Wanted To Know", "slug": "marijuana-much-more-than-you-wanted-to-know", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c8khnHoRTSGjmHLLf/marijuana-much-more-than-you-wanted-to-know", "pageUrlRelative": "/posts/c8khnHoRTSGjmHLLf/marijuana-much-more-than-you-wanted-to-know", "linkUrl": "https://www.lesswrong.com/posts/c8khnHoRTSGjmHLLf/marijuana-much-more-than-you-wanted-to-know", "postedAtFormatted": "Sunday, January 5th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Marijuana%3A%20Much%20More%20Than%20You%20Wanted%20To%20Know&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMarijuana%3A%20Much%20More%20Than%20You%20Wanted%20To%20Know%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc8khnHoRTSGjmHLLf%2Fmarijuana-much-more-than-you-wanted-to-know%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Marijuana%3A%20Much%20More%20Than%20You%20Wanted%20To%20Know%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc8khnHoRTSGjmHLLf%2Fmarijuana-much-more-than-you-wanted-to-know", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc8khnHoRTSGjmHLLf%2Fmarijuana-much-more-than-you-wanted-to-know", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5662, "htmlBody": "<p>This month I work on my hospital&#8217;s Substance Abuse Team, which means we treat people who have been hospitalized for alcohol or drug-related problems and then gingerly suggest that maybe they should use drugs a little less.</p>\n<p>The two doctors leading the team are both very experienced and have kind of seen it all, so it&#8217;s interesting to get a perspective on drug issues from people on the front line. In particular, one of my attendings is an Obama-loving long-haired hippie who nevertheless vehemently opposes medical marijuana or any relaxation on marijuana&#8217;s status at all. He says that &#8220;just because I&#8217;m a Democrat doesn&#8217;t mean I have to support stupid policies I know are wrong&#8221; and he&#8217;s able to back up his opinion with an impressive variety of studies.</p>\n<p>To be honest, I had kind of forgotten that the Universe was allowed to contain negative consequences for legalizing drugs. What with all the mental energy it took protesting the the Drug War and getting outraged at police brutality and celebrating Colorado&#8217;s recently permitting recreational cannabis use and so on, it had completely slipped my mind that the legalization of marijuana might have negative consequences and that I couldn&#8217;t reject it out of hand until I had done some research.</p>\n<p>So I&#8217;ve been doing the research. Not to try to convince my attending of anything &#8211; as the old saying goes, do not meddle in the affairs of attendings, <A HREF=\"http://www.youtube.com/watch?v=TyT8uc6ath4\">because you are crunchy and taste good with ketchup</A> &#8211; but just to figure out where exactly things stand.</p>\n<p><b>I. Would Relaxation Of Penalties On Marijuana Increase Marijuana Use?</b></p>\n<p>Starting in the 1970s, several states decriminalized possession of marijuana &#8211; that is, possession could not be penalized by jail time. It could still be penalized by fines and other smaller penalties, and manufacture and sale could still be punished by jail time.</p>\n<p>Starting in the 1990s, several states legalized medical marijuana. People with medical marijuana cards, which in many cases were laughably easy to get with or without good evidence of disease, were allowed to grow and use marijuana, despite concerns that some of this would end up on the illegal market.</p>\n<p>Starting last week, Colorado legalized recreational use of marijuana, as well as cultivation and sale (subject to heavy regulations). Washington will follow later this year, and other states will be placing measures on their ballots to do the same.</p>\n<p>One should be able to evaluate to what degree marijuana use rose after these policy changes, and indeed, many people have tried &#8211; with greater or lesser levels of statistical sophistication.</p>\n<p>The <i>worst</i> arguments in favor of this proposition are those like <A HREF=\"http://webcache.googleusercontent.com/search?q=cache:WQVP0JAUAvYJ:www.cadca.org/files/policy_priorities/EffectsMedicalMarijuanaLegal.doc+&#038;cd=26&#038;hl=en&#038;ct=clnk&#038;gl=us&#038;client=firefox-a\">this CADCA paper</A>, which note that states with more liberal marijuana laws have higher rates of marijuana use among teenagers than states that do not. The proper counterspell to such nonsense is <i>Reverse Causal Arrows</i> &#8211; could it not be that states with more marijuana users are more likely to pass proposals liberalizing marijuana laws? Yes it could. Even more likely, some third variable &#8211; let&#8217;s call it &#8220;hippie attitudes&#8221; &#8211; could be behind both high rates of marijuana use and support for liberal marijuana regimes. The states involved are places like Colorado, California, Washington, and Oregon. I think that speaks for itself. In case it doesn&#8217;t, someone went through the statistics and found that these states had the highest rates of marijuana use among teens since <i>well</i> before they relaxed drug-related punishments. Argument successfully debunked.</p>\n<p>A slightly more sophisticated version &#8211; used by the DEA <A HREF=\"http://archive.is/Sa5Qm\">here</A> &#8211;  takes the teenage marijuana use in a state one year before legalization of medical marijuana and compares it to the teenage marijuana use in a state one (or several years) after such legalization. They often find that it has increased, and blame the increase on the new laws. <A HREF=\"http://www.michelepolak.com/200spring11/Weekly_Schedule_files/Single.pdf\">For example</A>, 28% of Californians used marijuana before it was decriminalized in the 70s, compared to 35% a few years after.  This falls victim to a different confounder &#8211; marijuana use has undergone some very large swings nationwide, so the rate of increase in medical marijuana states may be the same as the rate anywhere else. Indeed, this is what was going on in California &#8211; its marijuana use actually rose slightly <i>less</i> than the national average.</p>\n<p>What we want is a study that compares the average marijuana use in a set of states before liberalization to the average marijuana use in the country as a whole, and then does the same after liberalization to see if the ratio has increased. There are several studies that purport to try this, of which by far the best is <A HREF=\"http://www.monitoringthefuture.org/pubs/occpapers/occ13.pdf\">Johnston, O&#8217;Malley &#038; Bachman 1981</A>, which monitored the effect of the decriminalization campaigns of the 70s. They survey thousand of high school seniors on marijuana use in seven states that decriminalize marijuana both before and for five years after the decriminalization, and find absolutely no sign of increased marijuana use (in fact, there is a negative trend). Several other studies (eg <A HREF=\"http://www.sciencedirect.com/science/article/pii/036233199390016O\">Thies &#038; Register 1993</A>) confirm this finding.</p>\n<p>There is only a hint of some different results. <A HREF=\"http://tigger.uic.edu/~fjc/Presentations/Scans/Final%20PDFs/ei1999.pdf\">Saffer and Chaloukpa 1999</A> and <A HREF=\"http://www.nber.org/chapters/c11158.pdf\">Chaloupka, Grossman &#038; Tauras 1999</A> try to use complicated econometric simulations to estimate the way marijuana demand will respond to different variables. They simulate (as opposed to detecting in real evidence) that marijuana decriminalization should raise past-year use by about 5 &#8211; 8%, but have no effect on more frequent use (ie a few more people try it but do not become regular users). More impressively, <A HREF=\"http://www.drugpolicy.org/docUploads/model.pdf\">Model 1993</A> (a source of <A HREF=\"https://twitter.com/slatestarcodex/status/417494354710654978\">some exasperation</A> for me earlier) finds that after decriminalization, marijuana-related emergency room visits went up (trying to interpret their tables, I think they went up by a whopping 90%, but I&#8217;m not sure of this). This is sufficiently different from every other study that I don&#8217;t give it much weight, although we&#8217;ll return to it later.</p>\n<p>Overall I think the evidence is pretty strong that decriminalization probably led to no increase in marijuana use among teens, and may at most have led to a small single-digit increase. </p>\n<p>Proponents of stricter marijuana penalties say the experiment isn&#8217;t fair. In practice, decriminalization does not affect the average user very much &#8211; even in states without decriminalization, marijuana possession very rarely leads to jail time. The only hard number I have is from Australia, where in &#8220;non-decriminalized&#8221; Australian states <A HREF=\"http://192.5.14.43/content/dam/rand/pubs/working_papers/2010/RAND_WR771.pdf\">only 0.3% of marijuana arrests lead to jail time</A>, but a quick back-of-the-envelope calculation suggests US numbers are very similar. And even in supposedly decriminalized states, it&#8217;s not hard for a cop who wants to get a pot user in jail to find a way (possession of even small amounts can be &#8220;possession with intent to sell&#8221; if someone doesn&#8217;t like you). So the overall real difference between decriminalized and not decriminalized is small and it&#8217;s not surprising the results are small as well. I mostly agree with them; decriminalization is fine as far as it goes, but it&#8217;s a bigger psychological step than an actual one.</p>\n<p>The next major milestone in cannabis history was the legalization of medical marijuana. <A HREF=\"http://depts.washington.edu/phenom/docs/Anderson_Hansen_Rees_2012.pdf\">Anderson, Hansen &#038; Rees (2012)</A> did the same kind of study we have seen above, and despite trying multiple different measures of youth marijuana use found pretty much no evidence that medical marijuana legalization caused it to increase. <A HREF=\"http://medicalmarijuana.procon.org/sourcefiles/2005TeenUseReport.pdf\">Other studies</A> find pretty much the same.</p>\n<p>This could potentially suffer from the same problems as decriminalization studies &#8211; the laws don&#8217;t always change the facts on the ground. Indeed, for about ten years after medical marijuana legalization, the federal government kept on prosecuting marijuana users even when their use accorded with state laws, and many states had so few dispensaries that in reality not a whole lot of medical marijuana was being given out. I haven&#8217;t found any great studies that purport to overcome these problems.</p>\n<p>When we examined decriminalization, we found that the studies based on surveys of teens looked pretty good, but that the one study that examined outcomes &#8211; marijuana-related ER visits &#8211; was a lot less encouraging. We find the same pattern here, and the rain on our parade is <A HREF=\"https://www.msu.edu/~chuyuwei/mjdraft.pdf\">Chu 2013</A>, who finds that medical marijuana laws increased marijuana-related arrests by 15-20% and marijuana-related drug rehab admissions by 10-15%.</p>\n<p>So what&#8217;s going on here? I have two theories. First, maybe medical marijuana use (and decriminalization) increase use among adults only. This could be because the system is working &#8211; giving adults access to medical marijuana while keeping it out of the hands of children &#8211; or because kids are dumb and don&#8217;t understand consequences but adults are more responsive to incentives and punishments. Second, we know that medical marijuana has <A HREF=\"https://www.procon.org/files/current_psychiatry_psychosis.pdf\">twice as much THC</A> as street marijuana. Maybe everyone keeps using the same amount of marijuana, but when medical marijuana inevitably gets diverted to the street, addicts can&#8217;t handle it and end up behaving much worse than they expected.</p>\n<p>Or the studies are wrong. Studies being wrong is always a pretty good bet.</p>\n<p>I can&#8217;t close this section without mentioning the Colorado expulsion controversy. Nearly everyone who teaches in Colorado says <A HREF=\"http://www.denverpost.com/breakingnews/ci_24501596/pot-problems-colorado-schools-increase-legalization\">there has been an explosion of marijuana-related problems</A> since medical marijuana was legalized. Meanwhile, the actual surveys of Colorado high school students say that <A HREF=\"http://www.huffingtonpost.com/2012/09/07/marijuana-usage-down-in-t_n_1865095.html\">marijuana use, if anything, is going down</A>. A Colorado drug warrior has some <A HREF=\"http://drthurstone.com/jumping-to-conclusions-with-cdc-data/\">strong objections</A> to the survey results, but they center around not really being able to prove that there is a real downward trend (which is an entirely correct complaint) without denying that in fact they show no evidence at all of going <i>up</i>.</p>\n<p>The consensus on medical marijuana seems to be that it does not increase teen marijuana use either, although there is some murky and suggestive evidence that it might increase illicit or dangerous marijuana use among adults.</p>\n<p>There is less information on the effects of full legalization of marijuana, which has never been tried before in the United States. To make even wild guesses we will have to look at a few foreign countries plus some econometric simulations.</p>\n<p>No one will be surprised to hear that the first foreign country involved is the Netherlands, which was famously permissive of cannabis up until a crackdown a few years ago. Despite popular belief they never fully legalized the drug and they were still pretty harsh on production and manufacture; distribution, on the other hand, could occur semi-openly in coffee shops. This is another case where we have to be careful to distinguish legal regimes from actual effects, but during the period when there were actually a lot of pot-serving coffee shops, the Netherlands did experience <A HREF=\"**http://www.stopthewarondrugs.org/wp-content/uploads/2012/06/MacCoun-Robert-J.-2010-Estimating-the-Non-Price-Effects-of-Legalization-on-Cannabis-Consumption.pdf\">an otherwise-inexplicable 35% rise in marijuana consumption</A> relative to the rest of Europe. This is true even among teenagers, and covers both heavy use as well as occasional experimentation. Some scientists studying the Netherlands&#8217; example expect Colorado to see a similar rise; others think it will be even larger because the legalization is complete rather than partial.</p>\n<p>The second foreign country involved is Portugal, which was maybe more of a decriminalization than a legalization case but which is forever linked with the idea of lax drug regimes in the minds of most Americans. They decriminalized all drugs (including heroin and cocaine) in 2001, choosing to replace punishment with increased treatment opportunities, and <A HREF=\"http://www.cato.org/publications/white-paper/drug-decriminalization-portugal-lessons-creating-fair-successful-drug-policies\">as we all have been told</A>, no one in Portugal ever used drugs ever again, or even remembers that drugs exist. Except it turns out it&#8217;s more complicated; for example, the percent of Portuguese who admit to lifetime use of drugs <A HREF=\"http://www.wfad.se/latest-news/1-articles/123-decriminalization-of-drugs-in-portugal--the-real-facts\">has doubled</A> since the law took effect. Two very patient scientists <A HREF=\"http://www.undrugcontrol.info/images/stories/documents/A_resounding_success_or_a_disastrous_failure.pdf\">have sifted through all the conflicting claims</A> and found that in reality, the number of people who briefly experiment with drugs has gone way up, but the number of addicts hasn&#8217;t, nor has the number of bad outcomes like overdose-related deaths. There are many more people receiving drug treatment, but that might just be because Portugal upped its drug treatment game in a separate law at the same time they decriminalized drugs. Overall they seem to have been a modest success &#8211; neither really raising nor decreasing the number of addicts &#8211; but they seem more related to decriminalization (which we&#8217;ve already determined doesn&#8217;t have much effect) than to legalization per se.</p>\n<p>Returning to America, what if you just <i>ask</i> people whether they would use more marijuana if it&#8217;s legal? Coloradans were asked if they plan to smoke marijuana once it becomes legal; comparing survey results to current usage numbers suggests <A HREF=\"http://learnaboutsam.com/legalization-of-marijuana-could-increase-marijuana-use-almost-40-versus-current-rates-among-those-aged-18-and-older-and-in-the-18-25-year-old-age-group-in-colorado/\">40% more users</A> above the age of 18; it is unclear what the effect will be on younger teens and children.</p>\n<p>Finally, we let the economists have their say. They crunch all the data and predict <A HREF=\"http://www.rand.org/content/dam/rand/pubs/testimonies/2010/RAND_CT351.pdf\">an increase of 50 &#8211; 100%</A> based solely on the likely price drop (even with taxes factored in). And if there&#8217;s one group we can trust to make infallible predictions about the future, it&#8217;s economists.</p>\n<p>Overall I find the Dutch evidence most convincing, and predict a 25 &#8211; 50% increase in adult marijuana use with legalization. I would expect a lower increase &#8211; 15 &#8211; 30% &#8211; among youth, but the data are also perfectly consistent with no increase at all.</p>\n<p>Conclusion for this section: that decriminalization and legalization of medical marijuana do not increase youth marijuana use rates, although there is some shaky and indirect evidence they do increase adult use and bad behavior. There is no good data yet on full legalization, but there&#8217;s good reason to think it would substantially increase adult use and it might also increase youth use somewhat.</p>\n<p><b>II. Is Marijuana Bad For You?</b></p>\n<p><A HREF=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3371269/\">About 9% of marijuana users</A> eventually become addicted to the drug, exposing them to various potential side effects.</p>\n<p>Marijuana smoke contains a lot of the same chemicals in tobacco smoke and so it would not be at all surprising if it had some of the same ill effects, like cardiovascular disease and lung cancer. But when people look for these effects, <A HREF=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380837/\">they can&#8217;t find any increase in mortality among marijuana smokers</A>. I predict that larger studies will one day pick something up, but for now let&#8217;s take this at face value.</p>\n<p>Much more concerning are the attempts to link marijuana to cognitive and psychiatric side effects. <A HREF=\"http://www.pnas.org/content/109/40/E2657.full\">Meier et al (2012)</A> analyzed a study of a thousand people in New Zealand and found that heavy marijuana use was linked to an IQ decline of 8 points. <A HREF=\"http://www.pnas.org/content/110/11/4251.full\">Rogeberg 2012</A> developed an alternative explanation &#8211; poor people saw their IQs drop in their 20s more than rich people because their IQs had been artificially inflated by schooling; what Meier et al had thought to be an effect of cannabis was really an effect of poor people having an apparent IQ drop and using cannabis more often. Meier et al <A HREF=\"http://www.pnas.org/content/110/11/E980.full\">pointed out</A> that actually, poor people didn&#8217;t use cannabis any more often than anyone else and effects remained when controlled for class. Other studies, like <A HREF=\"http://www.cmaj.ca/content/166/7/887.full\">Fried et al (2002)</A> find the same effect, and there is a plausible biological mechanism (cannabinoids something something neurotransmitters something brain maturation). As far as I can tell the finding still seems legit, and marijuana use does decrease IQ. It is still unclear whether this only applies in teenagers (who are undergoing a &#8220;sensitive period of brain development&#8221;) or full stop. </p>\n<p>More serious still is the link with psychosis. A number of studies have found that marijuana use is heavily correlated with development of schizophrenia and related psychotic disorders later in life. Some of them find relative risks as high as 2 &#8211; heavy marijuana use doubles your chance of getting schizophrenia, which is already a moderately high 1%. But of course correlation is not causation, and many people have come up with alternative theories. For example, maybe people who are already kind of psychotic use marijuana to self-medicate, or just make poor life choices like starting drugs. Maybe people of low socioeconomic status who come from broken homes are more likely to both use marijuana and get schizophrenia. Maybe some gene both makes marijuana really pleasant and increases schizophrenia risk.</p>\n<p>I know of three good studies attempting to tease out causation. <A HREF=\"http://bjp.rcpsych.org/content/184/2/110.full.pdf\">Arseneault et al (2004)</A> checks to see which came first &#8211; the marijuana use or the psychotic symptoms &#8211; and finds it was the marijuana use, thus supporting an increase in risk from the drug. <A HREF=\"http://onlinelibrary.wiley.com/doi/10.1111/add.12050/abstract\">Griffith-Lendering et al (2012)</A> try the same, and find <i>bidirectional</i> causation &#8211; previous marijuana use seems to predict future psychosis, but previous psychosis seems to predict future marijuana use. A <A HREF=\"**http://psychcentral.com/news/2013/12/10/harvard-marijuana-doesnt-cause-schizophrenia/63148.html\">very new study from last month</A> boxes clever and checks whether your marijuana use can predict schizophrenia in <i>your relatives</i>, and find that it does &#8211; presumably suggesting that genetic tendencies towards schizophrenia cause marijuana use and not vice versa (although Ozy points out to meet that the relatives of marijuana users are more likely to use marijuana themselves; the plot thickens). When <A HREF=\"http://www.clinique-transculturelle.org/pdf/lancet_2007.pdf\">a meta-analysis</A> tries to control for all of these factors, they get a relative risk of 1.4 (they call it an odds ratio, but from their discussion section I think they mean relative risk).</p>\n<p>Is this true, or just the confounders they failed to pick up? One argument for the latter is that marijuana use has increased very much over the past 50 years. If marijuana use caused schizophrenia, we would expect to see much more schizophrenia, but in fact as far as anyone can tell (which is not very far) <A HREF=\"http://www.mentalhealth.com/mag1/scz/sb-time.html\">schizophrenia incidence is decreasing</A>. The decrease might be due (maybe! if it even exists at all!) to obstetric advances which prevent fetal brain damage which could later lead to the disease. The effect of this variable is insufficiently known to pretend we can tease out some supposed contrary effect of increased marijuana use. Also, some people say that <A HREF=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0447.2012.01913.x/abstract\">schizophrenia is increasing in young people</A>, so who knows?</p>\n<p>The <i>exact</i> nature of the marijuana-psychosis link is still very controversial. Some people say that marijuana causes psychosis. Other people say it &#8220;activates latent psychosis&#8221;, a term without a very good meaning but which might mean that it pushes people on the borderline of psychosis &#8211; eg those with a strong family history but who might otherwise have escaped &#8211; over the edge. Still others say all it does is get people who would have developed psychosis eventually to develop it a few years earlier. You can read a comparison of all the different hypotheses <A HREF=\"https://www.procon.org/files/current_psychiatry_psychosis.pdf\">here</A>.</p>\n<p>I&#8217;ve saved the most annoying for last: is marijuana a &#8220;gateway drug&#8221;? Would legalizing it make it more or less of a &#8220;gateway drug&#8221;? This claim seems tailor-made to torture statisticians. We know that marijuana users are <i>definitely</i> more likely to use other drugs later &#8211; for example, <A HREF=\"http://www.columbia.edu/cu/record/archives/vol20/vol20_iss10/record2010.24.html\">marijuana users are 85x more likely than non-marijuana users to use cocaine</A>. but that could be either because marijuana affects them in some way (implying that legalizing marijuana would increase other drug use), because <A HREF=\"http://healthland.time.com/2010/10/29/marijuna-as-a-gateway-drug-the-myth-that-will-not-die/\">they have factors</A> like genetics or stressful life situation that makes them more likely to use all drugs (implying that legalizing marijuana would not affect other drug use), or because using illegal marijuana without ill effect connects them to the illegal drug market and convinces them illegal drugs are okay (implying that legalizing marijuana would decrease other drug use). RAND comes very close to investigating this properly by saying that <A HREF=\"http://www.rand.org/content/dam/rand/pubs/working_papers/2010/RAND_WR768.pdf\">when the Dutch pseudo-legalized marijuana, use of harder drugs stayed stable or went down</A>, but all their study actually shows is that the ratio of marijuana users : hard drug users went down. This is to be expected when you make marijuana much easier to get, but it&#8217;s still consistent with the absolute number of hard drug users going way up. The best that can be said is that there is no direct causal evidence for the gateway theory and <A HREF=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3600369/\">some good alternative explanations</A> for the effect. Let us accept their word for it and never speak of this matter again.</p>\n<p>Conclusion for this section: Marijuana does not have a detectable effect on mortality and there is surprisingly scarce evidence of tobacco-like side effects. It probably does decrease IQ if used early and often, possibly by as many as 8 IQ points. It may increase risk of psychosis by as much as 40%, but it&#8217;s not clear who is at risk or whether the risk is even real. The gateway drug hypothesis is too complicated to evaluate effectively but there is no clear casual evidence in its support.</p>\n<p><b>III. What Are The Costs Of The Drug War?</b></p>\n<p>There are not really that many people in jail for using marijuana.</p>\n<p>I learned this from <A HREF=\"https://www.ncjrs.gov/ondcppubs/publications/pdf/whos_in_prison_for_marij.pdf\">Who&#8217;s Really In Prison For Marijuana?</A>, a publication of the National Office Of Drug Control Policy, which was clearly written by someone with the same ability to take personal offense at bad statistics that inspires <A HREF=\"http://slatestarcodex.com/2013/04/04/lies-damned-lies-and-facebook-part-1-of-%E2%88%9E/\">my</A> <A HREF=\"http://slatestarcodex.com/2013/04/04/lies-damned-lies-and-facebook-part-2-of-%E2%88%9E/\">posts</A> <A HREF=\"http://slatestarcodex.com/2013/06/11/lies-damned-lies-and-facebook-part-3-of-%E2%88%9E/\">about</A> <A HREF=\"http://slatestarcodex.com/2013/11/08/lies-damned-lies-and-facebook-part-4-of-%E2%88%9E/\">Facebook</A>. The whole thing seethes with indignation and makes me want to hug the drug czar and tell him everything will be okay.</p>\n<p>Only 1.6% of state prisoners are serving time for marijuana, only 0.7% are serving for marijuana possession, and only 0.3% are first time offenders. Some of those are &#8220;possession&#8221; in the sense of &#8220;possessing a warehouse full of marijuana bales&#8221;, and others are people who committed much more dangerous crimes but were nailed for marijuana, in the same sense that Al Capone was nailed for tax evasion. The percent of normal law-abiding people who just had a gram or two of marijuana and were thrown in jail is a rounding error, and the stories of such you read in the news are extremely dishonest (read the document for examples).</p>\n<p>Federal numbers are even lower; in the entire federal prison system, they could only find 63 people imprisoned with marijuana possession as the sole crime, and those people were possessing a median of one hundred fifteen <i>pounds</i> of marijuana (enough to make over 100,000 joints).</p>\n<p>In total, federal + state prison and counting all the kingpins, dealers, manufacturers, et cetera, there are probably about 16,000 people in prison solely for marijuana-related offenses, serving average actual sentence lengths of three year. But it&#8217;s anybody&#8217;s guess whether those people would be free today if marijuana were legal, or whether their drug cartels would just switch to something else. </p>\n<p>Looking at the other side&#8217;s statistics, I don&#8217;t see much difference. <A HREF=\"http://norml.org/news/2006/10/12/nearly-one-in-eight-us-drug-prisoners-are-behind-bars-for-pot-taxpayers-spending-over-1-billion-annually-to-incarcerate-pot-offenders\">NORML claims that</A> there are 40,000 people in prison for marijuana use, but they admit that half of those people were arrested for using harder drugs and marijuana was a tack-on charge, so they seem to agree with the Feds about around 20,000 pure marijuana prisoners. <A HREF=\"http://learnaboutsam.com/the-issues/marijuana-and-whos-in-prison/\">SAM agrees</A> that only 0.5% of the prison population is in there for marijuana possession alone. I see no reason to doubt any of these numbers.</p>\n<p>A much more serious problem is marijuana-related arrests, of which there are 700,000 a year. <A HREF=\"https://www.aclu.org/drug-law-reform/marijuana-arrests-punishments\">90% of them are for simple possession</A>, and the vast majority do not end in prison terms; they do however result in criminal records, community service, a couple days of jail time until a judge is available to hear the case, heavy fines, high cost of legal representation, and moderate costs to the state for funding the whole thing. Fines can be up to $1500, and legal representation <A HREF=\"http://www.aclu-wa.org/library_files/BeckettandHerbert.pdf\">can cost up to $5000</A> (though I am suspicious of this paper and think it may be exaggerating for effect). These costs are often borne by poor people who will have to give up all their savings for years to pay them back.</p>\n<p>Costs paid by the government, which cover everything from police officers to trials to prison time, are estimated at about $2 billion by <A HREF=\"http://www.huffingtonpost.com/2012/10/29/one-marijuana-arrest-occu_n_2041236.html\">multiple</A> <A HREF=\"http://www.aclu-wa.org/library_files/BeckettandHerbert.pdf\">sources</A>. This is only 3% of the total law enforcement budget, so legalizing marijuana wouldn&#8217;t create some kind of sudden revolution in policing, but as the saying goes, a billion here, a billion there, and eventually it adds up to real money. And a Harvard economist claims that the total monetary benefits from legalization, including potential tax revenues, <A HREF=\"www.huffingtonpost.com/2012/04/17/economists-marijuana-legalization_n_1431840.html\">could reach $14 billion</A>.</p>\n<p>Some people worry that legalizing marijuana would cause an increase in car accidents by &#8220;stoned drivers&#8221;, who, like drunk drivers, have impaired reflexes and poor judgment, and indeed there is <A HREF=\"http://www.drugabuse.gov/publications/drugfacts/drugged-driving\">a small but real problem of marijuana-induced car accidents</A>. But <A HREF=\"http://www.nber.org/papers/w4662\">Chaloukpa and Laixuthai (1994)</A> crunch the numbers and find that decreased price/increased availability of marijuana is actually associated with <i>decreased</i> car accidents, probably because marijuana is substituting for alcohol in the &#8220;have impairing substances and then go driving&#8221; population. This finding &#8211; that marijuana and alcohol substitute for each other &#8211; <A HREF=\"http://www.nytimes.com/2013/11/04/opinion/marijuana-and-alcohol.html?_r=0\">has been spotted again and again</A>. <A HREF=\"http://dmarkanderson.com/Point_Counterpoint_07_31_13_v5.pdf\">Anderson &#038; Rees (2013)</A> find that states that legalize medical marijuana see a 5% drop in beer sales. There are however a few dissenting opinions: <A HREF=\"http://cms.sem.tsinghua.edu.cn/semcms/res_base/semcms_com_www/upload/home/store/2008/10/15/3229.pdf\">Cameron &#038; Williams (2001)</A>, in complex econometric simulations that may or may not resemble the real world in any respect, find that increasing the price of alcohol increases marijuana use, but increasing the price of marijuana does not affect alcohol use, and <A HREF=\"http://www.impacteen.org/generalarea_PDFs/WEA062001_presentation.pdf\">the same researcher</A> finds that banning alcohol on a college campus also decreases marijuana use. Also, possibly marijuana use increases smoking? This whole area is confusing, but I am most sympathetic to to the Andersen and Rees statistics which say that medical marijuana states are associated with 13% fewer traffic fatalities.</p>\n<p>Overall conclusion for this section: full legalization of marijuana would free about 20,000 people from jail (although most of them would not be exactly fine upstanding citizens), prevent 700,000 arrests not resulting in jail time per year, save between 2 and 14 billion dollars, and possibly reduce traffic fatalities a few percent (or, for all we know, increase them).</p>\n<p><b>IV. An Irresponsible Utilitarian Analysis</b></p>\n<p>Decriminalization and legalization of medical marijuana seem, if we are to trust the statistics in (I) saying they do not increase use among youth, like almost unalloyed good things. Although there are some nagging hints of doubt, they are not especially quantifiable and therefore not amenable to analysis. Without a very strong predisposition to try as hard as possible to fit the evidence into a pessimistic picture, I don&#8217;t think there&#8217;s a great argument against either of these two propositions. Let&#8217;s concentrate on legalization, which would mean something like &#8220;People can grow and sell as much marijuana as they want and it&#8217;s totally legal for people over 21, with the same level of penalties as today for people under 21&#8221;.</p>\n<p>Section (I) concludes that legalization could lead to an increase in adult marijuana use up to 50%. There&#8217;s not a lot of evidence on what it could do to teen marijuana use, but since it seems teen marijuana use is less responsive to legal changes, I made up a number and said 20%. Lest you think I am being unfair, note that this is well below the percent increase predicted by the survey that asked 18 year olds if they would start using marijuana if it were legal.</p>\n<p>Right now about 1.5 million teenagers <A HREF=\"https://www.drugfree.org/newsroom/pats-2011\">use marijuana &#8220;heavily&#8221;</A>. Most of the detrimental effects of marijuana seem concentrated in teens and people in their early twenties; I&#8217;m going to artificially round that up to 2 million to catch the early 20 year olds. If this 2 million number increased 20%, 400,000 extra teens would start heavily using marijuana.</p>\n<p>Those 400,000 teens would lose 8 IQ points each. IQ increases your yearly earnings by about $500 per point, so these people would lose about $4,000 a year. Making very strong assumptions about salary being a measure of value to society, society would lose about $1.6 billion a year directly, plus various intangibles from potential artists and scientists losing the ability to create masterpieces and inventions, plus various <i>really</i> intangibles like a slightly dumber electorate.</p>\n<p>We need to use a different number to calculate psychosis risk, since the studies were done on &#8220;people who had used marijuana at least once&#8221;. The appropriate number turns out to be 8 million teenagers; of those, 1%, or 80,000, would naturally develop schizophrenia. If the 1.4 relative risk number is correct, marijuana use will increase that to 112,000, for a total increase of 32,000 people. Schizophrenia pretty much always presents in the 15 &#8211; 25 age window, so we&#8217;ll say we get 3,200 extra cases per year.</p>\n<p><A HREF=\"http://en.wikipedia.org/wiki/List_of_motor_vehicle_deaths_in_U.S._by_year\">There were</A> 35000 road traffic accident fatalities in the US last year. If greater availability of marijuana decreases those fatalities by 13% (note that I am using the number from medical marijuana legalization and not for marijuana legalization per se, solely because it is a number I actually have), that will cause 4500 fewer road traffic deaths per year. There may be additional positive effects of alcohol substitution from, for example, less liver disease. But there may also be additional negative effects from increasing use of tobacco, so let&#8217;s just pretend those cancel out.</p>\n<p>So here is my guess at the yearly results of marijuana legalization:</p>\n<p>&#8211; 20,000 fewer prisoners (but they might switch to other criminal enterprises)<br />\n&#8211; 700,000 fewer arrests<br />\n&#8211; $2 billion less in law enforcement costs<br />\n&#8211; Some amount of positive gain (let&#8217;s say $5 billion) in taxes<br />\n&#8211; 4500 fewer road traffic deaths (if you believe the preliminary alcohol substitution numbers)</p>\n<p>&#8211; 400,000 people with lower IQ<br />\n&#8211; $2 billion in social costs from above dumber people<br />\n&#8211; 3,200 more cases of schizophrenia a year</p>\n<p>We&#8217;ll proceed to calculate the nonmonetary burden of each of these in QALYs, then add the monetary burden in dollars, then convert.</p>\n<p>The <A HREF=\"https://research.tufts-nemc.org/cear4/SearchingtheCEARegistry/SearchtheCEARegistry.aspx\">searchable public database of utility weights for all diseases</A> (God I love the 21st century) tells me that schizophrenia has a QALY weight of 0.73. It generally starts around 20 and lasts a lifetime, so each case of schizophrenia costs us 0.27 * 50 or 13.5 QALYs. Therefore, the total burden of the 3,200 added schizophrenia cases is 43 kiloQALYs.</p>\n<p>There&#8217;s no good way to calculate the QALY weight of having 4-8 fewer IQ points, and unfortunately this is going to end up being among the most important numbers in our results. If we say the lifetime cost of this problem is 3 QALYs, and divide the number by eight to represent eight years worth of teenagers in our sample population, we end up with 400,000/8 * 3 = 150 kiloQALYs.</p>\n<p><A HREF=\"http://slatestarcodex.com/2013/04/30/utility-weight-results/\">My own survey</A> tells me that being in prison has a QALY weight around 0.5. Marijuana sentences generally last an average of three years, which suggests that 1/3 of these marijuana prisoners are arrested every year, so the total burden of the ~6000ish marijuana imprisonments each year is 3 * ~6000 * 0.5 = 10 kiloQALYs.</p>\n<p>Assume the average road traffic death occurs at age 30, costing 40 years of potential future life. The total cost of 4500 road traffic deaths is 40 * 4500 = 180 kiloQALYs.</p>\n<p>The arrests are going to require even more fudging than normal. Average jail time for a marijuana arrest (when awaiting trial) is &#8220;one to five days&#8221; &#8211; let&#8217;s round that off to two and then use our prison number to say that the jail from each arrest is 2/365 * 0.5 = three-thousandths of a QALY. I am going to arbitrarily round this up to one one-hundredth of a QALY to account for emotional trauma and the burden of fines, then even more arbitrarily round this up to a tenth of a QALY to account for possibility of getting a criminal record. This sets the burden of 700,000 arrests at 70 kiloQALYs.</p>\n<p>Now our accounting is:</p>\n<p>Costs from legalization compared to current system: 200 kQALYs and $2 billion<br />\nBenefits from legalization compared to current system: 260 kQALYs and $7 billion</p>\n<p>Although it&#8217;s not going to be necessary, we can interconvert QALYs and dollars at the going health-care rate of about $100,000/QALY ($100 million/kQALY):</p>\n<p>Costs from legalization compared to current system: 220 kQALYs<br />\nBenefits from legalization compared to current system: 330 kQALYs</p>\n<p>And get:</p>\n<p><i>Net benefits from legalization: +110 kQALYs</i></p>\n<p>Except that this is extremely speculative and irresponsible. By far the largest component of the benefits of legalization turned out to be the effect on road traffic accidents, which is based on only two studies and which may on further research turn out to be a cost. And by far the largest component of the costs of legalization turned out to be the effect on IQ, and we had to totally-wild-guess the QALY cost of an IQ point loss. The wiggle room in my ignorance and assumptions is more than large enough to cover the small gap between the two policies in the results.</p>\n<p>So my actual conclusion is:</p>\n<p><i>There is not a sufficiently obvious order-of-magnitude difference between the costs and benefits of marijuana legalization for a evidence-based utilitarian analysis of costs and benefits to inform the debate. You may return to your regularly scheduled wild speculation and shrill accusations.</i></p>\n<p>But I wouldn&#8217;t say this exercise is useless. For example, it suggests that whether marijuana legalization is positive or negative on net depends almost entirely on small changes in the road traffic accident rate. This is something I&#8217;ve never heard anyone else mention, but which in retrospect should be obvious; the few debatable health effects and the couple of people given short jail sentences absolutely can&#8217;t compare to the potential for thousands more (or fewer) traffic accidents which leave people permanently dead.</p>\n<p>So my actual actual conclusion is:</p>\n<p><i>We should probably stop caring about health effects of marijuana and about imprisonment for marijuana-related offenses, and concentrate all of our research and political energy on how marijuana affects driving.</i></p>\n<p>This cements <A HREF=\"http://slatestarcodex.com/2013/05/02/if-its-worth-doing-its-worth-doing-with-made-up-statistics/\">my previous intuitions on irresponsible use of statistics</A> &#8211; it&#8217;s unlikely to unilaterally solve the problem, but it can be very good at pointing out where you&#8217;re being irrational and suggesting new ways of looking at a question.</p>\n<p><B>EDIT</B>: People in the comments have pointed out several important factors left out, including:<br />\n&#8211; Some people enjoy smoking marijuana<br />\n&#8211; The opening of a permanent criminal record may mean arrests are worse than I estimate. I can&#8217;t find good statistics on how often this happens, but do note that decriminalization prevents a record from being opened.<br />\n&#8211; Loss of 8 IQ points may have wider social effects than I estimate, since IQ affects for example crime rate.<br />\n&#8211; Legalizing marijuana might remove a source of funding for organized crime</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TLrqSmzoGoA3v5tNP": 2, "3uE2pXvbcnS9nnZRE": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c8khnHoRTSGjmHLLf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 20, "extendedScore": null, "score": 5.1e-05, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "B384FrQNrxSq4hZoS", "canonicalCollectionSlug": "codex", "canonicalBookId": "YhQ39PPHNrRCgYXcs", "canonicalNextPostSlug": "wheat-much-more-than-you-wanted-to-know", "canonicalPrevPostSlug": "the-study-of-anglophysics", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This month I work on my hospital\u2019s Substance Abuse Team, which means we treat people who have been hospitalized for alcohol or drug-related problems and then gingerly suggest that maybe they should use drugs a little less.</p>\n<p>The two doctors leading the team are both very experienced and have kind of seen it all, so it\u2019s interesting to get a perspective on drug issues from people on the front line. In particular, one of my attendings is an Obama-loving long-haired hippie who nevertheless vehemently opposes medical marijuana or any relaxation on marijuana\u2019s status at all. He says that \u201cjust because I\u2019m a Democrat doesn\u2019t mean I have to support stupid policies I know are wrong\u201d and he\u2019s able to back up his opinion with an impressive variety of studies.</p>\n<p>To be honest, I had kind of forgotten that the Universe was allowed to contain negative consequences for legalizing drugs. What with all the mental energy it took protesting the the Drug War and getting outraged at police brutality and celebrating Colorado\u2019s recently permitting recreational cannabis use and so on, it had completely slipped my mind that the legalization of marijuana might have negative consequences and that I couldn\u2019t reject it out of hand until I had done some research.</p>\n<p>So I\u2019ve been doing the research. Not to try to convince my attending of anything \u2013 as the old saying goes, do not meddle in the affairs of attendings, <a href=\"http://www.youtube.com/watch?v=TyT8uc6ath4\">because you are crunchy and taste good with ketchup</a> \u2013 but just to figure out where exactly things stand.</p>\n<p><b id=\"I__Would_Relaxation_Of_Penalties_On_Marijuana_Increase_Marijuana_Use_\">I. Would Relaxation Of Penalties On Marijuana Increase Marijuana Use?</b></p>\n<p>Starting in the 1970s, several states decriminalized possession of marijuana \u2013 that is, possession could not be penalized by jail time. It could still be penalized by fines and other smaller penalties, and manufacture and sale could still be punished by jail time.</p>\n<p>Starting in the 1990s, several states legalized medical marijuana. People with medical marijuana cards, which in many cases were laughably easy to get with or without good evidence of disease, were allowed to grow and use marijuana, despite concerns that some of this would end up on the illegal market.</p>\n<p>Starting last week, Colorado legalized recreational use of marijuana, as well as cultivation and sale (subject to heavy regulations). Washington will follow later this year, and other states will be placing measures on their ballots to do the same.</p>\n<p>One should be able to evaluate to what degree marijuana use rose after these policy changes, and indeed, many people have tried \u2013 with greater or lesser levels of statistical sophistication.</p>\n<p>The <i>worst</i> arguments in favor of this proposition are those like <a href=\"http://webcache.googleusercontent.com/search?q=cache:WQVP0JAUAvYJ:www.cadca.org/files/policy_priorities/EffectsMedicalMarijuanaLegal.doc+&amp;cd=26&amp;hl=en&amp;ct=clnk&amp;gl=us&amp;client=firefox-a\">this CADCA paper</a>, which note that states with more liberal marijuana laws have higher rates of marijuana use among teenagers than states that do not. The proper counterspell to such nonsense is <i>Reverse Causal Arrows</i> \u2013 could it not be that states with more marijuana users are more likely to pass proposals liberalizing marijuana laws? Yes it could. Even more likely, some third variable \u2013 let\u2019s call it \u201chippie attitudes\u201d \u2013 could be behind both high rates of marijuana use and support for liberal marijuana regimes. The states involved are places like Colorado, California, Washington, and Oregon. I think that speaks for itself. In case it doesn\u2019t, someone went through the statistics and found that these states had the highest rates of marijuana use among teens since <i>well</i> before they relaxed drug-related punishments. Argument successfully debunked.</p>\n<p>A slightly more sophisticated version \u2013 used by the DEA <a href=\"http://archive.is/Sa5Qm\">here</a> \u2013  takes the teenage marijuana use in a state one year before legalization of medical marijuana and compares it to the teenage marijuana use in a state one (or several years) after such legalization. They often find that it has increased, and blame the increase on the new laws. <a href=\"http://www.michelepolak.com/200spring11/Weekly_Schedule_files/Single.pdf\">For example</a>, 28% of Californians used marijuana before it was decriminalized in the 70s, compared to 35% a few years after.  This falls victim to a different confounder \u2013 marijuana use has undergone some very large swings nationwide, so the rate of increase in medical marijuana states may be the same as the rate anywhere else. Indeed, this is what was going on in California \u2013 its marijuana use actually rose slightly <i>less</i> than the national average.</p>\n<p>What we want is a study that compares the average marijuana use in a set of states before liberalization to the average marijuana use in the country as a whole, and then does the same after liberalization to see if the ratio has increased. There are several studies that purport to try this, of which by far the best is <a href=\"http://www.monitoringthefuture.org/pubs/occpapers/occ13.pdf\">Johnston, O\u2019Malley &amp; Bachman 1981</a>, which monitored the effect of the decriminalization campaigns of the 70s. They survey thousand of high school seniors on marijuana use in seven states that decriminalize marijuana both before and for five years after the decriminalization, and find absolutely no sign of increased marijuana use (in fact, there is a negative trend). Several other studies (eg <a href=\"http://www.sciencedirect.com/science/article/pii/036233199390016O\">Thies &amp; Register 1993</a>) confirm this finding.</p>\n<p>There is only a hint of some different results. <a href=\"http://tigger.uic.edu/~fjc/Presentations/Scans/Final%20PDFs/ei1999.pdf\">Saffer and Chaloukpa 1999</a> and <a href=\"http://www.nber.org/chapters/c11158.pdf\">Chaloupka, Grossman &amp; Tauras 1999</a> try to use complicated econometric simulations to estimate the way marijuana demand will respond to different variables. They simulate (as opposed to detecting in real evidence) that marijuana decriminalization should raise past-year use by about 5 \u2013 8%, but have no effect on more frequent use (ie a few more people try it but do not become regular users). More impressively, <a href=\"http://www.drugpolicy.org/docUploads/model.pdf\">Model 1993</a> (a source of <a href=\"https://twitter.com/slatestarcodex/status/417494354710654978\">some exasperation</a> for me earlier) finds that after decriminalization, marijuana-related emergency room visits went up (trying to interpret their tables, I think they went up by a whopping 90%, but I\u2019m not sure of this). This is sufficiently different from every other study that I don\u2019t give it much weight, although we\u2019ll return to it later.</p>\n<p>Overall I think the evidence is pretty strong that decriminalization probably led to no increase in marijuana use among teens, and may at most have led to a small single-digit increase. </p>\n<p>Proponents of stricter marijuana penalties say the experiment isn\u2019t fair. In practice, decriminalization does not affect the average user very much \u2013 even in states without decriminalization, marijuana possession very rarely leads to jail time. The only hard number I have is from Australia, where in \u201cnon-decriminalized\u201d Australian states <a href=\"http://192.5.14.43/content/dam/rand/pubs/working_papers/2010/RAND_WR771.pdf\">only 0.3% of marijuana arrests lead to jail time</a>, but a quick back-of-the-envelope calculation suggests US numbers are very similar. And even in supposedly decriminalized states, it\u2019s not hard for a cop who wants to get a pot user in jail to find a way (possession of even small amounts can be \u201cpossession with intent to sell\u201d if someone doesn\u2019t like you). So the overall real difference between decriminalized and not decriminalized is small and it\u2019s not surprising the results are small as well. I mostly agree with them; decriminalization is fine as far as it goes, but it\u2019s a bigger psychological step than an actual one.</p>\n<p>The next major milestone in cannabis history was the legalization of medical marijuana. <a href=\"http://depts.washington.edu/phenom/docs/Anderson_Hansen_Rees_2012.pdf\">Anderson, Hansen &amp; Rees (2012)</a> did the same kind of study we have seen above, and despite trying multiple different measures of youth marijuana use found pretty much no evidence that medical marijuana legalization caused it to increase. <a href=\"http://medicalmarijuana.procon.org/sourcefiles/2005TeenUseReport.pdf\">Other studies</a> find pretty much the same.</p>\n<p>This could potentially suffer from the same problems as decriminalization studies \u2013 the laws don\u2019t always change the facts on the ground. Indeed, for about ten years after medical marijuana legalization, the federal government kept on prosecuting marijuana users even when their use accorded with state laws, and many states had so few dispensaries that in reality not a whole lot of medical marijuana was being given out. I haven\u2019t found any great studies that purport to overcome these problems.</p>\n<p>When we examined decriminalization, we found that the studies based on surveys of teens looked pretty good, but that the one study that examined outcomes \u2013 marijuana-related ER visits \u2013 was a lot less encouraging. We find the same pattern here, and the rain on our parade is <a href=\"https://www.msu.edu/~chuyuwei/mjdraft.pdf\">Chu 2013</a>, who finds that medical marijuana laws increased marijuana-related arrests by 15-20% and marijuana-related drug rehab admissions by 10-15%.</p>\n<p>So what\u2019s going on here? I have two theories. First, maybe medical marijuana use (and decriminalization) increase use among adults only. This could be because the system is working \u2013 giving adults access to medical marijuana while keeping it out of the hands of children \u2013 or because kids are dumb and don\u2019t understand consequences but adults are more responsive to incentives and punishments. Second, we know that medical marijuana has <a href=\"https://www.procon.org/files/current_psychiatry_psychosis.pdf\">twice as much THC</a> as street marijuana. Maybe everyone keeps using the same amount of marijuana, but when medical marijuana inevitably gets diverted to the street, addicts can\u2019t handle it and end up behaving much worse than they expected.</p>\n<p>Or the studies are wrong. Studies being wrong is always a pretty good bet.</p>\n<p>I can\u2019t close this section without mentioning the Colorado expulsion controversy. Nearly everyone who teaches in Colorado says <a href=\"http://www.denverpost.com/breakingnews/ci_24501596/pot-problems-colorado-schools-increase-legalization\">there has been an explosion of marijuana-related problems</a> since medical marijuana was legalized. Meanwhile, the actual surveys of Colorado high school students say that <a href=\"http://www.huffingtonpost.com/2012/09/07/marijuana-usage-down-in-t_n_1865095.html\">marijuana use, if anything, is going down</a>. A Colorado drug warrior has some <a href=\"http://drthurstone.com/jumping-to-conclusions-with-cdc-data/\">strong objections</a> to the survey results, but they center around not really being able to prove that there is a real downward trend (which is an entirely correct complaint) without denying that in fact they show no evidence at all of going <i>up</i>.</p>\n<p>The consensus on medical marijuana seems to be that it does not increase teen marijuana use either, although there is some murky and suggestive evidence that it might increase illicit or dangerous marijuana use among adults.</p>\n<p>There is less information on the effects of full legalization of marijuana, which has never been tried before in the United States. To make even wild guesses we will have to look at a few foreign countries plus some econometric simulations.</p>\n<p>No one will be surprised to hear that the first foreign country involved is the Netherlands, which was famously permissive of cannabis up until a crackdown a few years ago. Despite popular belief they never fully legalized the drug and they were still pretty harsh on production and manufacture; distribution, on the other hand, could occur semi-openly in coffee shops. This is another case where we have to be careful to distinguish legal regimes from actual effects, but during the period when there were actually a lot of pot-serving coffee shops, the Netherlands did experience <a href=\"**http://www.stopthewarondrugs.org/wp-content/uploads/2012/06/MacCoun-Robert-J.-2010-Estimating-the-Non-Price-Effects-of-Legalization-on-Cannabis-Consumption.pdf\">an otherwise-inexplicable 35% rise in marijuana consumption</a> relative to the rest of Europe. This is true even among teenagers, and covers both heavy use as well as occasional experimentation. Some scientists studying the Netherlands\u2019 example expect Colorado to see a similar rise; others think it will be even larger because the legalization is complete rather than partial.</p>\n<p>The second foreign country involved is Portugal, which was maybe more of a decriminalization than a legalization case but which is forever linked with the idea of lax drug regimes in the minds of most Americans. They decriminalized all drugs (including heroin and cocaine) in 2001, choosing to replace punishment with increased treatment opportunities, and <a href=\"http://www.cato.org/publications/white-paper/drug-decriminalization-portugal-lessons-creating-fair-successful-drug-policies\">as we all have been told</a>, no one in Portugal ever used drugs ever again, or even remembers that drugs exist. Except it turns out it\u2019s more complicated; for example, the percent of Portuguese who admit to lifetime use of drugs <a href=\"http://www.wfad.se/latest-news/1-articles/123-decriminalization-of-drugs-in-portugal--the-real-facts\">has doubled</a> since the law took effect. Two very patient scientists <a href=\"http://www.undrugcontrol.info/images/stories/documents/A_resounding_success_or_a_disastrous_failure.pdf\">have sifted through all the conflicting claims</a> and found that in reality, the number of people who briefly experiment with drugs has gone way up, but the number of addicts hasn\u2019t, nor has the number of bad outcomes like overdose-related deaths. There are many more people receiving drug treatment, but that might just be because Portugal upped its drug treatment game in a separate law at the same time they decriminalized drugs. Overall they seem to have been a modest success \u2013 neither really raising nor decreasing the number of addicts \u2013 but they seem more related to decriminalization (which we\u2019ve already determined doesn\u2019t have much effect) than to legalization per se.</p>\n<p>Returning to America, what if you just <i>ask</i> people whether they would use more marijuana if it\u2019s legal? Coloradans were asked if they plan to smoke marijuana once it becomes legal; comparing survey results to current usage numbers suggests <a href=\"http://learnaboutsam.com/legalization-of-marijuana-could-increase-marijuana-use-almost-40-versus-current-rates-among-those-aged-18-and-older-and-in-the-18-25-year-old-age-group-in-colorado/\">40% more users</a> above the age of 18; it is unclear what the effect will be on younger teens and children.</p>\n<p>Finally, we let the economists have their say. They crunch all the data and predict <a href=\"http://www.rand.org/content/dam/rand/pubs/testimonies/2010/RAND_CT351.pdf\">an increase of 50 \u2013 100%</a> based solely on the likely price drop (even with taxes factored in). And if there\u2019s one group we can trust to make infallible predictions about the future, it\u2019s economists.</p>\n<p>Overall I find the Dutch evidence most convincing, and predict a 25 \u2013 50% increase in adult marijuana use with legalization. I would expect a lower increase \u2013 15 \u2013 30% \u2013 among youth, but the data are also perfectly consistent with no increase at all.</p>\n<p>Conclusion for this section: that decriminalization and legalization of medical marijuana do not increase youth marijuana use rates, although there is some shaky and indirect evidence they do increase adult use and bad behavior. There is no good data yet on full legalization, but there\u2019s good reason to think it would substantially increase adult use and it might also increase youth use somewhat.</p>\n<p><b id=\"II__Is_Marijuana_Bad_For_You_\">II. Is Marijuana Bad For You?</b></p>\n<p><a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3371269/\">About 9% of marijuana users</a> eventually become addicted to the drug, exposing them to various potential side effects.</p>\n<p>Marijuana smoke contains a lot of the same chemicals in tobacco smoke and so it would not be at all surprising if it had some of the same ill effects, like cardiovascular disease and lung cancer. But when people look for these effects, <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380837/\">they can\u2019t find any increase in mortality among marijuana smokers</a>. I predict that larger studies will one day pick something up, but for now let\u2019s take this at face value.</p>\n<p>Much more concerning are the attempts to link marijuana to cognitive and psychiatric side effects. <a href=\"http://www.pnas.org/content/109/40/E2657.full\">Meier et al (2012)</a> analyzed a study of a thousand people in New Zealand and found that heavy marijuana use was linked to an IQ decline of 8 points. <a href=\"http://www.pnas.org/content/110/11/4251.full\">Rogeberg 2012</a> developed an alternative explanation \u2013 poor people saw their IQs drop in their 20s more than rich people because their IQs had been artificially inflated by schooling; what Meier et al had thought to be an effect of cannabis was really an effect of poor people having an apparent IQ drop and using cannabis more often. Meier et al <a href=\"http://www.pnas.org/content/110/11/E980.full\">pointed out</a> that actually, poor people didn\u2019t use cannabis any more often than anyone else and effects remained when controlled for class. Other studies, like <a href=\"http://www.cmaj.ca/content/166/7/887.full\">Fried et al (2002)</a> find the same effect, and there is a plausible biological mechanism (cannabinoids something something neurotransmitters something brain maturation). As far as I can tell the finding still seems legit, and marijuana use does decrease IQ. It is still unclear whether this only applies in teenagers (who are undergoing a \u201csensitive period of brain development\u201d) or full stop. </p>\n<p>More serious still is the link with psychosis. A number of studies have found that marijuana use is heavily correlated with development of schizophrenia and related psychotic disorders later in life. Some of them find relative risks as high as 2 \u2013 heavy marijuana use doubles your chance of getting schizophrenia, which is already a moderately high 1%. But of course correlation is not causation, and many people have come up with alternative theories. For example, maybe people who are already kind of psychotic use marijuana to self-medicate, or just make poor life choices like starting drugs. Maybe people of low socioeconomic status who come from broken homes are more likely to both use marijuana and get schizophrenia. Maybe some gene both makes marijuana really pleasant and increases schizophrenia risk.</p>\n<p>I know of three good studies attempting to tease out causation. <a href=\"http://bjp.rcpsych.org/content/184/2/110.full.pdf\">Arseneault et al (2004)</a> checks to see which came first \u2013 the marijuana use or the psychotic symptoms \u2013 and finds it was the marijuana use, thus supporting an increase in risk from the drug. <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/add.12050/abstract\">Griffith-Lendering et al (2012)</a> try the same, and find <i>bidirectional</i> causation \u2013 previous marijuana use seems to predict future psychosis, but previous psychosis seems to predict future marijuana use. A <a href=\"**http://psychcentral.com/news/2013/12/10/harvard-marijuana-doesnt-cause-schizophrenia/63148.html\">very new study from last month</a> boxes clever and checks whether your marijuana use can predict schizophrenia in <i>your relatives</i>, and find that it does \u2013 presumably suggesting that genetic tendencies towards schizophrenia cause marijuana use and not vice versa (although Ozy points out to meet that the relatives of marijuana users are more likely to use marijuana themselves; the plot thickens). When <a href=\"http://www.clinique-transculturelle.org/pdf/lancet_2007.pdf\">a meta-analysis</a> tries to control for all of these factors, they get a relative risk of 1.4 (they call it an odds ratio, but from their discussion section I think they mean relative risk).</p>\n<p>Is this true, or just the confounders they failed to pick up? One argument for the latter is that marijuana use has increased very much over the past 50 years. If marijuana use caused schizophrenia, we would expect to see much more schizophrenia, but in fact as far as anyone can tell (which is not very far) <a href=\"http://www.mentalhealth.com/mag1/scz/sb-time.html\">schizophrenia incidence is decreasing</a>. The decrease might be due (maybe! if it even exists at all!) to obstetric advances which prevent fetal brain damage which could later lead to the disease. The effect of this variable is insufficiently known to pretend we can tease out some supposed contrary effect of increased marijuana use. Also, some people say that <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0447.2012.01913.x/abstract\">schizophrenia is increasing in young people</a>, so who knows?</p>\n<p>The <i>exact</i> nature of the marijuana-psychosis link is still very controversial. Some people say that marijuana causes psychosis. Other people say it \u201cactivates latent psychosis\u201d, a term without a very good meaning but which might mean that it pushes people on the borderline of psychosis \u2013 eg those with a strong family history but who might otherwise have escaped \u2013 over the edge. Still others say all it does is get people who would have developed psychosis eventually to develop it a few years earlier. You can read a comparison of all the different hypotheses <a href=\"https://www.procon.org/files/current_psychiatry_psychosis.pdf\">here</a>.</p>\n<p>I\u2019ve saved the most annoying for last: is marijuana a \u201cgateway drug\u201d? Would legalizing it make it more or less of a \u201cgateway drug\u201d? This claim seems tailor-made to torture statisticians. We know that marijuana users are <i>definitely</i> more likely to use other drugs later \u2013 for example, <a href=\"http://www.columbia.edu/cu/record/archives/vol20/vol20_iss10/record2010.24.html\">marijuana users are 85x more likely than non-marijuana users to use cocaine</a>. but that could be either because marijuana affects them in some way (implying that legalizing marijuana would increase other drug use), because <a href=\"http://healthland.time.com/2010/10/29/marijuna-as-a-gateway-drug-the-myth-that-will-not-die/\">they have factors</a> like genetics or stressful life situation that makes them more likely to use all drugs (implying that legalizing marijuana would not affect other drug use), or because using illegal marijuana without ill effect connects them to the illegal drug market and convinces them illegal drugs are okay (implying that legalizing marijuana would decrease other drug use). RAND comes very close to investigating this properly by saying that <a href=\"http://www.rand.org/content/dam/rand/pubs/working_papers/2010/RAND_WR768.pdf\">when the Dutch pseudo-legalized marijuana, use of harder drugs stayed stable or went down</a>, but all their study actually shows is that the ratio of marijuana users : hard drug users went down. This is to be expected when you make marijuana much easier to get, but it\u2019s still consistent with the absolute number of hard drug users going way up. The best that can be said is that there is no direct causal evidence for the gateway theory and <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3600369/\">some good alternative explanations</a> for the effect. Let us accept their word for it and never speak of this matter again.</p>\n<p>Conclusion for this section: Marijuana does not have a detectable effect on mortality and there is surprisingly scarce evidence of tobacco-like side effects. It probably does decrease IQ if used early and often, possibly by as many as 8 IQ points. It may increase risk of psychosis by as much as 40%, but it\u2019s not clear who is at risk or whether the risk is even real. The gateway drug hypothesis is too complicated to evaluate effectively but there is no clear casual evidence in its support.</p>\n<p><b id=\"III__What_Are_The_Costs_Of_The_Drug_War_\">III. What Are The Costs Of The Drug War?</b></p>\n<p>There are not really that many people in jail for using marijuana.</p>\n<p>I learned this from <a href=\"https://www.ncjrs.gov/ondcppubs/publications/pdf/whos_in_prison_for_marij.pdf\">Who\u2019s Really In Prison For Marijuana?</a>, a publication of the National Office Of Drug Control Policy, which was clearly written by someone with the same ability to take personal offense at bad statistics that inspires <a href=\"http://slatestarcodex.com/2013/04/04/lies-damned-lies-and-facebook-part-1-of-%E2%88%9E/\">my</a> <a href=\"http://slatestarcodex.com/2013/04/04/lies-damned-lies-and-facebook-part-2-of-%E2%88%9E/\">posts</a> <a href=\"http://slatestarcodex.com/2013/06/11/lies-damned-lies-and-facebook-part-3-of-%E2%88%9E/\">about</a> <a href=\"http://slatestarcodex.com/2013/11/08/lies-damned-lies-and-facebook-part-4-of-%E2%88%9E/\">Facebook</a>. The whole thing seethes with indignation and makes me want to hug the drug czar and tell him everything will be okay.</p>\n<p>Only 1.6% of state prisoners are serving time for marijuana, only 0.7% are serving for marijuana possession, and only 0.3% are first time offenders. Some of those are \u201cpossession\u201d in the sense of \u201cpossessing a warehouse full of marijuana bales\u201d, and others are people who committed much more dangerous crimes but were nailed for marijuana, in the same sense that Al Capone was nailed for tax evasion. The percent of normal law-abiding people who just had a gram or two of marijuana and were thrown in jail is a rounding error, and the stories of such you read in the news are extremely dishonest (read the document for examples).</p>\n<p>Federal numbers are even lower; in the entire federal prison system, they could only find 63 people imprisoned with marijuana possession as the sole crime, and those people were possessing a median of one hundred fifteen <i>pounds</i> of marijuana (enough to make over 100,000 joints).</p>\n<p>In total, federal + state prison and counting all the kingpins, dealers, manufacturers, et cetera, there are probably about 16,000 people in prison solely for marijuana-related offenses, serving average actual sentence lengths of three year. But it\u2019s anybody\u2019s guess whether those people would be free today if marijuana were legal, or whether their drug cartels would just switch to something else. </p>\n<p>Looking at the other side\u2019s statistics, I don\u2019t see much difference. <a href=\"http://norml.org/news/2006/10/12/nearly-one-in-eight-us-drug-prisoners-are-behind-bars-for-pot-taxpayers-spending-over-1-billion-annually-to-incarcerate-pot-offenders\">NORML claims that</a> there are 40,000 people in prison for marijuana use, but they admit that half of those people were arrested for using harder drugs and marijuana was a tack-on charge, so they seem to agree with the Feds about around 20,000 pure marijuana prisoners. <a href=\"http://learnaboutsam.com/the-issues/marijuana-and-whos-in-prison/\">SAM agrees</a> that only 0.5% of the prison population is in there for marijuana possession alone. I see no reason to doubt any of these numbers.</p>\n<p>A much more serious problem is marijuana-related arrests, of which there are 700,000 a year. <a href=\"https://www.aclu.org/drug-law-reform/marijuana-arrests-punishments\">90% of them are for simple possession</a>, and the vast majority do not end in prison terms; they do however result in criminal records, community service, a couple days of jail time until a judge is available to hear the case, heavy fines, high cost of legal representation, and moderate costs to the state for funding the whole thing. Fines can be up to $1500, and legal representation <a href=\"http://www.aclu-wa.org/library_files/BeckettandHerbert.pdf\">can cost up to $5000</a> (though I am suspicious of this paper and think it may be exaggerating for effect). These costs are often borne by poor people who will have to give up all their savings for years to pay them back.</p>\n<p>Costs paid by the government, which cover everything from police officers to trials to prison time, are estimated at about $2 billion by <a href=\"http://www.huffingtonpost.com/2012/10/29/one-marijuana-arrest-occu_n_2041236.html\">multiple</a> <a href=\"http://www.aclu-wa.org/library_files/BeckettandHerbert.pdf\">sources</a>. This is only 3% of the total law enforcement budget, so legalizing marijuana wouldn\u2019t create some kind of sudden revolution in policing, but as the saying goes, a billion here, a billion there, and eventually it adds up to real money. And a Harvard economist claims that the total monetary benefits from legalization, including potential tax revenues, <a href=\"www.huffingtonpost.com/2012/04/17/economists-marijuana-legalization_n_1431840.html\">could reach $14 billion</a>.</p>\n<p>Some people worry that legalizing marijuana would cause an increase in car accidents by \u201cstoned drivers\u201d, who, like drunk drivers, have impaired reflexes and poor judgment, and indeed there is <a href=\"http://www.drugabuse.gov/publications/drugfacts/drugged-driving\">a small but real problem of marijuana-induced car accidents</a>. But <a href=\"http://www.nber.org/papers/w4662\">Chaloukpa and Laixuthai (1994)</a> crunch the numbers and find that decreased price/increased availability of marijuana is actually associated with <i>decreased</i> car accidents, probably because marijuana is substituting for alcohol in the \u201chave impairing substances and then go driving\u201d population. This finding \u2013 that marijuana and alcohol substitute for each other \u2013 <a href=\"http://www.nytimes.com/2013/11/04/opinion/marijuana-and-alcohol.html?_r=0\">has been spotted again and again</a>. <a href=\"http://dmarkanderson.com/Point_Counterpoint_07_31_13_v5.pdf\">Anderson &amp; Rees (2013)</a> find that states that legalize medical marijuana see a 5% drop in beer sales. There are however a few dissenting opinions: <a href=\"http://cms.sem.tsinghua.edu.cn/semcms/res_base/semcms_com_www/upload/home/store/2008/10/15/3229.pdf\">Cameron &amp; Williams (2001)</a>, in complex econometric simulations that may or may not resemble the real world in any respect, find that increasing the price of alcohol increases marijuana use, but increasing the price of marijuana does not affect alcohol use, and <a href=\"http://www.impacteen.org/generalarea_PDFs/WEA062001_presentation.pdf\">the same researcher</a> finds that banning alcohol on a college campus also decreases marijuana use. Also, possibly marijuana use increases smoking? This whole area is confusing, but I am most sympathetic to to the Andersen and Rees statistics which say that medical marijuana states are associated with 13% fewer traffic fatalities.</p>\n<p>Overall conclusion for this section: full legalization of marijuana would free about 20,000 people from jail (although most of them would not be exactly fine upstanding citizens), prevent 700,000 arrests not resulting in jail time per year, save between 2 and 14 billion dollars, and possibly reduce traffic fatalities a few percent (or, for all we know, increase them).</p>\n<p><b id=\"IV__An_Irresponsible_Utilitarian_Analysis\">IV. An Irresponsible Utilitarian Analysis</b></p>\n<p>Decriminalization and legalization of medical marijuana seem, if we are to trust the statistics in (I) saying they do not increase use among youth, like almost unalloyed good things. Although there are some nagging hints of doubt, they are not especially quantifiable and therefore not amenable to analysis. Without a very strong predisposition to try as hard as possible to fit the evidence into a pessimistic picture, I don\u2019t think there\u2019s a great argument against either of these two propositions. Let\u2019s concentrate on legalization, which would mean something like \u201cPeople can grow and sell as much marijuana as they want and it\u2019s totally legal for people over 21, with the same level of penalties as today for people under 21\u201d.</p>\n<p>Section (I) concludes that legalization could lead to an increase in adult marijuana use up to 50%. There\u2019s not a lot of evidence on what it could do to teen marijuana use, but since it seems teen marijuana use is less responsive to legal changes, I made up a number and said 20%. Lest you think I am being unfair, note that this is well below the percent increase predicted by the survey that asked 18 year olds if they would start using marijuana if it were legal.</p>\n<p>Right now about 1.5 million teenagers <a href=\"https://www.drugfree.org/newsroom/pats-2011\">use marijuana \u201cheavily\u201d</a>. Most of the detrimental effects of marijuana seem concentrated in teens and people in their early twenties; I\u2019m going to artificially round that up to 2 million to catch the early 20 year olds. If this 2 million number increased 20%, 400,000 extra teens would start heavily using marijuana.</p>\n<p>Those 400,000 teens would lose 8 IQ points each. IQ increases your yearly earnings by about $500 per point, so these people would lose about $4,000 a year. Making very strong assumptions about salary being a measure of value to society, society would lose about $1.6 billion a year directly, plus various intangibles from potential artists and scientists losing the ability to create masterpieces and inventions, plus various <i>really</i> intangibles like a slightly dumber electorate.</p>\n<p>We need to use a different number to calculate psychosis risk, since the studies were done on \u201cpeople who had used marijuana at least once\u201d. The appropriate number turns out to be 8 million teenagers; of those, 1%, or 80,000, would naturally develop schizophrenia. If the 1.4 relative risk number is correct, marijuana use will increase that to 112,000, for a total increase of 32,000 people. Schizophrenia pretty much always presents in the 15 \u2013 25 age window, so we\u2019ll say we get 3,200 extra cases per year.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/List_of_motor_vehicle_deaths_in_U.S._by_year\">There were</a> 35000 road traffic accident fatalities in the US last year. If greater availability of marijuana decreases those fatalities by 13% (note that I am using the number from medical marijuana legalization and not for marijuana legalization per se, solely because it is a number I actually have), that will cause 4500 fewer road traffic deaths per year. There may be additional positive effects of alcohol substitution from, for example, less liver disease. But there may also be additional negative effects from increasing use of tobacco, so let\u2019s just pretend those cancel out.</p>\n<p>So here is my guess at the yearly results of marijuana legalization:</p>\n<p>\u2013 20,000 fewer prisoners (but they might switch to other criminal enterprises)<br>\n\u2013 700,000 fewer arrests<br>\n\u2013 $2 billion less in law enforcement costs<br>\n\u2013 Some amount of positive gain (let\u2019s say $5 billion) in taxes<br>\n\u2013 4500 fewer road traffic deaths (if you believe the preliminary alcohol substitution numbers)</p>\n<p>\u2013 400,000 people with lower IQ<br>\n\u2013 $2 billion in social costs from above dumber people<br>\n\u2013 3,200 more cases of schizophrenia a year</p>\n<p>We\u2019ll proceed to calculate the nonmonetary burden of each of these in QALYs, then add the monetary burden in dollars, then convert.</p>\n<p>The <a href=\"https://research.tufts-nemc.org/cear4/SearchingtheCEARegistry/SearchtheCEARegistry.aspx\">searchable public database of utility weights for all diseases</a> (God I love the 21st century) tells me that schizophrenia has a QALY weight of 0.73. It generally starts around 20 and lasts a lifetime, so each case of schizophrenia costs us 0.27 * 50 or 13.5 QALYs. Therefore, the total burden of the 3,200 added schizophrenia cases is 43 kiloQALYs.</p>\n<p>There\u2019s no good way to calculate the QALY weight of having 4-8 fewer IQ points, and unfortunately this is going to end up being among the most important numbers in our results. If we say the lifetime cost of this problem is 3 QALYs, and divide the number by eight to represent eight years worth of teenagers in our sample population, we end up with 400,000/8 * 3 = 150 kiloQALYs.</p>\n<p><a href=\"http://slatestarcodex.com/2013/04/30/utility-weight-results/\">My own survey</a> tells me that being in prison has a QALY weight around 0.5. Marijuana sentences generally last an average of three years, which suggests that 1/3 of these marijuana prisoners are arrested every year, so the total burden of the ~6000ish marijuana imprisonments each year is 3 * ~6000 * 0.5 = 10 kiloQALYs.</p>\n<p>Assume the average road traffic death occurs at age 30, costing 40 years of potential future life. The total cost of 4500 road traffic deaths is 40 * 4500 = 180 kiloQALYs.</p>\n<p>The arrests are going to require even more fudging than normal. Average jail time for a marijuana arrest (when awaiting trial) is \u201cone to five days\u201d \u2013 let\u2019s round that off to two and then use our prison number to say that the jail from each arrest is 2/365 * 0.5 = three-thousandths of a QALY. I am going to arbitrarily round this up to one one-hundredth of a QALY to account for emotional trauma and the burden of fines, then even more arbitrarily round this up to a tenth of a QALY to account for possibility of getting a criminal record. This sets the burden of 700,000 arrests at 70 kiloQALYs.</p>\n<p>Now our accounting is:</p>\n<p>Costs from legalization compared to current system: 200 kQALYs and $2 billion<br>\nBenefits from legalization compared to current system: 260 kQALYs and $7 billion</p>\n<p>Although it\u2019s not going to be necessary, we can interconvert QALYs and dollars at the going health-care rate of about $100,000/QALY ($100 million/kQALY):</p>\n<p>Costs from legalization compared to current system: 220 kQALYs<br>\nBenefits from legalization compared to current system: 330 kQALYs</p>\n<p>And get:</p>\n<p><i>Net benefits from legalization: +110 kQALYs</i></p>\n<p>Except that this is extremely speculative and irresponsible. By far the largest component of the benefits of legalization turned out to be the effect on road traffic accidents, which is based on only two studies and which may on further research turn out to be a cost. And by far the largest component of the costs of legalization turned out to be the effect on IQ, and we had to totally-wild-guess the QALY cost of an IQ point loss. The wiggle room in my ignorance and assumptions is more than large enough to cover the small gap between the two policies in the results.</p>\n<p>So my actual conclusion is:</p>\n<p><i>There is not a sufficiently obvious order-of-magnitude difference between the costs and benefits of marijuana legalization for a evidence-based utilitarian analysis of costs and benefits to inform the debate. You may return to your regularly scheduled wild speculation and shrill accusations.</i></p>\n<p>But I wouldn\u2019t say this exercise is useless. For example, it suggests that whether marijuana legalization is positive or negative on net depends almost entirely on small changes in the road traffic accident rate. This is something I\u2019ve never heard anyone else mention, but which in retrospect should be obvious; the few debatable health effects and the couple of people given short jail sentences absolutely can\u2019t compare to the potential for thousands more (or fewer) traffic accidents which leave people permanently dead.</p>\n<p>So my actual actual conclusion is:</p>\n<p><i>We should probably stop caring about health effects of marijuana and about imprisonment for marijuana-related offenses, and concentrate all of our research and political energy on how marijuana affects driving.</i></p>\n<p>This cements <a href=\"http://slatestarcodex.com/2013/05/02/if-its-worth-doing-its-worth-doing-with-made-up-statistics/\">my previous intuitions on irresponsible use of statistics</a> \u2013 it\u2019s unlikely to unilaterally solve the problem, but it can be very good at pointing out where you\u2019re being irrational and suggesting new ways of looking at a question.</p>\n<p><b>EDIT</b>: People in the comments have pointed out several important factors left out, including:<br>\n\u2013 Some people enjoy smoking marijuana<br>\n\u2013 The opening of a permanent criminal record may mean arrests are worse than I estimate. I can\u2019t find good statistics on how often this happens, but do note that decriminalization prevents a record from being opened.<br>\n\u2013 Loss of 8 IQ points may have wider social effects than I estimate, since IQ affects for example crime rate.<br>\n\u2013 Legalizing marijuana might remove a source of funding for organized crime</p>", "sections": [{"title": "I. Would Relaxation Of Penalties On Marijuana Increase Marijuana Use?", "anchor": "I__Would_Relaxation_Of_Penalties_On_Marijuana_Increase_Marijuana_Use_", "level": 1}, {"title": "II. Is Marijuana Bad For You?", "anchor": "II__Is_Marijuana_Bad_For_You_", "level": 1}, {"title": "III. What Are The Costs Of The Drug War?", "anchor": "III__What_Are_The_Costs_Of_The_Drug_War_", "level": 1}, {"title": "IV. An Irresponsible Utilitarian Analysis", "anchor": "IV__An_Irresponsible_Utilitarian_Analysis", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-06T00:16:09.532Z", "modifiedAt": null, "url": null, "title": "[LINK] Why I'm not on the Rationalist Masterlist", "slug": "link-why-i-m-not-on-the-rationalist-masterlist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:06.962Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Apprentice", "createdAt": "2009-05-20T13:06:35.976Z", "isAdmin": false, "displayName": "Apprentice"}, "userId": "X5f7X9PkRFwGyxWsb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bMDJAcXfJdLtMArej/link-why-i-m-not-on-the-rationalist-masterlist", "pageUrlRelative": "/posts/bMDJAcXfJdLtMArej/link-why-i-m-not-on-the-rationalist-masterlist", "linkUrl": "https://www.lesswrong.com/posts/bMDJAcXfJdLtMArej/link-why-i-m-not-on-the-rationalist-masterlist", "postedAtFormatted": "Monday, January 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Why%20I'm%20not%20on%20the%20Rationalist%20Masterlist&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Why%20I'm%20not%20on%20the%20Rationalist%20Masterlist%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbMDJAcXfJdLtMArej%2Flink-why-i-m-not-on-the-rationalist-masterlist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Why%20I'm%20not%20on%20the%20Rationalist%20Masterlist%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbMDJAcXfJdLtMArej%2Flink-why-i-m-not-on-the-rationalist-masterlist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbMDJAcXfJdLtMArej%2Flink-why-i-m-not-on-the-rationalist-masterlist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 222, "htmlBody": "<p>A long blog post explains why the author, a feminist, is not comfortable with the rationalist community despite thinking it is \"super cool and interesting\". It's directed specifically at Yvain, but it's probably general enough to be of some interest here.</p>\n<p><a href=\"http://apophemi.wordpress.com/2014/01/04/why-im-not-on-the-rationalist-masterlist/\">http://apophemi.wordpress.com/2014/01/04/why-im-not-on-the-rationalist-masterlist/</a></p>\n<p>I'm not sure if I can summarize this fairly but the main thrust seems to be that we are overly willing to entertain offensive/taboo/hurtful ideas and this drives off many types of people. Here's a quote:</p>\n<blockquote>\n<p>In other words, prizing discourse without limitations (I tried to find a convenient analogy for said limitations and failed. Fenders? Safety belts?) will result in an environment in which people are more comfortable speaking the more social privilege they hold.</p>\n</blockquote>\n<p>The author perceives a link between LW type open discourse and danger to minority groups. I'm not sure whether that's true or not. Take race. Many LWers are willing to entertain ideas about the existence and possible importance of average group differences in psychological traits. So, maybe LWers are racists. But they're racists who continually obsess over optimizing their philanthropic contributions to African charities. So, maybe not racists in a dangerous way?</p>\n<p>An overly rosy view, perhaps, and I don't want to deny the reality of the blogger's experience. Clearly, the person is intelligent and attracted to some aspects of LW discourse while turned off by other aspects.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W9aNkPwtPhMrcfgj7": 1, "MXcpQvaPGtXpB6vkM": 1, "CYMR6p5iZG75QAT8a": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bMDJAcXfJdLtMArej", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 76, "baseScore": 39, "extendedScore": null, "score": 0.000108, "legacy": true, "legacyId": "25191", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 883, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-06T06:11:51.012Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014How to Live on 22 Hours a Day", "slug": "meetup-west-la-how-to-live-on-22-hours-a-day", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gyAtqJjr47DxWGyP7/meetup-west-la-how-to-live-on-22-hours-a-day", "pageUrlRelative": "/posts/gyAtqJjr47DxWGyP7/meetup-west-la-how-to-live-on-22-hours-a-day", "linkUrl": "https://www.lesswrong.com/posts/gyAtqJjr47DxWGyP7/meetup-west-la-how-to-live-on-22-hours-a-day", "postedAtFormatted": "Monday, January 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94How%20to%20Live%20on%2022%20Hours%20a%20Day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94How%20to%20Live%20on%2022%20Hours%20a%20Day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgyAtqJjr47DxWGyP7%2Fmeetup-west-la-how-to-live-on-22-hours-a-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94How%20to%20Live%20on%2022%20Hours%20a%20Day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgyAtqJjr47DxWGyP7%2Fmeetup-west-la-how-to-live-on-22-hours-a-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgyAtqJjr47DxWGyP7%2Fmeetup-west-la-how-to-live-on-22-hours-a-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 259, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/va'>West LA\u2014How to Live on 22 Hours a Day</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 January 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 24 hours a day. Sorry, I mean three hours. Just three.</p>\n\n<p><strong>Discussion</strong>: <a href=\"http://lesswrong.com/lw/37y/how_to_live_on_24_hours_a_day/\">Previously</a> discussed on Less Wrong, <a href=\"http://www.gutenberg.org/files/2274/2274-h/2274-h.htm\" rel=\"nofollow\">How to Live on 24 Hours a Day</a>, by Arnold Bennett, is a remarkable and <em>short</em> book from 1910 about how to make the most out of your time. Hustle and bustle have increased a tad since then, so it it is my goal to compress the book's 24-hour methodology into a paltry 22. Bennett gives you two hours with which to deviate from his programme\u2014I am giving you two more.</p>\n\n<p><strong>Recommended reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://www.gutenberg.org/files/2274/2274-h/2274-h.htm\" rel=\"nofollow\">How to Live on 24 Hours a Day</a>, by Arnold Bennett. I highly recommend reading this book, either before the meetup, so that you are prepared to discuss it, or after the meetup, when you've been sufficiently oiled for it. Either is fine, as I will prepare a summary.</li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary</em>; this will be generally accessible. It's been a while, comrades; this will be our first meetup of 2014! Might I say, welcome back, returners, and welcome, new-folk!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/va'>West LA\u2014How to Live on 22 Hours a Day</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gyAtqJjr47DxWGyP7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.50031739920961e-06, "legacy": true, "legacyId": "25193", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_How_to_Live_on_22_Hours_a_Day\">Discussion article for the meetup : <a href=\"/meetups/va\">West LA\u2014How to Live on 22 Hours a Day</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 January 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 24 hours a day. Sorry, I mean three hours. Just three.</p>\n\n<p><strong>Discussion</strong>: <a href=\"http://lesswrong.com/lw/37y/how_to_live_on_24_hours_a_day/\">Previously</a> discussed on Less Wrong, <a href=\"http://www.gutenberg.org/files/2274/2274-h/2274-h.htm\" rel=\"nofollow\">How to Live on 24 Hours a Day</a>, by Arnold Bennett, is a remarkable and <em>short</em> book from 1910 about how to make the most out of your time. Hustle and bustle have increased a tad since then, so it it is my goal to compress the book's 24-hour methodology into a paltry 22. Bennett gives you two hours with which to deviate from his programme\u2014I am giving you two more.</p>\n\n<p><strong>Recommended reading</strong>:</p>\n\n<ul>\n<li><a href=\"http://www.gutenberg.org/files/2274/2274-h/2274-h.htm\" rel=\"nofollow\">How to Live on 24 Hours a Day</a>, by Arnold Bennett. I highly recommend reading this book, either before the meetup, so that you are prepared to discuss it, or after the meetup, when you've been sufficiently oiled for it. Either is fine, as I will prepare a summary.</li>\n</ul>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary</em>; this will be generally accessible. It's been a while, comrades; this will be our first meetup of 2014! Might I say, welcome back, returners, and welcome, new-folk!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_How_to_Live_on_22_Hours_a_Day1\">Discussion article for the meetup : <a href=\"/meetups/va\">West LA\u2014How to Live on 22 Hours a Day</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014How to Live on 22 Hours a Day", "anchor": "Discussion_article_for_the_meetup___West_LA_How_to_Live_on_22_Hours_a_Day", "level": 1}, {"title": "Discussion article for the meetup : West LA\u2014How to Live on 22 Hours a Day", "anchor": "Discussion_article_for_the_meetup___West_LA_How_to_Live_on_22_Hours_a_Day1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6yW7hGvr5k33gxgkB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-06T06:43:54.147Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Social Meetup", "slug": "meetup-melbourne-social-meetup-13", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:00.997Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hg4ep5cKgtSESfane/meetup-melbourne-social-meetup-13", "pageUrlRelative": "/posts/hg4ep5cKgtSESfane/meetup-melbourne-social-meetup-13", "linkUrl": "https://www.lesswrong.com/posts/hg4ep5cKgtSESfane/meetup-melbourne-social-meetup-13", "postedAtFormatted": "Monday, January 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhg4ep5cKgtSESfane%2Fmeetup-melbourne-social-meetup-13%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhg4ep5cKgtSESfane%2Fmeetup-melbourne-social-meetup-13", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhg4ep5cKgtSESfane%2Fmeetup-melbourne-social-meetup-13", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vb'>Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 January 2014 05:42:30PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">5 / 52 Leicester St, Carlton, VIC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The January social meetup will be on Friday 17th at our usual location in Carlton. All welcome from 6:30pm.</p>\n\n<p>Our social meetups are informal events where we gather for games and conversation about any topics of interest. We sometimes play board/card games, and we will often play a parlour game like Mafia/Werewolf or The Resistance later in the evening. We usually arrange take-away food for dinner.</p>\n\n<p>Just ring number 5 when you get to the front door, and we'll buzz you up. If you get lost or have any problems, you can call me (Richard) on 0421-231-789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vb'>Melbourne Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hg4ep5cKgtSESfane", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5003519122354463e-06, "legacy": true, "legacyId": "25194", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/vb\">Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 January 2014 05:42:30PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">5 / 52 Leicester St, Carlton, VIC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The January social meetup will be on Friday 17th at our usual location in Carlton. All welcome from 6:30pm.</p>\n\n<p>Our social meetups are informal events where we gather for games and conversation about any topics of interest. We sometimes play board/card games, and we will often play a parlour game like Mafia/Werewolf or The Resistance later in the evening. We usually arrange take-away food for dinner.</p>\n\n<p>Just ring number 5 when you get to the front door, and we'll buzz you up. If you get lost or have any problems, you can call me (Richard) on 0421-231-789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/vb\">Melbourne Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-06T21:17:36.889Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham Meetup - Bias Defenses from Influence", "slug": "meetup-durham-meetup-bias-defenses-from-influence", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "curiousepic", "createdAt": "2010-04-15T14:35:25.116Z", "isAdmin": false, "displayName": "curiousepic"}, "userId": "wxLCJJwvPiQbkXjTe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HHDL6GQfsXKWmDmW3/meetup-durham-meetup-bias-defenses-from-influence", "pageUrlRelative": "/posts/HHDL6GQfsXKWmDmW3/meetup-durham-meetup-bias-defenses-from-influence", "linkUrl": "https://www.lesswrong.com/posts/HHDL6GQfsXKWmDmW3/meetup-durham-meetup-bias-defenses-from-influence", "postedAtFormatted": "Monday, January 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20Meetup%20-%20Bias%20Defenses%20from%20Influence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20Meetup%20-%20Bias%20Defenses%20from%20Influence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHHDL6GQfsXKWmDmW3%2Fmeetup-durham-meetup-bias-defenses-from-influence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20Meetup%20-%20Bias%20Defenses%20from%20Influence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHHDL6GQfsXKWmDmW3%2Fmeetup-durham-meetup-bias-defenses-from-influence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHHDL6GQfsXKWmDmW3%2Fmeetup-durham-meetup-bias-defenses-from-influence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vc'>Durham Meetup - Bias Defenses from Influence</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 January 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2411 N Roxboro St, Durham NC 27704</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will walk through some techniques to recognize and compensate for some biases from the book Influence, by Robert Cialdini.</p>\n\n<p>This Thursday 7:00 PM, at the House with the Redish Door, 2411 N Roxboro St, parking on Ellerbee. Hangouts, libations, and/or games to subsequently ensue in-situ or at Fullsteam.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vc'>Durham Meetup - Bias Defenses from Influence</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HHDL6GQfsXKWmDmW3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.5012932451355313e-06, "legacy": true, "legacyId": "25195", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_Meetup___Bias_Defenses_from_Influence\">Discussion article for the meetup : <a href=\"/meetups/vc\">Durham Meetup - Bias Defenses from Influence</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 January 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2411 N Roxboro St, Durham NC 27704</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will walk through some techniques to recognize and compensate for some biases from the book Influence, by Robert Cialdini.</p>\n\n<p>This Thursday 7:00 PM, at the House with the Redish Door, 2411 N Roxboro St, parking on Ellerbee. Hangouts, libations, and/or games to subsequently ensue in-situ or at Fullsteam.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_Meetup___Bias_Defenses_from_Influence1\">Discussion article for the meetup : <a href=\"/meetups/vc\">Durham Meetup - Bias Defenses from Influence</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham Meetup - Bias Defenses from Influence", "anchor": "Discussion_article_for_the_meetup___Durham_Meetup___Bias_Defenses_from_Influence", "level": 1}, {"title": "Discussion article for the meetup : Durham Meetup - Bias Defenses from Influence", "anchor": "Discussion_article_for_the_meetup___Durham_Meetup___Bias_Defenses_from_Influence1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-06T21:44:36.664Z", "modifiedAt": null, "url": null, "title": "January Monthly Bragging Thread", "slug": "january-monthly-bragging-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:36.570Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YRvDZCoktaN5vMpck/january-monthly-bragging-thread", "pageUrlRelative": "/posts/YRvDZCoktaN5vMpck/january-monthly-bragging-thread", "linkUrl": "https://www.lesswrong.com/posts/YRvDZCoktaN5vMpck/january-monthly-bragging-thread", "postedAtFormatted": "Monday, January 6th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20January%20Monthly%20Bragging%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJanuary%20Monthly%20Bragging%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYRvDZCoktaN5vMpck%2Fjanuary-monthly-bragging-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=January%20Monthly%20Bragging%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYRvDZCoktaN5vMpck%2Fjanuary-monthly-bragging-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYRvDZCoktaN5vMpck%2Fjanuary-monthly-bragging-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<p>As in Joshua Blaine's original description (below), but may be used to brag about things you've accomplished either this month (January) or the previous one (December), assuming that you haven't brought it up in any earlier Monthly Bragging Thread.</p>\n<blockquote>\n<p>In an attempt to encourage more people to <em>actually do awesome things</em> (a la instrumental rationality), I am proposing a new monthly thread (can be changed to bi-weekly, should that be demanded). Your job, should you choose to accept it, is to comment on this thread explaining <strong>the most awesome thing you've done this month</strong>. You may be as blatantly proud of yourself as you feel. You may unabashedly consider yourself <em>the coolest freaking person ever</em> because of that awesome thing you're dying to tell everyone about. This is the place to do just that.</p>\n<p>Remember, however, that this <strong>isn't</strong> any kind of progress thread. Nor is it any kind of proposal thread. <em>This thread is solely for people to talk about the awesomest thing they've done all month. not will do. not are working on</em>. <strong>have already done.</strong> This is to cultivate an environment of object level productivity rather than meta-productivity methods.</p>\n<p>So, what's the coolest thing you've done this month?</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YRvDZCoktaN5vMpck", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 1.5013223473532348e-06, "legacy": true, "legacyId": "25196", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-07T02:49:44.203Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston - Macroeconomic Theory (Joe Schneider)", "slug": "meetup-boston-macroeconomic-theory-joe-schneider", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K6Yz6CjXA5RPotw43/meetup-boston-macroeconomic-theory-joe-schneider", "pageUrlRelative": "/posts/K6Yz6CjXA5RPotw43/meetup-boston-macroeconomic-theory-joe-schneider", "linkUrl": "https://www.lesswrong.com/posts/K6Yz6CjXA5RPotw43/meetup-boston-macroeconomic-theory-joe-schneider", "postedAtFormatted": "Tuesday, January 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20-%20Macroeconomic%20Theory%20(Joe%20Schneider)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20-%20Macroeconomic%20Theory%20(Joe%20Schneider)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK6Yz6CjXA5RPotw43%2Fmeetup-boston-macroeconomic-theory-joe-schneider%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20-%20Macroeconomic%20Theory%20(Joe%20Schneider)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK6Yz6CjXA5RPotw43%2Fmeetup-boston-macroeconomic-theory-joe-schneider", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK6Yz6CjXA5RPotw43%2Fmeetup-boston-macroeconomic-theory-joe-schneider", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 236, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vd'>Boston - Macroeconomic Theory (Joe Schneider)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 January 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">MIT - 25 Ames St, Cambridge, MA, 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Joe Schneider will be discussing macroeconomic theory, and how to apply it to your world model. Some topics include:</p>\n\n<p>-different colors of money</p>\n\n<p>-idealistic supply/demand curves</p>\n\n<p>-special cases of supply/demand</p>\n\n<p>-actual official definitions (rather than LWisms)</p>\n\n<p>-how markets break (and what you can do about it)</p>\n\n<p>\"I am decidedly in the Keynesian camp, but I will include an explanation of supply side theory as well. It is important to at least understand how other people think before rejecting it. If you are a supply sider, please come and correct me.</p>\n\n<p>For those of you already familiar with macroeconomic theory, this will mostly be a review. However, I encourage you to come and share your insights on this critical topic.\"</p>\n\n<p>Cambridge/Boston-area Less Wrong 2nd and 4th Sunday meetups are at 2pm in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number. Meetups on other weeks are at Citadel in Porter Sq.</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 3pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vd'>Boston - Macroeconomic Theory (Joe Schneider)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K6Yz6CjXA5RPotw43", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.5016513454842334e-06, "legacy": true, "legacyId": "25198", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston___Macroeconomic_Theory__Joe_Schneider_\">Discussion article for the meetup : <a href=\"/meetups/vd\">Boston - Macroeconomic Theory (Joe Schneider)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 January 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">MIT - 25 Ames St, Cambridge, MA, 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Joe Schneider will be discussing macroeconomic theory, and how to apply it to your world model. Some topics include:</p>\n\n<p>-different colors of money</p>\n\n<p>-idealistic supply/demand curves</p>\n\n<p>-special cases of supply/demand</p>\n\n<p>-actual official definitions (rather than LWisms)</p>\n\n<p>-how markets break (and what you can do about it)</p>\n\n<p>\"I am decidedly in the Keynesian camp, but I will include an explanation of supply side theory as well. It is important to at least understand how other people think before rejecting it. If you are a supply sider, please come and correct me.</p>\n\n<p>For those of you already familiar with macroeconomic theory, this will mostly be a review. However, I encourage you to come and share your insights on this critical topic.\"</p>\n\n<p>Cambridge/Boston-area Less Wrong 2nd and 4th Sunday meetups are at 2pm in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number. Meetups on other weeks are at Citadel in Porter Sq.</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 3pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston___Macroeconomic_Theory__Joe_Schneider_1\">Discussion article for the meetup : <a href=\"/meetups/vd\">Boston - Macroeconomic Theory (Joe Schneider)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston - Macroeconomic Theory (Joe Schneider)", "anchor": "Discussion_article_for_the_meetup___Boston___Macroeconomic_Theory__Joe_Schneider_", "level": 1}, {"title": "Discussion article for the meetup : Boston - Macroeconomic Theory (Joe Schneider)", "anchor": "Discussion_article_for_the_meetup___Boston___Macroeconomic_Theory__Joe_Schneider_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-07T10:25:33.016Z", "modifiedAt": null, "url": null, "title": "Meetup : Meetup at CFAR, Wednesday: Nutritionally complete bread", "slug": "meetup-meetup-at-cfar-wednesday-nutritionally-complete-bread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:38.320Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9LKjSdoHW2YJM463t/meetup-meetup-at-cfar-wednesday-nutritionally-complete-bread", "pageUrlRelative": "/posts/9LKjSdoHW2YJM463t/meetup-meetup-at-cfar-wednesday-nutritionally-complete-bread", "linkUrl": "https://www.lesswrong.com/posts/9LKjSdoHW2YJM463t/meetup-meetup-at-cfar-wednesday-nutritionally-complete-bread", "postedAtFormatted": "Tuesday, January 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meetup%20at%20CFAR%2C%20Wednesday%3A%20Nutritionally%20complete%20bread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meetup%20at%20CFAR%2C%20Wednesday%3A%20Nutritionally%20complete%20bread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9LKjSdoHW2YJM463t%2Fmeetup-meetup-at-cfar-wednesday-nutritionally-complete-bread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meetup%20at%20CFAR%2C%20Wednesday%3A%20Nutritionally%20complete%20bread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9LKjSdoHW2YJM463t%2Fmeetup-meetup-at-cfar-wednesday-nutritionally-complete-bread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9LKjSdoHW2YJM463t%2Fmeetup-meetup-at-cfar-wednesday-nutritionally-complete-bread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 135, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ve'>Meetup at CFAR, Wednesday: Nutritionally complete bread</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 January 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2030 Addison, 3rd floor, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Due to popular demand, the Berkeley meetups will be held at the CFAR office from now on. This week, Romeo_Stevens will be showcasing their nutritionally complete bread. (Like Soylent, but better.) Romeo will answer your questions about nutrition, longevity, the bread business, and effective altruism. Also, you will get to taste their bread.</p>\n\n<p>Please arrive between 7pm and 7:30pm on Wednesday. Romeo's presentation will begin at 7:30pm. It won't take long, and we will hang out afterward. CFAR is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ve'>Meetup at CFAR, Wednesday: Nutritionally complete bread</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9LKjSdoHW2YJM463t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.5021430587447388e-06, "legacy": true, "legacyId": "25202", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meetup_at_CFAR__Wednesday__Nutritionally_complete_bread\">Discussion article for the meetup : <a href=\"/meetups/ve\">Meetup at CFAR, Wednesday: Nutritionally complete bread</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 January 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2030 Addison, 3rd floor, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Due to popular demand, the Berkeley meetups will be held at the CFAR office from now on. This week, Romeo_Stevens will be showcasing their nutritionally complete bread. (Like Soylent, but better.) Romeo will answer your questions about nutrition, longevity, the bread business, and effective altruism. Also, you will get to taste their bread.</p>\n\n<p>Please arrive between 7pm and 7:30pm on Wednesday. Romeo's presentation will begin at 7:30pm. It won't take long, and we will hang out afterward. CFAR is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meetup_at_CFAR__Wednesday__Nutritionally_complete_bread1\">Discussion article for the meetup : <a href=\"/meetups/ve\">Meetup at CFAR, Wednesday: Nutritionally complete bread</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meetup at CFAR, Wednesday: Nutritionally complete bread", "anchor": "Discussion_article_for_the_meetup___Meetup_at_CFAR__Wednesday__Nutritionally_complete_bread", "level": 1}, {"title": "Discussion article for the meetup : Meetup at CFAR, Wednesday: Nutritionally complete bread", "anchor": "Discussion_article_for_the_meetup___Meetup_at_CFAR__Wednesday__Nutritionally_complete_bread1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-07T17:48:11.363Z", "modifiedAt": null, "url": null, "title": "A big Singularity-themed Hollywood movie out in April offers many opportunities to talk about AI risk", "slug": "a-big-singularity-themed-hollywood-movie-out-in-april-offers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:39.277Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/68atEsN7jMzK7tGFp/a-big-singularity-themed-hollywood-movie-out-in-april-offers", "pageUrlRelative": "/posts/68atEsN7jMzK7tGFp/a-big-singularity-themed-hollywood-movie-out-in-april-offers", "linkUrl": "https://www.lesswrong.com/posts/68atEsN7jMzK7tGFp/a-big-singularity-themed-hollywood-movie-out-in-april-offers", "postedAtFormatted": "Tuesday, January 7th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20big%20Singularity-themed%20Hollywood%20movie%20out%20in%20April%20offers%20many%20opportunities%20to%20talk%20about%20AI%20risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20big%20Singularity-themed%20Hollywood%20movie%20out%20in%20April%20offers%20many%20opportunities%20to%20talk%20about%20AI%20risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F68atEsN7jMzK7tGFp%2Fa-big-singularity-themed-hollywood-movie-out-in-april-offers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20big%20Singularity-themed%20Hollywood%20movie%20out%20in%20April%20offers%20many%20opportunities%20to%20talk%20about%20AI%20risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F68atEsN7jMzK7tGFp%2Fa-big-singularity-themed-hollywood-movie-out-in-april-offers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F68atEsN7jMzK7tGFp%2Fa-big-singularity-themed-hollywood-movie-out-in-april-offers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 191, "htmlBody": "<p>There's a big Hollywood movie coming out with an apocalyptic Singularity-like story, called Transcendence. (<a href=\"http://www.imdb.com/title/tt2209764/\">IMDB</a>, <a href=\"http://en.wikipedia.org/wiki/Transcendence_%282014_film%29\">Wiki</a>, <a href=\"http://www.transcendencemovie.com/tagged/video\">official site</a>) With an A-list cast and big budget, I contend this movie is the front-runner to be 2014's most significant influence on discussions of superintelligence outside specialist circles. Anyone hoping to influence those discussions should start preparing some talking points.</p>\n<p>I don't see anybody here agree with me on this. The movie has been briefly <a href=\"/lw/h1b/link_transcendence_2014_a_movie_about/\">discussed on LW</a> when it was first announced in March 2013, but since then, only the trailer (out since December) has been <a href=\"/lw/jdt/open_thread_for_december_2431_2013/\">mentioned</a>. MIRI hasn't published a word about it. This amazes me. We have three months till millions of people who never considered superintelligence are going to start thinking about it - is nobody bothering to craft a response to the movie yet? Shouldn't there be something that lazy journalists, given the job to write about this movie, can find?</p>\n<p>Because if there isn't, they'll dismiss the danger of AI like Erik Sofge <a href=\"http://www.popsci.com/blog-network/zero-moment/her-smartest-movie-about-artificial-intelligence-years\">already did</a> in an early piece about the movie for Popular Science, and nudge their readers to do so too. And that'd be a shame, wouldn't it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "68atEsN7jMzK7tGFp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 49, "extendedScore": null, "score": 0.0005264750054754617, "legacy": true, "legacyId": "25204", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vwHBQKdYewXAMceWf", "Z43isMka9gvpZCnhF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-08T12:14:41.780Z", "modifiedAt": null, "url": null, "title": "Open Thread for January 8 - 16 2014", "slug": "open-thread-for-january-8-16-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:32.434Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tut", "createdAt": "2009-05-25T14:54:52.473Z", "isAdmin": false, "displayName": "tut"}, "userId": "HHzEcSBPJKmgx4mRp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vH6tTuiMyfzpHxSxY/open-thread-for-january-8-16-2014", "pageUrlRelative": "/posts/vH6tTuiMyfzpHxSxY/open-thread-for-january-8-16-2014", "linkUrl": "https://www.lesswrong.com/posts/vH6tTuiMyfzpHxSxY/open-thread-for-january-8-16-2014", "postedAtFormatted": "Wednesday, January 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%20for%20January%208%20-%2016%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%20for%20January%208%20-%2016%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvH6tTuiMyfzpHxSxY%2Fopen-thread-for-january-8-16-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%20for%20January%208%20-%2016%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvH6tTuiMyfzpHxSxY%2Fopen-thread-for-january-8-16-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvH6tTuiMyfzpHxSxY%2Fopen-thread-for-january-8-16-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>If it's worth saying but not worth its own thread even in discussion it goes here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vH6tTuiMyfzpHxSxY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.5038163462829032e-06, "legacy": true, "legacyId": "25211", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 347, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-08T13:12:53.132Z", "modifiedAt": null, "url": null, "title": "Meetup : London - The Schelling Point Strategy Game (plus socials)", "slug": "meetup-london-the-schelling-point-strategy-game-plus-socials", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:59.808Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sixes_and_sevens", "createdAt": "2009-11-11T14:42:23.502Z", "isAdmin": false, "displayName": "sixes_and_sevens"}, "userId": "n83meJ5yG2WQzygvw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LYrWRpAasPSwwKdit/meetup-london-the-schelling-point-strategy-game-plus-socials", "pageUrlRelative": "/posts/LYrWRpAasPSwwKdit/meetup-london-the-schelling-point-strategy-game-plus-socials", "linkUrl": "https://www.lesswrong.com/posts/LYrWRpAasPSwwKdit/meetup-london-the-schelling-point-strategy-game-plus-socials", "postedAtFormatted": "Wednesday, January 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20-%20The%20Schelling%20Point%20Strategy%20Game%20(plus%20socials)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20-%20The%20Schelling%20Point%20Strategy%20Game%20(plus%20socials)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYrWRpAasPSwwKdit%2Fmeetup-london-the-schelling-point-strategy-game-plus-socials%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20-%20The%20Schelling%20Point%20Strategy%20Game%20(plus%20socials)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYrWRpAasPSwwKdit%2Fmeetup-london-the-schelling-point-strategy-game-plus-socials", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLYrWRpAasPSwwKdit%2Fmeetup-london-the-schelling-point-strategy-game-plus-socials", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 383, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/vf\">London - The Schelling Point Strategy Game (plus socials)</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">19 January 2014 02:00:00PM (+0000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>For the first London practical session of 2014, we will be playing The <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Focal_point_(game_theory)\">Schelling Point</a> Strategy Game on the 19th of January. All attendees will have to deduce where all other attendees are going to meet in order to participate.</p>\n<p>(Hint: it's probably the listed venue. Please don't try and look for us at the information desk at Grand Central Station in New York. We will not be there.)</p>\n<p>(Just to be absolutely sure, in case that joke fell completely flat, we will be meeting at the listed, regular venue, the Shakespeare's Head pub in Holborn. Go there.)</p>\n<p>The Schelling Point Strategy Game will feature two teams in competition with each other. Not only must each team come up with clever strategies to coordinate amongst themselves on multiple-choice problems, they must also construct fiendish problems to confound the other team's strategies.</p>\n<p>No previous knowledge of game theory is required. The session will begin with a brief practical introduction to the relevant concepts before dividing into teams. Once the game is concluded we will spend some time discussing what people came up with. There is typically social / unstructured discussion afterwards.</p>\n<p>Please be in attendance by 2pm, as we will be starting shortly thereafter. If you have problems finding the venue or locating us in there, ring 07887 718458 and we'll come and find you.</p>\n<p><strong>Plus Socials on the 12th and 26th</strong></p>\n<p>We are also having social meetups on Sunday 12th of January and Sunday 26th of January, also at 2pm at the same venue. These are unstructured gatherings without a set agenda or discussion topic. Come along and talk about interesting things with interesting people. We make efforts to welcome newcomers and visitors.</p>\n<p>(For the past few months, with the exception of a break over the festive season, we have by default been holding social meetups every Sunday. These dates are included in this post to avoid weekly spamming of Discussion.)</p>\n<p>We also have a <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">Google Group</a> where we organise things and discuss local LW-related matters. If you live in / near London or you're a regular visitor, joining could be worth your while.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/vf\">London - The Schelling Point Strategy Game (plus socials)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LYrWRpAasPSwwKdit", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 1.5038792627302655e-06, "legacy": true, "legacyId": "25212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London___The_Schelling_Point_Strategy_Game__plus_socials_\">Discussion article for the meetup : <a href=\"/meetups/vf\">London - The Schelling Point Strategy Game (plus socials)</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">19 January 2014 02:00:00PM (+0000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Shakespeare's Head, Holborn, WC2B 6BG</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>For the first London practical session of 2014, we will be playing The <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Focal_point_(game_theory)\">Schelling Point</a> Strategy Game on the 19th of January. All attendees will have to deduce where all other attendees are going to meet in order to participate.</p>\n<p>(Hint: it's probably the listed venue. Please don't try and look for us at the information desk at Grand Central Station in New York. We will not be there.)</p>\n<p>(Just to be absolutely sure, in case that joke fell completely flat, we will be meeting at the listed, regular venue, the Shakespeare's Head pub in Holborn. Go there.)</p>\n<p>The Schelling Point Strategy Game will feature two teams in competition with each other. Not only must each team come up with clever strategies to coordinate amongst themselves on multiple-choice problems, they must also construct fiendish problems to confound the other team's strategies.</p>\n<p>No previous knowledge of game theory is required. The session will begin with a brief practical introduction to the relevant concepts before dividing into teams. Once the game is concluded we will spend some time discussing what people came up with. There is typically social / unstructured discussion afterwards.</p>\n<p>Please be in attendance by 2pm, as we will be starting shortly thereafter. If you have problems finding the venue or locating us in there, ring 07887 718458 and we'll come and find you.</p>\n<p><strong id=\"Plus_Socials_on_the_12th_and_26th\">Plus Socials on the 12th and 26th</strong></p>\n<p>We are also having social meetups on Sunday 12th of January and Sunday 26th of January, also at 2pm at the same venue. These are unstructured gatherings without a set agenda or discussion topic. Come along and talk about interesting things with interesting people. We make efforts to welcome newcomers and visitors.</p>\n<p>(For the past few months, with the exception of a break over the festive season, we have by default been holding social meetups every Sunday. These dates are included in this post to avoid weekly spamming of Discussion.)</p>\n<p>We also have a <a href=\"https://groups.google.com/forum/#!forum/lesswronglondon\">Google Group</a> where we organise things and discuss local LW-related matters. If you live in / near London or you're a regular visitor, joining could be worth your while.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___London___The_Schelling_Point_Strategy_Game__plus_socials_1\">Discussion article for the meetup : <a href=\"/meetups/vf\">London - The Schelling Point Strategy Game (plus socials)</a></h2>", "sections": [{"title": "Discussion article for the meetup : London - The Schelling Point Strategy Game (plus socials)", "anchor": "Discussion_article_for_the_meetup___London___The_Schelling_Point_Strategy_Game__plus_socials_", "level": 1}, {"title": "Plus Socials on the 12th and 26th", "anchor": "Plus_Socials_on_the_12th_and_26th", "level": 2}, {"title": "Discussion article for the meetup : London - The Schelling Point Strategy Game (plus socials)", "anchor": "Discussion_article_for_the_meetup___London___The_Schelling_Point_Strategy_Game__plus_socials_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-08T17:26:16.266Z", "modifiedAt": null, "url": null, "title": "Some thoughts on having children", "slug": "some-thoughts-on-having-children", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:37.448Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pianoforte611", "createdAt": "2012-05-01T15:15:46.011Z", "isAdmin": false, "displayName": "pianoforte611"}, "userId": "krJ7Se3Mhp9nTX6Hi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mkf7ypjtN2hkkSrdg/some-thoughts-on-having-children", "pageUrlRelative": "/posts/Mkf7ypjtN2hkkSrdg/some-thoughts-on-having-children", "linkUrl": "https://www.lesswrong.com/posts/Mkf7ypjtN2hkkSrdg/some-thoughts-on-having-children", "postedAtFormatted": "Wednesday, January 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20thoughts%20on%20having%20children&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20thoughts%20on%20having%20children%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMkf7ypjtN2hkkSrdg%2Fsome-thoughts-on-having-children%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20thoughts%20on%20having%20children%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMkf7ypjtN2hkkSrdg%2Fsome-thoughts-on-having-children", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMkf7ypjtN2hkkSrdg%2Fsome-thoughts-on-having-children", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1563, "htmlBody": "<p>Disclaimer: I am not a parent.</p>\n<p>I've seen a bit of discussion here on whether or not to have children. Most of the discussion that I have seen are about the moral case, but there are factors as well. I'd like to talk about three aspects of parenting that I suspect are the main reasons why people choose to have kids or not: the financial case, the moral case, and the practical case (for lack of a better term). The financial case is straightforward - how expensive is raising kids? The moral case has to do with the best use of resources: is it better to divert resources away from having kids towards charity? The practical case has to do with the actual process of being a parent - the effort it takes and the sense of responsibility.&nbsp;</p>\n<p><strong>The Practical Case</strong></p>\n<p>I suspect that the main reason for why people don't have kids is because they think that kids are a lot of responsibility because:</p>\n<p>1) It takes a lot of work and effort to raise children - effort that could be spent on other activities.</p>\n<p>2) Great parenting is extremely important for raising well adjusted, intelligent kids that will grow up to be successful and likable adults.&nbsp;</p>\n<p>Regarding 1) yes kids do take a lot of time and effort, but that's not necessarily a bad thing - lots of things that are rewarding require a lot of effort, such as learning a language or a new skill. I don't know what its like to a parent so I won't say much more on this topic.</p>\n<p>Regarding 2) it is actually far from a settled question whether parenting style significantly affects the kind of person that your child will grow up to be. There has been <a href=\"/lw/col/review_selfish_reasons_to_have_more_kids\">some</a>&nbsp;<a href=\"/lw/f9s/less_wrong_parents/7qoi\">discussion</a> here on the effects of parenting on children. The tentative consensus seems to be that within the range of normal parenting, parenting style has only small impact life outcomes pertaining to happiness, personality, educational achievement. That doesn't mean that how you treat your child doesn't matter. Steven Pinker puts it quite nicely:</p>\n<blockquote>\n<p>Judith Rich Harris is coming out with a book called The Nurture Assumption which argues that parents don&rsquo;t influence the long-term fates of their children; peers do. The reaction she often gets is, &ldquo;So are you saying it doesn&rsquo;t matter how I treat my child?&rdquo; She points out that this is like someone learning that you can&rsquo;t change the personality of your spouse and asking, &ldquo;So are you saying that it doesn&rsquo;t matter how you treat my spouse?&rdquo; People seem to think that the only reason to be nice to children is that it will mold their character as adults in the future &mdash; as opposed to the common-sense idea that you should be nice to people because it makes life better for them in the present. Child rearing has become a technological matter of which practices grow the best children, as opposed to a human relationship in which the happiness of the child (during childhood) is determined by how the child is treated. She has a wonderful quote: &ldquo;We may not control our children&rsquo;s tomorrows, but we surely control their todays, and we have the capacity to make them very, very miserable.&rdquo;</p>\n</blockquote>\n<p>The message I would take away is not to worry too much about creating an optimal child. Don't worry about finding the optimal set of extra-curricular activities or the perfect balance of authoritarianism and permissiveness. Instead, try to cultivate a healthy relationship with your child and most of all enjoy the parenting process.</p>\n<p><strong>The Financial Case</strong></p>\n<p>In agarian societies (and most societies quite frankly) children were/are cheap, in some cases free labor and a life insurance policy for when you retire. But in the post-industrial Western world that is no longer the case. For a middle-upper class family, having a child is a very large cost for two reasons: the first is that children cost a lot of money to raise. The second reason is that having a child might hold you back from advancing your career as much as you would have been able to do otherwise. I will focus on the first problem here. According to the United States department of Agriculture, the average cost of raising a child to age 18 was <a href=\"http://www.cnpp.usda.gov/Publications/CRC/crc2012.pdf\">about</a> $241,080 (in 2012 dollars). This doesn't count the cost of college which can exceed $250,000 at elite institutions. I'll assume the $250,000 figure for the purposes of the following calculations.</p>\n<p>Assuming that you are able to invest your money at a modest 5% rate of return, this amounts to having to put aside $8887 each year from your child's birth for college only, and approximately $13,000 (2012 dollars) per year on other expenses such as housing and food. That $13,000 per year figure does not account for inflation and in reality that figure would grow each year but this is just to provide a rough ball-park figure. This figure goes up if you have more than one child but the per child cost goes down.</p>\n<p>This brings up the issue of whether or not you \"owe\" your child an all expenses paid college education. I wouldn't rule out only paying partially for your child's college education especially since this calculation assumes only one child. I would be interested to hear more thoughts on this matter.</p>\n<p><strong>The Moral Case</strong></p>\n<p>Some effective altruists have <a href=\"/lw/ive/is_it_immoral_to_have_children/\">advanced</a> the idea that having children is immoral because the money spent on having kids would be better spent by donating it to charity. This assumes utilitarianism, and indeed if GiveWell recommended charities were perfect or even pretty good util maximizers then this argument would succeed, since by design whatever they did would be the best use of money under utilitarianism. However, I do not believe that this is the case. GiveWell recommended charities that focus almost exclusively on public health initiatives, and exclusively focus on providing aid to the poorest countries. While a simple diminishing marginal returns argument might suggest that this is the lowest hanging fruit and hence the best use of money there are other things that need to be considered.</p>\n<p>As Apprentice points out the heritability of prosocial behaviors such as cooperativeness, empathy and altruism <a href=\"http://scan.oxfordjournals.org/content/early/2010/10/28/scan.nsq083.full\">is</a> 0.5, and I think most people here are aware that IQ has a heritability around that number as well and is a pretty good predictor of life outcomes. If you want to increase the number of people in the world that are like yourself, then having children is a great way of doing so. This is particularly important since high IQ college educated individuals in Western countries have fertility rates that are below replacement levels and are some of the lowest in the world.</p>\n<p>Rachels anticipates this argument by pointing out than one child is unlikely to produce the same returns as an investment in charity. I believe this is a mistake because it is short sighted. If you stop the utilitarian analysis at one generation into the future then yes having a smart altruistic child will not give the same returns as saving lives through charity, however consequentialism need not be short sighted. If you have more than one child, and/or if your children have children then the returns get magnified significantly - and it is worth noting that intelligent people contribute a lot to society not just through charity but through their work as well. Moreover, the people you would save by donating to charity would also have children and those children would have children all of whom might require yet more aid in the future. Thus the short term gains in QALYs that giving to GiveWell recommended charities provides lead to a long term drain of resources and human capital. And as I have already mentioned, intelligent people already have the lowest fertility in society, I'd rather not see it go even lower.</p>\n<p>Jeff Kaufman provides two counterarguments that caught my eye: that this is an argument for sperm donation rather than having children; and that genetic engineering will solve the dysgenic fertility problem. However, sperm banks are already eugenic (in a sense) and it is fairly easy to saturate the supply of high quality sperm. Sperm donation is good idea for highly intelligent individuals (and to my surprise there are actually sperm donor shortages in some parts of the world making it an even better idea), but it is not a substitute for having children - the bottleneck quickly becomes the demand for said sperm. This is certainly a potential area worth investigating as a light form of eugenics, but I don't know of anyone who's trying to market eugenic sperm donation right now. With regard to genetic engineering, I have serious doubts that the field will develop to the point of commercialization in the next hundred years, and I have even stronger doubts that it will be widely accepted and used. While I realize that prediction of the future is very difficult, I would be very surprised if in a hundred years the average Joe will think about having genetically engineered children. Any mention of eugenics already invokes fear in the hearts of most people, and its pretty hard to deny that genetically engineering babies is the scariest kind of eugenics. Human genetic engineering might well solve the dysgenic problem, but I wouldn't bet strongly on that happening any time soon, whereas having children is an almost guaranteed way of helping to solve the problem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b7ZSAGimsbzrLR5CR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mkf7ypjtN2hkkSrdg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 5, "extendedScore": null, "score": 1.5041532875926745e-06, "legacy": true, "legacyId": "25199", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Disclaimer: I am not a parent.</p>\n<p>I've seen a bit of discussion here on whether or not to have children. Most of the discussion that I have seen are about the moral case, but there are factors as well. I'd like to talk about three aspects of parenting that I suspect are the main reasons why people choose to have kids or not: the financial case, the moral case, and the practical case (for lack of a better term). The financial case is straightforward - how expensive is raising kids? The moral case has to do with the best use of resources: is it better to divert resources away from having kids towards charity? The practical case has to do with the actual process of being a parent - the effort it takes and the sense of responsibility.&nbsp;</p>\n<p><strong id=\"The_Practical_Case\">The Practical Case</strong></p>\n<p>I suspect that the main reason for why people don't have kids is because they think that kids are a lot of responsibility because:</p>\n<p>1) It takes a lot of work and effort to raise children - effort that could be spent on other activities.</p>\n<p>2) Great parenting is extremely important for raising well adjusted, intelligent kids that will grow up to be successful and likable adults.&nbsp;</p>\n<p>Regarding 1) yes kids do take a lot of time and effort, but that's not necessarily a bad thing - lots of things that are rewarding require a lot of effort, such as learning a language or a new skill. I don't know what its like to a parent so I won't say much more on this topic.</p>\n<p>Regarding 2) it is actually far from a settled question whether parenting style significantly affects the kind of person that your child will grow up to be. There has been <a href=\"/lw/col/review_selfish_reasons_to_have_more_kids\">some</a>&nbsp;<a href=\"/lw/f9s/less_wrong_parents/7qoi\">discussion</a> here on the effects of parenting on children. The tentative consensus seems to be that within the range of normal parenting, parenting style has only small impact life outcomes pertaining to happiness, personality, educational achievement. That doesn't mean that how you treat your child doesn't matter. Steven Pinker puts it quite nicely:</p>\n<blockquote>\n<p>Judith Rich Harris is coming out with a book called The Nurture Assumption which argues that parents don\u2019t influence the long-term fates of their children; peers do. The reaction she often gets is, \u201cSo are you saying it doesn\u2019t matter how I treat my child?\u201d She points out that this is like someone learning that you can\u2019t change the personality of your spouse and asking, \u201cSo are you saying that it doesn\u2019t matter how you treat my spouse?\u201d People seem to think that the only reason to be nice to children is that it will mold their character as adults in the future \u2014 as opposed to the common-sense idea that you should be nice to people because it makes life better for them in the present. Child rearing has become a technological matter of which practices grow the best children, as opposed to a human relationship in which the happiness of the child (during childhood) is determined by how the child is treated. She has a wonderful quote: \u201cWe may not control our children\u2019s tomorrows, but we surely control their todays, and we have the capacity to make them very, very miserable.\u201d</p>\n</blockquote>\n<p>The message I would take away is not to worry too much about creating an optimal child. Don't worry about finding the optimal set of extra-curricular activities or the perfect balance of authoritarianism and permissiveness. Instead, try to cultivate a healthy relationship with your child and most of all enjoy the parenting process.</p>\n<p><strong id=\"The_Financial_Case\">The Financial Case</strong></p>\n<p>In agarian societies (and most societies quite frankly) children were/are cheap, in some cases free labor and a life insurance policy for when you retire. But in the post-industrial Western world that is no longer the case. For a middle-upper class family, having a child is a very large cost for two reasons: the first is that children cost a lot of money to raise. The second reason is that having a child might hold you back from advancing your career as much as you would have been able to do otherwise. I will focus on the first problem here. According to the United States department of Agriculture, the average cost of raising a child to age 18 was <a href=\"http://www.cnpp.usda.gov/Publications/CRC/crc2012.pdf\">about</a> $241,080 (in 2012 dollars). This doesn't count the cost of college which can exceed $250,000 at elite institutions. I'll assume the $250,000 figure for the purposes of the following calculations.</p>\n<p>Assuming that you are able to invest your money at a modest 5% rate of return, this amounts to having to put aside $8887 each year from your child's birth for college only, and approximately $13,000 (2012 dollars) per year on other expenses such as housing and food. That $13,000 per year figure does not account for inflation and in reality that figure would grow each year but this is just to provide a rough ball-park figure. This figure goes up if you have more than one child but the per child cost goes down.</p>\n<p>This brings up the issue of whether or not you \"owe\" your child an all expenses paid college education. I wouldn't rule out only paying partially for your child's college education especially since this calculation assumes only one child. I would be interested to hear more thoughts on this matter.</p>\n<p><strong id=\"The_Moral_Case\">The Moral Case</strong></p>\n<p>Some effective altruists have <a href=\"/lw/ive/is_it_immoral_to_have_children/\">advanced</a> the idea that having children is immoral because the money spent on having kids would be better spent by donating it to charity. This assumes utilitarianism, and indeed if GiveWell recommended charities were perfect or even pretty good util maximizers then this argument would succeed, since by design whatever they did would be the best use of money under utilitarianism. However, I do not believe that this is the case. GiveWell recommended charities that focus almost exclusively on public health initiatives, and exclusively focus on providing aid to the poorest countries. While a simple diminishing marginal returns argument might suggest that this is the lowest hanging fruit and hence the best use of money there are other things that need to be considered.</p>\n<p>As Apprentice points out the heritability of prosocial behaviors such as cooperativeness, empathy and altruism <a href=\"http://scan.oxfordjournals.org/content/early/2010/10/28/scan.nsq083.full\">is</a> 0.5, and I think most people here are aware that IQ has a heritability around that number as well and is a pretty good predictor of life outcomes. If you want to increase the number of people in the world that are like yourself, then having children is a great way of doing so. This is particularly important since high IQ college educated individuals in Western countries have fertility rates that are below replacement levels and are some of the lowest in the world.</p>\n<p>Rachels anticipates this argument by pointing out than one child is unlikely to produce the same returns as an investment in charity. I believe this is a mistake because it is short sighted. If you stop the utilitarian analysis at one generation into the future then yes having a smart altruistic child will not give the same returns as saving lives through charity, however consequentialism need not be short sighted. If you have more than one child, and/or if your children have children then the returns get magnified significantly - and it is worth noting that intelligent people contribute a lot to society not just through charity but through their work as well. Moreover, the people you would save by donating to charity would also have children and those children would have children all of whom might require yet more aid in the future. Thus the short term gains in QALYs that giving to GiveWell recommended charities provides lead to a long term drain of resources and human capital. And as I have already mentioned, intelligent people already have the lowest fertility in society, I'd rather not see it go even lower.</p>\n<p>Jeff Kaufman provides two counterarguments that caught my eye: that this is an argument for sperm donation rather than having children; and that genetic engineering will solve the dysgenic fertility problem. However, sperm banks are already eugenic (in a sense) and it is fairly easy to saturate the supply of high quality sperm. Sperm donation is good idea for highly intelligent individuals (and to my surprise there are actually sperm donor shortages in some parts of the world making it an even better idea), but it is not a substitute for having children - the bottleneck quickly becomes the demand for said sperm. This is certainly a potential area worth investigating as a light form of eugenics, but I don't know of anyone who's trying to market eugenic sperm donation right now. With regard to genetic engineering, I have serious doubts that the field will develop to the point of commercialization in the next hundred years, and I have even stronger doubts that it will be widely accepted and used. While I realize that prediction of the future is very difficult, I would be very surprised if in a hundred years the average Joe will think about having genetically engineered children. Any mention of eugenics already invokes fear in the hearts of most people, and its pretty hard to deny that genetically engineering babies is the scariest kind of eugenics. Human genetic engineering might well solve the dysgenic problem, but I wouldn't bet strongly on that happening any time soon, whereas having children is an almost guaranteed way of helping to solve the problem.</p>", "sections": [{"title": "The Practical Case", "anchor": "The_Practical_Case", "level": 1}, {"title": "The Financial Case", "anchor": "The_Financial_Case", "level": 1}, {"title": "The Moral Case", "anchor": "The_Moral_Case", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "107 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 107, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nzsHQzsvwLw6g4pyE", "qrP9Rawdq87RpwSBQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-08T18:11:51.690Z", "modifiedAt": null, "url": null, "title": "[Link] Consciousness as a State of Matter (Max Tegmark)", "slug": "link-consciousness-as-a-state-of-matter-max-tegmark", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.289Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "vpeitoi89GPGat77P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fwv7zkChRzjCSiidH/link-consciousness-as-a-state-of-matter-max-tegmark", "pageUrlRelative": "/posts/Fwv7zkChRzjCSiidH/link-consciousness-as-a-state-of-matter-max-tegmark", "linkUrl": "https://www.lesswrong.com/posts/Fwv7zkChRzjCSiidH/link-consciousness-as-a-state-of-matter-max-tegmark", "postedAtFormatted": "Wednesday, January 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Consciousness%20as%20a%20State%20of%20Matter%20(Max%20Tegmark)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Consciousness%20as%20a%20State%20of%20Matter%20(Max%20Tegmark)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwv7zkChRzjCSiidH%2Flink-consciousness-as-a-state-of-matter-max-tegmark%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Consciousness%20as%20a%20State%20of%20Matter%20(Max%20Tegmark)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwv7zkChRzjCSiidH%2Flink-consciousness-as-a-state-of-matter-max-tegmark", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwv7zkChRzjCSiidH%2Flink-consciousness-as-a-state-of-matter-max-tegmark", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>Max Tegmark <a href=\"http://arxiv.org/abs/1401.1219\">publishes a preprint of a paper</a> arguing from physical principles that consciousness is &ldquo;what information processing feels like from the inside,&rdquo; a position I've <a href=\"/lw/qx/timeless_identity/9txe\">previously</a> <a href=\"/lw/iya/singularity_or_bust_full_documentary/a0fw\">articulated</a> on lesswrong. It's a very physics-rich paper, but here's the most accessable description I was able to find within it:</p>\n<blockquote>\n<p>If we understood consciousness as a physical phenomenon, we could in principle answer all of these questions [about consciousness] by studying the equations of physics: we could identify all conscious entities in any physical system, and calculate what they would perceive. However, this approach is typically not pursued by physicists, with the argument that we do not understand consciousness well enough.</p>\n<p>In this paper, I argue that recent progress in neuroscience has fundamentally changed this situation, and that we physicists can no longer blame neuroscientists for our own lack of progress. I have long contended that consciousness is the way information feels when being processed in certain complex ways, i.e., that it corresponds to certain complex patterns in spacetime that obey the same laws of physics as other complex systems, with no \"secret sauce\" required.</p>\n</blockquote>\n<p>The whole paper is very rich, and worth a read.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fwv7zkChRzjCSiidH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 22, "extendedScore": null, "score": 1.5042026008625079e-06, "legacy": true, "legacyId": "25213", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-08T19:23:47.830Z", "modifiedAt": null, "url": null, "title": "Literature-review on cognitive effects of modafinil (my bachelor thesis) ", "slug": "literature-review-on-cognitive-effects-of-modafinil-my", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:37.460Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wallowinmaya", "createdAt": "2011-03-21T00:39:18.855Z", "isAdmin": false, "displayName": "David Althaus"}, "userId": "xY8DDzk6TyvRroJEo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dkqwQAgLemS6vBdRk/literature-review-on-cognitive-effects-of-modafinil-my", "pageUrlRelative": "/posts/dkqwQAgLemS6vBdRk/literature-review-on-cognitive-effects-of-modafinil-my", "linkUrl": "https://www.lesswrong.com/posts/dkqwQAgLemS6vBdRk/literature-review-on-cognitive-effects-of-modafinil-my", "postedAtFormatted": "Wednesday, January 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Literature-review%20on%20cognitive%20effects%20of%20modafinil%20(my%20bachelor%20thesis)%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALiterature-review%20on%20cognitive%20effects%20of%20modafinil%20(my%20bachelor%20thesis)%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkqwQAgLemS6vBdRk%2Fliterature-review-on-cognitive-effects-of-modafinil-my%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Literature-review%20on%20cognitive%20effects%20of%20modafinil%20(my%20bachelor%20thesis)%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkqwQAgLemS6vBdRk%2Fliterature-review-on-cognitive-effects-of-modafinil-my", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkqwQAgLemS6vBdRk%2Fliterature-review-on-cognitive-effects-of-modafinil-my", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 810, "htmlBody": "<p>Modafinil is probably the most popular cognitive enhancer. LessWrong seems <a href=\"/lw/10u/ask_lesswrong_human_cognitive_enhancement_now/v6e\">pretty</a> <a href=\"/lw/dbc/anybody_know_some_good_modafinil_suppliers/\">interested</a> <a href=\"/lw/bes/generic_modafinil_sales_begin/\">in</a> <a href=\"/\">it</a>.&nbsp;The incredible Gwern&nbsp;<a href=\"http://www.gwern.net/Modafinil\">wrote an excellent and extensive article about it</a>.&nbsp;</p>\n<p>Of all the stimulants I tried, modafinil is my favorite one. There are more powerful substances like e.g. amphetamine or methylphenidate, but modafinil has much less negative effects on physical as well as mental health and is far less addictive. All things considered, the cost-benefit-ratio of modafinil is unparalleled.&nbsp;</p>\n<p>For those reasons&nbsp;I decided to publish my bachelor thesis on the cognitive effects of modafinil in healthy, non-sleep deprived individuals on LessWrong. Forgive me its shortcomings.&nbsp;</p>\n<p>Here are some relevant quotes:</p>\n<h3>Introduction:</h3>\n<blockquote>\n<div title=\"Page 4\">\n<p>...the main research question of this thesis is if and to what extent modafinil has positive effects on cognitive performance (operationalized as performance improvements in a variety of cognitive tests) in healthy, non-sleep deprived individuals.... The abuse liability and adverse effects of modafinil are also discussed. A literature research of all available, randomized, placebo-controlled, double-blind studies which examined those effects was therefore conducted.</p>\n</div>\n</blockquote>\n<h3>Overview of effects in healthy individuals:</h3>\n<blockquote>\n<div title=\"Page 5\">\n<p>...Altogether 19 randomized, double-blind, placebo-controlled studies about the effects of modafinil on cognitive functioning in healthy, non sleep-deprived individuals were reviewed. One of them (Randall et al., 2005b) was a retrospect analysis of 2 other studies (Randall et al., 2002 and 2005a), so 18 independent studies remain.</p>\n<p>Out of the 19 studies, 14 found performance improvements in at least one of the administered cognitive tests through modafinil in healthy volunteers.<br />Modafinil significantly improved performance in 26 out of 102 cognitive tests, but significantly decreased performance in 3 cognitive tests.</p>\n<p>...Several studies suggest that modafinil is only effective in subjects with lower IQ or lower baseline performance (Randall et al., 2005b; M&uuml;ller et al., 2004; Finke et al., 2010). Significant differences between modafinil and placebo also often only emerge in the most difficult conditions of cognitive tests (M&uuml;ller et al., 2004; M&uuml;ller et al., 2012; Winder-Rhodes et al., 2010; Marchant et al., 2009).</p>\n</div>\n</blockquote>\n<h3>Adverse effects:</h3>\n<blockquote>\n<div title=\"Page 5\">\n<p>...A study by Wong et al. (1999) of 32 healthy, male volunteers showed that the most frequently observed adverse effects among modafinil subjects were headache (34%), followed by insomnia, palpitations and anxiety (each occurring in 21% of participants). Adverse events were clearly dose- dependent: 50%, 83%, 100% and 100% of the participants in the 200 mg, 400 mg, 600 mg, and 800 mg dose groups respectively experienced at least one adverse event. According to the authors of this study the maximal safe dosage of modafinil is 600 mg.</p>\n</div>\n</blockquote>\n<h3>Abuse potential:</h3>\n<blockquote>\n<div title=\"Page 5\">\n<p>...Using a randomized, double-blind, placebo-controlled design Rush et al. (2002) examined subjective and behavioral effects of cocaine (100, 200 or 300 mg), modafinil (200, 400 or 600 mg) and placebo in cocaine users&hellip;.Of note, while subjects taking cocaine were willing to pay $3 for 100 mg, $6 for 200 mg and $10 for 300 mg cocaine, participants on modafinil were willing to pay $2, regardless of the dose. These results suggest that modafinil has a low abuse liability, but the rather small sample size (n=9) limits the validity of this study.</p>\n<p>The study by Marchant et al. (2009) which is discussed in more detail in part 2.4.12 found that subjects receiving modafinil were significantly less (p&lt;0,05) content than subjects receiving placebo which indicates a low abuse potential of modafinil. In contrast, in a study by M&uuml;ller et al. (2012) which is also discussed in more detail above, modafinil significantly increased (p&lt;0,05) ratings of \"task-enjoyment\" which may suggest a moderate potential for abuse.</p>\n</div>\n<div title=\"Page 47\">\n<p>...Overall, these results indicate that although modafinil promotes wakefulness, its effects are distinct from those of more typical stimulants like amphetamine and methylphenidate and more similar to the effects of caffeine which suggests a relatively low abuse liability.</p>\n</div>\n</blockquote>\n<h3>Conclusion:</h3>\n<blockquote>\n<div title=\"Page 48\">\n<p>In healthy individuals modafinil seems to improve cognitive performance, especially on the Stroop Task, stop-signal and serial reaction time tasks and tests of visual memory, working memory, spatial planning ability and sustained attention. However, these cognitive enhancing effects did only emerge in a subset of the reviewed studies. Additionally, significant performance increases may be limited to subjects with low baseline performance. Modafinil also appears to have detrimental effects on mental flexibility.</p>\n<p>...The abuse liability of modafinil seems to be small, particularly in comparison with other stimulants such as amphetamine and methylphenidate. Headache and insomnia are the most common adverse effects of modafinil.</p>\n<p>...Because several studies suggest that modafinil may only provide substantial beneficial effects to individuals with low baseline performance, ultimately the big question remains if modafinil can really improve the cognitive performance of already high-functioning, healthy individuals. Only in the latter case modafinil can justifiably be called a genuine cognitive enhancer.</p>\n</div>\n</blockquote>\n<p>You can download the whole thing below. (Just skip the sections on substance-dependent individuals and patients with dementia. My professor wanted them.)</p>\n<p><a href=\"http://wallowinmaya.files.wordpress.com/2013/12/bachelorarbeit-david-althaus-finale-version.pdf\">Effects of modafinil on cognitive performance in healthy individuals, substance-dependent individuals and patients with dementia</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 3, "e6j9tnTfaabjCgK6d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dkqwQAgLemS6vBdRk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 53, "extendedScore": null, "score": 1.5042804165494314e-06, "legacy": true, "legacyId": "25206", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Modafinil is probably the most popular cognitive enhancer. LessWrong seems <a href=\"/lw/10u/ask_lesswrong_human_cognitive_enhancement_now/v6e\">pretty</a> <a href=\"/lw/dbc/anybody_know_some_good_modafinil_suppliers/\">interested</a> <a href=\"/lw/bes/generic_modafinil_sales_begin/\">in</a> <a href=\"/\">it</a>.&nbsp;The incredible Gwern&nbsp;<a href=\"http://www.gwern.net/Modafinil\">wrote an excellent and extensive article about it</a>.&nbsp;</p>\n<p>Of all the stimulants I tried, modafinil is my favorite one. There are more powerful substances like e.g. amphetamine or methylphenidate, but modafinil has much less negative effects on physical as well as mental health and is far less addictive. All things considered, the cost-benefit-ratio of modafinil is unparalleled.&nbsp;</p>\n<p>For those reasons&nbsp;I decided to publish my bachelor thesis on the cognitive effects of modafinil in healthy, non-sleep deprived individuals on LessWrong. Forgive me its shortcomings.&nbsp;</p>\n<p>Here are some relevant quotes:</p>\n<h3 id=\"Introduction_\">Introduction:</h3>\n<blockquote>\n<div title=\"Page 4\">\n<p>...the main research question of this thesis is if and to what extent modafinil has positive effects on cognitive performance (operationalized as performance improvements in a variety of cognitive tests) in healthy, non-sleep deprived individuals.... The abuse liability and adverse effects of modafinil are also discussed. A literature research of all available, randomized, placebo-controlled, double-blind studies which examined those effects was therefore conducted.</p>\n</div>\n</blockquote>\n<h3 id=\"Overview_of_effects_in_healthy_individuals_\">Overview of effects in healthy individuals:</h3>\n<blockquote>\n<div title=\"Page 5\">\n<p>...Altogether 19 randomized, double-blind, placebo-controlled studies about the effects of modafinil on cognitive functioning in healthy, non sleep-deprived individuals were reviewed. One of them (Randall et al., 2005b) was a retrospect analysis of 2 other studies (Randall et al., 2002 and 2005a), so 18 independent studies remain.</p>\n<p>Out of the 19 studies, 14 found performance improvements in at least one of the administered cognitive tests through modafinil in healthy volunteers.<br>Modafinil significantly improved performance in 26 out of 102 cognitive tests, but significantly decreased performance in 3 cognitive tests.</p>\n<p>...Several studies suggest that modafinil is only effective in subjects with lower IQ or lower baseline performance (Randall et al., 2005b; M\u00fcller et al., 2004; Finke et al., 2010). Significant differences between modafinil and placebo also often only emerge in the most difficult conditions of cognitive tests (M\u00fcller et al., 2004; M\u00fcller et al., 2012; Winder-Rhodes et al., 2010; Marchant et al., 2009).</p>\n</div>\n</blockquote>\n<h3 id=\"Adverse_effects_\">Adverse effects:</h3>\n<blockquote>\n<div title=\"Page 5\">\n<p>...A study by Wong et al. (1999) of 32 healthy, male volunteers showed that the most frequently observed adverse effects among modafinil subjects were headache (34%), followed by insomnia, palpitations and anxiety (each occurring in 21% of participants). Adverse events were clearly dose- dependent: 50%, 83%, 100% and 100% of the participants in the 200 mg, 400 mg, 600 mg, and 800 mg dose groups respectively experienced at least one adverse event. According to the authors of this study the maximal safe dosage of modafinil is 600 mg.</p>\n</div>\n</blockquote>\n<h3 id=\"Abuse_potential_\">Abuse potential:</h3>\n<blockquote>\n<div title=\"Page 5\">\n<p>...Using a randomized, double-blind, placebo-controlled design Rush et al. (2002) examined subjective and behavioral effects of cocaine (100, 200 or 300 mg), modafinil (200, 400 or 600 mg) and placebo in cocaine users\u2026.Of note, while subjects taking cocaine were willing to pay $3 for 100 mg, $6 for 200 mg and $10 for 300 mg cocaine, participants on modafinil were willing to pay $2, regardless of the dose. These results suggest that modafinil has a low abuse liability, but the rather small sample size (n=9) limits the validity of this study.</p>\n<p>The study by Marchant et al. (2009) which is discussed in more detail in part 2.4.12 found that subjects receiving modafinil were significantly less (p&lt;0,05) content than subjects receiving placebo which indicates a low abuse potential of modafinil. In contrast, in a study by M\u00fcller et al. (2012) which is also discussed in more detail above, modafinil significantly increased (p&lt;0,05) ratings of \"task-enjoyment\" which may suggest a moderate potential for abuse.</p>\n</div>\n<div title=\"Page 47\">\n<p>...Overall, these results indicate that although modafinil promotes wakefulness, its effects are distinct from those of more typical stimulants like amphetamine and methylphenidate and more similar to the effects of caffeine which suggests a relatively low abuse liability.</p>\n</div>\n</blockquote>\n<h3 id=\"Conclusion_\">Conclusion:</h3>\n<blockquote>\n<div title=\"Page 48\">\n<p>In healthy individuals modafinil seems to improve cognitive performance, especially on the Stroop Task, stop-signal and serial reaction time tasks and tests of visual memory, working memory, spatial planning ability and sustained attention. However, these cognitive enhancing effects did only emerge in a subset of the reviewed studies. Additionally, significant performance increases may be limited to subjects with low baseline performance. Modafinil also appears to have detrimental effects on mental flexibility.</p>\n<p>...The abuse liability of modafinil seems to be small, particularly in comparison with other stimulants such as amphetamine and methylphenidate. Headache and insomnia are the most common adverse effects of modafinil.</p>\n<p>...Because several studies suggest that modafinil may only provide substantial beneficial effects to individuals with low baseline performance, ultimately the big question remains if modafinil can really improve the cognitive performance of already high-functioning, healthy individuals. Only in the latter case modafinil can justifiably be called a genuine cognitive enhancer.</p>\n</div>\n</blockquote>\n<p>You can download the whole thing below. (Just skip the sections on substance-dependent individuals and patients with dementia. My professor wanted them.)</p>\n<p><a href=\"http://wallowinmaya.files.wordpress.com/2013/12/bachelorarbeit-david-althaus-finale-version.pdf\">Effects of modafinil on cognitive performance in healthy individuals, substance-dependent individuals and patients with dementia</a></p>", "sections": [{"title": "Introduction:", "anchor": "Introduction_", "level": 1}, {"title": "Overview of effects in healthy individuals:", "anchor": "Overview_of_effects_in_healthy_individuals_", "level": 1}, {"title": "Adverse effects:", "anchor": "Adverse_effects_", "level": 1}, {"title": "Abuse potential:", "anchor": "Abuse_potential_", "level": 1}, {"title": "Conclusion:", "anchor": "Conclusion_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "42 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CpemMhCxeyWGCWs4J", "g6kKTtNevCwGxa5px"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-08T19:36:39.463Z", "modifiedAt": null, "url": null, "title": "Physics grad student: how to build employability in programming & finance", "slug": "physics-grad-student-how-to-build-employability-in", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:04.332Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stabilizer", "createdAt": "2011-12-02T09:36:56.841Z", "isAdmin": false, "displayName": "Stabilizer"}, "userId": "Qa3pLZx3o2TApyfgq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zTfaYnR7px4FG5JRK/physics-grad-student-how-to-build-employability-in", "pageUrlRelative": "/posts/zTfaYnR7px4FG5JRK/physics-grad-student-how-to-build-employability-in", "linkUrl": "https://www.lesswrong.com/posts/zTfaYnR7px4FG5JRK/physics-grad-student-how-to-build-employability-in", "postedAtFormatted": "Wednesday, January 8th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Physics%20grad%20student%3A%20how%20to%20build%20employability%20in%20programming%20%26%20finance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhysics%20grad%20student%3A%20how%20to%20build%20employability%20in%20programming%20%26%20finance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzTfaYnR7px4FG5JRK%2Fphysics-grad-student-how-to-build-employability-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Physics%20grad%20student%3A%20how%20to%20build%20employability%20in%20programming%20%26%20finance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzTfaYnR7px4FG5JRK%2Fphysics-grad-student-how-to-build-employability-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzTfaYnR7px4FG5JRK%2Fphysics-grad-student-how-to-build-employability-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 285, "htmlBody": "<p>I'm a theoretical physics (quantum computing) grad student. I really like what I do, and would like to continue doing it for a long time.&nbsp;</p>\n<p>But I'm aware that the job market in academia for freshly minted physics PhDs is not spectacular. For personal reasons, I may not be able to go through the post-doc treadmill and I might want to make good money. Thus: programming &amp; finance. I currently lean towards programming.</p>\n<p>I thought LW is a good place to ask for advice related to this.</p>\n<p>Current skills: Good at math, definitely not \"gifted\". I know C++, and some Python; neither inside out. I don't know specific techniques to design good algorithms for problems. For example, I tried my hand a few times at programming contests (including those at small scales) and got my ass handed to me. I've only taken basic college courses in programming.&nbsp;</p>\n<p>I'm not very aware of the skills tested in quant interviews. I'm sure googling and talking to a few people will fix this, but please feel free to add your thoughts.</p>\n<p>I have about a couple of years left till I graduate, so I can do this properly: What is the best way to make sure that when I graduate I can easily take a job in software or finance after the PhD? Looking for the most bang-for-the-buck (the buck here being time and money) way to do this.</p>\n<p>Also, I may have blinders on. Are there other well-paying jobs out there for physics PhDs? I'm not an American citizen, so many of the government/government-funded lab jobs are out of the question.&nbsp;</p>\n<p>Thanks in advance.</p>\n<hr />\n<p>Some resources I've identified:</p>\n<p>1. <a href=\"http://cerberus.delos.com:790/usacogate\">USACO training gateway.</a></p>\n<p>2. <a href=\"http://mitpress.mit.edu/sicp/\">SICP.</a> (How much is it worth going through this?)</p>\n<p>3.&nbsp;<a href=\"http://www.amazon.com/dp/098478280X/ref=rdr_ext_tmb\">Cracking the Coding Interview.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zTfaYnR7px4FG5JRK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 14, "extendedScore": null, "score": 1.504294329062692e-06, "legacy": true, "legacyId": "25214", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-09T00:25:05.757Z", "modifiedAt": null, "url": null, "title": "We need new humans, please help", "slug": "we-need-new-humans-please-help", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:37.267Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Apprentice", "createdAt": "2009-05-20T13:06:35.976Z", "isAdmin": false, "displayName": "Apprentice"}, "userId": "X5f7X9PkRFwGyxWsb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XrrKapHtHkD6TxDiZ/we-need-new-humans-please-help", "pageUrlRelative": "/posts/XrrKapHtHkD6TxDiZ/we-need-new-humans-please-help", "linkUrl": "https://www.lesswrong.com/posts/XrrKapHtHkD6TxDiZ/we-need-new-humans-please-help", "postedAtFormatted": "Thursday, January 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20We%20need%20new%20humans%2C%20please%20help&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWe%20need%20new%20humans%2C%20please%20help%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXrrKapHtHkD6TxDiZ%2Fwe-need-new-humans-please-help%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=We%20need%20new%20humans%2C%20please%20help%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXrrKapHtHkD6TxDiZ%2Fwe-need-new-humans-please-help", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXrrKapHtHkD6TxDiZ%2Fwe-need-new-humans-please-help", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 396, "htmlBody": "<p class=\"MsoNormal\">This topic is in vogue, so here's my pitch.</p>\n<p class=\"MsoNormal\">My fellow humans, I have some bad news and some good news. The bad news is that you are likely to eventually enter an enfeebled state, during which you will not be able to independently provide for yourself. Even worse, you will at some point altogether cease to function and then you can no longer contribute to the things you care about. The good news is that both of those problems can be ameliorated by the same scheme &ndash; the creation of new humans. The new humans can provide us with the assistance we need as our own abilities diminish. And when we cease to function, the new humans can carry on with the projects we value.</p>\n<p class=\"MsoNormal\">Now, the thing is, creating fully functioning new humans is a huge project, consuming many man-years of work. A person engaged in preparing and outfitting a new human will need to sacrifice a lot of time that could otherwise be devoted to personal leisure and other projects. We currently have a volunteer system for replenishing the population and in many ways this works well. Not everyone is well-placed for creating humans while some people are in a good position to create many. But this system is not perfect and it can be exploited. There are some freeloaders who do not create humans even though they are in a suitable position to do so. Those same people almost always value receiving care in old age and value humanity having a future. But they are relying on the rest of us to provide enough new humans for this to happen while they can devote all their time to other projects and zero time to diapers with poop in them.</p>\n<p class=\"MsoNormal\">Sometimes the non-child-creators justify their decision by suggesting that the projects they are working on are especially socially valuable and thus they can spend time on them in preference to child-creation without violating their duty to society. While it is *possible* that this argument goes through in some cases, it seems suspiciously self-serving. What is especially worth taking into account is that if the humans in question really are so highly valuable, they would statistically have highly valuable offspring. Thus, it seems doubtful in the general case that high-value people refraining from procreating is a net gain for society.</p>\n<p class=\"MsoNormal\">[Poorly conceived section on my personal experiences removed.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XrrKapHtHkD6TxDiZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": -16, "extendedScore": null, "score": -4.1e-05, "legacy": true, "legacyId": "25216", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-09T02:30:19.908Z", "modifiedAt": null, "url": null, "title": "The mechanics of my recent productivity", "slug": "the-mechanics-of-my-recent-productivity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:09.845Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uX3HjXo6BWos3Zgy5/the-mechanics-of-my-recent-productivity", "pageUrlRelative": "/posts/uX3HjXo6BWos3Zgy5/the-mechanics-of-my-recent-productivity", "linkUrl": "https://www.lesswrong.com/posts/uX3HjXo6BWos3Zgy5/the-mechanics-of-my-recent-productivity", "postedAtFormatted": "Thursday, January 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20mechanics%20of%20my%20recent%20productivity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20mechanics%20of%20my%20recent%20productivity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuX3HjXo6BWos3Zgy5%2Fthe-mechanics-of-my-recent-productivity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20mechanics%20of%20my%20recent%20productivity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuX3HjXo6BWos3Zgy5%2Fthe-mechanics-of-my-recent-productivity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuX3HjXo6BWos3Zgy5%2Fthe-mechanics-of-my-recent-productivity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2578, "htmlBody": "<p>A decade ago, I decided to save the world. I was fourteen, and the world certainly wasn't going to save itself.</p>\n<p>I fumbled around for nine years; it's surprising how long one can fumble around. I somehow managed to miss the whole idea of existential risk and the whole concept of an intelligence explosion. I had plenty of other ideas in my head, and while I spent a lot of time honing them, I wasn't particularly looking for new ones.</p>\n<p>A year ago, I finally read the LessWrong sequences. My road here was roundabout, almost comical. It took me a while to come to terms with the implications of what I'd read.</p>\n<p>Five months ago, after resolving a few internal crises, I started donating to MIRI and studying math.</p>\n<p>Three weeks ago, I attended the December MIRI workshop on logic, probability, and reflection. I was invited to visit for the first two days and stay longer if things went well. They did: I was able to make some meaningful contributions.</p>\n<p>On Saturday I was invited to become a MIRI research associate.</p>\n<p>It's been an exciting year, to say the least.</p>\n<p>(ETA: Note that being a research associate gives me access to a number of MIRI resources, but is not a full time position. I will be doing FAI research, but it will be done outside of work. I will be retaining my day job and continuing to <a href=\"https://intelligence.org/donate/\">donate</a>.)</p>\n<p>(ETA: As of 1 April 2014, I am a full-time researcher at MIRI.)</p>\n<p>(ETA: As of 1 June 2015, I am now the executive director of MIRI.)</p>\n<p>To commemorate the occasion &mdash; and because a few people have expressed interest in my efforts &mdash; I'll be writing a series of posts about my experience, about what I did and how I did it. This is the first post in the series.</p>\n<p><a id=\"more\"></a></p>\n<hr />\n<p>First and foremost, know that I am not done with my aggressive autodidacting. I have a long way to go yet before I'm anywhere near as productive as others who do research with MIRI. I find myself at a checkpoint of sorts, collecting my thoughts in the wake of my first workshop, but next week I will be back to business.</p>\n<p>One goal of this post is to give you a feel for how much effort is required to become good at MIRI-relevant mathematics in a short time, and perhaps inspire others to follow my path. It was difficult, but not as difficult as you might think.</p>\n<p>Another goal is to provide data for fellow autodidacts. At the least I can provide you with an anchor point, a single datum about how much effort is required to learn at this pace. As always, remember that I am only one person and that what worked for me may not work for you.</p>\n<p>In order to understand what I achieved it's important to know where I started from. Thus, allow me to briefly discuss my relevant prior experience.</p>\n<h1 id=\"mybackground\">Background</h1>\n<p>I was born in 1989. I have bachelor's degrees of science in both computer science and economics. I started programming TI-83 calculators in late 2002. I've been programming professionally since 2008. I currently work for Google and live in Seattle.</p>\n<p>In high school I had a knack for math. I was placed two years ahead of my classmates. I aced some AP tests, I won some regional math competitions, nothing much came of it. I explicitly decided not to pursue mathematics: I reasoned that in order to save the world I would need charisma, knowledge of how the world economy works, and a reliable source of cash. This (and my love of programming) drove my choice of majors.</p>\n<p>During college I soaked up computer science like a sponge. (Economics, too, but that's not as relevant here.) I came out of college with a strong understanding of the foundations of computing: algorithms, data structures, discrete math, etcetera. I cultivated a love for information theory. Outside of the computer science department I took two math classes: multi variable calculus and real analysis.</p>\n<p>I was careful not to let schooling get in the way of my education. On my own time I learned Haskell &nbsp;in 2008 and started flirting with type theory and category theory. I read <em>G&ouml;del, Escher, Bach </em>early in 2011.</p>\n<p>This should paint a rough picture of my background: I never explicitly studied mathematical logic, but my interests never strayed too far from it. While I didn't have much formal training in this particular subject area, I certainly wasn't starting from a blank slate.</p>\n<h1 id=\"myaccomplishments\">Accomplishments</h1>\n<p>In broad strokes, I'm writing this because I was able to learn a lot very quickly. In the space of eighteen weeks I went from being a professional programmer to helping Benja discover&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2013/12/fallensteins-monster.pdf\">Fallenstein's Monster</a>, a result concerning tiling agents (in the field of mathematical logic).</p>\n<p>I studied math at a fervent pace from August 11th to December 12th and gained enough knowledge to contribute at a MIRI workshop. In that timeframe I read seven textbooks, five of which I finished:</p>\n<ol>\n<li><a href=\"/lw/ii0/book_review_heuristics_and_biases_miri_course_list/\">Heuristics and Biases</a></li>\n<li><a href=\"/lw/il1/book_review_cognitive_science_miri_course_list/\">Cognitive Science</a></li>\n<li><a href=\"/r/lesswrong/lw/ioo/book_review_basic_category_theory_for_computer/\">Basic Category Theory for Computer Scientists</a></li>\n<li><a href=\"/r/lesswrong/lw/ir6/book_review_na%C3%AFve_set_theory_miri_course_list/\">Na&iuml;ve Set Theory</a></li>\n<li><a href=\"/r/lesswrong/lw/ix5/mental_context_for_model_theory/\">Model</a> <a href=\"/r/lesswrong/lw/ixn/very_basic_model_theory/\">Theory</a> (first half)</li>\n<li><a href=\"/lw/j4r/book_review_computability_and_logic/\">Computability and Logic</a></li>\n<li>The Logic of Provability (first half, unreviewed)</li>\n</ol>\n<p>In retrospect, the first two were not particularly relevant to MIRI's current research. Regardless, <em>Heuristics and Biases</em> was quite useful on a personal level.</p>\n<p>I also studied a number of MIRI research papers, two of which I summarized:</p>\n<ul>\n<li>The <a href=\"/r/lesswrong/lw/jbe/walkthrough_of_definability_of_truth_in/\">Probabilistic Logic</a> paper</li>\n<li>The <a href=\"/lw/jca/walkthrough_of_the_tiling_agents_for/\">Tiling Agents</a> paper</li>\n</ul>\n<p>I made use of a number of other minor resources as well, mostly papers found via web search. I successfully signaled my competence and my drive to the right people. While this played a part in my success, it is not the focus of this post.</p>\n<p>I estimate my total study time to be slightly less than 500 hours. I achieved high retention and validated my understanding against other participants of the December workshop. I did this without seriously impacting my job or my social life. I retained enough spare time to&nbsp;<a href=\"http://nanowrimo.org/participants/so8res/novels/lucky-number-eight/stats\">participate in NaNoWriMo</a> during November.</p>\n<p>In sum, I achieved a high level of productivity for an extended period. In the remainder of this post I'll discuss the mechanics of how I did this: my study schedule, my study techniques, and so on. The psychological aspects &mdash; where I found my drive, how I avoid akrasia &mdash; will be covered in later posts.</p>\n<h1 id=\"myschedule\">Schedule</h1>\n<p>I estimate I studied 30-40 hours per week except in November, when I studied 5-15 hours per week. On average, I studied six days a week.</p>\n<p>On the normal weekday I studied for an hour and a half in the morning, a half hour during lunch, and three to four hours in the evening. On the average weekend day I studied 8 to 12 hours on and off throughout the day.</p>\n<p>Believe it or not, I didn't have to alter my schedule much to achieve this pace. I've been following roughly the same schedule for a number of years: I aim to spend one evening per workweek and one day per weekend on social endeavors and the rest of my time toying with something interesting. This is a loose target, I don't sweat deviations.</p>\n<p>There were some changes to my routines, but they were minimal:</p>\n<ul>\n<li>I have many side projects, most were dropped as studying took precedence.</li>\n<li>The number of weeknights I took off per week fell from a little more than one to a little less than one.</li>\n<li>Before this endeavor I traveled for leisure about once every two months. In the past five months I traveled for leisure once.</li>\n</ul>\n<p>While my studying did not affect my schedule much, it <em>definitely</em> affected my pacing. Don't get me wrong; this sprint was not easy. I suspended many other projects and drastically increased my intensity and my pace. I spent roughly the same amount of time per day studying as I used to spend on side projects, but there is a <em>vast</em> difference between spending three hours casually tinkering on open source code and spending three hours learning logic as fast as possible.</p>\n<p>The point here is that aggressive autodidacting certainly takes quite a bit of time and effort, but it need not be all consuming: you can do this sort of thing and maintain a social life.</p>\n<h1 id=\"studytechnique\">Study Technique</h1>\n<p>My methods were simple: read textbooks, do exercises, rephrase and write down the hard parts.</p>\n<p>I had a number of techniques for handling difficult exercises. First, I'd put them aside and come back to them later. If that failed, I'd restate the problem (and all relevant material) in my own words. If this didn't work, it at least helped me identify the point of confusion, which set me up for a question math.stackexchange.com.</p>\n<p>I wasn't above skipping exercises when I was convinced that the exercise was tedious and that I know the underlying material.</p>\n<p>This sounds cleaner than it was: I made a lot of stupid mistakes and experienced my fair share of frustration. For more details on my study methods refer to <a href=\"/lw/j10/on_learning_difficult_things/\">On Learning Difficult Things</a>, a post I wrote while in the midst of my struggles.</p>\n<p>Upon finishing a book, I would immediately start the next one. Concurrently, I would start writing a review of the book I'd finished. I generally wrote the first draft of my book reviews on the Sunday after completing the book, alternating between studying the new and summarizing the old. On subsequent weekdays I'd edit in the morning and study in the evening until I was ready to post my review.</p>\n<p>It's worth noting that summarizing content, especially the research papers, went a long way towards solidifying my knowledge and ensuring that I wasn't glossing over anything.</p>\n<h1 id=\"impactonsociallife\">Impact on Social Life</h1>\n<p>The impact on my social life was minimal. I decreased contact with some periphery friend groups but maintained healthy relationships within my core circles. That I was able to do this is due in part to my circumstances:</p>\n<ul>\n<li>I live with two close friends. This meant that social contact was never out of reach. Even when spending an entire day sequestered in my room pouring over a textbook I was able to maintain a small amount of social interaction. If ever I had a spare hour and a thirst for company, I found it readily available.</li>\n<li>My primary partner was, up until early 2014, going to school full time while holding down a full time job. Thus, her schedule was more restrictive than my own and we had been working around it for some time. Our relationship was not further constrained by my efforts.</li>\n<li>My core friend groups knew and respected what I was doing. I was more tense and exhausted than usual, but I had warned my friends to expect this and no friendships suffered as a result.</li>\n</ul>\n<h1 id=\"impactonworklife\">Impact on Work Life</h1>\n<p>The additional cognitive load did have an impact on my day job. I had less focus and willpower to dedicate to work. Fortunately, I was exceeding expectations before this endeavor. During this sprint, with my cognitive reserves significantly depleted, I had to settle for merely meeting expectations. My performance at work was not poor, by any means: rather, it fell from \"exemplary\" to \"good\".</p>\n<p>I'd rather not settle for merely good performance at work for any extended period of time. Going forward, I'll be reducing my pace somewhat, in large part to ensure that I can dedicate appropriate resources to my day job.</p>\n<h1 id=\"mentalhealth\">Mental Health</h1>\n<p>It's not like I was working from dawn till dusk every day. There was ample time for other activities: I had a few hours of downtime on the average day to read books or surf the web. I participated in a biweekly <a href=\"http://paizo.com/pathfinderRPG\">Pathfinder</a> campaign and spent the occasional Sunday playing <a href=\"http://www.fantasyflightgames.com/edge_minisite.asp?eidm=21\">Twilight Imperium</a>. In September I went camping in the Olympic mountain range. I spent four days in October visiting friends in Cape Cod. I spent a day in December hiking to some hot springs. I entertained guests, went to birthday parties, and so on. There were ample opportunities to get away from math textbooks.</p>\n<p>Most important of all, I had friends I could call on when I needed a mental health day. I could rely on them to find time where we could just sit around, play with LEGO bricks, and shoot the breeze. This went a long way towards keeping me sane.</p>\n<p>All that said, this stint was rough. I experienced far more stress than my norm. I lost a little weight and twice caught myself grinding my teeth in my sleep (a new experience). There were days that I became mentally exhausted, growing obstinate and stubborn as if sleep- or food-deprived. This tended to happen immediately before planned breaks in the routine, as if my mind was rebelling when it thought it could get away with it.</p>\n<p>The stress was manageable, but built up over time. It's hard to tell whether the stress was cumulative or whether the increase was due to circumstance. Doing NaNoWriMo in November while continuing studying didn't particularly help matters. The weeks leading up to the workshop were particularly stressful due to a lack of information: I worried that I would not know nearly enough to be useful, that I would make a fool of myself, and so on. So while the stress surely mounted as time wore on, I can't tell how much of that was cumulative versus circumstantial.</p>\n<p>I tentatively believe that someone could sustain my pace for significantly longer than I did, so long as they were willing to live with the strain. I don't plan to test this myself: I'll be slowing down both to improve performance at work and to reduce my general stress levels. Five months of fervent studying is no walk in the park.</p>\n<h1 id=\"advice\">Advice</h1>\n<p>So you want to follow in my footsteps? Awesome. I commend your enthusiasm. My next post will delve into my mindset and a few of the quirks of my behavior that helped me be productive. For now, I will leave you with this advice:</p>\n<ul>\n<li>There is no magic to it. If you study the right material, do the exercises, and write what you've learned in your own words, then you can indeed learn MIRI-relevant math in a reasonable amount of time.</li>\n<li>Learning fast does not need to dominate your life. There can be time for social activities and even significant side projects. You will have to work really hard, but that work does not have to consume your life.</li>\n<li>If you're going to do something like this, let people know what you're doing. This is much easier if you have people you can turn to for support who don't mind you being extra snappy, people who can drag you away for a day every week or two. Also, stating your goals publicly helps to stop you from giving up.</li>\n</ul>\n<div>The difficult part is making a commitment and sticking to it. Akrasia is a formidable enemy, here. If you can avoid it, the actual autodidacting is not overly difficult.</div>\n<div><br /></div>\n<div>As for specific advice, if your background is similar to mine then I recommend reading&nbsp;<em>Na&iuml;ve Set Theory, Computability and Logic, </em>and the first two chapters of&nbsp;<em>Model Theory </em>in that order, these will get you off to a good start. Feel free to PM me if you get stuck or if you want more recommendations.</div>\n<div><em><br /></em></div>\n<div>Following posts will cover the other sides of my experience: how I got interested in this field, where I draw my motivation from, and the dark arts that I use to maintain productivity. In the meantime, questions are welcome.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"udPbn9RthmgTtHMiG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uX3HjXo6BWos3Zgy5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 130, "baseScore": 167, "extendedScore": null, "score": 0.000435, "legacy": true, "legacyId": "25203", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 167, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>A decade ago, I decided to save the world. I was fourteen, and the world certainly wasn't going to save itself.</p>\n<p>I fumbled around for nine years; it's surprising how long one can fumble around. I somehow managed to miss the whole idea of existential risk and the whole concept of an intelligence explosion. I had plenty of other ideas in my head, and while I spent a lot of time honing them, I wasn't particularly looking for new ones.</p>\n<p>A year ago, I finally read the LessWrong sequences. My road here was roundabout, almost comical. It took me a while to come to terms with the implications of what I'd read.</p>\n<p>Five months ago, after resolving a few internal crises, I started donating to MIRI and studying math.</p>\n<p>Three weeks ago, I attended the December MIRI workshop on logic, probability, and reflection. I was invited to visit for the first two days and stay longer if things went well. They did: I was able to make some meaningful contributions.</p>\n<p>On Saturday I was invited to become a MIRI research associate.</p>\n<p>It's been an exciting year, to say the least.</p>\n<p>(ETA: Note that being a research associate gives me access to a number of MIRI resources, but is not a full time position. I will be doing FAI research, but it will be done outside of work. I will be retaining my day job and continuing to <a href=\"https://intelligence.org/donate/\">donate</a>.)</p>\n<p>(ETA: As of 1 April 2014, I am a full-time researcher at MIRI.)</p>\n<p>(ETA: As of 1 June 2015, I am now the executive director of MIRI.)</p>\n<p>To commemorate the occasion \u2014 and because a few people have expressed interest in my efforts \u2014 I'll be writing a series of posts about my experience, about what I did and how I did it. This is the first post in the series.</p>\n<p><a id=\"more\"></a></p>\n<hr>\n<p>First and foremost, know that I am not done with my aggressive autodidacting. I have a long way to go yet before I'm anywhere near as productive as others who do research with MIRI. I find myself at a checkpoint of sorts, collecting my thoughts in the wake of my first workshop, but next week I will be back to business.</p>\n<p>One goal of this post is to give you a feel for how much effort is required to become good at MIRI-relevant mathematics in a short time, and perhaps inspire others to follow my path. It was difficult, but not as difficult as you might think.</p>\n<p>Another goal is to provide data for fellow autodidacts. At the least I can provide you with an anchor point, a single datum about how much effort is required to learn at this pace. As always, remember that I am only one person and that what worked for me may not work for you.</p>\n<p>In order to understand what I achieved it's important to know where I started from. Thus, allow me to briefly discuss my relevant prior experience.</p>\n<h1 id=\"Background\">Background</h1>\n<p>I was born in 1989. I have bachelor's degrees of science in both computer science and economics. I started programming TI-83 calculators in late 2002. I've been programming professionally since 2008. I currently work for Google and live in Seattle.</p>\n<p>In high school I had a knack for math. I was placed two years ahead of my classmates. I aced some AP tests, I won some regional math competitions, nothing much came of it. I explicitly decided not to pursue mathematics: I reasoned that in order to save the world I would need charisma, knowledge of how the world economy works, and a reliable source of cash. This (and my love of programming) drove my choice of majors.</p>\n<p>During college I soaked up computer science like a sponge. (Economics, too, but that's not as relevant here.) I came out of college with a strong understanding of the foundations of computing: algorithms, data structures, discrete math, etcetera. I cultivated a love for information theory. Outside of the computer science department I took two math classes: multi variable calculus and real analysis.</p>\n<p>I was careful not to let schooling get in the way of my education. On my own time I learned Haskell &nbsp;in 2008 and started flirting with type theory and category theory. I read <em>G\u00f6del, Escher, Bach </em>early in 2011.</p>\n<p>This should paint a rough picture of my background: I never explicitly studied mathematical logic, but my interests never strayed too far from it. While I didn't have much formal training in this particular subject area, I certainly wasn't starting from a blank slate.</p>\n<h1 id=\"Accomplishments\">Accomplishments</h1>\n<p>In broad strokes, I'm writing this because I was able to learn a lot very quickly. In the space of eighteen weeks I went from being a professional programmer to helping Benja discover&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2013/12/fallensteins-monster.pdf\">Fallenstein's Monster</a>, a result concerning tiling agents (in the field of mathematical logic).</p>\n<p>I studied math at a fervent pace from August 11th to December 12th and gained enough knowledge to contribute at a MIRI workshop. In that timeframe I read seven textbooks, five of which I finished:</p>\n<ol>\n<li><a href=\"/lw/ii0/book_review_heuristics_and_biases_miri_course_list/\">Heuristics and Biases</a></li>\n<li><a href=\"/lw/il1/book_review_cognitive_science_miri_course_list/\">Cognitive Science</a></li>\n<li><a href=\"/r/lesswrong/lw/ioo/book_review_basic_category_theory_for_computer/\">Basic Category Theory for Computer Scientists</a></li>\n<li><a href=\"/r/lesswrong/lw/ir6/book_review_na%C3%AFve_set_theory_miri_course_list/\">Na\u00efve Set Theory</a></li>\n<li><a href=\"/r/lesswrong/lw/ix5/mental_context_for_model_theory/\">Model</a> <a href=\"/r/lesswrong/lw/ixn/very_basic_model_theory/\">Theory</a> (first half)</li>\n<li><a href=\"/lw/j4r/book_review_computability_and_logic/\">Computability and Logic</a></li>\n<li>The Logic of Provability (first half, unreviewed)</li>\n</ol>\n<p>In retrospect, the first two were not particularly relevant to MIRI's current research. Regardless, <em>Heuristics and Biases</em> was quite useful on a personal level.</p>\n<p>I also studied a number of MIRI research papers, two of which I summarized:</p>\n<ul>\n<li>The <a href=\"/r/lesswrong/lw/jbe/walkthrough_of_definability_of_truth_in/\">Probabilistic Logic</a> paper</li>\n<li>The <a href=\"/lw/jca/walkthrough_of_the_tiling_agents_for/\">Tiling Agents</a> paper</li>\n</ul>\n<p>I made use of a number of other minor resources as well, mostly papers found via web search. I successfully signaled my competence and my drive to the right people. While this played a part in my success, it is not the focus of this post.</p>\n<p>I estimate my total study time to be slightly less than 500 hours. I achieved high retention and validated my understanding against other participants of the December workshop. I did this without seriously impacting my job or my social life. I retained enough spare time to&nbsp;<a href=\"http://nanowrimo.org/participants/so8res/novels/lucky-number-eight/stats\">participate in NaNoWriMo</a> during November.</p>\n<p>In sum, I achieved a high level of productivity for an extended period. In the remainder of this post I'll discuss the mechanics of how I did this: my study schedule, my study techniques, and so on. The psychological aspects \u2014 where I found my drive, how I avoid akrasia \u2014 will be covered in later posts.</p>\n<h1 id=\"Schedule\">Schedule</h1>\n<p>I estimate I studied 30-40 hours per week except in November, when I studied 5-15 hours per week. On average, I studied six days a week.</p>\n<p>On the normal weekday I studied for an hour and a half in the morning, a half hour during lunch, and three to four hours in the evening. On the average weekend day I studied 8 to 12 hours on and off throughout the day.</p>\n<p>Believe it or not, I didn't have to alter my schedule much to achieve this pace. I've been following roughly the same schedule for a number of years: I aim to spend one evening per workweek and one day per weekend on social endeavors and the rest of my time toying with something interesting. This is a loose target, I don't sweat deviations.</p>\n<p>There were some changes to my routines, but they were minimal:</p>\n<ul>\n<li>I have many side projects, most were dropped as studying took precedence.</li>\n<li>The number of weeknights I took off per week fell from a little more than one to a little less than one.</li>\n<li>Before this endeavor I traveled for leisure about once every two months. In the past five months I traveled for leisure once.</li>\n</ul>\n<p>While my studying did not affect my schedule much, it <em>definitely</em> affected my pacing. Don't get me wrong; this sprint was not easy. I suspended many other projects and drastically increased my intensity and my pace. I spent roughly the same amount of time per day studying as I used to spend on side projects, but there is a <em>vast</em> difference between spending three hours casually tinkering on open source code and spending three hours learning logic as fast as possible.</p>\n<p>The point here is that aggressive autodidacting certainly takes quite a bit of time and effort, but it need not be all consuming: you can do this sort of thing and maintain a social life.</p>\n<h1 id=\"Study_Technique\">Study Technique</h1>\n<p>My methods were simple: read textbooks, do exercises, rephrase and write down the hard parts.</p>\n<p>I had a number of techniques for handling difficult exercises. First, I'd put them aside and come back to them later. If that failed, I'd restate the problem (and all relevant material) in my own words. If this didn't work, it at least helped me identify the point of confusion, which set me up for a question math.stackexchange.com.</p>\n<p>I wasn't above skipping exercises when I was convinced that the exercise was tedious and that I know the underlying material.</p>\n<p>This sounds cleaner than it was: I made a lot of stupid mistakes and experienced my fair share of frustration. For more details on my study methods refer to <a href=\"/lw/j10/on_learning_difficult_things/\">On Learning Difficult Things</a>, a post I wrote while in the midst of my struggles.</p>\n<p>Upon finishing a book, I would immediately start the next one. Concurrently, I would start writing a review of the book I'd finished. I generally wrote the first draft of my book reviews on the Sunday after completing the book, alternating between studying the new and summarizing the old. On subsequent weekdays I'd edit in the morning and study in the evening until I was ready to post my review.</p>\n<p>It's worth noting that summarizing content, especially the research papers, went a long way towards solidifying my knowledge and ensuring that I wasn't glossing over anything.</p>\n<h1 id=\"Impact_on_Social_Life\">Impact on Social Life</h1>\n<p>The impact on my social life was minimal. I decreased contact with some periphery friend groups but maintained healthy relationships within my core circles. That I was able to do this is due in part to my circumstances:</p>\n<ul>\n<li>I live with two close friends. This meant that social contact was never out of reach. Even when spending an entire day sequestered in my room pouring over a textbook I was able to maintain a small amount of social interaction. If ever I had a spare hour and a thirst for company, I found it readily available.</li>\n<li>My primary partner was, up until early 2014, going to school full time while holding down a full time job. Thus, her schedule was more restrictive than my own and we had been working around it for some time. Our relationship was not further constrained by my efforts.</li>\n<li>My core friend groups knew and respected what I was doing. I was more tense and exhausted than usual, but I had warned my friends to expect this and no friendships suffered as a result.</li>\n</ul>\n<h1 id=\"Impact_on_Work_Life\">Impact on Work Life</h1>\n<p>The additional cognitive load did have an impact on my day job. I had less focus and willpower to dedicate to work. Fortunately, I was exceeding expectations before this endeavor. During this sprint, with my cognitive reserves significantly depleted, I had to settle for merely meeting expectations. My performance at work was not poor, by any means: rather, it fell from \"exemplary\" to \"good\".</p>\n<p>I'd rather not settle for merely good performance at work for any extended period of time. Going forward, I'll be reducing my pace somewhat, in large part to ensure that I can dedicate appropriate resources to my day job.</p>\n<h1 id=\"Mental_Health\">Mental Health</h1>\n<p>It's not like I was working from dawn till dusk every day. There was ample time for other activities: I had a few hours of downtime on the average day to read books or surf the web. I participated in a biweekly <a href=\"http://paizo.com/pathfinderRPG\">Pathfinder</a> campaign and spent the occasional Sunday playing <a href=\"http://www.fantasyflightgames.com/edge_minisite.asp?eidm=21\">Twilight Imperium</a>. In September I went camping in the Olympic mountain range. I spent four days in October visiting friends in Cape Cod. I spent a day in December hiking to some hot springs. I entertained guests, went to birthday parties, and so on. There were ample opportunities to get away from math textbooks.</p>\n<p>Most important of all, I had friends I could call on when I needed a mental health day. I could rely on them to find time where we could just sit around, play with LEGO bricks, and shoot the breeze. This went a long way towards keeping me sane.</p>\n<p>All that said, this stint was rough. I experienced far more stress than my norm. I lost a little weight and twice caught myself grinding my teeth in my sleep (a new experience). There were days that I became mentally exhausted, growing obstinate and stubborn as if sleep- or food-deprived. This tended to happen immediately before planned breaks in the routine, as if my mind was rebelling when it thought it could get away with it.</p>\n<p>The stress was manageable, but built up over time. It's hard to tell whether the stress was cumulative or whether the increase was due to circumstance. Doing NaNoWriMo in November while continuing studying didn't particularly help matters. The weeks leading up to the workshop were particularly stressful due to a lack of information: I worried that I would not know nearly enough to be useful, that I would make a fool of myself, and so on. So while the stress surely mounted as time wore on, I can't tell how much of that was cumulative versus circumstantial.</p>\n<p>I tentatively believe that someone could sustain my pace for significantly longer than I did, so long as they were willing to live with the strain. I don't plan to test this myself: I'll be slowing down both to improve performance at work and to reduce my general stress levels. Five months of fervent studying is no walk in the park.</p>\n<h1 id=\"Advice\">Advice</h1>\n<p>So you want to follow in my footsteps? Awesome. I commend your enthusiasm. My next post will delve into my mindset and a few of the quirks of my behavior that helped me be productive. For now, I will leave you with this advice:</p>\n<ul>\n<li>There is no magic to it. If you study the right material, do the exercises, and write what you've learned in your own words, then you can indeed learn MIRI-relevant math in a reasonable amount of time.</li>\n<li>Learning fast does not need to dominate your life. There can be time for social activities and even significant side projects. You will have to work really hard, but that work does not have to consume your life.</li>\n<li>If you're going to do something like this, let people know what you're doing. This is much easier if you have people you can turn to for support who don't mind you being extra snappy, people who can drag you away for a day every week or two. Also, stating your goals publicly helps to stop you from giving up.</li>\n</ul>\n<div>The difficult part is making a commitment and sticking to it. Akrasia is a formidable enemy, here. If you can avoid it, the actual autodidacting is not overly difficult.</div>\n<div><br></div>\n<div>As for specific advice, if your background is similar to mine then I recommend reading&nbsp;<em>Na\u00efve Set Theory, Computability and Logic, </em>and the first two chapters of&nbsp;<em>Model Theory </em>in that order, these will get you off to a good start. Feel free to PM me if you get stuck or if you want more recommendations.</div>\n<div><em><br></em></div>\n<div>Following posts will cover the other sides of my experience: how I got interested in this field, where I draw my motivation from, and the dark arts that I use to maintain productivity. In the meantime, questions are welcome.</div>", "sections": [{"title": "Background", "anchor": "Background", "level": 1}, {"title": "Accomplishments", "anchor": "Accomplishments", "level": 1}, {"title": "Schedule", "anchor": "Schedule", "level": 1}, {"title": "Study Technique", "anchor": "Study_Technique", "level": 1}, {"title": "Impact on Social Life", "anchor": "Impact_on_Social_Life", "level": 1}, {"title": "Impact on Work Life", "anchor": "Impact_on_Work_Life", "level": 1}, {"title": "Mental Health", "anchor": "Mental_Health", "level": 1}, {"title": "Advice", "anchor": "Advice", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "47 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gc6foBcbozvEJ3HbG", "ucpD7YtcvKo6C9fBK", "Jar4BGrJ7BiQemBDM", "Ee8CZW7wzaNdCENYG", "MG8Yhsxqu9JY4xRPr", "F6BrJFkqEhh22rFsZ", "CvhPTwSMPqNju7hhw", "kFDikC8kbukAhSnbe", "QGrX3qK3qxQYK9D4C", "w5F4w8tNZc6LcBKRP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-09T06:44:08.080Z", "modifiedAt": null, "url": null, "title": "Habitual Productivity", "slug": "habitual-productivity", "viewCount": null, "lastCommentedAt": "2021-06-29T14:24:06.323Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/srwKRt9TsS5oxvJsh/habitual-productivity", "pageUrlRelative": "/posts/srwKRt9TsS5oxvJsh/habitual-productivity", "linkUrl": "https://www.lesswrong.com/posts/srwKRt9TsS5oxvJsh/habitual-productivity", "postedAtFormatted": "Thursday, January 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Habitual%20Productivity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHabitual%20Productivity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsrwKRt9TsS5oxvJsh%2Fhabitual-productivity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Habitual%20Productivity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsrwKRt9TsS5oxvJsh%2Fhabitual-productivity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsrwKRt9TsS5oxvJsh%2Fhabitual-productivity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1709, "htmlBody": "<p><em>I was able to maintain <a href=\"/lw/jg3/the_mechanics_of_my_recent_productivity/\">high productivity</a> for extended periods of time and achieve some difficult goals. In this and the following posts I will discuss some personality quirks and techniques that helped me do this. This post is fairly self-expository. I claim no originality, this is simply an account of how I operate.</em></p>\n<p>Secret number one: Productivity is a habit of mine. As I mentioned in the previous post, I've been following a similar schedule for years: two days doing social things, five days doing something constructive. Before I turned my efforts towards FAI research, this mainly consistent of programming, writing, and self-education.</p>\n<p>This habit was not sufficient to get the high productivity I attained in the last few months, but it was definitely necessary.</p>\n<p>I understand that this is not helpful advice: \"I'm habitually productive\" just passes the buck. \"Ah\", you ask, \"but how did you turn productivity into a habit?\" For that, I have an ace up my sleeve:</p>\n<p>I <em>deplore</em> fun.</p>\n<p><a id=\"more\"></a></p>\n<p>Ok, not really. However, I do have a strong aversion to activities that I find unproductive. This aversion is partly innate and partly developed. It first became explicit at the age of nine or ten, when I read <em>The Phantom Tollbooth</em>:</p>\n<blockquote>\n<p>\"KILLING TIME!\" roared the dog&mdash;so furiously that his alarm went off. \"It's bad enough wasting time without killing it.\" And he shuddered at the thought.</p>\n</blockquote>\n<p>- Norton Juster, <a href=\"http://books.google.com/books?id=r3jFlrbsACMC&amp;pg=PT32&amp;lpg=PT32&amp;dq=killing+time+phantom+tollbooth&amp;source=bl&amp;ots=k1o2aZYICZ&amp;sig=_m76RAD0PKbhQdXQWwKVSpanKFU&amp;hl=en&amp;sa=X&amp;ei=s-3MUvSlOsLgoATtroGwBQ&amp;ved=0CGoQ6AEwCA#v=onepage&amp;q&amp;f=false\">The Phantom Tollbooth</a></p>\n<p>This quote stuck with me. Time is scarce, and I certainly didn't want to <em>kill</em> any.</p>\n<p>I developed an explicit distaste for boredom, and went out of my way to avoid it. I kept books near me at all times. I invented stories and thought up new plots when drifting off to sleep. I invented mental puzzles to keep me entertained during class, including a stint in my teens where I worked out the base 12 multiplication tables. Later, I put spare mental cycles towards considering my code, probing edge cases or considering alternative designs (a practice that is no doubt familiar to all programmers).</p>\n<p>This distaste broadened as I aged. I grew to realize that I didn't just want to be doing things, I wanted to be doing <em>useful</em> things. My disdain started spreading towards other activities, ones that didn't forward my long-term goals. The memories are hazy, and I'm not sure whether this caused or was caused by my na&iuml;ve resolution to save the world (or a whole tangle of other factors), but I know the two were linked.</p>\n<p>Before long, I began to view escapism as a guilty pleasure: fun and addictive, but unsatisfying. Things like hiking and going to parties became almost a chore: I superficially enjoyed them, sure, but I yearned to be elsewhere, doing something <em>permanent</em>. Even reading fiction took on a pang of guilt. I valued things that moved me forward, that honed my skills or moved me closer to my terminal goals. I wanted to be <em>building</em> things, <em>improving</em> things.</p>\n<p>This is my first secret weapon: I lost the ability to be satisfied by unproductive activity.</p>\n<p>This was not particularly pleasant.</p>\n<p>As I got older, I struggled to balance social activities that were supposed to be fun with all of the things that I wanted to learn and build. All forms of entertainment were weighed against their opportunity cost. This wasn't an elegant phase of my life: I was still a teenager, and I yearned for social validation, strong friendships, and adventures just as much as my peers. Trouble was, I was caught in a catch 22: when I squirreled away in my room being \"productive\" I felt like I was missing out, and when I went outside to have \"adventures\" I only wanted to be elsewhere. I vacillated wildly for a few years before coming to terms with myself.</p>\n<p>These days, I aim to spend about two evenings a week (one on weekdays, one on weekends) doing something that's traditionally fun. I spend the rest of my time doing things that sate my neverending desire to march towards my goals.</p>\n<p>It's interesting to note that, in the end, there wasn't really a compromise. The productivity side just flat-out won: I eventually realized that human interaction is necessary for mental health and that a solid social network is invaluable. I don't mean to imply that I engage in social interaction because I've calculated that it's necessary: I <em>really do</em> enjoy social interaction, and I <em>really want</em> to be able to enjoy it without guilt. Rather, it's more like I've found an excuse that allows me to both enjoy myself and sate the thirst. That said, it's still difficult for me to disengage sometimes.</p>\n<hr />\n<p>This is also not the most helpful advice, I realize: I'm good at being productive in part because I'm bad at being satisfied unless my current task forwards my active goals. This isn't exactly something you can practice.</p>\n<p>Unless you're into mind hacking, I suppose. (Note: At this point in the post, set your \"humor\" dials to \"dry\".)</p>\n<p>When I was quite young, one of the guests at our house refused to eat processed food. I remember that I offered her some fritos and she refused. I was fairly astonished, and young enough to be socially inept. I asked, incredulous, how someone could <em>not like</em> fritos. To my surprise, she didn't brush me off or feed me banal lines about how different people have different tastes. She gave me the answer of someone who had recently stopped liking fritos through an act of will. Her answer went something like this: \"Just start noticing how greasy they are, and how the grease gets all over your fingers and coats the inside of the bag. Notice that you don't want to eat things soaked in that much grease. Become repulsed by it, and then you won't like them either.\"</p>\n<p>Now, I was a stubborn and contrary child, so her ploy failed. But to this day, I still notice the grease. This woman's technique stuck with me. She picked out a <em>very specific</em> property of a thing she wanted to stop enjoying and convinced herself that it repulsed her.</p>\n<p>If I were <em>trying </em>to start hating fun (and I remind you that I'm not trying, because I already do, and that you shouldn't try, because it's no fun) then this is the route I would recommend: Recognize those little discomforts that underlie your escapism, latch on to them, and blow them <em>completely</em>&nbsp;out of proportion. (Disclaimer: I am not a mindwizard; I've no doubt there are better ways to change your affections if you're in to mindhacking.)</p>\n<p>Note that such mindhacking is a Dark Art which you should not pursue.&nbsp;Side effects may include:</p>\n<ul>\n<li>Experiencing guilt when you should be having a grand old time.</li>\n<li>Attempting to complete hikes as fast as possible so you can get back to what you were working on.</li>\n<li>A propensity to get more tense when you're supposed to be relaxing.</li>\n<li>A tendency to bring books to live concerts so that you can multitask.</li>\n</ul>\n<p>Furthermore, I imagine that this can backfire <em>reaaaly</em> hard: if you manage to develop a strong revulsion for unproductive activities but <em>still</em> can't force yourself to stop browsing reddit (or whatever your vice) then you run a big risk of hitting a willpower-draining death spiral.</p>\n<p>So I'm <em>really</em> not recommending that you try this mindhack. But if&nbsp;you <em>already</em> have spikes of guilt after bouts of escapism, or if you house an arrogant disdain for wasting your time on TV shows, here are a few mantras you can latch on to to help yourself develop a solid hatred of fun (I warn you that these are calibrated for a 14 year old mind and may be somewhat stale):</p>\n<ul>\n<li>When skiing, partying, or generally having a good time, try remembering that this is exactly the type of thing people should have an opportunity to do <em>after</em> we stop everyone from dying.</li>\n<li>When doing something transient like watching TV or playing video games, reflect upon how it's not building any skills that are going to make the world a better place, nor really having a lasting impact on the world.</li>\n<li>Notice that if the world is to be saved then it <em>really does</em> need to be you who saves it, because everybody else is busy skiing, partying, reading fantasy, or dying in third world countries.</li>\n</ul>\n<p>It also helps if you're extraordinarily arrogant and you house a deep-seated belief in civilizational inadequacy.</p>\n<p>(You may now disengage your humor shielding.)</p>\n<hr />\n<p>I strongly recommend finding a different and preferably healthier route to habitual productivity. The point of this exposition is that <em>for me</em>, a quirk of my psychology led me to a schedule where I spend my days doing things that lead towards my goals.</p>\n<p>My distaste for other activities is not the thing that is driving&nbsp;me, per se: it has merely pushed me towards a certain lifestyle, it has helped me develop a certain habit. That habit is the foundation for my recent achievements.</p>\n<p>If you can structure your life such that productive things are the things that you do&nbsp;<em>by default</em>, the things that you do in your free time when you have nothing else on your plate, then you will be in good shape. When \"do something that forwards your goals\" is the <em>fallback </em>plan then it becomes much easier to scale your efforts up.</p>\n<p>The way that I built such structure into my own life was pretty personalized and likely unhealthy, but I'm quite content with the end result. So that's my advice for the day: if you can, try to make your default actions useful. Find a way to make productivity habitual.</p>\n<p>When forming habits, repetition is very important. If you're trying to be highly productive, consider starting by being a little productive with high regularity<em>.</em>&nbsp;Humans are very habitual creatures, and establishing a habit of completing easier tasks may pay off in the long run.</p>\n<p>Even if you start with the easier tasks, though, you're going to need a good chunk of motivation to successfully form a habit of doing things that require effort. In these waters swims Akrasia, a most ancient enemy. I meant to delve more into the sources of my motivation and some tricks I use to avoid akrasia, but I've run out of time. Further posts will follow.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"udPbn9RthmgTtHMiG": 2, "XYHzLjwYiqpeqaf4c": 1, "r7qAjcbfhj2256EHH": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "srwKRt9TsS5oxvJsh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 64, "baseScore": 72, "extendedScore": null, "score": 0.000188, "legacy": true, "legacyId": "25217", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 73, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uX3HjXo6BWos3Zgy5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-09T13:16:46.247Z", "modifiedAt": null, "url": null, "title": "Variables in Arguments as a Source of Confusion", "slug": "variables-in-arguments-as-a-source-of-confusion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:40.374Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kremlin", "createdAt": "2011-11-20T13:42:17.401Z", "isAdmin": false, "displayName": "kremlin"}, "userId": "hs8N5xYvR4GGkDg8f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6oBYtNkafZatGsHkW/variables-in-arguments-as-a-source-of-confusion", "pageUrlRelative": "/posts/6oBYtNkafZatGsHkW/variables-in-arguments-as-a-source-of-confusion", "linkUrl": "https://www.lesswrong.com/posts/6oBYtNkafZatGsHkW/variables-in-arguments-as-a-source-of-confusion", "postedAtFormatted": "Thursday, January 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Variables%20in%20Arguments%20as%20a%20Source%20of%20Confusion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVariables%20in%20Arguments%20as%20a%20Source%20of%20Confusion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6oBYtNkafZatGsHkW%2Fvariables-in-arguments-as-a-source-of-confusion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Variables%20in%20Arguments%20as%20a%20Source%20of%20Confusion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6oBYtNkafZatGsHkW%2Fvariables-in-arguments-as-a-source-of-confusion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6oBYtNkafZatGsHkW%2Fvariables-in-arguments-as-a-source-of-confusion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 273, "htmlBody": "<p>I was reading an argument happening in the comments of an article about Light Table switching to open source. The argument was about freedom in relation to software, and it went basically something like this:</p>\n<blockquote>\n<p>People who use OSX are less free [than Linux users], because they don't have the freedom to modify their OS source code.</p>\n</blockquote>\n<blockquote>\n<p>No, they have the exact same freedom. People who use OSX and people who use Linux both have the freedom to modify the source code of Linux.</p>\n</blockquote>\n<p>I'm not entirely sure, but this conversation reminded me immediately of arguing about <a href=\"/lw/of/dissolving_the_question/\">a tree falling and making a sound when nobody's around to hear</a>.</p>\n<p>The first persons statement uses a variable in the place that the second persons statement uses a constant.</p>\n<p>X's freedom is [partially] a function of [X's OS].<br />vs<br />X's freedom is [partially] a function of OS_List. (where OS_List is just a list of the OSs that he could in principle modify, regardless of if he wants to or is using any of those OSs)</p>\n<p>(Obviously OS_List is a variable as well, but with respect to each person it's relatively unchanging).</p>\n<p>I've seen this crop up in various conversations before - one person arguing using a variable where another person is using a constant (if that's the right way to describe it).</p>\n<p>How does one diagnose the problem with this argument, if there is a problem? Is it a similar problem to the Tree in the Forest problem? Is there a standard rationalist way to dissolve the dispute so that both parties can leave not only agreeing, but also having a high probability of being correct when they leave?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6oBYtNkafZatGsHkW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 1.5054419415728852e-06, "legacy": true, "legacyId": "25219", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Mc6QcrsbH5NRXbCRX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-09T20:28:11.785Z", "modifiedAt": null, "url": null, "title": "[LINK] People become more utilitarian in VR moral dilemmas as compared to text based.", "slug": "link-people-become-more-utilitarian-in-vr-moral-dilemmas-as", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:38.688Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "trifith", "createdAt": "2013-11-13T21:10:15.223Z", "isAdmin": false, "displayName": "trifith"}, "userId": "cDsuSwt4gXSrHdNsQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qLaSqRpG26ZYpmsNT/link-people-become-more-utilitarian-in-vr-moral-dilemmas-as", "pageUrlRelative": "/posts/qLaSqRpG26ZYpmsNT/link-people-become-more-utilitarian-in-vr-moral-dilemmas-as", "linkUrl": "https://www.lesswrong.com/posts/qLaSqRpG26ZYpmsNT/link-people-become-more-utilitarian-in-vr-moral-dilemmas-as", "postedAtFormatted": "Thursday, January 9th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20People%20become%20more%20utilitarian%20in%20VR%20moral%20dilemmas%20as%20compared%20to%20text%20based.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20People%20become%20more%20utilitarian%20in%20VR%20moral%20dilemmas%20as%20compared%20to%20text%20based.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqLaSqRpG26ZYpmsNT%2Flink-people-become-more-utilitarian-in-vr-moral-dilemmas-as%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20People%20become%20more%20utilitarian%20in%20VR%20moral%20dilemmas%20as%20compared%20to%20text%20based.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqLaSqRpG26ZYpmsNT%2Flink-people-become-more-utilitarian-in-vr-moral-dilemmas-as", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqLaSqRpG26ZYpmsNT%2Flink-people-become-more-utilitarian-in-vr-moral-dilemmas-as", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 191, "htmlBody": "<p>A new study indicates that people become more utilitarian (save more lives) when viewing a moral dilemma in a virtual reality situation, as compared to reading the same situation in text.</p>\n<h4>Abstract.</h4>\n<blockquote>\n<p>Although research in moral psychology in the last decade has relied heavily on hypothetical moral dilemmas and has been effective in understanding moral judgment, how these judgments translate into behaviors remains a largely unexplored issue due to the harmful nature of the acts involved. To study this link, we follow a new approach based on a desktop virtual reality environment. In our within-subjects experiment, participants exhibited an order-dependent judgment-behavior discrepancy across temporally-separated sessions, with many of them behaving in utilitarian manner in virtual reality dilemmas despite their non-utilitarian judgments for the same dilemmas in textual descriptions. This change in decisions reflected in the autonomic arousal of participants, with dilemmas in virtual reality being perceived more emotionally arousing than the ones in text, after controlling for general differences between the two presentation modalities (virtual reality vs. text). This suggests that moral decision-making in hypothetical moral dilemmas is susceptible to contextual saliency of the presentation of these dilemmas.</p>\n</blockquote>\n<p><a href=\"http://hcilab.uniud.it/publications/2014-01.html\">Full paper</a></p>\n<p><a href=\"http://youtu.be/ebdU3HhhYs8\">Video of simulations</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qLaSqRpG26ZYpmsNT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.5059094197387843e-06, "legacy": true, "legacyId": "25220", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-10T10:21:25.364Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Biweekly Sequences Discussion: Politics Is the Mind-killer", "slug": "meetup-vancouver-biweekly-sequences-discussion-politics-is", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eggman", "createdAt": "2011-10-09T00:24:15.183Z", "isAdmin": false, "displayName": "eggman"}, "userId": "irkySx7hExrK2XG53", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Bc2TdbJYSS3EhgvKg/meetup-vancouver-biweekly-sequences-discussion-politics-is", "pageUrlRelative": "/posts/Bc2TdbJYSS3EhgvKg/meetup-vancouver-biweekly-sequences-discussion-politics-is", "linkUrl": "https://www.lesswrong.com/posts/Bc2TdbJYSS3EhgvKg/meetup-vancouver-biweekly-sequences-discussion-politics-is", "postedAtFormatted": "Friday, January 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Biweekly%20Sequences%20Discussion%3A%20Politics%20Is%20the%20Mind-killer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Biweekly%20Sequences%20Discussion%3A%20Politics%20Is%20the%20Mind-killer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBc2TdbJYSS3EhgvKg%2Fmeetup-vancouver-biweekly-sequences-discussion-politics-is%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Biweekly%20Sequences%20Discussion%3A%20Politics%20Is%20the%20Mind-killer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBc2TdbJYSS3EhgvKg%2Fmeetup-vancouver-biweekly-sequences-discussion-politics-is", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBc2TdbJYSS3EhgvKg%2Fmeetup-vancouver-biweekly-sequences-discussion-politics-is", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vg'>Vancouver Biweekly Sequences Discussion: Politics Is the Mind-killer</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 January 2014 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Benny's Bagels:  2505 W Broadway, Vancouver, British Columbia V6K 2E9</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><pre><code>For those not in the know, the Less Wrong sequences are a corpus of essays written by the self-educated blogger and AI theorist Eliezer Yudkowsky on applying rationality further into and also beyond the domains of science and skepticism. Less Wrong is the website where Mr. Yudkowsky and others blog about rationality.\n</code></pre>\n\n<p>This week we'll be reading \"Politics is the Mind-Killer\". It's a series of essays about how errors in thinking about politics and social dichotomies prevent people from reaching more accurate beliefs, and how these errors in thinking might be identified and corrected.</p>\n\n<p>If you're interested in attending, please read the posts under the heading \"most important posts\" at this link:\n<a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Politics_is_the_Mind-Killer\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Politics_is_the_Mind-Killer</a></p>\n\n<p>We'll primarily be discussing those seven essays, but feel free to read the rest of the ones in the sequence if you have the time.</p>\n\n<p>This is the first biweekly meetup for this reading and discussion group. We hope to see you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vg'>Vancouver Biweekly Sequences Discussion: Politics Is the Mind-killer</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Bc2TdbJYSS3EhgvKg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.506813002778398e-06, "legacy": true, "legacyId": "25222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Biweekly_Sequences_Discussion__Politics_Is_the_Mind_killer\">Discussion article for the meetup : <a href=\"/meetups/vg\">Vancouver Biweekly Sequences Discussion: Politics Is the Mind-killer</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 January 2014 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Benny's Bagels:  2505 W Broadway, Vancouver, British Columbia V6K 2E9</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><pre><code>For those not in the know, the Less Wrong sequences are a corpus of essays written by the self-educated blogger and AI theorist Eliezer Yudkowsky on applying rationality further into and also beyond the domains of science and skepticism. Less Wrong is the website where Mr. Yudkowsky and others blog about rationality.\n</code></pre>\n\n<p>This week we'll be reading \"Politics is the Mind-Killer\". It's a series of essays about how errors in thinking about politics and social dichotomies prevent people from reaching more accurate beliefs, and how these errors in thinking might be identified and corrected.</p>\n\n<p>If you're interested in attending, please read the posts under the heading \"most important posts\" at this link:\n<a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Politics_is_the_Mind-Killer\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Politics_is_the_Mind-Killer</a></p>\n\n<p>We'll primarily be discussing those seven essays, but feel free to read the rest of the ones in the sequence if you have the time.</p>\n\n<p>This is the first biweekly meetup for this reading and discussion group. We hope to see you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Biweekly_Sequences_Discussion__Politics_Is_the_Mind_killer1\">Discussion article for the meetup : <a href=\"/meetups/vg\">Vancouver Biweekly Sequences Discussion: Politics Is the Mind-killer</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Biweekly Sequences Discussion: Politics Is the Mind-killer", "anchor": "Discussion_article_for_the_meetup___Vancouver_Biweekly_Sequences_Discussion__Politics_Is_the_Mind_killer", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Biweekly Sequences Discussion: Politics Is the Mind-killer", "anchor": "Discussion_article_for_the_meetup___Vancouver_Biweekly_Sequences_Discussion__Politics_Is_the_Mind_killer1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-10T15:52:07.327Z", "modifiedAt": null, "url": null, "title": "Dr. Jubjub predicts a crisis", "slug": "dr-jubjub-predicts-a-crisis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:00.395Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Apprentice", "createdAt": "2009-05-20T13:06:35.976Z", "isAdmin": false, "displayName": "Apprentice"}, "userId": "X5f7X9PkRFwGyxWsb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XYd67qYjjEDmeiPiL/dr-jubjub-predicts-a-crisis", "pageUrlRelative": "/posts/XYd67qYjjEDmeiPiL/dr-jubjub-predicts-a-crisis", "linkUrl": "https://www.lesswrong.com/posts/XYd67qYjjEDmeiPiL/dr-jubjub-predicts-a-crisis", "postedAtFormatted": "Friday, January 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dr.%20Jubjub%20predicts%20a%20crisis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADr.%20Jubjub%20predicts%20a%20crisis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXYd67qYjjEDmeiPiL%2Fdr-jubjub-predicts-a-crisis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dr.%20Jubjub%20predicts%20a%20crisis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXYd67qYjjEDmeiPiL%2Fdr-jubjub-predicts-a-crisis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXYd67qYjjEDmeiPiL%2Fdr-jubjub-predicts-a-crisis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 454, "htmlBody": "<p class=\"MsoNormal\"><span lang=\"IS\"><strong>Dr. Jubjub</strong>: Sir, I have been running some calculations and I&rsquo;m worried about the way our slithy toves are heading.</span></p>\n<p class=\"MsoNormal\"><strong>Prof. Bandersnatch</strong>: Huh? Why? The toves seem fine to me. Just look at them, gyring and gimbling in the wabe over there.</p>\n<p class=\"MsoNormal\"><strong>Dr. Jubjub</strong>: Yes, but there is a distinct negative trend in my data. The toves are gradually losing their slithiness.</p>\n<p class=\"MsoNormal\"><strong>Prof. Bandersnatch</strong>: Hmm, okay. That does sound serious. How long until it becomes a problem?</p>\n<p class=\"MsoNormal\"><strong>Dr. Jubjub</strong>: Well, I&rsquo;d argue that it&rsquo;s already having negative effects but I&rsquo;d say we will reach a real crisis in around 120 years.</p>\n<p class=\"MsoNormal\"><strong>Prof. Bandersnatch</strong>: Phew, okay, you had me worried there for a moment. But it sounds like this is actually a non-problem. We can carry on working on the important stuff &ndash; technology will bail us out here in time.</p>\n<p class=\"MsoNormal\"><strong>Dr. Jubjub</strong>: Sir! We already have the technology to fix the toves. The most straightforward way would be to whiffle their tulgey wood but we could also...</p>\n<p class=\"MsoNormal\"><strong>Prof. Bandersnatch</strong>: What?? Whiffle their tulgey wood? Do you have any idea what that would cost? And besides, people won&rsquo;t stand for it &ndash; slithy toves with unwhiffled tulgey wood are a part of our way of life.</p>\n<p class=\"MsoNormal\"><strong>Dr. Jubjub</strong>: So, when you say technology will bail us out you mean you expect a solution that will be cheap, socially acceptable and developed soon?</p>\n<p class=\"MsoNormal\"><strong>Prof. Bandersnatch</strong>: Of course! Prof. Jabberwock assures me the singularity will be here around tea-time on Tuesday. That is, if we roll up our sleeves and don&rsquo;t waste time with trivialities like your tove issue.</p>\n<p class=\"MsoNormal\">Maybe it&rsquo;s just me but I feel like I run into a lot of conversations like this around here. On any problem that won&rsquo;t become an absolute crisis in the next few decades, someone will take the Bandersnatch view that it will be more easily solved later (with cheaper or more socially acceptable technology) so we shouldn&rsquo;t work directly on it now. The way out is forward - let&rsquo;s step on the gas and get to the finish line before any annoying problems catch up with us.</p>\n<p class=\"MsoNormal\">For all I know, Bandersnatch is absolutely right. But my natural inclination is to take the Jubjub view. I think the chances of a basically business-as-usual future for the next 200 or 300 years are not epsilon. They may not be very high but they seem like they need to be seriously taken into account. Problems may prove harder than they look. Apparently promising technology may not become practical. Maybe we'll have the capacity for AI in 50 years - but need another 500 years to make it friendly. I'd prefer humanity to plan in such a way that things will gradually improve rather than gradually deteriorate, even in a slow-technology scenario.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hNFdS3rRiYgqqD8aM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XYd67qYjjEDmeiPiL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 74, "extendedScore": null, "score": 0.000207, "legacy": true, "legacyId": "25223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 74, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-10T17:04:51.917Z", "modifiedAt": null, "url": null, "title": "New LW Meetups: Portland, Sydney", "slug": "new-lw-meetups-portland-sydney", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zsEqMikJmTgPWhtHo/new-lw-meetups-portland-sydney", "pageUrlRelative": "/posts/zsEqMikJmTgPWhtHo/new-lw-meetups-portland-sydney", "linkUrl": "https://www.lesswrong.com/posts/zsEqMikJmTgPWhtHo/new-lw-meetups-portland-sydney", "postedAtFormatted": "Friday, January 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetups%3A%20Portland%2C%20Sydney&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetups%3A%20Portland%2C%20Sydney%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzsEqMikJmTgPWhtHo%2Fnew-lw-meetups-portland-sydney%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetups%3A%20Portland%2C%20Sydney%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzsEqMikJmTgPWhtHo%2Fnew-lw-meetups-portland-sydney", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzsEqMikJmTgPWhtHo%2Fnew-lw-meetups-portland-sydney", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 522, "htmlBody": "<p><strong>This summary was posted to LW main on January 3rd. The following week's summary is <a href=\"/lw/jgp/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/v7\">[Portland] I moved to Portland! I want to meet you!:&nbsp;<span class=\"date\">11 January 2014 11:50AM</span></a></li>\n<li><a href=\"/meetups/v5\">Sydney Meetup: January:&nbsp;<span class=\"date\">22 January 2014 06:30PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/uu\">Montreal - How to Actually Change your Mind:&nbsp;<span class=\"date\">07 January 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/v2\">Southeast Michigan:&nbsp;<span class=\"date\">04 January 2014 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">04 January 2020 01:30PM</span></a></li>\n<li><a href=\"/meetups/uo\">Brussels monthly meetup: [topic TBD]:&nbsp;<span class=\"date\">11 January 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/v4\">London 2014 Protospective:&nbsp;<span class=\"date\">05 January 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/uv\">Vienna:&nbsp;<span class=\"date\">18 January 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/v6\">Washington DC fun and games meetup:&nbsp;<span class=\"date\">05 January 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zsEqMikJmTgPWhtHo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5072508563043933e-06, "legacy": true, "legacyId": "25177", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["W26QHc4WXrCJWCDzu", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-10T19:25:02.313Z", "modifiedAt": null, "url": null, "title": "Free online course: How to Reason and Argue starting Mon. Any interest in study group?", "slug": "free-online-course-how-to-reason-and-argue-starting-mon-any", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:00.081Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pinyaka", "createdAt": "2012-09-21T12:11:45.980Z", "isAdmin": false, "displayName": "pinyaka"}, "userId": "FscpDmNcKZdbeDNZ2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nh2C9wpKsneu7LFpr/free-online-course-how-to-reason-and-argue-starting-mon-any", "pageUrlRelative": "/posts/Nh2C9wpKsneu7LFpr/free-online-course-how-to-reason-and-argue-starting-mon-any", "linkUrl": "https://www.lesswrong.com/posts/Nh2C9wpKsneu7LFpr/free-online-course-how-to-reason-and-argue-starting-mon-any", "postedAtFormatted": "Friday, January 10th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Free%20online%20course%3A%20How%20to%20Reason%20and%20Argue%20starting%20Mon.%20Any%20interest%20in%20study%20group%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFree%20online%20course%3A%20How%20to%20Reason%20and%20Argue%20starting%20Mon.%20Any%20interest%20in%20study%20group%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNh2C9wpKsneu7LFpr%2Ffree-online-course-how-to-reason-and-argue-starting-mon-any%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Free%20online%20course%3A%20How%20to%20Reason%20and%20Argue%20starting%20Mon.%20Any%20interest%20in%20study%20group%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNh2C9wpKsneu7LFpr%2Ffree-online-course-how-to-reason-and-argue-starting-mon-any", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNh2C9wpKsneu7LFpr%2Ffree-online-course-how-to-reason-and-argue-starting-mon-any", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1095, "htmlBody": "<p>I am going to take the free Coursera class \"<a href=\"https://www.coursera.org/course/thinkagain\">Think Again: How to Reason and Argue</a>\" starting Monday, January 13 (three days from now) and I thought I'd to see if there was any interest in going through this as a group. This is one of the <a href=\"http://intelligence.org/courses/\">MIRI recommended courses</a> under the \"Heuristics and Biases\" section. If you're interested and you will sign up if we get a group together, please leave a note in the comments (if you will only sign up if the group hits a specific size, please leave that requirement in the comments as well). If enough people are willing to sign up (5 or more? idk), I will start a group on Google (or somewhere else if that's preferred) so that we can have a forum to share thoughts, ask questions, etc. Otherwise, email may be a better way to maintain contact.&nbsp;</p>\n<p>EDIT: We hit five people willing to start, so I created a Google group <a href=\"https://groups.google.com/d/forum/lw-how-to-reason-and-argue\">here</a>. If you're interested in taking the course with us, please sign up there.</p>\n<p>&nbsp;</p>\n<p>The recommended text is fairly inexpensive on Amazon (&lt;$20 USD) and can be found on libgen.info for free if that's your thing. It's taught in English, lasts 12 weeks and predicts that it will take 5-6 hours/week. More info from the course website:</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><span style=\"font-size: 2em; font-family: sofiapro-light, Arial, sans-serif; line-height: 1.2em;\">Think Again: How to Reason and Argue</span></p>\n<p style=\"box-sizing: border-box; margin: 12px 0px 10.5px; color: #333333; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\">Reasoning is important. &nbsp;This course will teach you how to do it well. &nbsp;You will learn how to understand and assess arguments by other people and how to construct good arguments of your own about whatever matters to you.</p>\n<p style=\"box-sizing: border-box; margin: 12px 0px 10.5px; color: #333333; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\">&nbsp;</p>\n<h2 class=\"coursera-course-heading\" style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px;\">About the Course</h2>\n<div class=\"coursera-course-detail\" style=\"box-sizing: border-box; margin: 10px auto 20px; clear: both; color: #5f5f5f;\">Reasoning is important. &nbsp;This course will teach you how to do it well. &nbsp;You will learn some simple but vital rules to follow in thinking about any topic at all and some common and tempting mistakes to avoid in reasoning. &nbsp;We will discuss how to identify, analyze, and evaluate arguments by other people (including politicians, used car salesmen, and teachers) and how to construct arguments of your own in order to help you decide what to believe or what to do. These skills will be useful in dealing with whatever matters most to you.</div>\n<h2 class=\"coursera-course-heading\" style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px; clear: both;\">Course Syllabus</h2>\n<div class=\"coursera-course-detail\" style=\"box-sizing: border-box; margin: 10px auto 20px; clear: both; color: #5f5f5f;\">\n<div style=\"box-sizing: border-box;\">\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\"><strong style=\"box-sizing: border-box;\">PART I: HOW TO ANALYZE ARGUMENTS</strong></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Week 1: How to Spot an Argument<br style=\"box-sizing: border-box;\" />Week 2: How to Untangle an Argument&nbsp;<br style=\"box-sizing: border-box;\" />Week 3: How to Reconstruct an Argument&nbsp;<br style=\"box-sizing: border-box;\" />Quiz #1: At the end of Week 3, students will take their first quiz.&nbsp;<br style=\"box-sizing: border-box;\" /></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\"><strong style=\"box-sizing: border-box;\">PART II: HOW TO EVALUATE DEDUCTIVE ARGUMENTS</strong></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Week 4: Propositional Logic and Truth Tables<strong style=\"box-sizing: border-box;\">&nbsp;<br style=\"box-sizing: border-box;\" /></strong>Week 5: Categorical Logic and Syllogisms&nbsp;<br style=\"box-sizing: border-box;\" />Week 6: Representing Information<br style=\"box-sizing: border-box;\" />Quiz #2: At the end of Week 6, students will take their second quiz.&nbsp;</p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\"><strong style=\"box-sizing: border-box;\">PART III: HOW TO EVALUATE INDUCTIVE ARGUMENTS</strong></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Week 7: Inductive Arguments&nbsp;<br style=\"box-sizing: border-box;\" />Week 8: Causal Reasoning&nbsp;<br style=\"box-sizing: border-box;\" />Week 9: Chance and Choice&nbsp;<br style=\"box-sizing: border-box;\" />Quiz #3: At the end of Week 9, students will take their third quiz.&nbsp;</p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\"><strong style=\"box-sizing: border-box;\">PART IV: HOW TO MESS UP ARGUMENTS</strong></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Week 10: Fallacies of Unclarity&nbsp;<br style=\"box-sizing: border-box;\" />Week 11: Fallacies of Relevance and of Vacuity&nbsp;<br style=\"box-sizing: border-box;\" />Week 12: Refutation&nbsp;<br style=\"box-sizing: border-box;\" />Quiz #4: At the end of Week 12, students will take their fourth quiz.</p>\n</div>\n</div>\n<h2 class=\"coursera-course-heading\" style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px;\">Recommended Background</h2>\n<div class=\"coursera-course-detail\" style=\"box-sizing: border-box; margin: 10px auto 20px; clear: both; color: #5f5f5f;\">This material is appropriate for introductory college students or advanced high school students&mdash;or, indeed, anyone who is interested. No special background is required other than knowledge of English.</div>\n<div class=\"coursera-course-free-textbooks\" style=\"box-sizing: border-box;\">\n<h2 style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px;\">In-course Textbooks</h2>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">As a student enrolled in this course, you will have free access to selected chapters and content for the duration of the course. All chapters were selected by the instructor specifically for this course. You will be able to access the Coursera edition of the e-textbook via an e-reader in the class site hosted by Chegg. If you click on &ldquo;Buy this book&rdquo;, you will be able to purchase the full version of the textbook, rather than the limited chapter selection in the Coursera edition. This initiative is made possible by Coursera&rsquo;s collaboration with textbook publishers and Chegg.</p>\n</div>\n<div class=\"coursera-course-textbook-list\" style=\"box-sizing: border-box;\">\n<div class=\"coursera-course-textbook-container\" style=\"box-sizing: border-box; margin: 20px 20px 20px 0px;\">\n<div class=\"coursera-course-textbook-thumbnail\" style=\"box-sizing: border-box; float: left; width: 103.9914779663086px;\"><img style=\"box-sizing: border-box; max-width: 100%; height: auto; vertical-align: middle;\" src=\"http://c.cheggcdn.com/covers2/9630000/9637145_1326226759.jpg\" alt=\"\" /></div>\n<div class=\"coursera-course-textbook-body\" style=\"box-sizing: border-box; float: right; width: 390px;\">\n<h4 style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 21px; color: inherit; text-rendering: optimizelegibility; font-size: 17.5px;\"><a class=\"coursera-course-textbook-name\" style=\"box-sizing: border-box; color: #0367b0; text-decoration: none; cursor: pointer; font-family: 'helvetica neue', helvetica, sans-serif;\" href=\"http://www.tqlkg.com/click-7115529-10692263?sid=thinkagain&amp;URL=http://www.chegg.com/textbooks/9780495603955\" target=\"_blank\">Cengage Advantage Books: Understanding Arguments</a></h4>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Author: Sinnott-Armstrong, Walter, Sinnott-Armstrong, Walter (Walter Sinnott-Armstrong), Fogelin, Robert J.<br style=\"box-sizing: border-box;\" />Publisher: CENGAGE Learning</p>\n</div>\n</div>\n</div>\n<h2 class=\"coursera-course-heading\" style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px;\">Suggested Readings</h2>\n<div class=\"coursera-course-detail\" style=\"box-sizing: border-box; margin: 10px auto 20px; clear: both; color: #5f5f5f;\">Students who want more detailed explanations or additional exercises or who want to explore these topics in more depth should consult&nbsp;<a style=\"box-sizing: border-box; color: #0367b0; text-decoration: none; cursor: pointer;\" href=\"http://www.tqlkg.com/click-7115529-10692263?sid=thinkagain&amp;URL=http://www.chegg.com/textbooks/9780495603955\">Understanding Arguments: An Introduction to Informal Logic</a>.&nbsp;</div>\n<h2 class=\"coursera-course-heading\" style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px;\">Course Format</h2>\n<div class=\"coursera-course-detail\" style=\"box-sizing: border-box; margin: 10px auto 20px; clear: both; color: #5f5f5f;\">Each week will be divided into multiple video segments that can be grouped as three lectures or viewed separately. There will be short exercises after each segment (to check comprehension) and several longer midterm quizzes.</div>\n<h2 class=\"coursera-course-heading\" style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px;\">FAQ</h2>\n<div class=\"coursera-course-detail\" style=\"box-sizing: border-box; margin: 10px auto 20px; clear: both; color: #5f5f5f;\">\n<ul style=\"box-sizing: border-box; padding: 0px; margin: 0px 0px 10.5px 25px;\">\n<li style=\"box-sizing: border-box; line-height: 21px;\"><strong style=\"box-sizing: border-box;\">Will I get a&nbsp;<strong style=\"box-sizing: border-box;\">Statement of Accomplishment</strong>&nbsp;after completing this class?</strong>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Yes. Students who successfully complete the class will receive a&nbsp;Statement of Accomplishment signed by the instructor.</p>\n</li>\n<li style=\"box-sizing: border-box; line-height: 21px;\"><strong style=\"box-sizing: border-box;\">What resources will I need for this class?</strong>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Only a working computer and internet connection.</p>\n</li>\n<li style=\"box-sizing: border-box; line-height: 21px;\"><strong style=\"box-sizing: border-box;\">What is the coolest thing I'll learn if I take this class?</strong>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Nasty names (equivocator!) to call people who try to fool you with bad arguments.</p>\n</li>\n<li style=\"box-sizing: border-box; line-height: 21px;\"><strong style=\"box-sizing: border-box; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\">What are people saying about this class?</strong>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\">Here are some remarks from students that have taken the class:&nbsp;</p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"><em style=\"box-sizing: border-box;\"><em style=\"box-sizing: border-box;\">&ldquo;I'd like to thank both professors for the course. It was fun, instructive, and I loved the input from people from all over the world, with their different views and backgrounds.&rdquo;</em><br style=\"box-sizing: border-box;\" /></em></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"><em style=\"box-sizing: border-box;\">&ldquo;Somewhere in the first couple weeks of the course, I was ruminating over some concept or perhaps over one of the homework exercises and suddenly it occurred to me, \"'Is this what thinking is?\" Just to clarify, I come from a thinking family and have thought a lot about various concepts and issues throughout my life and career...but somehow I realized that, even though I seemed to be thinking all the time, I hadn't been doing this type of thinking for quite some time...so, thanks!&rdquo;</em></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"><em style=\"box-sizing: border-box;\"></em></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"><em style=\"box-sizing: border-box;\">&ldquo;The rapport between Dr. Sinott-Armstrong and Dr. Neta and their senses of humor made the lectures engaging and enjoyable. Their passion for the subject was apparent and they were patient and thorough in their explanations.&rdquo;</em></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"><em style=\"box-sizing: border-box;\"></em></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\">The course has also been featured in a number of news articles and news reports. &nbsp;Here are links to some of these:</p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"><a style=\"box-sizing: border-box; color: #0367b0; text-decoration: none; cursor: pointer;\" href=\"https://docs.google.com/file/d/0B-lDWKjkoRC4MjJPWU1TNjFid28/edit?usp=sharing\" target=\"_blank\">Raleigh News and Observer Article - January 20, 2013</a></p>\n<a style=\"box-sizing: border-box; color: #0367b0; text-decoration: none; cursor: pointer; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\" href=\"http://www.pbs.org/newshour/bb/education/jan-june13/online_01-08.html\" target=\"_blank\">\"How Free Online Courses are Changing the Traditional Liberal Arts Education\" PBS Newshour - January 8, 2013</a><br style=\"box-sizing: border-box; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\" /></li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nh2C9wpKsneu7LFpr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.5074030381000509e-06, "legacy": true, "legacyId": "25226", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I am going to take the free Coursera class \"<a href=\"https://www.coursera.org/course/thinkagain\">Think Again: How to Reason and Argue</a>\" starting Monday, January 13 (three days from now) and I thought I'd to see if there was any interest in going through this as a group. This is one of the <a href=\"http://intelligence.org/courses/\">MIRI recommended courses</a> under the \"Heuristics and Biases\" section. If you're interested and you will sign up if we get a group together, please leave a note in the comments (if you will only sign up if the group hits a specific size, please leave that requirement in the comments as well). If enough people are willing to sign up (5 or more? idk), I will start a group on Google (or somewhere else if that's preferred) so that we can have a forum to share thoughts, ask questions, etc. Otherwise, email may be a better way to maintain contact.&nbsp;</p>\n<p>EDIT: We hit five people willing to start, so I created a Google group <a href=\"https://groups.google.com/d/forum/lw-how-to-reason-and-argue\">here</a>. If you're interested in taking the course with us, please sign up there.</p>\n<p>&nbsp;</p>\n<p>The recommended text is fairly inexpensive on Amazon (&lt;$20 USD) and can be found on libgen.info for free if that's your thing. It's taught in English, lasts 12 weeks and predicts that it will take 5-6 hours/week. More info from the course website:</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><span style=\"font-size: 2em; font-family: sofiapro-light, Arial, sans-serif; line-height: 1.2em;\">Think Again: How to Reason and Argue</span></p>\n<p style=\"box-sizing: border-box; margin: 12px 0px 10.5px; color: #333333; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\">Reasoning is important. &nbsp;This course will teach you how to do it well. &nbsp;You will learn how to understand and assess arguments by other people and how to construct good arguments of your own about whatever matters to you.</p>\n<p style=\"box-sizing: border-box; margin: 12px 0px 10.5px; color: #333333; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\">&nbsp;</p>\n<h2 class=\"coursera-course-heading\" style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px;\" id=\"About_the_Course\">About the Course</h2>\n<div class=\"coursera-course-detail\" style=\"box-sizing: border-box; margin: 10px auto 20px; clear: both; color: #5f5f5f;\">Reasoning is important. &nbsp;This course will teach you how to do it well. &nbsp;You will learn some simple but vital rules to follow in thinking about any topic at all and some common and tempting mistakes to avoid in reasoning. &nbsp;We will discuss how to identify, analyze, and evaluate arguments by other people (including politicians, used car salesmen, and teachers) and how to construct arguments of your own in order to help you decide what to believe or what to do. These skills will be useful in dealing with whatever matters most to you.</div>\n<h2 class=\"coursera-course-heading\" style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px; clear: both;\" id=\"Course_Syllabus\">Course Syllabus</h2>\n<div class=\"coursera-course-detail\" style=\"box-sizing: border-box; margin: 10px auto 20px; clear: both; color: #5f5f5f;\">\n<div style=\"box-sizing: border-box;\">\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\"><strong style=\"box-sizing: border-box;\" id=\"PART_I__HOW_TO_ANALYZE_ARGUMENTS\">PART I: HOW TO ANALYZE ARGUMENTS</strong></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Week 1: How to Spot an Argument<br style=\"box-sizing: border-box;\">Week 2: How to Untangle an Argument&nbsp;<br style=\"box-sizing: border-box;\">Week 3: How to Reconstruct an Argument&nbsp;<br style=\"box-sizing: border-box;\">Quiz #1: At the end of Week 3, students will take their first quiz.&nbsp;<br style=\"box-sizing: border-box;\"></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\"><strong style=\"box-sizing: border-box;\" id=\"PART_II__HOW_TO_EVALUATE_DEDUCTIVE_ARGUMENTS\">PART II: HOW TO EVALUATE DEDUCTIVE ARGUMENTS</strong></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Week 4: Propositional Logic and Truth Tables<strong style=\"box-sizing: border-box;\">&nbsp;<br style=\"box-sizing: border-box;\"></strong>Week 5: Categorical Logic and Syllogisms&nbsp;<br style=\"box-sizing: border-box;\">Week 6: Representing Information<br style=\"box-sizing: border-box;\">Quiz #2: At the end of Week 6, students will take their second quiz.&nbsp;</p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\"><strong style=\"box-sizing: border-box;\" id=\"PART_III__HOW_TO_EVALUATE_INDUCTIVE_ARGUMENTS\">PART III: HOW TO EVALUATE INDUCTIVE ARGUMENTS</strong></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Week 7: Inductive Arguments&nbsp;<br style=\"box-sizing: border-box;\">Week 8: Causal Reasoning&nbsp;<br style=\"box-sizing: border-box;\">Week 9: Chance and Choice&nbsp;<br style=\"box-sizing: border-box;\">Quiz #3: At the end of Week 9, students will take their third quiz.&nbsp;</p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\"><strong style=\"box-sizing: border-box;\" id=\"PART_IV__HOW_TO_MESS_UP_ARGUMENTS\">PART IV: HOW TO MESS UP ARGUMENTS</strong></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Week 10: Fallacies of Unclarity&nbsp;<br style=\"box-sizing: border-box;\">Week 11: Fallacies of Relevance and of Vacuity&nbsp;<br style=\"box-sizing: border-box;\">Week 12: Refutation&nbsp;<br style=\"box-sizing: border-box;\">Quiz #4: At the end of Week 12, students will take their fourth quiz.</p>\n</div>\n</div>\n<h2 class=\"coursera-course-heading\" style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px;\" id=\"Recommended_Background\">Recommended Background</h2>\n<div class=\"coursera-course-detail\" style=\"box-sizing: border-box; margin: 10px auto 20px; clear: both; color: #5f5f5f;\">This material is appropriate for introductory college students or advanced high school students\u2014or, indeed, anyone who is interested. No special background is required other than knowledge of English.</div>\n<div class=\"coursera-course-free-textbooks\" style=\"box-sizing: border-box;\">\n<h2 style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px;\" id=\"In_course_Textbooks\">In-course Textbooks</h2>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">As a student enrolled in this course, you will have free access to selected chapters and content for the duration of the course. All chapters were selected by the instructor specifically for this course. You will be able to access the Coursera edition of the e-textbook via an e-reader in the class site hosted by Chegg. If you click on \u201cBuy this book\u201d, you will be able to purchase the full version of the textbook, rather than the limited chapter selection in the Coursera edition. This initiative is made possible by Coursera\u2019s collaboration with textbook publishers and Chegg.</p>\n</div>\n<div class=\"coursera-course-textbook-list\" style=\"box-sizing: border-box;\">\n<div class=\"coursera-course-textbook-container\" style=\"box-sizing: border-box; margin: 20px 20px 20px 0px;\">\n<div class=\"coursera-course-textbook-thumbnail\" style=\"box-sizing: border-box; float: left; width: 103.9914779663086px;\"><img style=\"box-sizing: border-box; max-width: 100%; height: auto; vertical-align: middle;\" src=\"http://c.cheggcdn.com/covers2/9630000/9637145_1326226759.jpg\" alt=\"\"></div>\n<div class=\"coursera-course-textbook-body\" style=\"box-sizing: border-box; float: right; width: 390px;\">\n<h4 style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 21px; color: inherit; text-rendering: optimizelegibility; font-size: 17.5px;\" id=\"Cengage_Advantage_Books__Understanding_Arguments\"><a class=\"coursera-course-textbook-name\" style=\"box-sizing: border-box; color: #0367b0; text-decoration: none; cursor: pointer; font-family: 'helvetica neue', helvetica, sans-serif;\" href=\"http://www.tqlkg.com/click-7115529-10692263?sid=thinkagain&amp;URL=http://www.chegg.com/textbooks/9780495603955\" target=\"_blank\">Cengage Advantage Books: Understanding Arguments</a></h4>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Author: Sinnott-Armstrong, Walter, Sinnott-Armstrong, Walter (Walter Sinnott-Armstrong), Fogelin, Robert J.<br style=\"box-sizing: border-box;\">Publisher: CENGAGE Learning</p>\n</div>\n</div>\n</div>\n<h2 class=\"coursera-course-heading\" style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px;\" id=\"Suggested_Readings\">Suggested Readings</h2>\n<div class=\"coursera-course-detail\" style=\"box-sizing: border-box; margin: 10px auto 20px; clear: both; color: #5f5f5f;\">Students who want more detailed explanations or additional exercises or who want to explore these topics in more depth should consult&nbsp;<a style=\"box-sizing: border-box; color: #0367b0; text-decoration: none; cursor: pointer;\" href=\"http://www.tqlkg.com/click-7115529-10692263?sid=thinkagain&amp;URL=http://www.chegg.com/textbooks/9780495603955\">Understanding Arguments: An Introduction to Informal Logic</a>.&nbsp;</div>\n<h2 class=\"coursera-course-heading\" style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px;\" id=\"Course_Format\">Course Format</h2>\n<div class=\"coursera-course-detail\" style=\"box-sizing: border-box; margin: 10px auto 20px; clear: both; color: #5f5f5f;\">Each week will be divided into multiple video segments that can be grouped as three lectures or viewed separately. There will be short exercises after each segment (to check comprehension) and several longer midterm quizzes.</div>\n<h2 class=\"coursera-course-heading\" style=\"box-sizing: border-box; margin: 10.5px 0px; font-family: sofiapro-light, Arial, sans-serif; font-weight: normal; line-height: 32px; color: inherit; text-rendering: optimizelegibility; font-size: 24px;\" id=\"FAQ\">FAQ</h2>\n<div class=\"coursera-course-detail\" style=\"box-sizing: border-box; margin: 10px auto 20px; clear: both; color: #5f5f5f;\">\n<ul style=\"box-sizing: border-box; padding: 0px; margin: 0px 0px 10.5px 25px;\">\n<li style=\"box-sizing: border-box; line-height: 21px;\"><strong style=\"box-sizing: border-box;\">Will I get a&nbsp;<strong style=\"box-sizing: border-box;\">Statement of Accomplishment</strong>&nbsp;after completing this class?</strong>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Yes. Students who successfully complete the class will receive a&nbsp;Statement of Accomplishment signed by the instructor.</p>\n</li>\n<li style=\"box-sizing: border-box; line-height: 21px;\"><strong style=\"box-sizing: border-box;\">What resources will I need for this class?</strong>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Only a working computer and internet connection.</p>\n</li>\n<li style=\"box-sizing: border-box; line-height: 21px;\"><strong style=\"box-sizing: border-box;\">What is the coolest thing I'll learn if I take this class?</strong>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px;\">Nasty names (equivocator!) to call people who try to fool you with bad arguments.</p>\n</li>\n<li style=\"box-sizing: border-box; line-height: 21px;\"><strong style=\"box-sizing: border-box; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\">What are people saying about this class?</strong>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\">Here are some remarks from students that have taken the class:&nbsp;</p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"><em style=\"box-sizing: border-box;\"><em style=\"box-sizing: border-box;\">\u201cI'd like to thank both professors for the course. It was fun, instructive, and I loved the input from people from all over the world, with their different views and backgrounds.\u201d</em><br style=\"box-sizing: border-box;\"></em></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"><em style=\"box-sizing: border-box;\">\u201cSomewhere in the first couple weeks of the course, I was ruminating over some concept or perhaps over one of the homework exercises and suddenly it occurred to me, \"'Is this what thinking is?\" Just to clarify, I come from a thinking family and have thought a lot about various concepts and issues throughout my life and career...but somehow I realized that, even though I seemed to be thinking all the time, I hadn't been doing this type of thinking for quite some time...so, thanks!\u201d</em></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"><em style=\"box-sizing: border-box;\"></em></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"><em style=\"box-sizing: border-box;\">\u201cThe rapport between Dr. Sinott-Armstrong and Dr. Neta and their senses of humor made the lectures engaging and enjoyable. Their passion for the subject was apparent and they were patient and thorough in their explanations.\u201d</em></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"><em style=\"box-sizing: border-box;\"></em></p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\">The course has also been featured in a number of news articles and news reports. &nbsp;Here are links to some of these:</p>\n<p style=\"box-sizing: border-box; margin: 0px 0px 10.5px; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"><a style=\"box-sizing: border-box; color: #0367b0; text-decoration: none; cursor: pointer;\" href=\"https://docs.google.com/file/d/0B-lDWKjkoRC4MjJPWU1TNjFid28/edit?usp=sharing\" target=\"_blank\">Raleigh News and Observer Article - January 20, 2013</a></p>\n<a style=\"box-sizing: border-box; color: #0367b0; text-decoration: none; cursor: pointer; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\" href=\"http://www.pbs.org/newshour/bb/education/jan-june13/online_01-08.html\" target=\"_blank\">\"How Free Online Courses are Changing the Traditional Liberal Arts Education\" PBS Newshour - January 8, 2013</a><br style=\"box-sizing: border-box; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 13.63636302947998px; line-height: 20.99431800842285px;\"></li>\n</ul>\n</div>", "sections": [{"title": "About the Course", "anchor": "About_the_Course", "level": 1}, {"title": "Course Syllabus", "anchor": "Course_Syllabus", "level": 1}, {"title": "PART I: HOW TO ANALYZE ARGUMENTS", "anchor": "PART_I__HOW_TO_ANALYZE_ARGUMENTS", "level": 3}, {"title": "PART II: HOW TO EVALUATE DEDUCTIVE ARGUMENTS", "anchor": "PART_II__HOW_TO_EVALUATE_DEDUCTIVE_ARGUMENTS", "level": 3}, {"title": "PART III: HOW TO EVALUATE INDUCTIVE ARGUMENTS", "anchor": "PART_III__HOW_TO_EVALUATE_INDUCTIVE_ARGUMENTS", "level": 3}, {"title": "PART IV: HOW TO MESS UP ARGUMENTS", "anchor": "PART_IV__HOW_TO_MESS_UP_ARGUMENTS", "level": 3}, {"title": "Recommended Background", "anchor": "Recommended_Background", "level": 1}, {"title": "In-course Textbooks", "anchor": "In_course_Textbooks", "level": 1}, {"title": "Cengage Advantage Books: Understanding Arguments", "anchor": "Cengage_Advantage_Books__Understanding_Arguments", "level": 2}, {"title": "Suggested Readings", "anchor": "Suggested_Readings", "level": 1}, {"title": "Course Format", "anchor": "Course_Format", "level": 1}, {"title": "FAQ", "anchor": "FAQ", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 14}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-11T06:41:15.191Z", "modifiedAt": null, "url": null, "title": "Calculating an expected value", "slug": "calculating-an-expected-value", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:37.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zvPwoL5a53ejRJRm5/calculating-an-expected-value", "pageUrlRelative": "/posts/zvPwoL5a53ejRJRm5/calculating-an-expected-value", "linkUrl": "https://www.lesswrong.com/posts/zvPwoL5a53ejRJRm5/calculating-an-expected-value", "postedAtFormatted": "Saturday, January 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Calculating%20an%20expected%20value&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACalculating%20an%20expected%20value%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzvPwoL5a53ejRJRm5%2Fcalculating-an-expected-value%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Calculating%20an%20expected%20value%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzvPwoL5a53ejRJRm5%2Fcalculating-an-expected-value", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzvPwoL5a53ejRJRm5%2Fcalculating-an-expected-value", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>This is a tiny question that I wouldn't be asking if I had paid more attention in economics class. Anyway, a friend of mine was at the mall with me and he needed to go to the mall parking to retrieve his car. However, if he played at the mall casino, the parking fee would be waived. Without much interest, I heard him calculate his options out loud, until he got to this part:</p>\n<p>\"The parking fee is $4. I might get that amount waived yet lose more than that at the casino. Or I could play at the casino and win, in which case my expected value is whatever I win plus $4...\"</p>\n<p>At that moment I felt I had to intervene:</p>\n<p>\"You don't get to <em>add</em>&nbsp;the parking fee to your expected value if you win at the casino; you merely <em>don't substract it</em>.\"</p>\n<p>But he kept insisting that he could add it. We didn't meet later to check his numbers, but I was left with this question.</p>\n<p>Was my objection accurate?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zvPwoL5a53ejRJRm5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 3, "extendedScore": null, "score": 1.50813756567638e-06, "legacy": true, "legacyId": "25228", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-11T10:15:49.807Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Effective Altruism Meetup", "slug": "meetup-washington-dc-effective-altruism-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nsk2sWYPsNZbgjeej/meetup-washington-dc-effective-altruism-meetup", "pageUrlRelative": "/posts/Nsk2sWYPsNZbgjeej/meetup-washington-dc-effective-altruism-meetup", "linkUrl": "https://www.lesswrong.com/posts/Nsk2sWYPsNZbgjeej/meetup-washington-dc-effective-altruism-meetup", "postedAtFormatted": "Saturday, January 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Effective%20Altruism%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Effective%20Altruism%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNsk2sWYPsNZbgjeej%2Fmeetup-washington-dc-effective-altruism-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Effective%20Altruism%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNsk2sWYPsNZbgjeej%2Fmeetup-washington-dc-effective-altruism-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNsk2sWYPsNZbgjeej%2Fmeetup-washington-dc-effective-altruism-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 49, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vh'>Washington DC Effective Altruism Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 January 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to discuss effective altruism, or whatever else people want to talk about.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vh'>Washington DC Effective Altruism Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nsk2sWYPsNZbgjeej", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5083707788743924e-06, "legacy": true, "legacyId": "25229", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Effective_Altruism_Meetup\">Discussion article for the meetup : <a href=\"/meetups/vh\">Washington DC Effective Altruism Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 January 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to discuss effective altruism, or whatever else people want to talk about.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Effective_Altruism_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/vh\">Washington DC Effective Altruism Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Effective Altruism Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Effective_Altruism_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Effective Altruism Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Effective_Altruism_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-11T10:39:04.856Z", "modifiedAt": null, "url": null, "title": "I Will Pay $500 To Anyone Who Can Convince Me To Cancel My Cryonics Subscription", "slug": "i-will-pay-usd500-to-anyone-who-can-convince-me-to-cancel-my", "viewCount": null, "lastCommentedAt": "2020-04-14T05:21:20.999Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HxGRCquTQPSJE2k9g/i-will-pay-usd500-to-anyone-who-can-convince-me-to-cancel-my", "pageUrlRelative": "/posts/HxGRCquTQPSJE2k9g/i-will-pay-usd500-to-anyone-who-can-convince-me-to-cancel-my", "linkUrl": "https://www.lesswrong.com/posts/HxGRCquTQPSJE2k9g/i-will-pay-usd500-to-anyone-who-can-convince-me-to-cancel-my", "postedAtFormatted": "Saturday, January 11th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20Will%20Pay%20%24500%20To%20Anyone%20Who%20Can%20Convince%20Me%20To%20Cancel%20My%20Cryonics%20Subscription&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20Will%20Pay%20%24500%20To%20Anyone%20Who%20Can%20Convince%20Me%20To%20Cancel%20My%20Cryonics%20Subscription%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxGRCquTQPSJE2k9g%2Fi-will-pay-usd500-to-anyone-who-can-convince-me-to-cancel-my%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20Will%20Pay%20%24500%20To%20Anyone%20Who%20Can%20Convince%20Me%20To%20Cancel%20My%20Cryonics%20Subscription%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxGRCquTQPSJE2k9g%2Fi-will-pay-usd500-to-anyone-who-can-convince-me-to-cancel-my", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHxGRCquTQPSJE2k9g%2Fi-will-pay-usd500-to-anyone-who-can-convince-me-to-cancel-my", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 525, "htmlBody": "<p>Background:</p>\n<p>On the most recent <a href=\"/lw/j4y/2013_less_wrong_censussurvey/\">LessWrong readership survey</a>, I assigned a probability of 0.30 on the cryonics question. I had previously been persuaded to sign up for cryonics by reading the sequences, but <a href=\"/lw/iul/looking_for_opinions_of_people_like_nick_bostrom/\">this thread</a>&nbsp;and particularly <a href=\"/lw/iul/looking_for_opinions_of_people_like_nick_bostrom/9ylr\">this comment</a>&nbsp;lowered my estimate of the chances of cryonics working considerably. Also relevant from the same thread was <a href=\"/lw/iul/looking_for_opinions_of_people_like_nick_bostrom/9xc6\">ciphergoth's</a> comment:</p>\n<blockquote>\n<p>By and large cryonics critics don't make clear exactly what part of the cryonics argument they mean to target, so it's hard to say exactly whether it covers an area of their expertise, but it's at least plausible to read them as asserting that cryopreserved people are information-theoretically dead, which is not guesswork about future technology and would fall under their area of expertise.</p>\n</blockquote>\n<p>Based on this, I think there's a substantial chance that there's information out there that would convince me that the folks who dismiss cryonics as pseudoscience are essentially correct, that the right answer to the survey question was epsilon. I've seen what seem like convincing objections to cryonics, and it seems possible that an expanded version of those arguments, with full references and replies to pro-cryonics arguments, would convince me. Or someone could just go to the trouble of showing that a large majority of cryobiologists really do think cryopreserved people are information-theoretically dead.</p>\n<p>However, it's not clear to me how well worth my time it is to seek out such information. It seems coming up with decisive information would be hard, especially since e.g. ciphergoth has put a lot of energy into trying to figure out what the experts think about cryonics and come away without a clear answer. And part of the reason I signed up for cryonics in the first place is because it doesn't cost me much: the largest component is the life insurance for funding, only $50 / month.</p>\n<p>So I've decided to put a bounty on being persuaded to cancel my cryonics subscription. If no one succeeds in convincing me, it costs me nothing, and if someone does succeed in convincing me the cost is less than the cost of being signed up for cryonics for a year. And yes, I'm aware that providing one-sided financial incentives like this requires me to take the fact that I've done this into account when evaluating anti-cryonics arguments, and apply extra scrutiny to them.</p>\n<p>Note that there are several issues that ultimately go in to whether you should sign up for cryonics (the neuroscience / evaluation of current technology, estimate of the probability of a \"good\" future, various philosophical issues), I anticipate the greatest chance of being persuaded from scientific arguments. In particular, I find questions about personal identity and consciousness of uploads made from preserved brains confusing, but think there are <em>very </em>few people in the world, if any, who are likely to have much chance of getting me un-confused about those issues. The offer is blind to the exact nature of the arguments given, but I mostly foresee being persuaded by the neuroscience arguments.</p>\n<p>And of course, I'm happy to listen to people tell me why the anti-cryonics arguments are wrong and I should stay signed up for cryonics. There's just no prize for doing so.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"khReijeucXJTnsyMT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HxGRCquTQPSJE2k9g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 53, "extendedScore": null, "score": 0.000159, "legacy": true, "legacyId": "25230", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 182, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Sx26Aj3xuMzmnKE4A", "A8oGJC3kwvYJLkqWN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-12T01:25:56.497Z", "modifiedAt": null, "url": null, "title": "Try more things.", "slug": "try-more-things", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:59.993Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whales", "createdAt": "2014-01-07T06:31:44.661Z", "isAdmin": false, "displayName": "whales"}, "userId": "n7p4iZqwmT3whXoAD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZzCxs2AFThcTfFeKr/try-more-things", "pageUrlRelative": "/posts/ZzCxs2AFThcTfFeKr/try-more-things", "linkUrl": "https://www.lesswrong.com/posts/ZzCxs2AFThcTfFeKr/try-more-things", "postedAtFormatted": "Sunday, January 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Try%20more%20things.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATry%20more%20things.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZzCxs2AFThcTfFeKr%2Ftry-more-things%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Try%20more%20things.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZzCxs2AFThcTfFeKr%2Ftry-more-things", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZzCxs2AFThcTfFeKr%2Ftry-more-things", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2208, "htmlBody": "<p><em><a href=\"http://whaaales.com/try-more-things/\">(Cross-posted from my personal site.)</a></em></p>\n<p>Several months ago I began a list of \"things to try,\"&nbsp;which I share at the bottom of this post. It suggests many mundane, trivial-to-medium-cost changes to lifestyle and routine. Now that I've spent some time with most of them and pursued at least as many more personal items in the same spirit, I'll suggest you do something similar. Why?</p>\n<ul type=\"disc\">\n<li><a href=\"/lw/2sh/break_your_habits_be_more_empirical/\">Raise the temperature in your optimization algorithm</a>: avoid the trap of doing too much analysis on too little data and escape local optima.</li>\n<li>You can think of this as a system for self-improvement; something that operates on a meta level, unlike an object-level goal or technique; something that helps you <a href=\"/lw/jdr/review_of_scott_adams_how_to_fail_at_almost/\">fail at almost everything but still win big</a>.</li>\n<li>Variety of experience is an intrinsic pleasure to many, and it <a href=\"http://www.nytimes.com/2013/07/21/opinion/sunday/fast-time-and-the-aging-mind.html?_r=0\">may make you feel less that time has flown</a> as you look back on your life.</li>\n<li>Practice implementing small life changes, practice observing the effects of the changes, practice noticing further opportunities for changes, practice <a href=\"/lw/85x/value_of_information_four_examples/\">value of information</a> calculations, and reinforce your self-image as an empiricist working to improve your life. <a href=\"/lw/58m/build_small_skills_in_the_right_order/\">Build small skills in the right order</a> and you'll have better chances at bigger wins in the future.</li>\n<li>Advice often falls prey to the typical-mind (or typical-body) fallacy. That doesn't mean you should dismiss it out of hand. Think about not just how likely it is to work for you, but how beneficial it would be if it worked, how much it would cost to try, and how likely it is that trying it would give you enough information to change your behavior. Then <a href=\"/lw/5a5/no_seriously_just_try_it/\">just try it</a> anyway if it's cheap enough, because you forgot to account for uncertainty in your model inputs.</li>\n<li>Speaking of value of information: don't ignore tweakable variables just because you don't yet have a gwern-tier tracking and evaluation apparatus for the perfect self-experiment. Sometimes you can expect consciously noticeable non-placebo effects from a successful trial. You might do better picking the low hanging fruit to gain momentum before you invest in a Zeo and a statistics textbook.</li>\n<li>You know what, if there's an effect, it may not even need to be non-placebo. C.f. \"<a href=\"/lw/21l/lampshading/\">Lampshading</a>,\" as well as the often-observed \"honeymoon\" period of success with new productivity systems.</li>\n<li>It's very tempting, especially in certain communities, to focus exclusively on shiny, counterintuitive, \"rational,\" tech-based, hackeresque, or otherwise clever interventions and grand personal development schemes. Some of these are even good, but one suspects that some are optimized for punchiness, not effectiveness. Conversely, mundane ideas <a href=\"/r/discussion/lw/9q5/on_saying_the_obvious/\">may not propagate as well</a>, despite being potentially <a href=\"/lw/gx5/boring_advice_repository/\">equally or more likely to succeed</a>.</li>\n<li>If you were already convinced of all of the above, then great! I hope you have the agency to try stuff like this all the time. If not, you might find it useful, as I did, just to have a list like this available. It's one less <a href=\"/lw/f1/beware_trivial_inconveniences/\">trivial inconvenience</a> between thinking \"I should try more things\" and actually trying something. I've also found that I'm more likely to notice and remember optimization opportunities now that I have a place to capture them. And having spent the time to write them down and occasionally look over them, I'm more likely to notice when I'm in a position to enact something context-dependent on the list.</li>\n</ul>\n<p>I removed the terribly personal items from my list, but what remains is still somewhat tailored to my own situation and habits. These are not recommendations; they are just things that struck me as having enough potential value to try for a week or two. The list isn't not remotely comprehensive, even as far as mundane self-experiments are concerned, but it's left as an exercise to the reader to find and fill the gaps.&nbsp;Take this list as an example or as a starting point, and brainstorm ideas of your own in the comments. The usual recommendation applies against going overboard in domains where you're currently impulsive or unreflective.</p>\n<p><span style=\"line-height: 1.5em;\">Related posts:&nbsp;</span><a href=\"/lw/gx5/boring_advice_repository\">Boring Advice Repository</a>,&nbsp;<a href=\"/lw/2sh/break_your_habits_be_more_empirical/\">Break your habits: Be more empirical</a>,&nbsp;<a href=\"/r/discussion/lw/9q5/on_saying_the_obvious/\">On saying the obvious</a>,&nbsp;<a href=\"/lw/85x/value_of_information_four_examples/\">Value of Information: Four Examples</a>,&nbsp;<a href=\"/lw/5r6/spend_money_on_ergonomics/\">Spend money on ergonomics</a>,&nbsp;<a href=\"/lw/4ln/go_try_things/\">Go try things</a>,&nbsp;<a href=\"/lw/4up/dont_fear_failure/\">Don't fear failure</a>,&nbsp;<a href=\"/lw/53e/just_try_it_quantity_trumps_quality/\">Just try it: Quantity trumps quality</a>,&nbsp;<a href=\"/lw/5a5/no_seriously_just_try_it/\">No, seriously, just try it</a>,&nbsp;etc.<a id=\"more\"></a></p>\n<h3><span style=\"font-size: 15px;\">META</span></h3>\n<ul type=\"disc\">\n<li><em>Before</em> you read the rest of this list, spend two minutes brainstorming ideas to try! \n<ul type=\"circle\">\n<li>In what domains are you in a rut? What do you do frequently, and what are alternative ways to do it? What do others do differently? What vague dissatisfactions tickle your attention?</li>\n</ul>\n</li>\n<li><span style=\"line-height: 1.5em;\">Incorporate trying things from your list into your routine</span></li>\n<li><span style=\"line-height: 1.5em;\">Incorporate adding things to your list into your routine</span></li>\n<li><span style=\"line-height: 1.5em;\">Do something you've tried in the past (this is \"try more things,\" not \"new things\")</span></li>\n<li><span style=\"line-height: 1.5em;\">Attempt some value of information calculations for trying or researching items below</span></li>\n<li><span style=\"line-height: 1.5em;\">Ask friends/coworkers for recommendations (or bring them in on the adventures herein)</span></li>\n<li><span style=\"line-height: 1.5em;\">Create a system to reliably capture ideas before you forget them and later add them to your list&nbsp;(e.g. take notes on your phone)</span></li>\n<li><span style=\"line-height: 1.5em;\">Learn about and implement some more rigorous self-experimentation</span></li>\n<li>Learn to break down desired new behaviors into <a href=\"/lw/ita/how_habits_work_and_how_you_may_control_them/\">cue, routine, reward</a>&nbsp;and <a href=\"http://www.stevepavlina.com/blog/2006/04/how-to-get-up-right-away-when-your-alarm-goes-off/\">practice them offline</a></li>\n</ul>\n<h3>SLEEP</h3>\n<ul type=\"disc\">\n<li><a href=\"http://www.amazon.com/Howard-Leight-Laser-Earplugs-Cords/dp/B00362CNMM/ref=sr_1_7?s=industrial&amp;ie=UTF8&amp;qid=1388855430&amp;sr=1-7\">Earplugs</a>, or a change in style or brand if you already use them</li>\n<li><a href=\"http://www.gwern.net/Melatonin\">Melatonin</a>, or vary dose and timing</li>\n<li><a href=\"http://www.amazon.com/gp/product/B000WNX21Y/ref=s9_hps_bw_g121_i1?pf_rd_m=ATVPDKIKX0DER&amp;pf_rd_s=merchandised-search-3&amp;pf_rd_r=09JXE8EA7BJ6BKFWJT8Q&amp;pf_rd_t=101&amp;pf_rd_p=1615573342&amp;pf_rd_i=2586470011\">Sleep mask</a>; an extra pillowcase as a blindfold might be sufficient</li>\n<li>Wear socks or slippers to bed</li>\n<li>Different pillows, sheets or bedding</li>\n<li><span style=\"line-height: 1.5em;\">Side/back sleeping</span></li>\n<li><span style=\"line-height: 1.5em;\">Windows open/closed</span></li>\n<li><span style=\"line-height: 1.5em;\">White noise (perhaps a fan or a recording)</span></li>\n<li><span style=\"line-height: 1.5em;\">Humidifier</span></li>\n<li><span style=\"line-height: 1.5em;\">Air filter</span></li>\n<li><span style=\"line-height: 1.5em;\">Blackout curtains (particularly if you find a sleep mask uncomfortable but like the darkness)</span></li>\n<li><span style=\"line-height: 1.5em;\">Napping</span></li>\n<li><span style=\"line-height: 1.5em;\">Morning vitamin D</span></li>\n<li><span style=\"line-height: 1.5em;\">\"Sleep-tracking\" phone app (can record movement and noise, which is sometimes informative)</span></li>\n<li><span style=\"line-height: 1.5em;\">Antihistamines in the case of allergies disrupting breathing</span></li>\n<li><span style=\"line-height: 1.5em;\">Dream journal</span></li>\n<li><span style=\"line-height: 1.5em;\">Sleep journal (notes on sleep time, quality, etc.)</span></li>\n<li><span style=\"line-height: 1.5em;\">Lucid dreaming</span></li>\n<li><strong><span style=\"line-height: 1.5em;\">BEDTIME ROUTINE</span></strong> \n<ul>\n<li>Construct a nighttime ritual</li>\n<li><span style=\"line-height: 1.5em;\">Keep to a specific bedtime</span></li>\n<li><span style=\"line-height: 1.5em;\">Use bed for sleep only</span></li>\n<li><span style=\"line-height: 1.5em;\">Stop using a computer by T minus X hours</span></li>\n<li><span style=\"line-height: 1.5em;\">Stop working by T minus Y hours</span></li>\n<li><span style=\"line-height: 1.5em;\">Don't eat after T minus Z hours</span></li>\n<li><span style=\"line-height: 1.5em;\">Alternatively, light snack before bed</span></li>\n<li><span style=\"line-height: 1.5em;\">Don't drink after T minus V hours</span></li>\n<li><span style=\"line-height: 1.5em;\">Change lighting by T minus W hours (warm/dim lights)</span></li>\n<li><span style=\"line-height: 1.5em;\">Use flux&nbsp; or alternatives </span><a style=\"line-height: 1.5em;\" href=\"http://alternativeto.net/software/f46lux/\">http://alternativeto.net/software/f46lux/</a></li>\n<li><span style=\"line-height: 1.5em;\">Stretching/breathing exercise</span></li>\n<li><span style=\"line-height: 1.5em;\">Intense exercise (most likely to be beneficial well before bedtime ritual starts; I recall reading at least three hours)</span></li>\n<li><span style=\"line-height: 1.5em;\">Further research on sleep habits</span></li>\n</ul>\n</li>\n<li><strong><span style=\"line-height: 1.5em;\">WAKING ROUTINE</span></strong> \n<ul>\n<li><span style=\"line-height: 1.5em;\">Use an alarm, or use a different sound, or place it somewhere new</span> \n<ul>\n<li><span style=\"line-height: 1.5em;\">set up difficult tasks to turn off alarm</span></li>\n<li><span style=\"line-height: 1.5em;\">use a light on a timer rather than sound</span></li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><span style=\"line-height: 1.5em;\">Don't use an alarm -- wake up to daylight or use your natural cycle</span></li>\n<li><a style=\"line-height: 1.5em;\" href=\"/lw/h9b/post_ridiculous_munchkin_ideas/8yfw\">Practice offline</a><span style=\"line-height: 1.5em;\"> (either with naps, or just getting in bed and then out again)</span></li>\n<li><span style=\"line-height: 1.5em;\">Have a 'halfway point' to getting out of bed which makes things take much less than half the effort</span></li>\n<li><span style=\"line-height: 1.5em;\">Count down from 10, intensely focusing on your plan to get out of bed when you get to 0</span></li>\n<li><span style=\"line-height: 1.5em;\">Open windows or go outside after waking (particularly if it's sunny)</span></li>\n<li>Splash cold water on your face</li>\n<li><span style=\"line-height: 1.5em;\">Morning stretching , light exercise, or intense exercise routine</span></li>\n</ul>\n</li>\n</ul>\n<h3>WORK ENVIRONMENT</h3>\n<ul type=\"disc\">\n<li><span style=\"line-height: 1.5em;\">Change relative heights of chair, keyboard, monitor (can stack books under desk items)</span></li>\n<li><span style=\"line-height: 1.5em;\">Different desk chairs (ask coworkers if they want to trade for a day)</span></li>\n<li><span style=\"line-height: 1.5em;\">Lighting</span> \n<ul>\n<li><span style=\"line-height: 1.5em;\">Really bright daylight bulbs</span></li>\n<li><span style=\"line-height: 1.5em;\">'Warm light' bulbs</span></li>\n</ul>\n</li>\n<li><span style=\"line-height: 1.5em;\">Different keyboards</span></li>\n<li><a style=\"line-height: 1.5em;\" href=\"/lw/5r6/spend_money_on_ergonomics/\">Other ergonomics</a></li>\n<li><span style=\"line-height: 1.5em;\">Music</span></li>\n<li><span style=\"line-height: 1.5em;\">White/brown noise</span></li>\n<li><span style=\"line-height: 1.5em;\">Earplugs</span></li>\n<li><span style=\"line-height: 1.5em;\">Establish a policy for interruptions</span></li>\n<li><span style=\"line-height: 1.5em;\">Decorations</span></li>\n</ul>\n<h3><span style=\"line-height: 1.5em;\">WORK ROUTINE</span></h3>\n<ul>\n<li>Take breaks to stretch, stand, walk, or meditate</li>\n<li><span style=\"line-height: 1.5em;\">Try different kinds of work at different times of day</span></li>\n<li><span style=\"line-height: 1.5em;\">Create a routine for entering deep focus</span></li>\n<li><span style=\"line-height: 1.5em;\">Create a ritual or checklist for ending procrastination and starting work</span></li>\n<li><span style=\"line-height: 1.5em;\">Naps</span></li>\n<li><span style=\"line-height: 1.5em;\">Snacks</span></li>\n<li><span style=\"line-height: 1.5em;\">Co-working</span></li>\n<li><span style=\"line-height: 1.5em;\">Collaboration</span></li>\n<li><span style=\"line-height: 1.5em;\">Learn keyboard shortcuts for any application you use frequently</span></li>\n<li><span style=\"line-height: 1.5em;\">Voice recording (e.g. as notetaking while reading)</span></li>\n<li><span style=\"line-height: 1.5em;\">Voice input for computer work (or for your phone)</span></li>\n<li><span style=\"line-height: 1.5em;\">Getting Things Done, or a different system for implementing the key principles of attention saving and strategic review</span></li>\n<li><span style=\"line-height: 1.5em;\">Time-tracking</span></li>\n<li><span style=\"line-height: 1.5em;\">Time-boxing</span> \n<ul>\n<li><span style=\"line-height: 1.5em;\">Seriously, spend 15 minutes blocking out hourly plans every day</span></li>\n</ul>\n</li>\n<li><span style=\"line-height: 1.5em;\">Pomodoros (may also help by forcing you to break down tasks)</span></li>\n<li><a style=\"line-height: 1.5em;\" href=\"http://www.tinychat.com/lw\">LW Study Hall</a></li>\n<li><span style=\"line-height: 1.5em;\">Anything on </span><a style=\"line-height: 1.5em;\" href=\"/lw/1sm/akrasia_tactics_review/\">http://lesswrong.com/lw/1sm/akrasia_tactics_review/</a></li>\n<li><span style=\"line-height: 1.5em;\">Inbox Zero</span></li>\n<li><span style=\"line-height: 1.5em;\">Email filtering</span></li>\n<li><span style=\"line-height: 1.5em;\">Process email or other routine tasks in batches</span></li>\n<li><span style=\"line-height: 1.5em;\">Virtual assistant</span></li>\n<li><span style=\"line-height: 1.5em;\">Remove browser autocomplete suggestions for impulse browsing. It's &lt;shift&gt;&lt;del&gt; with the suggestion highlighted in Chrome.</span></li>\n</ul>\n<h3>LEISURE</h3>\n<ul type=\"disc\">\n<li>Brainstorm a list of endorsed activities to supplant unenjoyable, unrefreshing procrastination</li>\n<li>Walk around, go outside, listen to music, listen to <a href=\"http://www.maximumfun.org/shows/my-brother-my-brother-and-me\">a comedy podcast</a>, meditate, read, do recreational math, sing, dance, exercise, etc.</li>\n</ul>\n<h3>COMMUTE</h3>\n<ul type=\"disc\">\n<li>Different routes, weighing stress, safety, scenery, length as you see fit</li>\n<li><span style=\"line-height: 1.5em;\">Different times of day</span></li>\n<li><span style=\"line-height: 1.5em;\">Music/podcasts/audiobooks</span></li>\n<li><span style=\"line-height: 1.5em;\">Biking: This can be logistically complicated but still worthwhile. Try seriously thinking for two minutes about what is stopping you from trying it, and whether those obstacles can be removed.</span></li>\n</ul>\n<h3>EXERCISE</h3>\n<ul type=\"disc\">\n<li>Bodyweight workout (various push-ups, sit-ups, pistol squats, etc.)</li>\n<li>Biking</li>\n<li><span style=\"line-height: 1.5em;\">Running</span></li>\n<li><span style=\"line-height: 1.5em;\">Yoga</span></li>\n<li><span style=\"line-height: 1.5em;\">A </span><a style=\"line-height: 1.5em;\" href=\"/lw/h9b/post_ridiculous_munchkin_ideas/8yet\">pullup bar</a><span style=\"line-height: 1.5em;\"> or dumbbells</span></li>\n<li>Dance</li>\n<li><span style=\"line-height: 1.5em;\">Gym</span></li>\n</ul>\n<h3>FOOD</h3>\n<ul type=\"disc\">\n<li>Write a <a href=\"/lw/gx5/boring_advice_repository/917i\">weekly meal plan</a></li>\n<li><span style=\"line-height: 1.5em;\">Find some recipe blogs to follow</span></li>\n<li><span style=\"line-height: 1.5em;\">Try a new recipe (bonus points if it's more difficult than usual/from an unfamiliar genre/otherwise stretches your cooking skills)</span></li>\n<li><span style=\"line-height: 1.5em;\">Calculate recipe costs</span></li>\n<li><span style=\"line-height: 1.5em;\">Go somewhere new or just order something new unusual</span></li>\n<li><span style=\"line-height: 1.5em;\">Try snacks/sugar/caffeine at different times of the day</span></li>\n<li><span style=\"line-height: 1.5em;\">Reduce/eliminate something (e.g. sugar/caffeine/dairy)</span></li>\n<li><span style=\"line-height: 1.5em;\">Try </span><a style=\"line-height: 1.5em;\" href=\"/lw/h9b/post_ridiculous_munchkin_ideas/8yhj\">Soylent</a></li>\n<li><span style=\"line-height: 1.5em;\">I don't actually know anything about nutrition or dieting; maybe fix that?</span></li>\n</ul>\n<h3>MUSIC</h3>\n<ul type=\"disc\">\n<li>Check whether your favorite musicians (or even ones you only kind of liked before) have released new music</li>\n<li><span style=\"line-height: 1.5em;\">Find a service where you can listen to music for free with minimal inconvenience (YouTube and Spotify are usable for me, but just barely)</span></li>\n<li><span style=\"line-height: 1.5em;\">List artists you've \"been meaning to get around to listening to\" and use the above to actually do that</span></li>\n<li><span style=\"line-height: 1.5em;\">Listen to things outside your usual tastes</span></li>\n<li><span style=\"line-height: 1.5em;\">Listen to (internet) radio stations</span> \n<ul>\n<li><span style=\"line-height: 1.5em;\">Your local college radio (or any college radio, since they usually stream online) will have a huge variety of programs, some of which should be good</span></li>\n<li><span style=\"line-height: 1.5em;\">Use Google to find a good one; I really like </span><a style=\"line-height: 1.5em;\" href=\"http://kbaq.org/\">KBAQ</a><span style=\"line-height: 1.5em;\"> for classical</span></li>\n</ul>\n</li>\n<li><span style=\"line-height: 1.5em;\">Try different headphones (go to a store, or ask your friends if you can borrow theirs for a bit)</span></li>\n<li><span style=\"line-height: 1.5em;\">Find a \"best album of the year\" thread from a non-music-related forum; you'll get some pretty diverse picks</span></li>\n</ul>\n<h3>OTHER</h3>\n<ul>\n<li>Different soap/shampoo/shaving cream/razors/other grooming products</li>\n<li><span style=\"line-height: 1.5em;\">Different socks</span></li>\n<li><span style=\"line-height: 1.5em;\">Barefoot shoes</span></li>\n<li><span style=\"line-height: 1.5em;\">Journal</span></li>\n<li><span style=\"line-height: 1.5em;\">Gratitude journal</span></li>\n<li>Comfort zone expansion</li>\n<li><span style=\"line-height: 1.5em;\">Cold showers</span></li>\n<li><span style=\"line-height: 1.5em;\">Meditation</span></li>\n<li><span style=\"line-height: 1.5em;\">Various reading, lecture-watching, note-taking, or review strategies</span></li>\n<li><span style=\"line-height: 1.5em;\">Alternatives to software and online services you use frequently (whether or not you feel happy with your choice already)</span> \n<ul>\n<li><a style=\"line-height: 1.5em;\" href=\"http://alternativeto.net/\">http://alternativeto.net</a><span style=\"line-height: 1.5em;\"> and of course </span><a style=\"line-height: 1.5em;\" href=\"http://alternativeto.net/software/alternativeto/\">http://alternativeto.net/software/alternativeto/</a></li>\n</ul>\n</li>\n<li><a style=\"line-height: 1.5em;\" href=\"/lw/h9b/post_ridiculous_munchkin_ideas/91lq\">Watch videos at higher speeds</a></li>\n<li><a style=\"line-height: 1.5em;\" href=\"https://habitrpg.com/static/front\">HabitRPG</a><span style=\"line-height: 1.5em;\">, </span><a style=\"line-height: 1.5em;\" href=\"https://www.beeminder.com/\">Beeminder</a><span style=\"line-height: 1.5em;\">, and/or </span><a style=\"line-height: 1.5em;\" href=\"http://stickk.com\">Stickk</a><span style=\"line-height: 1.5em;\">; brainstorm some goals</span> \n<ul>\n<li><span style=\"line-height: 1.5em;\">e.g. pomodoros, Anki reviews/card-making, trying more things, reading, endorsed leisure, meditation, journaling, strategic reviews</span></li>\n</ul>\n</li>\n<li><a style=\"line-height: 1.5em;\" href=\"http://ankisrs.net\">Anki</a>&nbsp;(or <a href=\"http://www.gwern.net/Spaced%20repetition?2\">spaced repetition</a> more generally)</li>\n<li><span style=\"line-height: 1.5em;\">The </span><a style=\"line-height: 1.5em;\" href=\"http://tinyhabits.com/\">http://tinyhabits.com/</a><span style=\"line-height: 1.5em;\"> course</span></li>\n<li><span style=\"line-height: 1.5em;\"><span style=\"line-height: 1.5em;\">Typing practice to improve speed + accuracy </span></span></li>\n<li><span style=\"line-height: 1.5em;\">An alternative keyboard layout</span></li>\n<li><span style=\"line-height: 1.5em;\">Reading practice for speed + comprehension</span></li>\n<li><span style=\"line-height: 1.5em;\">Learn to juggle</span></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2, "AodfCFefLAuwDyj7Z": 2, "8nAXyYLu8eT72Hwuh": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZzCxs2AFThcTfFeKr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 57, "baseScore": 78, "extendedScore": null, "score": 0.00021, "legacy": true, "legacyId": "25234", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 78, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em><a href=\"http://whaaales.com/try-more-things/\">(Cross-posted from my personal site.)</a></em></p>\n<p>Several months ago I began a list of \"things to try,\"&nbsp;which I share at the bottom of this post. It suggests many mundane, trivial-to-medium-cost changes to lifestyle and routine. Now that I've spent some time with most of them and pursued at least as many more personal items in the same spirit, I'll suggest you do something similar. Why?</p>\n<ul type=\"disc\">\n<li><a href=\"/lw/2sh/break_your_habits_be_more_empirical/\">Raise the temperature in your optimization algorithm</a>: avoid the trap of doing too much analysis on too little data and escape local optima.</li>\n<li>You can think of this as a system for self-improvement; something that operates on a meta level, unlike an object-level goal or technique; something that helps you <a href=\"/lw/jdr/review_of_scott_adams_how_to_fail_at_almost/\">fail at almost everything but still win big</a>.</li>\n<li>Variety of experience is an intrinsic pleasure to many, and it <a href=\"http://www.nytimes.com/2013/07/21/opinion/sunday/fast-time-and-the-aging-mind.html?_r=0\">may make you feel less that time has flown</a> as you look back on your life.</li>\n<li>Practice implementing small life changes, practice observing the effects of the changes, practice noticing further opportunities for changes, practice <a href=\"/lw/85x/value_of_information_four_examples/\">value of information</a> calculations, and reinforce your self-image as an empiricist working to improve your life. <a href=\"/lw/58m/build_small_skills_in_the_right_order/\">Build small skills in the right order</a> and you'll have better chances at bigger wins in the future.</li>\n<li>Advice often falls prey to the typical-mind (or typical-body) fallacy. That doesn't mean you should dismiss it out of hand. Think about not just how likely it is to work for you, but how beneficial it would be if it worked, how much it would cost to try, and how likely it is that trying it would give you enough information to change your behavior. Then <a href=\"/lw/5a5/no_seriously_just_try_it/\">just try it</a> anyway if it's cheap enough, because you forgot to account for uncertainty in your model inputs.</li>\n<li>Speaking of value of information: don't ignore tweakable variables just because you don't yet have a gwern-tier tracking and evaluation apparatus for the perfect self-experiment. Sometimes you can expect consciously noticeable non-placebo effects from a successful trial. You might do better picking the low hanging fruit to gain momentum before you invest in a Zeo and a statistics textbook.</li>\n<li>You know what, if there's an effect, it may not even need to be non-placebo. C.f. \"<a href=\"/lw/21l/lampshading/\">Lampshading</a>,\" as well as the often-observed \"honeymoon\" period of success with new productivity systems.</li>\n<li>It's very tempting, especially in certain communities, to focus exclusively on shiny, counterintuitive, \"rational,\" tech-based, hackeresque, or otherwise clever interventions and grand personal development schemes. Some of these are even good, but one suspects that some are optimized for punchiness, not effectiveness. Conversely, mundane ideas <a href=\"/r/discussion/lw/9q5/on_saying_the_obvious/\">may not propagate as well</a>, despite being potentially <a href=\"/lw/gx5/boring_advice_repository/\">equally or more likely to succeed</a>.</li>\n<li>If you were already convinced of all of the above, then great! I hope you have the agency to try stuff like this all the time. If not, you might find it useful, as I did, just to have a list like this available. It's one less <a href=\"/lw/f1/beware_trivial_inconveniences/\">trivial inconvenience</a> between thinking \"I should try more things\" and actually trying something. I've also found that I'm more likely to notice and remember optimization opportunities now that I have a place to capture them. And having spent the time to write them down and occasionally look over them, I'm more likely to notice when I'm in a position to enact something context-dependent on the list.</li>\n</ul>\n<p>I removed the terribly personal items from my list, but what remains is still somewhat tailored to my own situation and habits. These are not recommendations; they are just things that struck me as having enough potential value to try for a week or two. The list isn't not remotely comprehensive, even as far as mundane self-experiments are concerned, but it's left as an exercise to the reader to find and fill the gaps.&nbsp;Take this list as an example or as a starting point, and brainstorm ideas of your own in the comments. The usual recommendation applies against going overboard in domains where you're currently impulsive or unreflective.</p>\n<p><span style=\"line-height: 1.5em;\">Related posts:&nbsp;</span><a href=\"/lw/gx5/boring_advice_repository\">Boring Advice Repository</a>,&nbsp;<a href=\"/lw/2sh/break_your_habits_be_more_empirical/\">Break your habits: Be more empirical</a>,&nbsp;<a href=\"/r/discussion/lw/9q5/on_saying_the_obvious/\">On saying the obvious</a>,&nbsp;<a href=\"/lw/85x/value_of_information_four_examples/\">Value of Information: Four Examples</a>,&nbsp;<a href=\"/lw/5r6/spend_money_on_ergonomics/\">Spend money on ergonomics</a>,&nbsp;<a href=\"/lw/4ln/go_try_things/\">Go try things</a>,&nbsp;<a href=\"/lw/4up/dont_fear_failure/\">Don't fear failure</a>,&nbsp;<a href=\"/lw/53e/just_try_it_quantity_trumps_quality/\">Just try it: Quantity trumps quality</a>,&nbsp;<a href=\"/lw/5a5/no_seriously_just_try_it/\">No, seriously, just try it</a>,&nbsp;etc.<a id=\"more\"></a></p>\n<h3 id=\"META\"><span style=\"font-size: 15px;\">META</span></h3>\n<ul type=\"disc\">\n<li><em>Before</em> you read the rest of this list, spend two minutes brainstorming ideas to try! \n<ul type=\"circle\">\n<li>In what domains are you in a rut? What do you do frequently, and what are alternative ways to do it? What do others do differently? What vague dissatisfactions tickle your attention?</li>\n</ul>\n</li>\n<li><span style=\"line-height: 1.5em;\">Incorporate trying things from your list into your routine</span></li>\n<li><span style=\"line-height: 1.5em;\">Incorporate adding things to your list into your routine</span></li>\n<li><span style=\"line-height: 1.5em;\">Do something you've tried in the past (this is \"try more things,\" not \"new things\")</span></li>\n<li><span style=\"line-height: 1.5em;\">Attempt some value of information calculations for trying or researching items below</span></li>\n<li><span style=\"line-height: 1.5em;\">Ask friends/coworkers for recommendations (or bring them in on the adventures herein)</span></li>\n<li><span style=\"line-height: 1.5em;\">Create a system to reliably capture ideas before you forget them and later add them to your list&nbsp;(e.g. take notes on your phone)</span></li>\n<li><span style=\"line-height: 1.5em;\">Learn about and implement some more rigorous self-experimentation</span></li>\n<li>Learn to break down desired new behaviors into <a href=\"/lw/ita/how_habits_work_and_how_you_may_control_them/\">cue, routine, reward</a>&nbsp;and <a href=\"http://www.stevepavlina.com/blog/2006/04/how-to-get-up-right-away-when-your-alarm-goes-off/\">practice them offline</a></li>\n</ul>\n<h3 id=\"SLEEP\">SLEEP</h3>\n<ul type=\"disc\">\n<li><a href=\"http://www.amazon.com/Howard-Leight-Laser-Earplugs-Cords/dp/B00362CNMM/ref=sr_1_7?s=industrial&amp;ie=UTF8&amp;qid=1388855430&amp;sr=1-7\">Earplugs</a>, or a change in style or brand if you already use them</li>\n<li><a href=\"http://www.gwern.net/Melatonin\">Melatonin</a>, or vary dose and timing</li>\n<li><a href=\"http://www.amazon.com/gp/product/B000WNX21Y/ref=s9_hps_bw_g121_i1?pf_rd_m=ATVPDKIKX0DER&amp;pf_rd_s=merchandised-search-3&amp;pf_rd_r=09JXE8EA7BJ6BKFWJT8Q&amp;pf_rd_t=101&amp;pf_rd_p=1615573342&amp;pf_rd_i=2586470011\">Sleep mask</a>; an extra pillowcase as a blindfold might be sufficient</li>\n<li>Wear socks or slippers to bed</li>\n<li>Different pillows, sheets or bedding</li>\n<li><span style=\"line-height: 1.5em;\">Side/back sleeping</span></li>\n<li><span style=\"line-height: 1.5em;\">Windows open/closed</span></li>\n<li><span style=\"line-height: 1.5em;\">White noise (perhaps a fan or a recording)</span></li>\n<li><span style=\"line-height: 1.5em;\">Humidifier</span></li>\n<li><span style=\"line-height: 1.5em;\">Air filter</span></li>\n<li><span style=\"line-height: 1.5em;\">Blackout curtains (particularly if you find a sleep mask uncomfortable but like the darkness)</span></li>\n<li><span style=\"line-height: 1.5em;\">Napping</span></li>\n<li><span style=\"line-height: 1.5em;\">Morning vitamin D</span></li>\n<li><span style=\"line-height: 1.5em;\">\"Sleep-tracking\" phone app (can record movement and noise, which is sometimes informative)</span></li>\n<li><span style=\"line-height: 1.5em;\">Antihistamines in the case of allergies disrupting breathing</span></li>\n<li><span style=\"line-height: 1.5em;\">Dream journal</span></li>\n<li><span style=\"line-height: 1.5em;\">Sleep journal (notes on sleep time, quality, etc.)</span></li>\n<li><span style=\"line-height: 1.5em;\">Lucid dreaming</span></li>\n<li><strong><span style=\"line-height: 1.5em;\">BEDTIME ROUTINE</span></strong> \n<ul>\n<li>Construct a nighttime ritual</li>\n<li><span style=\"line-height: 1.5em;\">Keep to a specific bedtime</span></li>\n<li><span style=\"line-height: 1.5em;\">Use bed for sleep only</span></li>\n<li><span style=\"line-height: 1.5em;\">Stop using a computer by T minus X hours</span></li>\n<li><span style=\"line-height: 1.5em;\">Stop working by T minus Y hours</span></li>\n<li><span style=\"line-height: 1.5em;\">Don't eat after T minus Z hours</span></li>\n<li><span style=\"line-height: 1.5em;\">Alternatively, light snack before bed</span></li>\n<li><span style=\"line-height: 1.5em;\">Don't drink after T minus V hours</span></li>\n<li><span style=\"line-height: 1.5em;\">Change lighting by T minus W hours (warm/dim lights)</span></li>\n<li><span style=\"line-height: 1.5em;\">Use flux&nbsp; or alternatives </span><a style=\"line-height: 1.5em;\" href=\"http://alternativeto.net/software/f46lux/\">http://alternativeto.net/software/f46lux/</a></li>\n<li><span style=\"line-height: 1.5em;\">Stretching/breathing exercise</span></li>\n<li><span style=\"line-height: 1.5em;\">Intense exercise (most likely to be beneficial well before bedtime ritual starts; I recall reading at least three hours)</span></li>\n<li><span style=\"line-height: 1.5em;\">Further research on sleep habits</span></li>\n</ul>\n</li>\n<li><strong><span style=\"line-height: 1.5em;\">WAKING ROUTINE</span></strong> \n<ul>\n<li><span style=\"line-height: 1.5em;\">Use an alarm, or use a different sound, or place it somewhere new</span> \n<ul>\n<li><span style=\"line-height: 1.5em;\">set up difficult tasks to turn off alarm</span></li>\n<li><span style=\"line-height: 1.5em;\">use a light on a timer rather than sound</span></li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><span style=\"line-height: 1.5em;\">Don't use an alarm -- wake up to daylight or use your natural cycle</span></li>\n<li><a style=\"line-height: 1.5em;\" href=\"/lw/h9b/post_ridiculous_munchkin_ideas/8yfw\">Practice offline</a><span style=\"line-height: 1.5em;\"> (either with naps, or just getting in bed and then out again)</span></li>\n<li><span style=\"line-height: 1.5em;\">Have a 'halfway point' to getting out of bed which makes things take much less than half the effort</span></li>\n<li><span style=\"line-height: 1.5em;\">Count down from 10, intensely focusing on your plan to get out of bed when you get to 0</span></li>\n<li><span style=\"line-height: 1.5em;\">Open windows or go outside after waking (particularly if it's sunny)</span></li>\n<li>Splash cold water on your face</li>\n<li><span style=\"line-height: 1.5em;\">Morning stretching , light exercise, or intense exercise routine</span></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"WORK_ENVIRONMENT\">WORK ENVIRONMENT</h3>\n<ul type=\"disc\">\n<li><span style=\"line-height: 1.5em;\">Change relative heights of chair, keyboard, monitor (can stack books under desk items)</span></li>\n<li><span style=\"line-height: 1.5em;\">Different desk chairs (ask coworkers if they want to trade for a day)</span></li>\n<li><span style=\"line-height: 1.5em;\">Lighting</span> \n<ul>\n<li><span style=\"line-height: 1.5em;\">Really bright daylight bulbs</span></li>\n<li><span style=\"line-height: 1.5em;\">'Warm light' bulbs</span></li>\n</ul>\n</li>\n<li><span style=\"line-height: 1.5em;\">Different keyboards</span></li>\n<li><a style=\"line-height: 1.5em;\" href=\"/lw/5r6/spend_money_on_ergonomics/\">Other ergonomics</a></li>\n<li><span style=\"line-height: 1.5em;\">Music</span></li>\n<li><span style=\"line-height: 1.5em;\">White/brown noise</span></li>\n<li><span style=\"line-height: 1.5em;\">Earplugs</span></li>\n<li><span style=\"line-height: 1.5em;\">Establish a policy for interruptions</span></li>\n<li><span style=\"line-height: 1.5em;\">Decorations</span></li>\n</ul>\n<h3 id=\"WORK_ROUTINE\"><span style=\"line-height: 1.5em;\">WORK ROUTINE</span></h3>\n<ul>\n<li>Take breaks to stretch, stand, walk, or meditate</li>\n<li><span style=\"line-height: 1.5em;\">Try different kinds of work at different times of day</span></li>\n<li><span style=\"line-height: 1.5em;\">Create a routine for entering deep focus</span></li>\n<li><span style=\"line-height: 1.5em;\">Create a ritual or checklist for ending procrastination and starting work</span></li>\n<li><span style=\"line-height: 1.5em;\">Naps</span></li>\n<li><span style=\"line-height: 1.5em;\">Snacks</span></li>\n<li><span style=\"line-height: 1.5em;\">Co-working</span></li>\n<li><span style=\"line-height: 1.5em;\">Collaboration</span></li>\n<li><span style=\"line-height: 1.5em;\">Learn keyboard shortcuts for any application you use frequently</span></li>\n<li><span style=\"line-height: 1.5em;\">Voice recording (e.g. as notetaking while reading)</span></li>\n<li><span style=\"line-height: 1.5em;\">Voice input for computer work (or for your phone)</span></li>\n<li><span style=\"line-height: 1.5em;\">Getting Things Done, or a different system for implementing the key principles of attention saving and strategic review</span></li>\n<li><span style=\"line-height: 1.5em;\">Time-tracking</span></li>\n<li><span style=\"line-height: 1.5em;\">Time-boxing</span> \n<ul>\n<li><span style=\"line-height: 1.5em;\">Seriously, spend 15 minutes blocking out hourly plans every day</span></li>\n</ul>\n</li>\n<li><span style=\"line-height: 1.5em;\">Pomodoros (may also help by forcing you to break down tasks)</span></li>\n<li><a style=\"line-height: 1.5em;\" href=\"http://www.tinychat.com/lw\">LW Study Hall</a></li>\n<li><span style=\"line-height: 1.5em;\">Anything on </span><a style=\"line-height: 1.5em;\" href=\"/lw/1sm/akrasia_tactics_review/\">http://lesswrong.com/lw/1sm/akrasia_tactics_review/</a></li>\n<li><span style=\"line-height: 1.5em;\">Inbox Zero</span></li>\n<li><span style=\"line-height: 1.5em;\">Email filtering</span></li>\n<li><span style=\"line-height: 1.5em;\">Process email or other routine tasks in batches</span></li>\n<li><span style=\"line-height: 1.5em;\">Virtual assistant</span></li>\n<li><span style=\"line-height: 1.5em;\">Remove browser autocomplete suggestions for impulse browsing. It's &lt;shift&gt;&lt;del&gt; with the suggestion highlighted in Chrome.</span></li>\n</ul>\n<h3 id=\"LEISURE\">LEISURE</h3>\n<ul type=\"disc\">\n<li>Brainstorm a list of endorsed activities to supplant unenjoyable, unrefreshing procrastination</li>\n<li>Walk around, go outside, listen to music, listen to <a href=\"http://www.maximumfun.org/shows/my-brother-my-brother-and-me\">a comedy podcast</a>, meditate, read, do recreational math, sing, dance, exercise, etc.</li>\n</ul>\n<h3 id=\"COMMUTE\">COMMUTE</h3>\n<ul type=\"disc\">\n<li>Different routes, weighing stress, safety, scenery, length as you see fit</li>\n<li><span style=\"line-height: 1.5em;\">Different times of day</span></li>\n<li><span style=\"line-height: 1.5em;\">Music/podcasts/audiobooks</span></li>\n<li><span style=\"line-height: 1.5em;\">Biking: This can be logistically complicated but still worthwhile. Try seriously thinking for two minutes about what is stopping you from trying it, and whether those obstacles can be removed.</span></li>\n</ul>\n<h3 id=\"EXERCISE\">EXERCISE</h3>\n<ul type=\"disc\">\n<li>Bodyweight workout (various push-ups, sit-ups, pistol squats, etc.)</li>\n<li>Biking</li>\n<li><span style=\"line-height: 1.5em;\">Running</span></li>\n<li><span style=\"line-height: 1.5em;\">Yoga</span></li>\n<li><span style=\"line-height: 1.5em;\">A </span><a style=\"line-height: 1.5em;\" href=\"/lw/h9b/post_ridiculous_munchkin_ideas/8yet\">pullup bar</a><span style=\"line-height: 1.5em;\"> or dumbbells</span></li>\n<li>Dance</li>\n<li><span style=\"line-height: 1.5em;\">Gym</span></li>\n</ul>\n<h3 id=\"FOOD\">FOOD</h3>\n<ul type=\"disc\">\n<li>Write a <a href=\"/lw/gx5/boring_advice_repository/917i\">weekly meal plan</a></li>\n<li><span style=\"line-height: 1.5em;\">Find some recipe blogs to follow</span></li>\n<li><span style=\"line-height: 1.5em;\">Try a new recipe (bonus points if it's more difficult than usual/from an unfamiliar genre/otherwise stretches your cooking skills)</span></li>\n<li><span style=\"line-height: 1.5em;\">Calculate recipe costs</span></li>\n<li><span style=\"line-height: 1.5em;\">Go somewhere new or just order something new unusual</span></li>\n<li><span style=\"line-height: 1.5em;\">Try snacks/sugar/caffeine at different times of the day</span></li>\n<li><span style=\"line-height: 1.5em;\">Reduce/eliminate something (e.g. sugar/caffeine/dairy)</span></li>\n<li><span style=\"line-height: 1.5em;\">Try </span><a style=\"line-height: 1.5em;\" href=\"/lw/h9b/post_ridiculous_munchkin_ideas/8yhj\">Soylent</a></li>\n<li><span style=\"line-height: 1.5em;\">I don't actually know anything about nutrition or dieting; maybe fix that?</span></li>\n</ul>\n<h3 id=\"MUSIC\">MUSIC</h3>\n<ul type=\"disc\">\n<li>Check whether your favorite musicians (or even ones you only kind of liked before) have released new music</li>\n<li><span style=\"line-height: 1.5em;\">Find a service where you can listen to music for free with minimal inconvenience (YouTube and Spotify are usable for me, but just barely)</span></li>\n<li><span style=\"line-height: 1.5em;\">List artists you've \"been meaning to get around to listening to\" and use the above to actually do that</span></li>\n<li><span style=\"line-height: 1.5em;\">Listen to things outside your usual tastes</span></li>\n<li><span style=\"line-height: 1.5em;\">Listen to (internet) radio stations</span> \n<ul>\n<li><span style=\"line-height: 1.5em;\">Your local college radio (or any college radio, since they usually stream online) will have a huge variety of programs, some of which should be good</span></li>\n<li><span style=\"line-height: 1.5em;\">Use Google to find a good one; I really like </span><a style=\"line-height: 1.5em;\" href=\"http://kbaq.org/\">KBAQ</a><span style=\"line-height: 1.5em;\"> for classical</span></li>\n</ul>\n</li>\n<li><span style=\"line-height: 1.5em;\">Try different headphones (go to a store, or ask your friends if you can borrow theirs for a bit)</span></li>\n<li><span style=\"line-height: 1.5em;\">Find a \"best album of the year\" thread from a non-music-related forum; you'll get some pretty diverse picks</span></li>\n</ul>\n<h3 id=\"OTHER\">OTHER</h3>\n<ul>\n<li>Different soap/shampoo/shaving cream/razors/other grooming products</li>\n<li><span style=\"line-height: 1.5em;\">Different socks</span></li>\n<li><span style=\"line-height: 1.5em;\">Barefoot shoes</span></li>\n<li><span style=\"line-height: 1.5em;\">Journal</span></li>\n<li><span style=\"line-height: 1.5em;\">Gratitude journal</span></li>\n<li>Comfort zone expansion</li>\n<li><span style=\"line-height: 1.5em;\">Cold showers</span></li>\n<li><span style=\"line-height: 1.5em;\">Meditation</span></li>\n<li><span style=\"line-height: 1.5em;\">Various reading, lecture-watching, note-taking, or review strategies</span></li>\n<li><span style=\"line-height: 1.5em;\">Alternatives to software and online services you use frequently (whether or not you feel happy with your choice already)</span> \n<ul>\n<li><a style=\"line-height: 1.5em;\" href=\"http://alternativeto.net/\">http://alternativeto.net</a><span style=\"line-height: 1.5em;\"> and of course </span><a style=\"line-height: 1.5em;\" href=\"http://alternativeto.net/software/alternativeto/\">http://alternativeto.net/software/alternativeto/</a></li>\n</ul>\n</li>\n<li><a style=\"line-height: 1.5em;\" href=\"/lw/h9b/post_ridiculous_munchkin_ideas/91lq\">Watch videos at higher speeds</a></li>\n<li><a style=\"line-height: 1.5em;\" href=\"https://habitrpg.com/static/front\">HabitRPG</a><span style=\"line-height: 1.5em;\">, </span><a style=\"line-height: 1.5em;\" href=\"https://www.beeminder.com/\">Beeminder</a><span style=\"line-height: 1.5em;\">, and/or </span><a style=\"line-height: 1.5em;\" href=\"http://stickk.com\">Stickk</a><span style=\"line-height: 1.5em;\">; brainstorm some goals</span> \n<ul>\n<li><span style=\"line-height: 1.5em;\">e.g. pomodoros, Anki reviews/card-making, trying more things, reading, endorsed leisure, meditation, journaling, strategic reviews</span></li>\n</ul>\n</li>\n<li><a style=\"line-height: 1.5em;\" href=\"http://ankisrs.net\">Anki</a>&nbsp;(or <a href=\"http://www.gwern.net/Spaced%20repetition?2\">spaced repetition</a> more generally)</li>\n<li><span style=\"line-height: 1.5em;\">The </span><a style=\"line-height: 1.5em;\" href=\"http://tinyhabits.com/\">http://tinyhabits.com/</a><span style=\"line-height: 1.5em;\"> course</span></li>\n<li><span style=\"line-height: 1.5em;\"><span style=\"line-height: 1.5em;\">Typing practice to improve speed + accuracy </span></span></li>\n<li><span style=\"line-height: 1.5em;\">An alternative keyboard layout</span></li>\n<li><span style=\"line-height: 1.5em;\">Reading practice for speed + comprehension</span></li>\n<li><span style=\"line-height: 1.5em;\">Learn to juggle</span></li>\n</ul>", "sections": [{"title": "META", "anchor": "META", "level": 1}, {"title": "SLEEP", "anchor": "SLEEP", "level": 1}, {"title": "WORK ENVIRONMENT", "anchor": "WORK_ENVIRONMENT", "level": 1}, {"title": "WORK ROUTINE", "anchor": "WORK_ROUTINE", "level": 1}, {"title": "LEISURE", "anchor": "LEISURE", "level": 1}, {"title": "COMMUTE", "anchor": "COMMUTE", "level": 1}, {"title": "EXERCISE", "anchor": "EXERCISE", "level": 1}, {"title": "FOOD", "anchor": "FOOD", "level": 1}, {"title": "MUSIC", "anchor": "MUSIC", "level": 1}, {"title": "OTHER", "anchor": "OTHER", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "20 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iA25AvZqAr6G8mAXR", "QTkij5fmPXPd7GD4Z", "vADtvr9iDeYsCDfxd", "qwdupkFd6kmeZHYXy", "Zmfo388RA9oky3KYe", "goCfoiQkniQwPryki", "6phFYpNQH9SmWL9Jt", "HEn2qiMxk5BggN83J", "reitXJgJXFzKpdKyd", "Gy8fy7rTgTocNLKfT", "ADaZaEsmJMnKKhRqS", "83naYmTXYcupTRGRW", "hY86FhYysQ7dBg3d8", "5wMTZLZZmZEbXdoMD", "rRmisKb45dN7DK4BW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-12T02:18:04.159Z", "modifiedAt": "2021-11-12T05:19:47.311Z", "url": null, "title": "AALWA: Ask any LessWronger anything", "slug": "aalwa-ask-any-lesswronger-anything", "viewCount": null, "lastCommentedAt": "2022-05-25T19:46:14.337Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YdfpDyRpNyypivgdu/aalwa-ask-any-lesswronger-anything", "pageUrlRelative": "/posts/YdfpDyRpNyypivgdu/aalwa-ask-any-lesswronger-anything", "linkUrl": "https://www.lesswrong.com/posts/YdfpDyRpNyypivgdu/aalwa-ask-any-lesswronger-anything", "postedAtFormatted": "Sunday, January 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AALWA%3A%20Ask%20any%20LessWronger%20anything&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAALWA%3A%20Ask%20any%20LessWronger%20anything%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYdfpDyRpNyypivgdu%2Faalwa-ask-any-lesswronger-anything%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AALWA%3A%20Ask%20any%20LessWronger%20anything%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYdfpDyRpNyypivgdu%2Faalwa-ask-any-lesswronger-anything", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYdfpDyRpNyypivgdu%2Faalwa-ask-any-lesswronger-anything", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<p><strong>If you want people to ask you stuff reply to this post with a comment to that effect.</strong></p>\n<p>More accurately, ask any participating LessWronger anything that is in the category of questions they indicate they would answer.</p>\n<p>If you want to talk about this post you can reply to my comment below that says \"Discussion of this post goes here.\", or not.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1, "izp6eeJJEg9v5zcur": 1, "YgizoZqa7LEb3LEJn": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YdfpDyRpNyypivgdu", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 52, "extendedScore": null, "score": 0.000156, "legacy": true, "legacyId": "25235", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 633, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2014-01-12T02:18:04.159Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-12T03:21:21.364Z", "modifiedAt": null, "url": null, "title": "Deregulating Distraction, Moving Towards the Goal, and Level Hopping", "slug": "deregulating-distraction-moving-towards-the-goal-and-level", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:07.643Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yFALNnscB2qgehnJv/deregulating-distraction-moving-towards-the-goal-and-level", "pageUrlRelative": "/posts/yFALNnscB2qgehnJv/deregulating-distraction-moving-towards-the-goal-and-level", "linkUrl": "https://www.lesswrong.com/posts/yFALNnscB2qgehnJv/deregulating-distraction-moving-towards-the-goal-and-level", "postedAtFormatted": "Sunday, January 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deregulating%20Distraction%2C%20Moving%20Towards%20the%20Goal%2C%20and%20Level%20Hopping&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeregulating%20Distraction%2C%20Moving%20Towards%20the%20Goal%2C%20and%20Level%20Hopping%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyFALNnscB2qgehnJv%2Fderegulating-distraction-moving-towards-the-goal-and-level%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deregulating%20Distraction%2C%20Moving%20Towards%20the%20Goal%2C%20and%20Level%20Hopping%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyFALNnscB2qgehnJv%2Fderegulating-distraction-moving-towards-the-goal-and-level", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyFALNnscB2qgehnJv%2Fderegulating-distraction-moving-towards-the-goal-and-level", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2534, "htmlBody": "<p><em>This is the third post in a series discussing my recent <a href=\"/lw/jg3/the_mechanics_of_my_recent_productivity/\">bout of productivity</a>. Within, I discuss two techniques I use to avoid akrasia and one technique I use to be especially productive.</em></p>\n<h1 id=\"deregulatingdistraction\">Deregulating Distraction</h1>\n<p>I like to pretend that I have higher-than-normal willpower, because my ability to Get Things Done seems to be somewhat above average. In fact, this is not the case. I'm not good at fighting akrasia. I merely have a knack for avoiding it.</p>\n<p>When I was young, my parents were very good at convincing me to manage my money. They gave me an allowance, perhaps a dollar a week. When we would go to the store, I'd get excited about some trite toy and ask my parents whether I could buy it.</p>\n<p>Their answers were similar. My mother would crouch down, put a hand on my shoulder, and say \"Of course you can. But before you do, think carefully about how much you will enjoy this after you've bought it, and what other things you would be able to buy if instead you saved up.\"</p>\n<p>My father was a bit more direct. He'd just shrug and say \"It's your money\", with the barest hint of derision.</p>\n<p>I rarely spent my allowance.</p>\n<p>I now use a similar technique when dealing with distractions.</p>\n<p>(It's worth noting that it's always been very easy to put me into far mode, perhaps in part because I decided at a very young age that I wasn't going to die.)</p>\n<p>As <a href=\"/lw/jgh/habitual_productivity/abpa\">Kaj Sotala</a> and a few others noted, assigning guilt to non-productive tasks is not especially healthy. Nor is it, in my experience, sustainable. In a few different cases, I experienced scenarios where I wanted to do something but couldn't will myself to do it. I suffered ego depletion and hit a vicious cycle of unproductivity and depression. I never fell completely into the self-hate death spiral, but I flirted around at the edges. It became clear that I needed a new strategy.</p>\n<p>To break the cycle, I decided to stop fighting myself.</p>\n<p><a id=\"more\"></a></p>\n<p>The world is full of distractions, and I have plenty of vices. I am just as susceptible as anyone to binging on TV shows or video games or book series. Instead of trying (and often failing) to stop myself from indulging, I decided to allow myself to indulge whenever I really wanted to.</p>\n<p>\"It's your time\", I told myself.</p>\n<p>This changed the game entirely. I no longer willed myself to avoid temptation: I weighed temptations alongside my other options, took their pros and cons into account, and made an informed decision. Did I <em>need</em> to distract myself? Sometimes, the answer was yes.</p>\n<p>Knowing that I could no longer trust myself to bail me out if I got addicted to new media, I took special care in removing as many distractions as I could from my environment. Because I'd resolved not to spend willpower to cancel addictions, I became much more cautious at the point of entry. These days, I ignore recommendations about new TV shows and books, preferring not even to learn the premises, thus dodging the temptation entirely.</p>\n<p>By allowing distractions a place in my mental calculus I allowed myself to choose between them with more care: I am able to watch movies instead of TV shows, to read standalone books instead of entire series.</p>\n<p>I know full well that my resolution against spending willpower against myself means that once I get addicted to something, it has to run its full course before I can be productive again. This is a nuclear option: because I know that I <em>won't</em> stop, I am <em>very</em> leery of lengthy media. I avoid open-ended addictions (ongoing online games, chemical addictions, etc.) like the plague.</p>\n<p>I refer to this strategy as \"playing chicken against myself\": because I know that I'll let long addictions run their course, I seldom have to.</p>\n<p>From another perspective, you could say that I deregulated a black market on distractions: By lifting the mental ban on entertainment, I was able to price it accurately and weigh the tradeoffs. If there is a new book I want to read, the answer is not an outright and unenforcible \"No\". Rather, it's \"can we afford to be underproductive for the next few days?\". And when the answer is negative, it's significantly easier for me to postpone gratification than to resist the temptation entirely. The end result is that I have much more control over when I indulge in escapism.</p>\n<p>Finally, I've found that this <em>feels</em> a lot better than feeling guilty about being unproductive. It's a healthier state of mind, and it's led to a general increase in happiness.</p>\n<h1 id=\"movingtowardsthegoal\">Moving Towards the Goal</h1>\n<p>My teachers used to tell my parents that I have two modes of operation: I either put in the minimum possible effort or I blow expectations completely out of the water. They claimed I have no middle ground.</p>\n<p>This isn't quite accurate. The truth is, I <em>always</em> put in the minimum effort. Anything else would be wasted motion. The discrepancy they observed was not due to some whim of passion, it was an artifact of how our incentives were misaligned.</p>\n<p>In school I was incentivized to ace classes with minimal work. I was very good at obeying the letter of the law while blatantly flouting the spirit, and I had a knack for knowing <em>exactly</em> how far I could push my luck. My teachers had&hellip; polarized opinions of me, to say the least. I was an arrogant kid.</p>\n<p>Yet when my schoolwork happened to align with some personal goal &mdash; mastering a new technique, figuring out new secrets of the universe &mdash; then I was relentless, shattering expectations with apparent ease. A number of my teachers took it upon themselves to press upon me just how much I could do if I actually <em>applied</em> myself. I didn't bother correcting them. If they weren't going to invent a grade higher than 'A', why should I waste my efforts in the classroom? I had better things to do.</p>\n<p>Like I said, I was an arrogant kid.</p>\n<p>This experience in school had two important repercussions. First, it taught me to seek out the gap between the <em>intended</em> rules and the <em>actual</em> rules. I developed a knack for it, and this has served me well in many walks of life. Noticing the space between what you meant and what you said is a fundamental skill for programmers. Math is a tool designed to narrow such gaps. Logical incompleteness theorems are statements about the gap between what logic <em>can</em> say and what mathematicians <em>want</em> to say.</p>\n<p>Secondly, and more relevant to this post, school helped me make explicit the virtue of putting in the minimum possible effort. Authority figures parroted the value of hard work, but that's only half the story. You should <em>always</em> be putting forth the least amount of effort that it takes to achieve your goals. That's not to say that you should never do hard work: in many situations, the easiest way to achieve your goals is to do things right the first time. I'm not condoning shoddy work, either: if quality is part of your goal then you'd best do things correctly. If you're trying to signal competence, then by all means, put in extra effort. But you should <em>never</em> expend extra effort just for effort's sake.</p>\n<p>This leads us to my second trick for avoiding akrasia: I am not Trying Really Hard. People who are Trying Really Hard give themselves rewards for progress or punishments for failure. They incentivize the behavior that they want to have. They keep on deciding to continue doing what they're doing, and they engage in valiant battle against akrasia. I don't do any of that.</p>\n<p>Instead, I simply Move Towards the Goal.</p>\n<p>I don't will myself to study. It is not a chore, it is not something I force myself to do. That's not to say I enjoy studying, per se: it's hard work, and the reward structure is pathetic compared to programming. If I had to force or convince myself to study lots of math continously, I don't think I'd get very far.</p>\n<p>That's not how I operate. I don't Try Really Hard. I simply Move Towards the Goal.</p>\n<p>This is where the previous post ties in. I've mostly eliminated the guilt I feel while unproductive, but I've maintained two very important things from that era of my life:</p>\n<ol>\n<li>In my head, long-term satisfaction is linked to productivity.</li>\n<li>I have maintained habitual productivity for years.</li>\n</ol>\n<p>Between these two points, I know that once I've settled on a goal, I'm going to more towards it.</p>\n<p>This is, internally, an immutable fact, made so both by habit and by crude Pavlovian training. None of this is explicit, mind you, it's just the <em>nature of goals</em>. I can change the goal and I can drop the goal, but I can't hold the goal and <em>not pursue</em> it.</p>\n<p>I never <em>decided</em> to study really hard. You can \"decide\" not to watch the next episode of that TV show only to sternly berate yourself three episodes later. My decision to study hard was made on a lower level, it's been internalized. Acting on goals the thing that System 1 does regardless of what System 2 \"decides\".</p>\n<p>System 2 controls things by <em>picking</em> the goals. It was a long and arduous process to internalize my most recent set of goals, the ones that have driven me to study hard and become a research associate and so on. It took a few months and a bit of mindhacking, and that's a story for another day. But once the goal was <em>chosen</em>, marching towards it was out of my hands.</p>\n<p>System 2 isn't in control of <em>whether</em> I move towards the goal. Instead, it spends its time doing something it's very good at: finding the most efficient path. Minimizing effort.</p>\n<p>I don't actively force myself to study hard. Rather, the structure of the environment is such that the shortest path to the goal requires hard studying. I merely follow that path.</p>\n<p>Moving Towards the Goal might look a lot like Trying Really Hard from the outside. Superficially, the two are similar. On the inside, though, they feel very different. I've Tried Really Hard before, and I'm not good at it. It requires exertion of willpower and results in depletion of ego.</p>\n<p>When I'm Moving Towards the Goal, I don't worry about whether things will be done. I've outsourced that concern to habit. Instead, mental effort is spent look for the shortest path, the easiest route. Difficult paths do not require additional willpower, because the internal narrative is not one of expending effort. If anything, a difficult path is worth extra points, because it means I'm pursuing admirable goals. Internally, I'm not Struggling Against Akrasia. I'm Finding an Efficient Route.</p>\n<p>Don't get me wrong, studying math at high speed for five months was hard. However, I have built myself a headspace where hardness is not an obstacle to overcome but a <em>feature of the terrain</em>. I am going to march on regardless. System 2 doesn't have to spend effort convincing System 1 to move forward, because System 1 is going to move forward come hell or high water. Thus, System 2 spends its time making sure that the march is as easy as possible.</p>\n<p>This leaves me free to try new techniques to achieve my goals more effectively, and that leads us to our final trick for the day.</p>\n<h1 id=\"levelhopping\">Level Hopping</h1>\n<p>I started doing NaNoWriMo in 2011, and I noticed something interesting: a vast majority of winners <em>barely</em> made it to 50,000 words. The goal of NaNoWriMo is to write 50k words in a month, so I wasn't particularly surprised. However, from my interactions with others I found that a vast majority of these winners <em>felt</em> like they were pushing themselves to the limit, even though many of them were probably psychologically anchored below their actual limits. After all, in my experience, the hardest part of NaNoWriMo is <em>writing every day</em>: the most difficult part of being productive is switching contexts, once you get rolling it's not difficult to keep rolling.</p>\n<p>It seemed clear that if the goal had been 60k, many of the same people would have eeked out a victory with similar margins and the same narrative of butting against their limits. The natural conclusion was that I can't trust myself to feel out my own limits.</p>\n<p>This is when I decided to start hopping to higher levels of productivity. These days, I occasionally throw wrenches into my study plans when I think I'm growing complacent.</p>\n<p>\"Those set theory and category theory books were easy\", I'll say, \"Let's try skipping introductory logic and going <a href=\"http://nanowrimo.org/participants/chasejyd/novels/the-accidental-inquisitor/stats\">straight to model theory</a>\".</p>\n<p>Or, \"All this studying is great, but I bet I could keep it up and also do a NaNoWriMo for 75k words\".</p>\n<p>Often, this fails spectacularly. Sometimes, I <em>am</em> at or near my limits, and skipping an intro logic textbook to dive straight into Model Theory is a <em>really bad idea</em>. Other times, I find out that I actually was just hovering around an anchor point, seduced by a narrative of linear improvement.</p>\n<p>This is not an original idea, by any means. In fact, there's a relevant Bruce Lee quote:</p>\n<blockquote>\n<p>There are no limits. There are plateaus, but you must not stay there, you must go beyond them. If it kills you, it kills you. A man must constantly exceed his level.</p>\n</blockquote>\n<p>-&nbsp;<a href=\"http://zenpencils.com/comics/2012-04-11-bruce-lee-2.jpg\">Bruce Lee</a></p>\n<p>My point, more broadly, is that this is the type of thing that occupies my mental narrative. I'm not wondering whether I will be able to convince myself to study each day. Instead, I'm gauging whether I'm reading the most effective material. I'm noticing that it won't be enough for me to just <em>learn</em> the material, I also have to <em>signal</em> that I've learned the material (and that I should start doing book reviews). I'm monitoring to see when I've grown complacent and looking for ways to keep me on my toes. This is process is doubly useful: It helps me sidestep akrasia and it also helps me become more effective.</p>\n<hr />\n<p>These are my three Light Side tools:</p>\n<ol>\n<li>I've constructed an environment in which productivity is habitual. In the absence of distractions, I trust myself to get things done.</li>\n<li>I've lifted my mental ban on distractions, and trust myself to use them wisely.</li>\n<li>My mental narrative is one of expending minimal effort, not one of trying to succeed: instead of worrying about whether I can continue, I worry about how to perform better.</li>\n</ol>\n<p>Most of these tricks are likely familiar: I do not claim originality; this is merely an account of the methods that I use, the things that work for me. Consider this to be evidence that these techniques work for people who share my personality (which I've tried to illustrate along the way).</p>\n<p>You now have a broad sketch of how I maintain productivity, but it may seem somewhat unstable, difficult to maintain indefinitely. The next post will detail my Dark Side tactics: tricks I use to remain unrelenting and sustain my vigorous pace, but which may make rationalists uncomfortable.</p>\n<p>After that, I'll tell the story of a kid who decided he would save the world for reasons completely unrelated to existential risk, and how he came to align himself with MIRI's mission. This will help you understand the source of my passion, and will conclude the series.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"udPbn9RthmgTtHMiG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yFALNnscB2qgehnJv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 67, "baseScore": 89, "extendedScore": null, "score": 0.000244, "legacy": true, "legacyId": "25236", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 89, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This is the third post in a series discussing my recent <a href=\"/lw/jg3/the_mechanics_of_my_recent_productivity/\">bout of productivity</a>. Within, I discuss two techniques I use to avoid akrasia and one technique I use to be especially productive.</em></p>\n<h1 id=\"Deregulating_Distraction\">Deregulating Distraction</h1>\n<p>I like to pretend that I have higher-than-normal willpower, because my ability to Get Things Done seems to be somewhat above average. In fact, this is not the case. I'm not good at fighting akrasia. I merely have a knack for avoiding it.</p>\n<p>When I was young, my parents were very good at convincing me to manage my money. They gave me an allowance, perhaps a dollar a week. When we would go to the store, I'd get excited about some trite toy and ask my parents whether I could buy it.</p>\n<p>Their answers were similar. My mother would crouch down, put a hand on my shoulder, and say \"Of course you can. But before you do, think carefully about how much you will enjoy this after you've bought it, and what other things you would be able to buy if instead you saved up.\"</p>\n<p>My father was a bit more direct. He'd just shrug and say \"It's your money\", with the barest hint of derision.</p>\n<p>I rarely spent my allowance.</p>\n<p>I now use a similar technique when dealing with distractions.</p>\n<p>(It's worth noting that it's always been very easy to put me into far mode, perhaps in part because I decided at a very young age that I wasn't going to die.)</p>\n<p>As <a href=\"/lw/jgh/habitual_productivity/abpa\">Kaj Sotala</a> and a few others noted, assigning guilt to non-productive tasks is not especially healthy. Nor is it, in my experience, sustainable. In a few different cases, I experienced scenarios where I wanted to do something but couldn't will myself to do it. I suffered ego depletion and hit a vicious cycle of unproductivity and depression. I never fell completely into the self-hate death spiral, but I flirted around at the edges. It became clear that I needed a new strategy.</p>\n<p>To break the cycle, I decided to stop fighting myself.</p>\n<p><a id=\"more\"></a></p>\n<p>The world is full of distractions, and I have plenty of vices. I am just as susceptible as anyone to binging on TV shows or video games or book series. Instead of trying (and often failing) to stop myself from indulging, I decided to allow myself to indulge whenever I really wanted to.</p>\n<p>\"It's your time\", I told myself.</p>\n<p>This changed the game entirely. I no longer willed myself to avoid temptation: I weighed temptations alongside my other options, took their pros and cons into account, and made an informed decision. Did I <em>need</em> to distract myself? Sometimes, the answer was yes.</p>\n<p>Knowing that I could no longer trust myself to bail me out if I got addicted to new media, I took special care in removing as many distractions as I could from my environment. Because I'd resolved not to spend willpower to cancel addictions, I became much more cautious at the point of entry. These days, I ignore recommendations about new TV shows and books, preferring not even to learn the premises, thus dodging the temptation entirely.</p>\n<p>By allowing distractions a place in my mental calculus I allowed myself to choose between them with more care: I am able to watch movies instead of TV shows, to read standalone books instead of entire series.</p>\n<p>I know full well that my resolution against spending willpower against myself means that once I get addicted to something, it has to run its full course before I can be productive again. This is a nuclear option: because I know that I <em>won't</em> stop, I am <em>very</em> leery of lengthy media. I avoid open-ended addictions (ongoing online games, chemical addictions, etc.) like the plague.</p>\n<p>I refer to this strategy as \"playing chicken against myself\": because I know that I'll let long addictions run their course, I seldom have to.</p>\n<p>From another perspective, you could say that I deregulated a black market on distractions: By lifting the mental ban on entertainment, I was able to price it accurately and weigh the tradeoffs. If there is a new book I want to read, the answer is not an outright and unenforcible \"No\". Rather, it's \"can we afford to be underproductive for the next few days?\". And when the answer is negative, it's significantly easier for me to postpone gratification than to resist the temptation entirely. The end result is that I have much more control over when I indulge in escapism.</p>\n<p>Finally, I've found that this <em>feels</em> a lot better than feeling guilty about being unproductive. It's a healthier state of mind, and it's led to a general increase in happiness.</p>\n<h1 id=\"Moving_Towards_the_Goal\">Moving Towards the Goal</h1>\n<p>My teachers used to tell my parents that I have two modes of operation: I either put in the minimum possible effort or I blow expectations completely out of the water. They claimed I have no middle ground.</p>\n<p>This isn't quite accurate. The truth is, I <em>always</em> put in the minimum effort. Anything else would be wasted motion. The discrepancy they observed was not due to some whim of passion, it was an artifact of how our incentives were misaligned.</p>\n<p>In school I was incentivized to ace classes with minimal work. I was very good at obeying the letter of the law while blatantly flouting the spirit, and I had a knack for knowing <em>exactly</em> how far I could push my luck. My teachers had\u2026 polarized opinions of me, to say the least. I was an arrogant kid.</p>\n<p>Yet when my schoolwork happened to align with some personal goal \u2014 mastering a new technique, figuring out new secrets of the universe \u2014 then I was relentless, shattering expectations with apparent ease. A number of my teachers took it upon themselves to press upon me just how much I could do if I actually <em>applied</em> myself. I didn't bother correcting them. If they weren't going to invent a grade higher than 'A', why should I waste my efforts in the classroom? I had better things to do.</p>\n<p>Like I said, I was an arrogant kid.</p>\n<p>This experience in school had two important repercussions. First, it taught me to seek out the gap between the <em>intended</em> rules and the <em>actual</em> rules. I developed a knack for it, and this has served me well in many walks of life. Noticing the space between what you meant and what you said is a fundamental skill for programmers. Math is a tool designed to narrow such gaps. Logical incompleteness theorems are statements about the gap between what logic <em>can</em> say and what mathematicians <em>want</em> to say.</p>\n<p>Secondly, and more relevant to this post, school helped me make explicit the virtue of putting in the minimum possible effort. Authority figures parroted the value of hard work, but that's only half the story. You should <em>always</em> be putting forth the least amount of effort that it takes to achieve your goals. That's not to say that you should never do hard work: in many situations, the easiest way to achieve your goals is to do things right the first time. I'm not condoning shoddy work, either: if quality is part of your goal then you'd best do things correctly. If you're trying to signal competence, then by all means, put in extra effort. But you should <em>never</em> expend extra effort just for effort's sake.</p>\n<p>This leads us to my second trick for avoiding akrasia: I am not Trying Really Hard. People who are Trying Really Hard give themselves rewards for progress or punishments for failure. They incentivize the behavior that they want to have. They keep on deciding to continue doing what they're doing, and they engage in valiant battle against akrasia. I don't do any of that.</p>\n<p>Instead, I simply Move Towards the Goal.</p>\n<p>I don't will myself to study. It is not a chore, it is not something I force myself to do. That's not to say I enjoy studying, per se: it's hard work, and the reward structure is pathetic compared to programming. If I had to force or convince myself to study lots of math continously, I don't think I'd get very far.</p>\n<p>That's not how I operate. I don't Try Really Hard. I simply Move Towards the Goal.</p>\n<p>This is where the previous post ties in. I've mostly eliminated the guilt I feel while unproductive, but I've maintained two very important things from that era of my life:</p>\n<ol>\n<li>In my head, long-term satisfaction is linked to productivity.</li>\n<li>I have maintained habitual productivity for years.</li>\n</ol>\n<p>Between these two points, I know that once I've settled on a goal, I'm going to more towards it.</p>\n<p>This is, internally, an immutable fact, made so both by habit and by crude Pavlovian training. None of this is explicit, mind you, it's just the <em>nature of goals</em>. I can change the goal and I can drop the goal, but I can't hold the goal and <em>not pursue</em> it.</p>\n<p>I never <em>decided</em> to study really hard. You can \"decide\" not to watch the next episode of that TV show only to sternly berate yourself three episodes later. My decision to study hard was made on a lower level, it's been internalized. Acting on goals the thing that System 1 does regardless of what System 2 \"decides\".</p>\n<p>System 2 controls things by <em>picking</em> the goals. It was a long and arduous process to internalize my most recent set of goals, the ones that have driven me to study hard and become a research associate and so on. It took a few months and a bit of mindhacking, and that's a story for another day. But once the goal was <em>chosen</em>, marching towards it was out of my hands.</p>\n<p>System 2 isn't in control of <em>whether</em> I move towards the goal. Instead, it spends its time doing something it's very good at: finding the most efficient path. Minimizing effort.</p>\n<p>I don't actively force myself to study hard. Rather, the structure of the environment is such that the shortest path to the goal requires hard studying. I merely follow that path.</p>\n<p>Moving Towards the Goal might look a lot like Trying Really Hard from the outside. Superficially, the two are similar. On the inside, though, they feel very different. I've Tried Really Hard before, and I'm not good at it. It requires exertion of willpower and results in depletion of ego.</p>\n<p>When I'm Moving Towards the Goal, I don't worry about whether things will be done. I've outsourced that concern to habit. Instead, mental effort is spent look for the shortest path, the easiest route. Difficult paths do not require additional willpower, because the internal narrative is not one of expending effort. If anything, a difficult path is worth extra points, because it means I'm pursuing admirable goals. Internally, I'm not Struggling Against Akrasia. I'm Finding an Efficient Route.</p>\n<p>Don't get me wrong, studying math at high speed for five months was hard. However, I have built myself a headspace where hardness is not an obstacle to overcome but a <em>feature of the terrain</em>. I am going to march on regardless. System 2 doesn't have to spend effort convincing System 1 to move forward, because System 1 is going to move forward come hell or high water. Thus, System 2 spends its time making sure that the march is as easy as possible.</p>\n<p>This leaves me free to try new techniques to achieve my goals more effectively, and that leads us to our final trick for the day.</p>\n<h1 id=\"Level_Hopping\">Level Hopping</h1>\n<p>I started doing NaNoWriMo in 2011, and I noticed something interesting: a vast majority of winners <em>barely</em> made it to 50,000 words. The goal of NaNoWriMo is to write 50k words in a month, so I wasn't particularly surprised. However, from my interactions with others I found that a vast majority of these winners <em>felt</em> like they were pushing themselves to the limit, even though many of them were probably psychologically anchored below their actual limits. After all, in my experience, the hardest part of NaNoWriMo is <em>writing every day</em>: the most difficult part of being productive is switching contexts, once you get rolling it's not difficult to keep rolling.</p>\n<p>It seemed clear that if the goal had been 60k, many of the same people would have eeked out a victory with similar margins and the same narrative of butting against their limits. The natural conclusion was that I can't trust myself to feel out my own limits.</p>\n<p>This is when I decided to start hopping to higher levels of productivity. These days, I occasionally throw wrenches into my study plans when I think I'm growing complacent.</p>\n<p>\"Those set theory and category theory books were easy\", I'll say, \"Let's try skipping introductory logic and going <a href=\"http://nanowrimo.org/participants/chasejyd/novels/the-accidental-inquisitor/stats\">straight to model theory</a>\".</p>\n<p>Or, \"All this studying is great, but I bet I could keep it up and also do a NaNoWriMo for 75k words\".</p>\n<p>Often, this fails spectacularly. Sometimes, I <em>am</em> at or near my limits, and skipping an intro logic textbook to dive straight into Model Theory is a <em>really bad idea</em>. Other times, I find out that I actually was just hovering around an anchor point, seduced by a narrative of linear improvement.</p>\n<p>This is not an original idea, by any means. In fact, there's a relevant Bruce Lee quote:</p>\n<blockquote>\n<p>There are no limits. There are plateaus, but you must not stay there, you must go beyond them. If it kills you, it kills you. A man must constantly exceed his level.</p>\n</blockquote>\n<p>-&nbsp;<a href=\"http://zenpencils.com/comics/2012-04-11-bruce-lee-2.jpg\">Bruce Lee</a></p>\n<p>My point, more broadly, is that this is the type of thing that occupies my mental narrative. I'm not wondering whether I will be able to convince myself to study each day. Instead, I'm gauging whether I'm reading the most effective material. I'm noticing that it won't be enough for me to just <em>learn</em> the material, I also have to <em>signal</em> that I've learned the material (and that I should start doing book reviews). I'm monitoring to see when I've grown complacent and looking for ways to keep me on my toes. This is process is doubly useful: It helps me sidestep akrasia and it also helps me become more effective.</p>\n<hr>\n<p>These are my three Light Side tools:</p>\n<ol>\n<li>I've constructed an environment in which productivity is habitual. In the absence of distractions, I trust myself to get things done.</li>\n<li>I've lifted my mental ban on distractions, and trust myself to use them wisely.</li>\n<li>My mental narrative is one of expending minimal effort, not one of trying to succeed: instead of worrying about whether I can continue, I worry about how to perform better.</li>\n</ol>\n<p>Most of these tricks are likely familiar: I do not claim originality; this is merely an account of the methods that I use, the things that work for me. Consider this to be evidence that these techniques work for people who share my personality (which I've tried to illustrate along the way).</p>\n<p>You now have a broad sketch of how I maintain productivity, but it may seem somewhat unstable, difficult to maintain indefinitely. The next post will detail my Dark Side tactics: tricks I use to remain unrelenting and sustain my vigorous pace, but which may make rationalists uncomfortable.</p>\n<p>After that, I'll tell the story of a kid who decided he would save the world for reasons completely unrelated to existential risk, and how he came to align himself with MIRI's mission. This will help you understand the source of my passion, and will conclude the series.</p>", "sections": [{"title": "Deregulating Distraction", "anchor": "Deregulating_Distraction", "level": 1}, {"title": "Moving Towards the Goal", "anchor": "Moving_Towards_the_Goal", "level": 1}, {"title": "Level Hopping", "anchor": "Level_Hopping", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uX3HjXo6BWos3Zgy5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-12T05:16:55.458Z", "modifiedAt": null, "url": null, "title": "Why I haven't signed up for cryonics", "slug": "why-i-haven-t-signed-up-for-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:31.515Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fy3JvBALB8HaJxzMv/why-i-haven-t-signed-up-for-cryonics", "pageUrlRelative": "/posts/fy3JvBALB8HaJxzMv/why-i-haven-t-signed-up-for-cryonics", "linkUrl": "https://www.lesswrong.com/posts/fy3JvBALB8HaJxzMv/why-i-haven-t-signed-up-for-cryonics", "postedAtFormatted": "Sunday, January 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20I%20haven't%20signed%20up%20for%20cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20I%20haven't%20signed%20up%20for%20cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffy3JvBALB8HaJxzMv%2Fwhy-i-haven-t-signed-up-for-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20I%20haven't%20signed%20up%20for%20cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffy3JvBALB8HaJxzMv%2Fwhy-i-haven-t-signed-up-for-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffy3JvBALB8HaJxzMv%2Fwhy-i-haven-t-signed-up-for-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1409, "htmlBody": "<p>(OR)</p>\n<h2>How I'm now on the fence about whether to sign up for cryonics</h2>\n<p>I'm not currently signed up for cryonics. In my social circle, that makes me a bit of an oddity. I disagree with Eliezer Yudkowsky; heaven forbid.&nbsp;</p>\n<p>My true rejection is that I don't feel a visceral urge to sign up. When I query my brain on why, what I get is that I don't feel <em>that </em>upset about me personally dying. It would suck, sure. It would suck a lot. But it wouldn't suck infinitely. I've seen a lot of people die. It's sad and wasteful and upsetting, but not like a civilization collapsing. It's neutral from a point of pleasure vs suffering for the dead person, and negative for the family, but they cope with it and find a bit of meaning and move on.&nbsp;</p>\n<p>(I'm desensitized. I have to be, to stay sane in a job where I watch people die on a day to day basis. This is a bias; I'm just not convinced that it's a bias in a negative direction.)</p>\n<p>I think the deeper cause behind my rejection may be that I don't have <a href=\"/lw/nb/something_to_protect/\">enough to protect</a>. Individuals may be unique, but as an individual, I'm fairly replaceable. All the things I'm currently doing can and are being done by other people. I'm not the sole support person in anyone's life, and if I were, I would be trying really, really hard to fix the situation. Part of me is convinced that wanting to personally survive and thinking that I deserve to is selfish and un-virtuous or something. (EDIT: or that it's non-altruistic to value my life above the amount Givewell thinks is <a href=\"http://www.givewell.org/international/technical/criteria/cost-effectiveness\">reasonable to save a life</a>&ndash;about $5,000. My revealed preference is that I obviously value my life more than this.) &nbsp;</p>\n<p>However, I don't think cryonics is wrong, or bad. It has obvious upsides, like being the only chance an average citizen has right now to do something that might lead to them not permanently dying. I say \"average citizen\" because people working on biological life extension and immortality research are arguably doing something about not dying.&nbsp;</p>\n<p>When queried, my brain tells me that it's doing an expected-value calculation and the expected value of cryonics to me is is too low to justify the costs; it's unlikely to succeed and the only reason some people have positive expected value for it is that they're multiplying that tiny number by the huge, huge number that they place on the value of my life. And my number doesn't feel big enough to outweigh those odds at that price.&nbsp;</p>\n<h2>Putting some numbers in that</h2>\n<p>If my brain thinks this is a matter of expected-value calculations, I ought to do one. With actual numbers, even if they're made-up, and actual multiplication.</p>\n<p>So: my death feels bad, but not infinitely bad. Obvious thing to do: assign a monetary value. Through a variety of helpful thought experiments (how much would I pay to cure a fatal illness if I were the only person in the world with it and research wouldn't help anyone but me and I could otherwise donate the money to EA charities; does the awesomeness of 3 million dewormings outway the suckiness of my death; is my death more or less sucky than the destruction of a high-end MRI machine), I've converged on a subjective value for my life of about $1 million. Like, give or take a lot.&nbsp;</p>\n<p>Cryonics feels unlikely to work for me. I think the basic principle is sound, but if someone were to tell me that cryonics had been shown to work for a human, I would be surprised. That's not a number, though, so I took the final result of Steve Harris' calculations <a href=\"http://www.alcor.org/Library/html/WillCryonicsWork.html\">here</a>&nbsp;(inspired by the Sagan-Drake equation). His optimistic number is a 0.15 chance of success, or 1 in 7; his pessimistic number is 0.0023, or less than 1/400. My brain thinks 15% is too high and 0.23% sounds reasonable, but I'll use his numbers for upper and lower bounds.&nbsp;</p>\n<p>I started out trying to calculate the expected cost by some convoluted method where I was going to estimate my expected chance of dying each year and repeatedly subtract it from one and multiply by the amount I'd pay each year to calculate how much I could expect pay in total. <a href=\"/user/Benquo/overview/\">Benquo</a> pointed out to me that calculation like this are usually done using perpetuities, or <a href=\"http://accountingexplained.com/misc/tvm/pv-perpetuity\">PV calculations</a>, so I made one in Excel and plugged in some numbers, approximating the Alcor annual membership fee as $600. Assuming my own discount rate is somewhere between 2% and 5%, I ran two calculations with those numbers. For 2%, the total expected, time-discounted cost would be $30,000; for a 5% discount rate, $12,000.</p>\n<p>Excel also lets you do calculations on perpetuities that aren't perpetual, so I plugged in 62 years, the time by which I'll have a 50% chance of dying according to <a href=\"http://www.ssa.gov/oact/STATS/table4c6.html\">this</a> actuarial table. It didn't change the final results much; $11,417 for a 5% discount rate and $21,000 for the 2% discount rate.&nbsp;</p>\n<p>That's not including the life insurance payout you need to pay for the actual freezing. So, life insurance premiums. Benquo's plan is five years of $2200 a year and then nothing from then on, which apparently isn't uncommon among plans for young healthy people. I could probably get something as good or better; I'm younger. So, $11,00 for total life insurance premiums. If I went with permanent annual payment, I could do a perpetuity calculation instead.&nbsp;</p>\n<p>In short: around $40,000 total, rounding up.</p>\n<p><strong>What's my final number?</strong></p>\n<p>There are two numbers I can output. When I started this article, one of them seemed like the obvious end product, so I calculated that. When I went back to finish this article days later, I walked through all the calculations again while writing the actual paragraphs, did what seemed obvious, ended up with a different number, and realized I'd calculated a different thing. So I'm not sure which one is right, although I suspect they're symmetrical.&nbsp;</p>\n<p>If I multiply the value of my life by the success chance of cryonics, I get a number that represents (I think) the monetary value of cryonics to me, given my factual beliefs and values. It would go up if the value of my life to me went up, or if the chances of cryonics succeeding went up. I can compare it directly to the actual cost of cryonics.</p>\n<p>I take $1 million and plug in either 0.15 or 0.00023, and I get $150,000 as an upper bound and $2300 as a lower bound, to compare to a total cost somewhere in the ballpark of $40,000.</p>\n<p>If I take the price of cryonics and divide it by the chance of success (because if I sign up, I'm optimistically paying for 100 worlds of which I survive in 15, or pessimistically paying for 10,000 worlds in which I survive in 23), I get the total expected cost per my life being saved, which I can compare to the figure I place on the value of my life. It goes down if the cost of cryonics goes down or the chances of success go up.&nbsp;</p>\n<p>I plug in my numbers and get a lower bound of $267,000 and an upper bound of 17 million.&nbsp;</p>\n<p>In both those cases, the optimistic success estimates make it seem worthwhile and the pessimistic success estimates don't, and my personal estimate of cryonics succeeding falls closer to pessimism. But it's close. It's a lot closer than I thought it would be.&nbsp;</p>\n<p>Updating somewhat in favour that I'll end up signed up for cryonics.&nbsp;</p>\n<h2>Fine-tuning and next steps</h2>\n<p>I could get better numbers for the value of my life to me. It's kind of squicky to think about, but that's a bad reason. I could ask other people about their numbers and compare what they're accomplishing in their lives to my own life. I could do more thought experiments to better acquaint my brain with how much value $1 million actually is, because scope insensitivity. I could do upper and lower bounds.</p>\n<p>I could include the cost of organizations cheaper than Alcor as a lower bound; the info is all <a href=\"http://www.longecity.org/forum/page/index.html/_/articles/cryonics-r40\">here</a>&nbsp;and the calculation wouldn't be too nasty but I have work in 7 hours and need to get to bed.&nbsp;</p>\n<p>I could do my own version of the cryonics success equation, plugging in my own estimates. (Although I suspect this data is less informed and less valuable than what's already there).</p>\n<p>I could ask what other people think. Thus, write this post.&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fy3JvBALB8HaJxzMv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 42, "extendedScore": null, "score": 0.00013, "legacy": true, "legacyId": "25227", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>(OR)</p>\n<h2 id=\"How_I_m_now_on_the_fence_about_whether_to_sign_up_for_cryonics\">How I'm now on the fence about whether to sign up for cryonics</h2>\n<p>I'm not currently signed up for cryonics. In my social circle, that makes me a bit of an oddity. I disagree with Eliezer Yudkowsky; heaven forbid.&nbsp;</p>\n<p>My true rejection is that I don't feel a visceral urge to sign up. When I query my brain on why, what I get is that I don't feel <em>that </em>upset about me personally dying. It would suck, sure. It would suck a lot. But it wouldn't suck infinitely. I've seen a lot of people die. It's sad and wasteful and upsetting, but not like a civilization collapsing. It's neutral from a point of pleasure vs suffering for the dead person, and negative for the family, but they cope with it and find a bit of meaning and move on.&nbsp;</p>\n<p>(I'm desensitized. I have to be, to stay sane in a job where I watch people die on a day to day basis. This is a bias; I'm just not convinced that it's a bias in a negative direction.)</p>\n<p>I think the deeper cause behind my rejection may be that I don't have <a href=\"/lw/nb/something_to_protect/\">enough to protect</a>. Individuals may be unique, but as an individual, I'm fairly replaceable. All the things I'm currently doing can and are being done by other people. I'm not the sole support person in anyone's life, and if I were, I would be trying really, really hard to fix the situation. Part of me is convinced that wanting to personally survive and thinking that I deserve to is selfish and un-virtuous or something. (EDIT: or that it's non-altruistic to value my life above the amount Givewell thinks is <a href=\"http://www.givewell.org/international/technical/criteria/cost-effectiveness\">reasonable to save a life</a>\u2013about $5,000. My revealed preference is that I obviously value my life more than this.) &nbsp;</p>\n<p>However, I don't think cryonics is wrong, or bad. It has obvious upsides, like being the only chance an average citizen has right now to do something that might lead to them not permanently dying. I say \"average citizen\" because people working on biological life extension and immortality research are arguably doing something about not dying.&nbsp;</p>\n<p>When queried, my brain tells me that it's doing an expected-value calculation and the expected value of cryonics to me is is too low to justify the costs; it's unlikely to succeed and the only reason some people have positive expected value for it is that they're multiplying that tiny number by the huge, huge number that they place on the value of my life. And my number doesn't feel big enough to outweigh those odds at that price.&nbsp;</p>\n<h2 id=\"Putting_some_numbers_in_that\">Putting some numbers in that</h2>\n<p>If my brain thinks this is a matter of expected-value calculations, I ought to do one. With actual numbers, even if they're made-up, and actual multiplication.</p>\n<p>So: my death feels bad, but not infinitely bad. Obvious thing to do: assign a monetary value. Through a variety of helpful thought experiments (how much would I pay to cure a fatal illness if I were the only person in the world with it and research wouldn't help anyone but me and I could otherwise donate the money to EA charities; does the awesomeness of 3 million dewormings outway the suckiness of my death; is my death more or less sucky than the destruction of a high-end MRI machine), I've converged on a subjective value for my life of about $1 million. Like, give or take a lot.&nbsp;</p>\n<p>Cryonics feels unlikely to work for me. I think the basic principle is sound, but if someone were to tell me that cryonics had been shown to work for a human, I would be surprised. That's not a number, though, so I took the final result of Steve Harris' calculations <a href=\"http://www.alcor.org/Library/html/WillCryonicsWork.html\">here</a>&nbsp;(inspired by the Sagan-Drake equation). His optimistic number is a 0.15 chance of success, or 1 in 7; his pessimistic number is 0.0023, or less than 1/400. My brain thinks 15% is too high and 0.23% sounds reasonable, but I'll use his numbers for upper and lower bounds.&nbsp;</p>\n<p>I started out trying to calculate the expected cost by some convoluted method where I was going to estimate my expected chance of dying each year and repeatedly subtract it from one and multiply by the amount I'd pay each year to calculate how much I could expect pay in total. <a href=\"/user/Benquo/overview/\">Benquo</a> pointed out to me that calculation like this are usually done using perpetuities, or <a href=\"http://accountingexplained.com/misc/tvm/pv-perpetuity\">PV calculations</a>, so I made one in Excel and plugged in some numbers, approximating the Alcor annual membership fee as $600. Assuming my own discount rate is somewhere between 2% and 5%, I ran two calculations with those numbers. For 2%, the total expected, time-discounted cost would be $30,000; for a 5% discount rate, $12,000.</p>\n<p>Excel also lets you do calculations on perpetuities that aren't perpetual, so I plugged in 62 years, the time by which I'll have a 50% chance of dying according to <a href=\"http://www.ssa.gov/oact/STATS/table4c6.html\">this</a> actuarial table. It didn't change the final results much; $11,417 for a 5% discount rate and $21,000 for the 2% discount rate.&nbsp;</p>\n<p>That's not including the life insurance payout you need to pay for the actual freezing. So, life insurance premiums. Benquo's plan is five years of $2200 a year and then nothing from then on, which apparently isn't uncommon among plans for young healthy people. I could probably get something as good or better; I'm younger. So, $11,00 for total life insurance premiums. If I went with permanent annual payment, I could do a perpetuity calculation instead.&nbsp;</p>\n<p>In short: around $40,000 total, rounding up.</p>\n<p><strong id=\"What_s_my_final_number_\">What's my final number?</strong></p>\n<p>There are two numbers I can output. When I started this article, one of them seemed like the obvious end product, so I calculated that. When I went back to finish this article days later, I walked through all the calculations again while writing the actual paragraphs, did what seemed obvious, ended up with a different number, and realized I'd calculated a different thing. So I'm not sure which one is right, although I suspect they're symmetrical.&nbsp;</p>\n<p>If I multiply the value of my life by the success chance of cryonics, I get a number that represents (I think) the monetary value of cryonics to me, given my factual beliefs and values. It would go up if the value of my life to me went up, or if the chances of cryonics succeeding went up. I can compare it directly to the actual cost of cryonics.</p>\n<p>I take $1 million and plug in either 0.15 or 0.00023, and I get $150,000 as an upper bound and $2300 as a lower bound, to compare to a total cost somewhere in the ballpark of $40,000.</p>\n<p>If I take the price of cryonics and divide it by the chance of success (because if I sign up, I'm optimistically paying for 100 worlds of which I survive in 15, or pessimistically paying for 10,000 worlds in which I survive in 23), I get the total expected cost per my life being saved, which I can compare to the figure I place on the value of my life. It goes down if the cost of cryonics goes down or the chances of success go up.&nbsp;</p>\n<p>I plug in my numbers and get a lower bound of $267,000 and an upper bound of 17 million.&nbsp;</p>\n<p>In both those cases, the optimistic success estimates make it seem worthwhile and the pessimistic success estimates don't, and my personal estimate of cryonics succeeding falls closer to pessimism. But it's close. It's a lot closer than I thought it would be.&nbsp;</p>\n<p>Updating somewhat in favour that I'll end up signed up for cryonics.&nbsp;</p>\n<h2 id=\"Fine_tuning_and_next_steps\">Fine-tuning and next steps</h2>\n<p>I could get better numbers for the value of my life to me. It's kind of squicky to think about, but that's a bad reason. I could ask other people about their numbers and compare what they're accomplishing in their lives to my own life. I could do more thought experiments to better acquaint my brain with how much value $1 million actually is, because scope insensitivity. I could do upper and lower bounds.</p>\n<p>I could include the cost of organizations cheaper than Alcor as a lower bound; the info is all <a href=\"http://www.longecity.org/forum/page/index.html/_/articles/cryonics-r40\">here</a>&nbsp;and the calculation wouldn't be too nasty but I have work in 7 hours and need to get to bed.&nbsp;</p>\n<p>I could do my own version of the cryonics success equation, plugging in my own estimates. (Although I suspect this data is less informed and less valuable than what's already there).</p>\n<p>I could ask what other people think. Thus, write this post.&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "How I'm now on the fence about whether to sign up for cryonics", "anchor": "How_I_m_now_on_the_fence_about_whether_to_sign_up_for_cryonics", "level": 1}, {"title": "Putting some numbers in that", "anchor": "Putting_some_numbers_in_that", "level": 1}, {"title": "What's my final number?", "anchor": "What_s_my_final_number_", "level": 2}, {"title": "Fine-tuning and next steps", "anchor": "Fine_tuning_and_next_steps", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "252 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 252, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SGR4GxFK7KmW7ckCB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-12T07:24:25.537Z", "modifiedAt": "2021-01-05T15:45:01.705Z", "url": null, "title": "Even Odds", "slug": "even-odds", "viewCount": null, "lastCommentedAt": "2016-07-30T02:25:10.074Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aiz4FCKTgFBtKiWsE/even-odds", "pageUrlRelative": "/posts/aiz4FCKTgFBtKiWsE/even-odds", "linkUrl": "https://www.lesswrong.com/posts/aiz4FCKTgFBtKiWsE/even-odds", "postedAtFormatted": "Sunday, January 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Even%20Odds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEven%20Odds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Faiz4FCKTgFBtKiWsE%2Feven-odds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Even%20Odds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Faiz4FCKTgFBtKiWsE%2Feven-odds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Faiz4FCKTgFBtKiWsE%2Feven-odds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 874, "htmlBody": "<p>(<a href=\"http://bywayofcontradiction.com/?p=40\">Cross-posted</a> on my <a href=\"http://bywayofcontradiction.com/\">personal blog</a>, which has LaTeX, and is easier to read.)</p>\n<p>Let's say that you are are at your local less wrong meet up and someone makes some strong claim and seems very sure of himself, \"blah blah blah resurrected blah blah alicorn princess blah blah 99 percent sure.\" You think he is probably correct, you estimate a 67 percent chance, but you think he is way over confident. \"Wanna bet?\" You ask.</p>\n<p>\"Sure,\" he responds, and you both check your wallets and have 25 dollars each. \"Okay,\" he says, \"now you pick some betting odds, and I'll choose which side I want to pick.\"</p>\n<p>\"That's crazy,\" you say, \"I am going to pick the odds so that I cannot be taken advantage of, which means that I will be indifferent between which of the two options you pick, which means that I will expect to gain 0 dollars from this transaction. I wont take it. It is not fair!\"</p>\n<p>\"Okay,\" he says, annoyed with you. \"We will both write down the probability we think that I am correct, average them together, and that will determine the betting odds. We'll bet as much as we can of our 25 dollars with those odds.\"</p>\n<p>\"What do you mean by 'average' I can think of at least <a href=\"/lw/hpe/how_should_eliezer_and_nicks_extra_20_be_split/\">four</a> possibilities. Also, since I know your probability is high, I will just choose a high probability that is still less than it to maximize the odds in my favor regardless of my actual belief. Your proposition is not strategy proof.\"</p>\n<p>\"Fine, what do you suggest?\"</p>\n<p>You take out some paper, solve some differential equations, and explain how the bet should go.</p>\n<p>Satisfied with your math, you share your probability, he puts 13.28 on the table, and you put 2.72 on the table.</p>\n<p>\"Now what?\" He asks.</p>\n<p>A third meet up member takes quickly takes the 16 dollars from the table and answers, \"You wait.\"</p>\n<p>I will now derive a general algorithm for determining a bet from two probabilities and a maximum amount of money that people are willing to bet. This algorithm is both strategy proof and fair. The solution turns out to be simple, so if you like, you can skip to the last paragraph, and use it next time you want to make a friendly bet. If you want to try to derive the solution on your own, you might want to stop reading now.</p>\n<p>First, we have to be clear about what we mean by strategy proof and fair. \"Strategy proof\" is clear. Our algorithm should ensure that neither person believes that they can increase their expected profit by lying about their probabilities. \"Fair\" will be a little harder to define. There is more than one way to define \"fair\" in this context, but there is one way which I think is probably the best. When the players make the bet, they both will expect to make some profit. They will not both be correct, but they will both believe they are expected to make profit. I claim the bet is fair if both players expect to make the same profit on average.</p>\n<p>Now, lets formalize the problem:</p>\n<p>Alice believes S is true with probability p. Bob believes S is false with probability q. Both players are willing to bet up to d dollars. Without loss of generality, assume p+q&gt;1. Our betting algorithm will output a dollar amount, f(p,q), for Alice to put on the table and a dollar amount, g(p,q) for Bob to put on the table. Then if S is true, Alice gets all the money, and if S is false, Bob gets all the money.</p>\n<p>From Alice's point of view, her expected profit for Alice will be p(g(p,q))+(1-p)(-f(p,q)).</p>\n<p>From Bob's point of view, his expected profit for Bob will be q(f(p,q))+(1-q)(-g(p,q)).</p>\n<p>Setting these two values equal, and simplifying, we get that (1+p-q)g(p,q)=(1+q-p)f(p,q), which is the condition that the betting algorithm is fair.</p>\n<p>For convenience of notation, we will define h(p,q) by h(p,q)=g(p,q)/(1+q-p)=f(p,q)/(1+p-q).</p>\n<p>Now, we want to look at what will happen if Alice lies about her probability. If instead of saying p, Alice were to say that her probability was r, then her expected profit would be p(g(r,q))+(1-p)(-f(r,q)), which equals p(1+q-r)h(r,q)+(1-p)(-(1+r-q)h(r,q))=(2p-1-r+q)h(r,q).</p>\n<p>We want this value as a function of r to be maximized when r=p, which means that -h+(2r-1-r+q)\f(dh/dr)=0.</p>\n<p>Separation of variables gives us (1/h)dh=1/(-1+r+q)dr,</p>\n<p>which integrates to ln(h)=C+ln(-1+r+q) at r=p,</p>\n<p>which simplifies to h=e^C(-1+r+q)=e^C(-1+p+q).</p>\n<p>This gives the solution f(p,q)=e^C(-1+p+q)(1+p-q)=e^C(p^2-(1-q)^2) and g(p,q)=e^C(-1+p+q)(1+q-p)=e^C(q^2-(1-p)^2).</p>\n<p>It is quick to verify that this solution is actually fair, and both players' expected profit is maximized by honest reporting of beliefs.</p>\n<p>The value of the constant multiplied out in front can be anything, and the most either player could ever have to put on the table is equal to this constant. Therefore, if both players are willing to bet up to d dollars, we should define e^C=d.</p>\n<p>Alice and Bob are willing to bet up to d dollars, Alice thinks S is true with probability p, and Bob thinks S is false with probability q. Assuming p+q&gt;1, Alice should put in d(p^2-(1-q)^2), while Bob should put in d(q^2-(1-p)^2). I suggest you use this algorithm next time you want to have a friendly wager (with a rational person), and I suggest you set d to 25 dollars and require both players to say an odd integer percent to ensure a whole number of cents.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"E8PHMuf7tsr8teXAe": 1, "b8FHrKqyXuYGWc6vn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aiz4FCKTgFBtKiWsE", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 62, "extendedScore": null, "score": 0.00019, "legacy": true, "legacyId": "25231", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NHFFzBF4b3SZLHckt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-01-12T07:24:25.537Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-12T13:55:24.241Z", "modifiedAt": null, "url": null, "title": "[LINK] Popular press account of social benefits of motivated reasoning", "slug": "link-popular-press-account-of-social-benefits-of-motivated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:38.204Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CfALFjQggMNjJb9uN/link-popular-press-account-of-social-benefits-of-motivated", "pageUrlRelative": "/posts/CfALFjQggMNjJb9uN/link-popular-press-account-of-social-benefits-of-motivated", "linkUrl": "https://www.lesswrong.com/posts/CfALFjQggMNjJb9uN/link-popular-press-account-of-social-benefits-of-motivated", "postedAtFormatted": "Sunday, January 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Popular%20press%20account%20of%20social%20benefits%20of%20motivated%20reasoning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Popular%20press%20account%20of%20social%20benefits%20of%20motivated%20reasoning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCfALFjQggMNjJb9uN%2Flink-popular-press-account-of-social-benefits-of-motivated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Popular%20press%20account%20of%20social%20benefits%20of%20motivated%20reasoning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCfALFjQggMNjJb9uN%2Flink-popular-press-account-of-social-benefits-of-motivated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCfALFjQggMNjJb9uN%2Flink-popular-press-account-of-social-benefits-of-motivated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 314, "htmlBody": "<p>The Monkey Cage: <a href=\"http://www.washingtonpost.com/blogs/monkey-cage/wp/2014/01/10/the-not-quite-as-depressing-psychological-theory-that-explains-washington/\">\"The Not Quite As Depressing Psychological Theory That Explains Washington\"</a>:</p>\n<blockquote>\n<p>In a landmark article in Behavioral and Brain Sciences, Hugo Mercier and Dan Sperber propose an &ldquo;argumentative&rdquo; theory of human reason which both directly acknowledges the problems of motivated reasoning and suggests that motivated reasoning in the right social contexts can be incredibly valuable and powerful.</p>\n<p>Mercier and Sperber have a straightforward account of why human beings reason. They reason to win arguments by convincing others that they are right. This means that human beings suffer from &ldquo;confirmation bias.&rdquo; They are far, far better at finding justifications for why they are right than they are at thinking carefully about reasons why they might be wrong. [...]</p>\n<p>However, where Mercier and Sperber depart from the skeptics is in pointing to the social value of reasoning. People are terrible judges of the flaws and weaknesses of their own arguments. However, they are much, much better at identifying weaknesses in the arguments of others. Furthermore, confirmation bias gives them good reason not only to try to confirm their own arguments, but also to try to demolish the arguments of people who disagree with them. This in turn means that groups &mdash; under the right conditions &mdash; are likely to be able to reach better judgments than any individual within the group. Real, substantial argument allows a kind of cognitive division of labor, in which different arguments get tested against each other.</p>\n<p>[...]</p>\n<p>When a group has to solve a problem, it is much more e\u000efficient if each individual looks mostly for arguments supporting a given solution. They can then present these arguments to the group, to be tested by the other members. This method will work as long as people can be swayed by good arguments, and the results reviewed . . . show that this is generally the case.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CfALFjQggMNjJb9uN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.5101766459895696e-06, "legacy": true, "legacyId": "25237", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-12T15:50:07.423Z", "modifiedAt": null, "url": null, "title": "New (proposal for) monthly thread: Meetup Reports", "slug": "new-proposal-for-monthly-thread-meetup-reports", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:05.528Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bartimaeus", "createdAt": "2013-05-07T17:14:04.389Z", "isAdmin": false, "displayName": "bartimaeus"}, "userId": "mqWrbcZHzhfPLnJqg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nHcuKaEPvFdJgpex4/new-proposal-for-monthly-thread-meetup-reports", "pageUrlRelative": "/posts/nHcuKaEPvFdJgpex4/new-proposal-for-monthly-thread-meetup-reports", "linkUrl": "https://www.lesswrong.com/posts/nHcuKaEPvFdJgpex4/new-proposal-for-monthly-thread-meetup-reports", "postedAtFormatted": "Sunday, January 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20(proposal%20for)%20monthly%20thread%3A%20Meetup%20Reports&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20(proposal%20for)%20monthly%20thread%3A%20Meetup%20Reports%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnHcuKaEPvFdJgpex4%2Fnew-proposal-for-monthly-thread-meetup-reports%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20(proposal%20for)%20monthly%20thread%3A%20Meetup%20Reports%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnHcuKaEPvFdJgpex4%2Fnew-proposal-for-monthly-thread-meetup-reports", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnHcuKaEPvFdJgpex4%2Fnew-proposal-for-monthly-thread-meetup-reports", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 134, "htmlBody": "<p>If you had an interesting Less Wrong meetup recently, but don't have the time to write up a big report to post to Discussion, feel free to write a comment here.&nbsp; Even if it's just a couple lines about what you did and how people felt about it, it might encourage some people to attend meetups or start meetups in their area.</p>\n<p>If you have the time, you can also describe what types of exercises you did, what worked and what didn't.&nbsp; This could help inspire meetups to try new things and improve themselves in various ways.</p>\n<p>If you're inspired by what's posted below and want to organize a meetup, check out <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\" target=\"_blank\">this page</a> for some resources to get started!&nbsp; You can also check FrankAdamek's weekly post on meetups for the week.</p>\n<p>Tell us about your meetup!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nHcuKaEPvFdJgpex4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 25, "extendedScore": null, "score": 1.510301619669524e-06, "legacy": true, "legacyId": "25238", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-12T19:32:41.358Z", "modifiedAt": null, "url": null, "title": "Double-thick transistors and other subjective phenomena", "slug": "double-thick-transistors-and-other-subjective-phenomena", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:09.678Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bJyYdyiESgfDsevw9/double-thick-transistors-and-other-subjective-phenomena", "pageUrlRelative": "/posts/bJyYdyiESgfDsevw9/double-thick-transistors-and-other-subjective-phenomena", "linkUrl": "https://www.lesswrong.com/posts/bJyYdyiESgfDsevw9/double-thick-transistors-and-other-subjective-phenomena", "postedAtFormatted": "Sunday, January 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Double-thick%20transistors%20and%20other%20subjective%20phenomena&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADouble-thick%20transistors%20and%20other%20subjective%20phenomena%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJyYdyiESgfDsevw9%2Fdouble-thick-transistors-and-other-subjective-phenomena%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Double-thick%20transistors%20and%20other%20subjective%20phenomena%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJyYdyiESgfDsevw9%2Fdouble-thick-transistors-and-other-subjective-phenomena", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJyYdyiESgfDsevw9%2Fdouble-thick-transistors-and-other-subjective-phenomena", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 785, "htmlBody": "<p>If I'm running on a silicon computer, do I have twice as much subjective experience if my computer is twice as thick?</p>\n<p>Why is this even a good question?</p>\n<p>Consider a computer that was printed on a flat sheet. If we stick two of these computers (one a mirror image) together face to face, we get a thicker computer. And then if we peel them apart again, we get two thin computers! Suppose that we simulate a person using these computers. It makes sense that a person running on two thin computers has twice as much \"experience\" as a person running on just one (for example, in the Sleeping Beauty problem, the correct betting strategy is to bet as if the probability of making the bet in a given world is proportional to the number of thin computers). So if we take two people-computers and stick them together into one thicker person-computer, the thicker person contains twice as much \"experience\" as a thinner one - each of their halves has as much experience as a thin person, so they have twice as much experience.</p>\n<p>Do I disagree? Well, I think it depends somewhat on how you cash out \"experience.\" Consider the Sleeping Beauty problem with these computers - in the classic version, our person is asked to give their probability that they're in the possibility where there's one thin computer, or the world where there are two thin computers. The correct betting strategy is to bet as if you think the probability that there are two computers is 2/3 - weighting each computer equally.</p>\n<p>Now, consider altering the experiment so that either there's one thin computer, or one double computer. We have two possibilities - either the correct betting probability is 1/2 and the computers seem to have equal \"experience\", or we bite the bullet and say that the correct betting probability is 2/3 for a double computer, 10/11 for a 10x thicker computer, 1000/1001 for a 1000x thicker computer, etc.</p>\n<p>The bullet-biting scenario is equivalent to saying that the selfish desires of the twice-thick computer are twice as important. If one computer is one person, a double computer is then two people in a box.</p>\n<p>But of course, if you have a box with two people in it, you can knock on the side and go \"hey, how many of you people are in there? &nbsp;I'm putting in an order for chinese food, how many entrees should I get?\" Instead, the double-thick computer is running exactly the same program as the thin computer, and will order exactly the same number of entrees. In particular, a double-thick computer will make evaluations of selfish vs. altruistic priorities exactly the same as a thin computer.</p>\n<p>There is one exception to the previous paragraph - what if the computer is programmed to care about its own thickness, and measure it with external instruments since introspection won't do, and weight its desires more when it's thicker? This is certainly possible, but by putting the caring straight into the utility function, it removes any possibility that the caring is some mysterious \"experience.\" It's just a term in the utility function - it doesn't have to be there, in fact by default it's not. Or, heck, your robot might as easily care more about things when the tides are high, that doesn't mean that high tides grant \"experience.\"</p>\n<p>The original Sleeping Beauty problem, now *that's* mysterious \"experience.\" Ordinary computers enter, weighting the possibility by the number of computers leaves. So something happens when you merge the two computers into a double computer, to destroy that experience rather than conserving it.</p>\n<p>What do I claim explains this? The simple fact that you only offer the double computer one bet, not two. Sure, the exact same signals go to the exact same wires in each case. Except for the prior information that says that the experimenter can only make 1 bet, not 2. &nbsp;In this sense, \"experience\" just comes from the ways in which our computer can interact with the world.</p>\n<p>So since the a double-thick computer is not more selfish than a thin one (neglecting the tides), and will not expect to be a thick computer more often in the Sleeping Beauty problem, I'd say it doesn't have more \"experience\" than a thin computer.</p>\n<p>EDIT: I use betting behavior as a proxy for probability here because it's easy to see which answer is correct. However, using betting behavior as a probability is not always valid - e.g. in the absent-minded driver problem. In the sleeping beauty case it only works because the payout structure is very simple. A safer way would be to derive the probabilities from the information available to the agents, which has been done elsewhere, but is harder to follow.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bJyYdyiESgfDsevw9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 1.5105441310185217e-06, "legacy": true, "legacyId": "25232", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-12T19:37:13.792Z", "modifiedAt": null, "url": null, "title": "Meetup : Lesswrong Boulder CO", "slug": "meetup-lesswrong-boulder-co-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:00.908Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "yakurbe0112", "createdAt": "2012-07-01T23:40:26.855Z", "isAdmin": false, "displayName": "yakurbe0112"}, "userId": "xXr6Jngw57uMXveQ9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wuJ4Ftj9qz7rYPLFT/meetup-lesswrong-boulder-co-1", "pageUrlRelative": "/posts/wuJ4Ftj9qz7rYPLFT/meetup-lesswrong-boulder-co-1", "linkUrl": "https://www.lesswrong.com/posts/wuJ4Ftj9qz7rYPLFT/meetup-lesswrong-boulder-co-1", "postedAtFormatted": "Sunday, January 12th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Lesswrong%20Boulder%20CO&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Lesswrong%20Boulder%20CO%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwuJ4Ftj9qz7rYPLFT%2Fmeetup-lesswrong-boulder-co-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Lesswrong%20Boulder%20CO%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwuJ4Ftj9qz7rYPLFT%2Fmeetup-lesswrong-boulder-co-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwuJ4Ftj9qz7rYPLFT%2Fmeetup-lesswrong-boulder-co-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vi'>Lesswrong Boulder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 January 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2690 Baseline Rd, Boulder, CO 80305</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet us here for awesome pizza and rationality. This week I'm going to see what I can learn about goal factoring and then tell you all about it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vi'>Lesswrong Boulder CO</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wuJ4Ftj9qz7rYPLFT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5105490791811855e-06, "legacy": true, "legacyId": "25241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Lesswrong_Boulder_CO\">Discussion article for the meetup : <a href=\"/meetups/vi\">Lesswrong Boulder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 January 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2690 Baseline Rd, Boulder, CO 80305</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet us here for awesome pizza and rationality. This week I'm going to see what I can learn about goal factoring and then tell you all about it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Lesswrong_Boulder_CO1\">Discussion article for the meetup : <a href=\"/meetups/vi\">Lesswrong Boulder CO</a></h2>", "sections": [{"title": "Discussion article for the meetup : Lesswrong Boulder CO", "anchor": "Discussion_article_for_the_meetup___Lesswrong_Boulder_CO", "level": 1}, {"title": "Discussion article for the meetup : Lesswrong Boulder CO", "anchor": "Discussion_article_for_the_meetup___Lesswrong_Boulder_CO1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-13T01:07:04.880Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup: Democracy", "slug": "meetup-west-la-meetup-democracy", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WiNFHYj7AKqpWGYkd/meetup-west-la-meetup-democracy", "pageUrlRelative": "/posts/WiNFHYj7AKqpWGYkd/meetup-west-la-meetup-democracy", "linkUrl": "https://www.lesswrong.com/posts/WiNFHYj7AKqpWGYkd/meetup-west-la-meetup-democracy", "postedAtFormatted": "Monday, January 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%3A%20Democracy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%3A%20Democracy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWiNFHYj7AKqpWGYkd%2Fmeetup-west-la-meetup-democracy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%3A%20Democracy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWiNFHYj7AKqpWGYkd%2Fmeetup-west-la-meetup-democracy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWiNFHYj7AKqpWGYkd%2Fmeetup-west-la-meetup-democracy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vj'>West LA Meetup: Democracy</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 January 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>How to get in: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the Westside Pavillion on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p>Parking is free for 3 hours.</p>\n\n<p>Discussion: This week, we will be talking about democracy. What properties would a good voting system have? Why is there no perfect voting system? What is the best voting system? Is there any reasonable voting system that we have any chance of ever being implemented? Should I vote for a third party or a primary party candidate in the next election? Is democracy even worth it in the first place? In spite of the topic, this discussion will probably be abstract enough to not kill your mind.</p>\n\n<p>No prior knowledge of or exposure to Less Wrong is necessary; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vj'>West LA Meetup: Democracy</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WiNFHYj7AKqpWGYkd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.5109086199516109e-06, "legacy": true, "legacyId": "25242", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup__Democracy\">Discussion article for the meetup : <a href=\"/meetups/vj\">West LA Meetup: Democracy</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 January 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>How to get in: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the Westside Pavillion on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p>Parking is free for 3 hours.</p>\n\n<p>Discussion: This week, we will be talking about democracy. What properties would a good voting system have? Why is there no perfect voting system? What is the best voting system? Is there any reasonable voting system that we have any chance of ever being implemented? Should I vote for a third party or a primary party candidate in the next election? Is democracy even worth it in the first place? In spite of the topic, this discussion will probably be abstract enough to not kill your mind.</p>\n\n<p>No prior knowledge of or exposure to Less Wrong is necessary; this will be generally accessible.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup__Democracy1\">Discussion article for the meetup : <a href=\"/meetups/vj\">West LA Meetup: Democracy</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup: Democracy", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup__Democracy", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup: Democracy", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup__Democracy1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-13T02:31:57.366Z", "modifiedAt": null, "url": null, "title": "Stupid Questions Thread - January 2014", "slug": "stupid-questions-thread-january-2014", "viewCount": null, "lastCommentedAt": "2018-10-24T01:13:00.069Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomeoStevens", "createdAt": "2011-10-28T20:59:30.426Z", "isAdmin": false, "displayName": "RomeoStevens"}, "userId": "5ZpAE3i54eEhMp2ib", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XxJDvPyqDN47uttCR/stupid-questions-thread-january-2014", "pageUrlRelative": "/posts/XxJDvPyqDN47uttCR/stupid-questions-thread-january-2014", "linkUrl": "https://www.lesswrong.com/posts/XxJDvPyqDN47uttCR/stupid-questions-thread-january-2014", "postedAtFormatted": "Monday, January 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stupid%20Questions%20Thread%20-%20January%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStupid%20Questions%20Thread%20-%20January%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXxJDvPyqDN47uttCR%2Fstupid-questions-thread-january-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stupid%20Questions%20Thread%20-%20January%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXxJDvPyqDN47uttCR%2Fstupid-questions-thread-january-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXxJDvPyqDN47uttCR%2Fstupid-questions-thread-january-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<p>Haven't had one of these for awhile. This thread is for questions or comments that you've felt silly about not knowing/understanding. Let's try to exchange info that seems obvious, knowing that due to the illusion of transparency it really isn't so obvious!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XxJDvPyqDN47uttCR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 1.5110011586049723e-06, "legacy": true, "legacyId": "25244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 298, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-13T03:16:28.394Z", "modifiedAt": null, "url": null, "title": "On Voting for Third Parties", "slug": "on-voting-for-third-parties", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:08.476Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BgADpG4q8Q8wvGvxc/on-voting-for-third-parties", "pageUrlRelative": "/posts/BgADpG4q8Q8wvGvxc/on-voting-for-third-parties", "linkUrl": "https://www.lesswrong.com/posts/BgADpG4q8Q8wvGvxc/on-voting-for-third-parties", "postedAtFormatted": "Monday, January 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Voting%20for%20Third%20Parties&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Voting%20for%20Third%20Parties%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBgADpG4q8Q8wvGvxc%2Fon-voting-for-third-parties%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Voting%20for%20Third%20Parties%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBgADpG4q8Q8wvGvxc%2Fon-voting-for-third-parties", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBgADpG4q8Q8wvGvxc%2Fon-voting-for-third-parties", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 815, "htmlBody": "<p><a href=\"http://bywayofcontradiction.com/?p=106\">Cross Posted</a> on my blog, <a href=\"http://bywayofcontradiction.com\">By Way of Contradiction</a>.&nbsp;</p>\n<p>Anti-Trigger Warning: There is not really any politics in this post. I doubt it will kill your mind.</p>\n<p>If your favorite candidate in an election is a third party candidate, should you vote for him?</p>\n<p>This question has confused me. I have changed my mind many times, and I have recently changed my mind again. I would like to talk about some of the arguments in both directions and explain the reason for my most recent change.</p>\n<p>Con 1) Voting for a third party is throwing your vote away.</p>\n<p>We have all heard this argument before, and it is true. It is an unfortunate consequence of the plurality voting system. Plurality is horrible and there are all better alternatives, but it is what we are stuck with for now. If you vote for a third party, the same candidate would be elected as if you did not vote at all.</p>\n<p>Pro 1) The probability that you vote changes the election is negligible. All your vote does is add one to the number of people who voted for a given candidate. Your vote for the third party candidate therefore matters more because it is changing a small number by relatively more.</p>\n<p>This argument is actually an empirical claim, and I am not sure how well it holds up. It is easy to study the likelihood that you vote changes the election. <a href=\"http://www.stat.columbia.edu/~gelman/research/published/probdecisive2.pdf\">One study</a> finds that it roughly varies from 10^-7 to 10^-11 in America for presidential elections. However, it is not clear to me just how much your vote affects the strategies of political candidates and voters in the future.</p>\n<p>Pro 2) The probability that your vote changes the election or future elections is negligible. The primary personal benefit for voting is the personal satisfaction of voting. This personal satisfaction is maximized by voting for the candidate you agree with the most.</p>\n<p>I think that many people if given the choice between changing the next president between the two primary parties or being paid an amount of money equal to the product of the amount of gas they spent to drive to vote and 10^7 would take the money. I am not one of them but any of those people must agree that voting is a bad investment if you do not consider the personal satisfaction. However, I think I might get more satisfaction out of doing my best to change the election, rather than placing a vote that does not matter.</p>\n<p>Con 2) Actually if you use a reflexive decision theory, you are much more likely to change the election, so you should vote like it matters.</p>\n<p>Looking at the problem like a timeless decision agent, you see that your choice on voting is probably correlated with that of many other people. You voting for a primary party is logically linked with other people voting for a primary party, and those people whose votes are logically linked with yours are more likely to agree with you politically. This could bring the chance of changing the election out of the negligible zone, where you should be deciding based on political consequences.</p>\n<p>Pro 3) Your morality should encourage you to vote honestly.</p>\n<p>It is not clear to me that I should view a vote for my favorite candidate as an honest vote. If we used the anti-plurality system where the person with the least votes wins, then a vote for my favorite candidate would clearly not be considered an honest one. The \"honest\" vote should be the vote that you think will maximize your preferences which might be a vote for a primary party.</p>\n<p>Pro 4) Strategic voting is like defecting in the prisoner's dilemma. If we all cooperate and vote honestly, we will get the favorite candidate of the largest number of people. If not, then we could end up with someone much worse.</p>\n<p>The problem with this is that if we all vote honestly, we get the plurality winner, and the plurality winner is probably not all that great a choice. The obvious voting strategy is not the only problem with plurality. Plurality also discourages compromise, and the results of plurality are changed drastically by honest vote splitting. The plurality candidate is not a good enough goal that I think we should all cooperate to achieve it.</p>\n<p>I have decided that in the next election, I will vote for a primary party candidate. I changed my mind almost a year ago after reading <a href=\"/lw/mi/stop_voting_for_nincompoops/\">Stop Voting for Nincompoops</a>, but after recent further reflection, I have changed my mind back. I believe that Con 1 is valid, Con 2 and the other criticisms above adequately respond to Pro 1 and Pro 2, and I believe that Pro 3 and Pro 4 are invalid for the reasons described above. I would love to hear any opinions on any of these arguments, and would love even more to hear arguments I have not thought of yet.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BgADpG4q8Q8wvGvxc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 1.5110496994311046e-06, "legacy": true, "legacyId": "25246", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["k5qPoHFgjyxtvYsm7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-13T05:00:23.495Z", "modifiedAt": null, "url": null, "title": "Anthropic Atheism", "slug": "anthropic-atheism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:08.155Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/93oeqzF7ZEKbd9jdx/anthropic-atheism", "pageUrlRelative": "/posts/93oeqzF7ZEKbd9jdx/anthropic-atheism", "linkUrl": "https://www.lesswrong.com/posts/93oeqzF7ZEKbd9jdx/anthropic-atheism", "postedAtFormatted": "Monday, January 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anthropic%20Atheism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnthropic%20Atheism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F93oeqzF7ZEKbd9jdx%2Fanthropic-atheism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anthropic%20Atheism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F93oeqzF7ZEKbd9jdx%2Fanthropic-atheism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F93oeqzF7ZEKbd9jdx%2Fanthropic-atheism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1442, "htmlBody": "<p>(Crossposted from <a href=\"http://nyansandwich.info/anthropics.html\">my blog</a>)</p>\n<p>I've been developing an approach to anthropic questions that I find less confusing than others, which I call Anthropic Atheism (AA). The name is a snarky reference to the ontologically basic status of observers (souls) in other anthropic theories. I'll have to explain myself.</p>\n<p>We'll start with what I call the &ldquo;Sherlock Holmes Axiom&rdquo; (SHA), which will form the epistemic background for my approach:</p>\n<blockquote>\n<p>How often have I said to you that when you have eliminated the impossible, whatever remains, however improbable, must be the truth?</p>\n</blockquote>\n<p>Which I reinterpret as &ldquo;Reason by eliminating those possibilities inconsistent with your observations. Period.&rdquo; I use this as a basis of epistemology. Basically, think of all possible world-histories, assign probability to each of them according to whatever principles (eg occams razor), eliminate inconsistencies, and renormalize your probabilities. I won&rsquo;t go into the details, but it turns out that probability theory (eg Bayes theorem) falls out of this just fine when you translate <code>P(E|H)</code> as &ldquo;portion of possible worlds consistent with H that predict E&rdquo;. So it&rsquo;s not really any different, but using SHA as our basis, I find certain confusing questions less confusing, and certain unholy temptations less tempting.</p>\n<p>With that out of the way, let&rsquo;s have a look at some confusing questions. First up is the <a href=\"https://en.wikipedia.org/wiki/Doomsday_argument\">Doomsday Argument</a>. From La Wik:</p>\n<blockquote>\n<p>Simply put, it says that supposing the humans alive today are in a random place in the whole human history timeline, chances are we are about halfway through it.</p>\n</blockquote>\n<p>The article goes on to claim that &ldquo;There is a 95% chance of extinction within 9120 years.&rdquo; Hard to refute, but nevertheless it makes one rather uncomfortable that the mere fact of one&rsquo;s existence should have predictive consequences.</p>\n<p>In response, Nick Bostrom formulated the &ldquo;<a href=\"https://en.wikipedia.org/wiki/Self-Indication_Assumption\">Self Indication Assumption</a>&rdquo;, which states that &ldquo;All other things equal, an observer should reason as if they are randomly selected from the set of all <em>possible</em> observers.&rdquo; Applied to the doomsday argument, it says that you are just as likely to exist in 2014 in a world where humanity grows up to create a glorious everlasting civilization, as one where we wipe ourselves out in the next hundred years, so you can&rsquo;t update on that mere fact of your existence. This is comforting, as it defuses the doomsday argument.</p>\n<p>By contrast, the Doomsday argument is the consequence of the &ldquo;<a href=\"https://en.wikipedia.org/wiki/Self-Sampling_Assumption\">Self Sampling Assumption</a>&rdquo;, which states that &ldquo;All other things equal, an observer should reason as if they are randomly selected from the set of all actually existent observers (past, present and future) in their reference class.&rdquo;</p>\n<p>Unfortunately for SIA, it implies that &ldquo;Given the fact that you exist, you should (other things equal) favor hypotheses according to which many observers exist over hypotheses on which few observers exist.&rdquo; Surely <em>that</em> should not follow, but clearly it does. So we can formulate another anthropic problem:</p>\n<blockquote>\n<p>It is the year 2100 and physicists have narrowed down the search for a theory of everything to only two remaining plausible candidate theories, T1 and T2 (using considerations from super-duper symmetry). According to T1 the world is very, very big but finite, and there are a total of a trillion trillion observers in the cosmos. According to T2, the world is very, very, very big but finite, and there are a trillion trillion trillion observers. The super-duper symmetry considerations seem to be roughly indifferent between these two theories. The physicists are planning on carrying out a simple experiment that will falsify one of the theories. Enter the presumptuous philosopher: &ldquo;Hey guys, it is completely unnecessary for you to do the experiment, because I can already show to you that T2 is about a trillion times more likely to be true than T1</p>\n</blockquote>\n<p>This one is called the &ldquo;<a href=\"/lw/175/torture_vs_dust_vs_the_presumptuous_philosopher/\">presumptuous philosopher</a>&rdquo;. Clearly the presumptuous philosopher should not get a Nobel prize.</p>\n<p>These questions have caused much psychological distress, and been beaten to death in certain corners of the internet, but as far as I know, few people have satisfactory answers. <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">Wei Dai&rsquo;s UDT</a> might be satisfactory for this, and might be equivalent to my answer, when the dust settles.</p>\n<p>So what&rsquo;s my objection to these schemes, and what&rsquo;s my scheme?</p>\n<p>My objection is aesthetic; I don&rsquo;t like that SIA and SSA seem to place some kind of ontological specialness on &ldquo;observers&rdquo;. This reminds me way too much of souls, which are nonsense. The whole &ldquo;reference-class&rdquo; thing rubs me the wrong way as well. Reference classes are useful tools for statistical approximation, not fundamental features of epistemology. So I'm hesitant to accept these theories.</p>\n<p>Instead, I take the position that you can never conclude anything from your own existence except that you exist. That is, I eliminate all hypotheses that don&rsquo;t predict my existence, and leave it at that, in accordance with SHA. No update happens in the Doomsday Argument; both glorious futures and impending doom are consistent with my existence, their relative probability comes from other reasoning. And the presumptuous philosopher is an idiot because both theories are consistent with us existing, so again we get no relative update.</p>\n<p>By reasoning purely from consistency of possible worlds with observations, SHA gives us a reasonably principled way to just punt on these questions. Let&rsquo;s see how it does on another anthropic question, the <a href=\"https://en.wikipedia.org/wiki/Sleeping_beauty_problem\">Sleeping Beauty Problem</a>:</p>\n<blockquote>\n<p>Sleeping Beauty volunteers to undergo the following experiment and is told all of the following details: On Sunday she will be put to sleep. Once or twice, during the experiment, Beauty will be wakened, interviewed, and put back to sleep with an amnesia-inducing drug that makes her forget that awakening. A fair coin will be tossed to determine which experimental procedure to undertake: if the coin comes up heads, Beauty will be wakened and interviewed on Monday only. If the coin comes up tails, she will be wakened and interviewed on Monday and Tuesday. In either case, she will be wakened on Wednesday without interview and the experiment ends.</p>\n<p>Any time Sleeping Beauty is wakened and interviewed, she is asked, &ldquo;What is your belief now for the proposition that the coin landed heads?&rdquo;</p>\n</blockquote>\n<p>SHA says that the coin came up heads in half of the worlds, and no further update happens based on existence. I'm slightly uncomfortable with this, because SHA is cheerfully biting a bullet that has confused many philosophers. However, I see no reason not to bite this bullet; it doesn&rsquo;t seem to have any particularly controversial implications for actual decision making. If she is paid for each correct <em>guess</em>, for example, she'll say that she thinks the coin came up tails (this way she gets $2 half the time instead of $1 half the time for heads). If she&rsquo;s paid only on Monday, she&rsquo;s indifferent between the options, as she should be.</p>\n<p>What if we modify the problem slightly, and ask sleeping beauty for her credence that it&rsquo;s Monday? That is, her credence that &ldquo;it&rdquo; &ldquo;is&rdquo; Monday. If the coin came up heads, there is only Monday, but if it came up tails, there is a Monday observer and a Tuesday observer. AA/SHA reasons purely from the perspective of possible worlds, and says that Monday is consistent with observations, as is Tuesday, and refuses to speculate further on which &ldquo;observer&rdquo; among possible observers she &ldquo;is&rdquo;. Again, given an actual decision problem with an actual payoff structure, AA/SHA will quickly reach the correct decision, even while refusing to assign probabilities &ldquo;between observers&rdquo;.</p>\n<p>I'd like to say that we've casually thrown out probability theory when it became inconvenient, but we haven&rsquo;t; we've just refused to answer a meaningless question. The meaninglessness of indexical uncertainty becomes apparent when you stop believing in the specialness of observers. It&rsquo;s like asking &ldquo;What&rsquo;s the probability that the Sun rather than the Earth?&rdquo;. That the Sun what? The Sun <em>and</em> the Earth both exist, for example, but maybe you meant something else. Want to know which one this here comet is going to hit? Sure I'll answer that, but these generic &ldquo;which one&rdquo; questions are meaningless.</p>\n<p>Not that I'm familiar with UDT, but this really is starting to remind me of UDT. Perhaps it even <em>is</em> part of UDT. In any case, Anthropic Atheism seems to easily give intuitive answers to anthropic questions. Maybe it breaks down on some edge case, though. If so, I'd like to see it. In the mean time, I don&rsquo;t believe in observers.</p>\n<p>ADDENDUM: As Wei Dai, DanielLC, and&nbsp;Tyrrell_McAllister point out below, it turns out this doesn't actually work. The objection is that by refusing to include the indexical hypothesis, we end up favoring universes with more variety of experiences (because they have a high chance of containing *our* experiences) and sacrificing the ability to predict much of anything. Oops. It was fun while it lasted ;)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "93oeqzF7ZEKbd9jdx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 34, "extendedScore": null, "score": 1.5111630213394244e-06, "legacy": true, "legacyId": "25240", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RcvyJjPQwimAeapNg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-13T11:25:44.474Z", "modifiedAt": null, "url": null, "title": "Karma query", "slug": "karma-query", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:45.815Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Chrysophylax", "createdAt": "2013-01-15T19:25:45.356Z", "isAdmin": false, "displayName": "Chrysophylax"}, "userId": "rk3XyPQ6eodfferp3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YDoNvjx6PxYgqvym4/karma-query", "pageUrlRelative": "/posts/YDoNvjx6PxYgqvym4/karma-query", "linkUrl": "https://www.lesswrong.com/posts/YDoNvjx6PxYgqvym4/karma-query", "postedAtFormatted": "Monday, January 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Karma%20query&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKarma%20query%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYDoNvjx6PxYgqvym4%2Fkarma-query%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Karma%20query%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYDoNvjx6PxYgqvym4%2Fkarma-query", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYDoNvjx6PxYgqvym4%2Fkarma-query", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<p>Yesterday I had 25 karma, having earned 15 (100% positive) in the last 30 days. Today I have 24 karma, still having 15 (100% positive) in the last 30 days. What is happening here?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YDoNvjx6PxYgqvym4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -8, "extendedScore": null, "score": -7e-06, "legacy": true, "legacyId": "25248", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-13T14:10:45.094Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava Meetup IX.", "slug": "meetup-bratislava-meetup-ix", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R6ttaQQLnvn6HBMn3/meetup-bratislava-meetup-ix", "pageUrlRelative": "/posts/R6ttaQQLnvn6HBMn3/meetup-bratislava-meetup-ix", "linkUrl": "https://www.lesswrong.com/posts/R6ttaQQLnvn6HBMn3/meetup-bratislava-meetup-ix", "postedAtFormatted": "Monday, January 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava%20Meetup%20IX.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%20Meetup%20IX.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR6ttaQQLnvn6HBMn3%2Fmeetup-bratislava-meetup-ix%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%20Meetup%20IX.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR6ttaQQLnvn6HBMn3%2Fmeetup-bratislava-meetup-ix", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR6ttaQQLnvn6HBMn3%2Fmeetup-bratislava-meetup-ix", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 95, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vk'>Bratislava Meetup IX.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 January 2014 06:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Bistro The Peach, Heydukova 21, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The same place; lecture and discussion in Slovak language.</p>\n\n<p>Stret\u00e1vame sa na rovnakom mieste v rovnakom \u010dase: pondelok o \u0161iestej. Dne\u0161n\u00fdm hos\u0165om je <a href=\"http://nataliablahova.blog.sme.sk/\" rel=\"nofollow\">Nat\u00e1lia Blahov\u00e1</a>, soci\u00e1lna poradky\u0148a a ob\u010dianska aktivistka zaoberaj\u00faca sa pr\u00e1vami det\u00ed. T\u00e9ma je soci\u00e1lny syst\u00e9m, altruizmus, ob\u010dianska anga\u017eovanos\u0165... a \u010doko\u013evek, \u010do v\u00e1s bude zauj\u00edma\u0165.</p>\n\n<p>Technick\u00e1 pozn\u00e1mka: Pani Blahov\u00e1 pr\u00edde a\u017e o siedmej, ale my sa stretneme klasicky o \u0161iestej, \u010di\u017ee prv\u00fa hodinu bude vo\u013en\u00e1 debata.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vk'>Bratislava Meetup IX.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R6ttaQQLnvn6HBMn3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.5117634342520306e-06, "legacy": true, "legacyId": "25249", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_IX_\">Discussion article for the meetup : <a href=\"/meetups/vk\">Bratislava Meetup IX.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 January 2014 06:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Bistro The Peach, Heydukova 21, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The same place; lecture and discussion in Slovak language.</p>\n\n<p>Stret\u00e1vame sa na rovnakom mieste v rovnakom \u010dase: pondelok o \u0161iestej. Dne\u0161n\u00fdm hos\u0165om je <a href=\"http://nataliablahova.blog.sme.sk/\" rel=\"nofollow\">Nat\u00e1lia Blahov\u00e1</a>, soci\u00e1lna poradky\u0148a a ob\u010dianska aktivistka zaoberaj\u00faca sa pr\u00e1vami det\u00ed. T\u00e9ma je soci\u00e1lny syst\u00e9m, altruizmus, ob\u010dianska anga\u017eovanos\u0165... a \u010doko\u013evek, \u010do v\u00e1s bude zauj\u00edma\u0165.</p>\n\n<p>Technick\u00e1 pozn\u00e1mka: Pani Blahov\u00e1 pr\u00edde a\u017e o siedmej, ale my sa stretneme klasicky o \u0161iestej, \u010di\u017ee prv\u00fa hodinu bude vo\u013en\u00e1 debata.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_IX_1\">Discussion article for the meetup : <a href=\"/meetups/vk\">Bratislava Meetup IX.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava Meetup IX.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_IX_", "level": 1}, {"title": "Discussion article for the meetup : Bratislava Meetup IX.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_IX_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-13T23:24:15.085Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston - Aversion factoring and calibration", "slug": "meetup-boston-aversion-factoring-and-calibration", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RBiZwm3QjnED4E8nJ/meetup-boston-aversion-factoring-and-calibration", "pageUrlRelative": "/posts/RBiZwm3QjnED4E8nJ/meetup-boston-aversion-factoring-and-calibration", "linkUrl": "https://www.lesswrong.com/posts/RBiZwm3QjnED4E8nJ/meetup-boston-aversion-factoring-and-calibration", "postedAtFormatted": "Monday, January 13th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20-%20Aversion%20factoring%20and%20calibration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20-%20Aversion%20factoring%20and%20calibration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRBiZwm3QjnED4E8nJ%2Fmeetup-boston-aversion-factoring-and-calibration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20-%20Aversion%20factoring%20and%20calibration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRBiZwm3QjnED4E8nJ%2Fmeetup-boston-aversion-factoring-and-calibration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRBiZwm3QjnED4E8nJ%2Fmeetup-boston-aversion-factoring-and-calibration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 145, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vl'>Boston - Aversion factoring and calibration</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 January 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Citadel, 98 Elm St Apt 1, Somerville, MA, 02144</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Victoria Krakovna will be running a session on Aversion Factoring and Calibration (based on a CFAR unit). While Goal Factoring is useful for understanding your motivations for things you do, Aversion Factoring helps you analyze the things you are averse to doing.</p>\n\n<p>Cambridge/Boston-area Less Wrong Wednesday meetups are once a month at 7pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square). All other meetups are on Sundays.</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 7:30pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vl'>Boston - Aversion factoring and calibration</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RBiZwm3QjnED4E8nJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5123676983979122e-06, "legacy": true, "legacyId": "25250", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston___Aversion_factoring_and_calibration\">Discussion article for the meetup : <a href=\"/meetups/vl\">Boston - Aversion factoring and calibration</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 January 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Citadel, 98 Elm St Apt 1, Somerville, MA, 02144</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Victoria Krakovna will be running a session on Aversion Factoring and Calibration (based on a CFAR unit). While Goal Factoring is useful for understanding your motivations for things you do, Aversion Factoring helps you analyze the things you are averse to doing.</p>\n\n<p>Cambridge/Boston-area Less Wrong Wednesday meetups are once a month at 7pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square). All other meetups are on Sundays.</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 7:30pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston___Aversion_factoring_and_calibration1\">Discussion article for the meetup : <a href=\"/meetups/vl\">Boston - Aversion factoring and calibration</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston - Aversion factoring and calibration", "anchor": "Discussion_article_for_the_meetup___Boston___Aversion_factoring_and_calibration", "level": 1}, {"title": "Discussion article for the meetup : Boston - Aversion factoring and calibration", "anchor": "Discussion_article_for_the_meetup___Boston___Aversion_factoring_and_calibration1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-14T01:21:51.143Z", "modifiedAt": null, "url": null, "title": "Philosophical Realism and Fairy Dust", "slug": "philosophical-realism-and-fairy-dust", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:58.630Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HalMorris", "createdAt": "2012-12-08T02:54:12.946Z", "isAdmin": false, "displayName": "HalMorris"}, "userId": "8cZxp4PS87vNbhmCf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6Cw7pTgfDL9H3bjLo/philosophical-realism-and-fairy-dust", "pageUrlRelative": "/posts/6Cw7pTgfDL9H3bjLo/philosophical-realism-and-fairy-dust", "linkUrl": "https://www.lesswrong.com/posts/6Cw7pTgfDL9H3bjLo/philosophical-realism-and-fairy-dust", "postedAtFormatted": "Tuesday, January 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Philosophical%20Realism%20and%20Fairy%20Dust&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhilosophical%20Realism%20and%20Fairy%20Dust%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Cw7pTgfDL9H3bjLo%2Fphilosophical-realism-and-fairy-dust%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Philosophical%20Realism%20and%20Fairy%20Dust%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Cw7pTgfDL9H3bjLo%2Fphilosophical-realism-and-fairy-dust", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Cw7pTgfDL9H3bjLo%2Fphilosophical-realism-and-fairy-dust", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 123, "htmlBody": "<p>\"Realism\" in the philosophical sense has to be relative to something - Plato's essences, \"collective imagination\", society, truth are among the subjects that evoke comments that this person is a realist (considers the \"something\" to be real), and that one isn't.</p>\n<p>A philosophical realist w.r.t. fairies is one who believes Fairies are real, while the non-realist says talk of fairies is due to overactive agency detectors or some such thing.&nbsp; It will tend to seem like the opposite of everyday use of the word \"realism\" -- at least if the subject is one non-philosophers would ever talk about.</p>\n<p>I mention this only because I found it a bit difficult to get, and I think I've now \"got\" it.&nbsp; Correct me if you think I'm wrong.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6Cw7pTgfDL9H3bjLo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.5124961401823782e-06, "legacy": true, "legacyId": "25251", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-14T02:15:53.470Z", "modifiedAt": null, "url": null, "title": "What rationality material should I teach in my game theory course", "slug": "what-rationality-material-should-i-teach-in-my-game-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:03.567Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f3ZfcMdPtjSwLSnHS/what-rationality-material-should-i-teach-in-my-game-theory", "pageUrlRelative": "/posts/f3ZfcMdPtjSwLSnHS/what-rationality-material-should-i-teach-in-my-game-theory", "linkUrl": "https://www.lesswrong.com/posts/f3ZfcMdPtjSwLSnHS/what-rationality-material-should-i-teach-in-my-game-theory", "postedAtFormatted": "Tuesday, January 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20rationality%20material%20should%20I%20teach%20in%20my%20game%20theory%20course&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20rationality%20material%20should%20I%20teach%20in%20my%20game%20theory%20course%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff3ZfcMdPtjSwLSnHS%2Fwhat-rationality-material-should-i-teach-in-my-game-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20rationality%20material%20should%20I%20teach%20in%20my%20game%20theory%20course%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff3ZfcMdPtjSwLSnHS%2Fwhat-rationality-material-should-i-teach-in-my-game-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff3ZfcMdPtjSwLSnHS%2Fwhat-rationality-material-should-i-teach-in-my-game-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 120, "htmlBody": "<p>I will soon be teaching a 50 person game theory class at Smith College and I want to include material on how to be rational. What do you think I should teach/assign? &nbsp;The course has a one semester calculus requirement. Here is material I'm considering:</p>\n<p>&nbsp;</p>\n<p>Litany of Gendlin,&nbsp;<a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">Humans are not automatically strategic</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Map\">The map is not the territory</a>,&nbsp;<a href=\"/lw/21b/\">Ugh fields</a>,&nbsp;Litany of Tarski,&nbsp;<a href=\"/lw/3pl/branches_of_rationality/\">Branches of rationality</a>,&nbsp;<a href=\"http://yudkowsky.net/rational/virtues\">Twelve Virtues of Rationality</a>,&nbsp;<a href=\"/lw/4su/how_to_be_happy/\">How to Be Happy</a>,&nbsp;<a href=\"/lw/3w3/how_to_beat_procrastination/\">How to Beat Procrastination</a>,&nbsp;<a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">Make Beliefs Pay Rent</a>,&nbsp;<a href=\"/lw/25t/navigating_disagreement_how_to_keep_your_eye_on/\">Navigating disagreement</a>,&nbsp;<a href=\"/lw/h3/superstimuli_and_the_collapse_of_western/\">Superstimuli and the Collapse of Western Civilization</a>,&nbsp;<a href=\"/lw/3gj/efficient_charity_do_unto_others/\">Efficient Charity</a>,&nbsp;<a href=\"/lw/5f/bayesians_vs_barbarians/\">Bayesians vs. Barbarians</a>,&nbsp;<a href=\"http://slatestarcodex.com/2013/06/13/arguments-from-my-opponent-believes-something/\">ARGUMENTS FROM MY OPPONENT BELIEVES</a>,&nbsp;<a href=\"/lw/i0/are_your_enemies_innately_evil/\">Are Your Enemies Innately Evil?</a>,&nbsp;<a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">Reversed Stupidity Is Not Intelligence</a>,&nbsp;<a href=\"/lw/ita/how_habits_work_and_how_you_may_control_them/\">How habits work and how you may control them</a>,&nbsp;<a href=\"/lw/lj/the_halo_effect/\">The Halo Effect</a>,&nbsp;<a href=\"http://slatestarcodex.com/2013/05/17/newtonian-ethics/\">NEWTONIAN ETHICS</a>,&nbsp;<a href=\"/lw/jg/planning_fallacy/\">Planning Fallacy</a>,&nbsp;<a href=\"/lw/52g/the_good_news_of_situationist_psychology/\">The Good News of Situationist Psychology</a>,&nbsp;<a href=\"/lw/ui/use_the_try_harder_luke/\">Use the Try Harder, Luke</a>,&nbsp;<a href=\"/lw/hl/lotteries_a_waste_of_hope/\">Lotteries: A Waste of Hope</a>. &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f3ZfcMdPtjSwLSnHS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 1.5125551668520166e-06, "legacy": true, "legacyId": "25252", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PBRWb2Em5SNeWYwwB", "EFQ3F6kmt4WHXRqik", "xvAkpCSdqgtYhEceo", "ZbgCx2ntD5eu8Cno9", "RWo4LwFzpHNQCTcYt", "a7n8GdKiAZRX86T5A", "cL4wNuHhM5gH4GxdC", "Jq73GozjsuhdwMLEG", "pC47ZTsPNAkjavkXs", "KsHmn6iJAEr9bACQW", "28bAMAxhoX3bwbAKC", "qNZM3EGoE5ZeMdCRt", "5wMTZLZZmZEbXdoMD", "ACGeaAk6KButv2xwQ", "CPm5LTwHrvBJCa9h5", "Q5CjE8pRiACqTvhRM", "fhEPnveFhb9tmd7Pe", "vYsuM8cpuRgZS5rYB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-14T18:18:40.112Z", "modifiedAt": null, "url": null, "title": "Interview with a Scientist.", "slug": "interview-with-a-scientist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:58.781Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Baruta07", "createdAt": "2012-10-18T02:35:18.633Z", "isAdmin": false, "displayName": "Baruta07"}, "userId": "reiXxhoN8Fj8xnHDF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Cy45Y693hokwFBRGi/interview-with-a-scientist", "pageUrlRelative": "/posts/Cy45Y693hokwFBRGi/interview-with-a-scientist", "linkUrl": "https://www.lesswrong.com/posts/Cy45Y693hokwFBRGi/interview-with-a-scientist", "postedAtFormatted": "Tuesday, January 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interview%20with%20a%20Scientist.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInterview%20with%20a%20Scientist.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCy45Y693hokwFBRGi%2Finterview-with-a-scientist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interview%20with%20a%20Scientist.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCy45Y693hokwFBRGi%2Finterview-with-a-scientist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCy45Y693hokwFBRGi%2Finterview-with-a-scientist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 33, "htmlBody": "<p>For my High-school chem course one of the requirements is to interview anyone occupied in a science relation occupation. Is there anyone interested in being interviewed: either over Skype or some other program?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Cy45Y693hokwFBRGi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": -2, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "25254", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-14T20:22:04.704Z", "modifiedAt": null, "url": null, "title": "Functional Side Effects", "slug": "functional-side-effects", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:02.766Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WjXYsSqdwTY9sJ5mP/functional-side-effects", "pageUrlRelative": "/posts/WjXYsSqdwTY9sJ5mP/functional-side-effects", "linkUrl": "https://www.lesswrong.com/posts/WjXYsSqdwTY9sJ5mP/functional-side-effects", "postedAtFormatted": "Tuesday, January 14th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Functional%20Side%20Effects&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFunctional%20Side%20Effects%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWjXYsSqdwTY9sJ5mP%2Ffunctional-side-effects%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Functional%20Side%20Effects%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWjXYsSqdwTY9sJ5mP%2Ffunctional-side-effects", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWjXYsSqdwTY9sJ5mP%2Ffunctional-side-effects", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 993, "htmlBody": "<p><a href=\"http://bywayofcontradiction.com/?p=201\">Cross Posted</a> on <a href=\"http://bywayofcontradiction.com\">By Way of Contradiction</a></p>\n<p>You have probably heard the argument in favor of functional programming languages that functions act like functions in mathematics, and therefore have no side effects. When you call a function, you get an output, and with the exception of possibly the running time nothing matters except for the output that you get. This is in contrast with other programming languages where a function might change the value of some other global variable and have a lasting effect.</p>\n<p>Unfortunately the truth is not that simple. All functions can have side effects. Let me illustrate this with Newcomb&rsquo;s problem. In front of you are two boxes. The first box contains 1000 dollars, while the second box contains either 1,000,000 or nothing. You may choose to take either both boxes or just the second box. An Artificial Intelligence, Omega, can predict your actions with high accuracy, and has put 1,000,000 in the second box if and only if he predicts that you will take only the second box.</p>\n<p>You, being a good reflexive decision agent, take only the second box, and it contains 1,000,000.</p>\n<p>Omega can be viewed as a single function in a functional programming language, which takes in all sorts of information about you and the universe, and outputs a single number, 1,000,000 or 0. This function has a side effect. The side effect is that you take only the second box. If Omega did not simulate you and just output 1,000,000, and you knew this, then you would take two boxes.</p>\n<p>Perhaps you are thinking &ldquo;No, I took one box because I BELIEVED I was being simulated. This was not a side effect of the function, but instead a side effect of my beliefs about the function. That doesn&rsquo;t count.&rdquo;</p>\n<p>Or, perhaps you are thinking &ldquo;No, I took one box because of the function from my actions to states of the box. The side effect is no way dependent on the interior workings of Omega, but only on the output of Omega&rsquo;s function in counterfactual universes. Omega&rsquo;s code does not matter. All that matters is the mathematical function from the input to the output.&rdquo;</p>\n<p>These are reasonable rebuttals, but they do not carry over to other situations.</p>\n<p>Imagine two programs, Omega 1 and Omega 2. They both simulate you for an hour, then output 0. The only difference is that Omega 1 tortures the simulation of you for an hour, while Omega 2 tries its best to simulate the values of the simulation of you. Which of these functions would your rather be run.</p>\n<p>The fact that you have a preference between these (assuming you do have a preference) shows that function has a side effect that is not just a consequence of the function application in counterfactual universes.</p>\n<p>Further, notice that even if you never know which function is run, you still have a preference. It is possible to have preference over things that you do not know about. Therefore, this side effect is not just a function of your beliefs about Omega.</p>\n<p>Sometimes the input-output model of computation is an over simplification.</p>\n<p>Let&rsquo;s look at an application of thinking about side effects to Wei Dai&rsquo;s <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">Updateless Decision Theory</a>. I will not try to explain UDT if you don&rsquo;t already know about it, so this post should not be viewed alone.</p>\n<p>UDT 1.0 is an attempt at a reflexive decision theory. It views a decision agent as a machine with code S, given input X, and having to choose an output Y. It advises the agent to consider different possible outputs, Y, and consider all consequences of the fact that the code S when run on X outputs Y. It then outputs the Y which maximizes his perceived utility of all the perceived consequences.</p>\n<p>Wei Dai noticed an error with UDT 1.0 with the following <a href=\"/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\">thought experiment</a>:</p>\n<p>&ldquo;Suppose Omega appears and tells you that you have just been copied, and each copy has been assigned a different number, either 1 or 2. Your number happens to be 1. You can choose between option A or option B. If the two copies choose different options without talking to each other, then each gets $10, otherwise they get $0.&rdquo;</p>\n<p>The problem is that all the reasons that S(1)=A are the exact same reasons why S(2)=A, so the two copies will probably the same result. Wei Dai proposes a fix, UDT 1.1 which is that instead of choosing an output S(1), you instead choose a function S, from 1,2 to A,B from the 4 available functions which maximizes utility. I think this was not the correct correction, which I will probably talk about in the future. I prefer UDT 1.0 to UDT 1.1.</p>\n<p>Instead, I would like to offer an alternative way of looking at this thought experiment. The error is in the fact that S only looked at the outputs, and ignored possible side effects. I am aware that when S looked at the outputs, he was also considering his output in simulations of himself, but those are not side effects of the function. Those are direct results of the output of the function.</p>\n<p>We should look at this problem and think, &rdquo;I want to output A or B, but in such a way that has the side effect that the other copy of me outputs B or A respectively.&rdquo; S could search through functions considering their output on input 1 and the side effects of that function. S might decide to run the UDT 1.1 algorithm, which would have the desired result.</p>\n<p>The difference between this and UDT 1.1 is that in UDT 1.1 S(1) is acting as though it had complete control over the output of S(2). In this thought experiment that seems like a fair assumption, but I do not think it is a fair assumption in general, so I am trying to construct a decision theory which does not have to make this assumption. This is because if the problem was different, then S(1) and S(2) might have had different utility functions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WjXYsSqdwTY9sJ5mP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 4, "extendedScore": null, "score": 1.5137424771795801e-06, "legacy": true, "legacyId": "25255", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["g8xh9R7RaNitKtkaa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-15T01:16:57.160Z", "modifiedAt": null, "url": null, "title": "Understanding and justifying Solomonoff induction", "slug": "understanding-and-justifying-solomonoff-induction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:05.292Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gedymin", "createdAt": "2011-02-19T16:22:18.343Z", "isAdmin": false, "displayName": "gedymin"}, "userId": "xPg56rQ4KieN3rPtQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BWNh2GctNzA26TqQx/understanding-and-justifying-solomonoff-induction", "pageUrlRelative": "/posts/BWNh2GctNzA26TqQx/understanding-and-justifying-solomonoff-induction", "linkUrl": "https://www.lesswrong.com/posts/BWNh2GctNzA26TqQx/understanding-and-justifying-solomonoff-induction", "postedAtFormatted": "Wednesday, January 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Understanding%20and%20justifying%20Solomonoff%20induction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnderstanding%20and%20justifying%20Solomonoff%20induction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBWNh2GctNzA26TqQx%2Funderstanding-and-justifying-solomonoff-induction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Understanding%20and%20justifying%20Solomonoff%20induction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBWNh2GctNzA26TqQx%2Funderstanding-and-justifying-solomonoff-induction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBWNh2GctNzA26TqQx%2Funderstanding-and-justifying-solomonoff-induction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2266, "htmlBody": "<p>I've been trying to understand the uses and limitations of Solomonoff induction. Following the principle that in order to fully understand something you should <a href=\"/lw/j10/on_learning_difficult_things/\">explain it others</a>, here's a try. I prefer to write such things in a form for dialogue, as that better reflects thought processes.<br /><br />This is not a very-in-depth technical article - for example, <a href=\"/user/cousin_it/\">cousin_it</a> and <a href=\"/lw/cw1/open_problems_related_to_solomonoff_induction/\">Wen Dai has obviously spent more time</a> pondering about SI. (I'm not a long-time LW reader, but have skimmed through existing <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/%20.\">LW</a> and wiki articles <a href=\"/lw/4gk/solomonoff_induction_by_shane_legg/\">on</a> <a href=\"/lw/qk/that_alien_message/\">related</a> <a href=\"/lw/4iy/does_solomonoff_always_win/\">topics</a> before posting this.)</p>\n<p><br /><br /><strong>Alice.</strong> Hi, I'm interested in the question of why and when should I prefer simpler hypotheses. I've heard about Occam's razor and I've read about <a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\">Solomonoff induction and the Universal Prior</a>. Now I'm looking for a philosophical justification of the math. I'd like to have something akin to de Finetti's justification for probability theory - \"if you don't believe in the axioms, you're going to be Dutch booked!\".<br /><strong><br />Bob.</strong> You're welcome. Do you have any problems with the formulations?<br /><br /><strong>A.</strong> I'm trying to understand how to connect the informal concept of Occam's razor with the mathematical formula of the Universal Prior in a meaningful way. Informally, a hypothesis is something that explains the data. Occam's razor tells us to prefer simpler hypotheses.<br /><br /><strong>B.</strong> Well, yes.<br /><br /><strong>A.</strong> But the Universal Prior seems to tell something that seems to be even <em>stronger</em>: that all shorter hypothesis are more likely! Clearly, that's not the case: if F and G are hypotheses, then \"F OR G\" is a hypothesis as well, and not less likely than either F or G individually! What's more, there exists a whole set of language and domain pairs where longer hypotheses have the same average probability. For one particular example consider propositional logic with AND, OR, and NOT operators. Because of symmetry between conjunction and disjunction, a well-formed 10-element formula is as likely to be a tautology (i.e. always correct) as a 100-element formula!</p>\n<p><strong>B.</strong> You're confused. In this formalism, you should interpret all hypotheses as programs for Universal Turing Machine. The machine reads a hypothesis from one of its tapes. It then executes the hypothesis, and outputs the result of the computation to another tape. The result is the data - the actual string, whose prefix we're observing, and whose remaining part we're trying to predict. Hypothesis is the input string - the string we're not observing. In order to predict future output, our best option is to guess which input program the machine is using. Some programs can be ruled out because they don't correspond to the observed output; but an infinite number of possible programs always remains. The Universal Prior says that shorter programs are more likely. So, a hypothesis is just a bunch of Turing Machine instructions. It makes sense to speak about the \"joint hypothesis of F and G\" - it's a program that does both what F and G do. On the other hand, it makes no sense to speak about \"F OR G\" in the way you were doing it before. <br /><br /><strong>A.</strong> I see, right! Actually I'm not a mathematician or AI designer, I just want to reason about the physical universe. Let's fix it as the domain of our conversation. It seems to me that hypotheses in some sense correspond to the <a href=\"http://i48.tinypic.com/hrfxao.gif\">laws of physics</a>. The output corresponds to the physical universe itself; everything that we can observe by using our senses. But what does the Turing Machine correspond to?<br /><br /><strong>B.</strong> You're looking at it from the wrong angle. The justification of Occam's razor is an epistemic, not an ontological issue. It's not essential whether there is or isn't a specific Turing Machine \"out there\" computing our universe; it's essential that our universe is behaving <strong><em>as if</em></strong> it was computable! Let's quote <a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\">Scholarpedia</a>: <em>\"It is clear that, in a world with computable processes, patterns which result from simple processes are relatively likely, while patterns that can only be produced by very complex processes are relatively unlikely.\"</em><br /><br /><strong>A.</strong> But when we're calculating the complexity of hypotheses, we're doing it in respect to a particular model of computing!<br /><strong><br />B.</strong> The Church-Turing thesis tells that the choice of a particular machine is not very important - any Universal Turing Machine can simulate any other model of computation.<br /><br /><strong>A.</strong> But who says it's a universal machine! It may be anything, really! For example, who says that the universe doesn't work as a quasi-Turing machine that outputs a huge number of \"1\" with 99% probability every time after it outputs a single \"0\"? The Universal Prior relies on the relation between inputs and outputs of the machine; if this relation is changed, the probabilities are going to be wrong.</p>\n<p>On the other hand, the physical model may be strictly more powerful than a Universal Turing machine - it may be a hypercomputer! If it is, the universe is uncomputable.</p>\n<p><strong>B.</strong> Well, Occam's razor says that the model should be the simplest possible... Ah, I see, appealing to the razor here is a kind of circular reasoning.<br /><br />At the moment I have another line of thought: consider what happens when we throw in the Simulation Hypothesis. The probability of being in a simulation is dependent on the computational power the simulated universes got. If the power is significantly smaller than in the parent universe, the recursive chain of simulations is shorter; if the power is closer to parent's power, the chain is longer. Therefore, the probability of being in a simulation is proportional to the length of the chain. This implies that if we're living in a simulation, then our universe is almost certainly not significantly less powerful than our \"parent\" universe. Either both our and the alien universe are computable, or they are both hypercomputable (in identical meanings of this word). Since it seems that we cannot create hypercomputers in our universe, its reasonable that the aliens cannot do that either. So it's evidence that our universe is computable.<br /><br /><strong>A.</strong> Still there is a small probability that uncomputable oracles are present in the physical world, and we have just failed to recognize them. Perhaps we could learn about them in the future and harness their power somehow.<br /><br /><strong>B.</strong> Sure - but the point is that we have yet to see any evidence for them. And there's evidence against them - the fact that our computer simulations match reality very well - as long as we have the computing power required for them! We've been looking for strange messages in the sky, <a href=\"http://www.seti.org\">we haven't found them</a>. We've been looking for messages in our cell, <a href=\"http://news.discovery.com/space/alien-life-exoplanets/could-an-alien-message-be-embedded-in-our-genetic-code-130401.htm\">we haven't found them</a>.<br /><br /><strong>A.</strong> In the worst case we can still justify Occam's razor for the physical universe by induction on empirical experience, right?<br /><br /><strong>B. </strong>Hmm... I thought for a while and now I've got a better justification! See, even if the universe itself is uncomputable, there's still a MYRIAD of processes in it that ARE COMPUTABLE. We know that gravity and electromagnetism do not behave in random ways; they are at least approximately computable. Molecular dynamics is approximately computable. <a href=\"http://www.sciencedirect.com/science/article/pii/S0092867412007763\">The cell is approximately computable</a>. The nervous system is approximately computable. Evolution should be approximately computable; we can compute some of its basic elements astonishingly well. When Mendel was determining how heredity works by doing his pea-hybridization experiments, he was investigating a discrete, beautifully computable process.</p>\n<p>Nature fights against randomness. Pure uncontrolled randomness does not allow complex systems to emerge. Even if the space-time of physics itself is completely continuous and uncomputable (<a href=\"http://en.wikipedia.org/wiki/Digital_physics\">which far from given</a>), Nature at higher levels favors computable and discrete processes. In a sense, Nature has an infinite number of monkeys - but these monkeys are not sitting a a typewriter and writing the whole damn thing. Instead, they are sitting at a computer and writing <em>input programs</em>. As Seth Lloyd says (<a href=\"http://diondetterer.com/type/video/\">source</a>):</p>\n<blockquote>\n<p><em>Quantum mechanics supplies the universe with &ldquo;monkeys&rdquo; in the form of random quantum fluctuations, such as those that seeded the locations of galaxies. The computer into which they type is the universe itself. From a simple initial state, obeying simple physical laws, the universe has systematically processed and amplified the bits of information embodied in those quantum fluctuations. The result of this information processing is the diverse, information-packed universe we see around us: programmed by quanta, physics gave rise first to chemistry and then to life; programmed by mutation and recombination, life gave rise to Shakespeare; programmed by experience and imagination, Shakespeare gave rise to Hamlet. You might say that the difference between a monkey at a typewriter and a monkey at a computer is all the difference in the world.</em></p>\n</blockquote>\n<p><strong>A.</strong> Fine. This really has convinced me that Occam's razor is in some sense justified. I agree that the majority of the processes in the universe are computable or computationally approximable. I agree that for these processes, the Universal Prior follows. On the other hand, Occam's razor isn't justified for the ultimate laws of physics, because our universe might be uncomputable.</p>\n<p><strong>B.</strong> Well, there might be no such thing as \"laws\" in the ontological sense. Look, it's probable that there were an infinite number of inflationary bubbles expanding in the primordial Multiverse. Random regularities generated in a particular their subset lead to the <em>appearance</em> of laws. In this case the Universal Prior is <em>exactly</em> what is needed to sort out which sets of physical laws are probable, and which are improbable. I think that the existence of this simple physical model is evidence for a simple computational model, therefore evidence against hypercomputation and the kind of probabilistic, quasi-Turing machines you mentioned before.<br /><br /><strong>A.</strong> May be. However, I still doubt that Occam's razor is always the best option to <em>use in practice</em>, even if it is <em>theoretically justified</em>.<br /><br />Solomonoff Induction guarantees good results when are able to observe the prefix of the output data <em>completely and precisely</em>. Instead, real-world reasoning requires working with data data that has measurement errors, random noise, missing samples... and so on. Furthermore, we often need not so much a single precise hypothesis, but rather something broader - a type or a class of hypotheses - what machine learning people call \"a model\". Indeed, it turns out that machine learning researchers are working precisely on these problems. They are not terribly excited about Occam's razor. Let me quote from [Domingos 1999]:</p>\n<blockquote>\n<p><em>\"<strong>First [version of Occam's] razor</strong>: Given two models with the same generalization error, the simpler one should be<br />preferred because simplicity is desirable in itself.<br />[..]<br /><strong>Second razor</strong>: Given two models with the same training-set error, the simpler one should be preferred because it is likely to have lower generalization error. <br /><br />We believe that it is important to distinguish clearly between these two versions of Occam's razor. The first one is largely uncontroversial, while the second one, taken literally, is false.<br /><br />Several theoretical arguments and pieces of empirical evidence have been advanced to support it, but each of these is reviewed below and found wanting.\"</em></p>\n</blockquote>\n<p><strong>B.</strong> So what do you suggest?</p>\n<p><strong>A.</strong> It seems to me that Occam's razor surely is a good option when we need to explain data from controlled scientific experiments. It's the best option if we <em>know</em> how to computationally simulate the process <em>and</em> are able to isolate the process from the environment. If we don't know the mechanics of this process, we cannot even approximately calculate the computational complexity of the data. If we cannot isolate the system from the environment (and the system doesn't have good enough built-in error-correction facilities), then the results of the simulations won't repeat real world data. (For example, we can simulate the heredity of the peas. We can get probability distribution of recessive and dominant traits across pea hybrids <em>in silico</em>, and then get nearly the same results <em>in vivo</em>. On the other hand, there's small hope that we'll ever be able to simulate the unfolding of the&nbsp; long-scale evolution and replicate its past results in our computers. Even though we <em>do</em> understand the mathematical basis of evolution, simulating the <em>environmental impact</em> is beyond our reach.)<br /><br /><strong>B.</strong> We also should not forget that there are other motivations behind the razor. When evaluating a theory, its universal a priori probability is not the only thing that counts: theories with lower cognitive complexity are preferable because they are easier to reason about; theories that lead to algorithms with lower time complexity are preferable because they save our processing power. The universal prior is uncomputable after all; it can only be approximated. So it's OK to trade off marginal gains in probability - the apparent gain may be an error anyway. It's perfectly rational to examine and trade-off Kolmogorov complexity for different kinds of complexities.<br /><br /><strong>A.</strong> So the bottom line is that situations in which Occam's razor (in the informal sense) is justified is neither a <em>superset</em> nor a <em>subset</em> of the situations in which Solomonoff induction is justified. Besides, the question of the nature of the universe remains open - as always...<br /><br /><br />References:</p>\n<hr />\n<p>[Domingos 1999] The role of Occam's razor in knowledge discovery. P Domingos - 1999 - Springer. [<a href=\"http://homes.cs.washington.edu/~pedrod/papers/dmkd99.pdf\">PDF</a>]<br /><br />[Scholarperdia 2007] Marcus Hutter et al. (2007), Scholarpedia, 2(8):2572. [<a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\">link</a>]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BWNh2GctNzA26TqQx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 1.5140650885290164e-06, "legacy": true, "legacyId": "25258", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 75, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w5F4w8tNZc6LcBKRP", "fC248GwrWLT4Dkjf6", "PM5MQzXCrvsoAewWZ", "5wMcKNAwB6X4mp9og", "oHwt2JmDBefiN8rvg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-15T05:23:32.825Z", "modifiedAt": null, "url": null, "title": "Thought Crimes", "slug": "thought-crimes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:03.457Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jhne8bdrencGi6SMn/thought-crimes", "pageUrlRelative": "/posts/jhne8bdrencGi6SMn/thought-crimes", "linkUrl": "https://www.lesswrong.com/posts/jhne8bdrencGi6SMn/thought-crimes", "postedAtFormatted": "Wednesday, January 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thought%20Crimes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThought%20Crimes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjhne8bdrencGi6SMn%2Fthought-crimes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thought%20Crimes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjhne8bdrencGi6SMn%2Fthought-crimes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjhne8bdrencGi6SMn%2Fthought-crimes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 588, "htmlBody": "<p><a href=\"http://bywayofcontradiction.com/?p=244\">Cross-posted</a> on <a href=\"http://bywayofcontradiction.com/\">By Way of Contradiction</a></p>\n<p>In my morals, at least up until recently, one of the most obvious universal rights was freedom of thought. Agents should be allowed to think whatever they want, and should not be discouraged for doing so. This feels like a terminal value to me, but it is also instrumentally useful. Freedom of thought encourages agents to be rational and search for the truth. If you are punished for believing something true, you might not want to search for truth. This could slow science and hurt everyone. On the other hand, religions often discourage freedom of thought, and this is a major reason for my moral problems with religions. It is not just that religions are wrong, everyone is wrong about lots of stuff. It is that many religious beliefs restrict freedom of thought by punishing doubters with ostracizing or eternal suffering. I recognize that there are some \"<a href=\"http://www.uua.org/homepage/index.shtml?utm_expid=1399327-0.5ox9GWY4QD6dGWBUsCvAog.1\">religions</a>\" which do not exhibit this flaw (as much).</p>\n<p>Recently, my tune has changed. There are two things which have caused me to question the universality of the virtue of freedom of thought:</p>\n<p>1) Some truths can hurt society</p>\n<p>Topics like unfriendly artificial intelligence make me question the assumption that I always want intellectual progress in all areas. If we as modern society were to choose any topic which restricting thought about might be very useful, UFAI seems like a good choice. Maybe the freedom of thought in this issue might be a necessary casualty to avoid a much worse conclusion.</p>\n<p>2) Simulations</p>\n<p>This is the main point I want to talk about. If we get to the point where minds can simulate other minds, then we run into major issues. Should one mind be allowed to simulate another mind and torture it? It seems like the answer should be no, but this rule seems very hard to enforce without sacrificing not only free thought, but what would seem like the most basic right to privacy. Even today, people can have preferences over the thoughts of other people, but our intuition tells us that the one who is doing the thinking should get the final say. If the mind is simulating another mind, shouldn't the simulated mind also have rights? What makes advanced minds simulating torture so much worse than a human today thinking about torture. (Or even worse, thinking about 3^^^^3 people with dust specks in their eyes. (That was a joke, I know we cant actually think about 3^^^^3 people.))</p>\n<p>The first thing seems like a possible practical concern, but it does not bother me nearly as much as the second one. The first seems like it is just and example of the basic right of freedom of thought contradicting another basic right of safety. However the second thing confuses me. It makes me wonder whether or not I should treat freedom of thought as a virtue as much as I currently do. I am also genuinely not sure whether or not I believe that advanced minds should not be free to do whatever they want to simulations in their own minds. I think they should not, but I am not sure about this, and I do not know if this restriction should be extended to humans.</p>\n<p>What do you think? What is your view on the morality of drawing the line between the rights of a simulator and the rights of a simulatee? Do simulations within human minds have any rights at all? What conditions (if any) would make you think rights should be given to simulations within human minds?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jhne8bdrencGi6SMn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 13, "extendedScore": null, "score": 1.5143349720249043e-06, "legacy": true, "legacyId": "25262", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-15T06:27:28.901Z", "modifiedAt": null, "url": null, "title": "To capture anti-death intuitions, include memory in utilitarianism", "slug": "to-capture-anti-death-intuitions-include-memory-in", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:05.946Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6ushcKgibJH6dQB6q/to-capture-anti-death-intuitions-include-memory-in", "pageUrlRelative": "/posts/6ushcKgibJH6dQB6q/to-capture-anti-death-intuitions-include-memory-in", "linkUrl": "https://www.lesswrong.com/posts/6ushcKgibJH6dQB6q/to-capture-anti-death-intuitions-include-memory-in", "postedAtFormatted": "Wednesday, January 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20To%20capture%20anti-death%20intuitions%2C%20include%20memory%20in%20utilitarianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATo%20capture%20anti-death%20intuitions%2C%20include%20memory%20in%20utilitarianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ushcKgibJH6dQB6q%2Fto-capture-anti-death-intuitions-include-memory-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=To%20capture%20anti-death%20intuitions%2C%20include%20memory%20in%20utilitarianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ushcKgibJH6dQB6q%2Fto-capture-anti-death-intuitions-include-memory-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ushcKgibJH6dQB6q%2Fto-capture-anti-death-intuitions-include-memory-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 985, "htmlBody": "<p><strong>EDIT:</strong> Mestroyer was the first one <a href=\"/r/discussion/lw/jhr/to_capture_antideath_intuitions_include_memory_in/adif\">to find a bug that breaks this idea</a>. Only took a couple of hours, that's ethics for you. :)</p>\n<p>In the last Stupid Questions Thread, <a href=\"/r/discussion/lw/jh8/stupid_questions_thread_january_2014/acql\">solipsist asked</a></p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; background-color: #f7f7f8;\">Making a person and unmaking a person seem like utilitarian inverses, yet I don't think contraception is tantamount to murder. Why isn't making a person as good as killing a person is bad?</span></p>\n</blockquote>\n<p>People raised valid points, such as ones about murder having generally bad effects on society, but most people probably have the intuition that murdering someone is bad even if the victim was a hermit whose death was never found out by anyone. It just occurred to me that the way to formalize this intuition would also solve more general problems with the way that the utility functions in utilitarianism (which I'll shorten to UFU from now on) behave.</p>\n<p>Consider these commonly held intuitions:</p>\n<ol>\n<li>If a person is painlessly murdered and a new (equally happy) person is instantly created in their place, this is worse than if there was a single person who lived for the whole time.</li>\n<li>If a living person X is painlessly murdered at time T, then this is worse than if the X's parents had simply chosen not to have a child at time T-20, even though both acts would have resulted in X not existing at time T+1.</li>\n</ol>\n<div>Also, the next intuition isn't necessarily <em>commonly</em>&nbsp;held, since it's probably deemed mostly science fictiony, but in transhumanist circles one also sees:</div>\n<div>\n<ul>\n<li>If someone is physically dead, but not information-theoretically dead and a close enough replica of them can be constructed and brought back, then bringing them back is better than creating an entirely new person.</li>\n</ul>\n</div>\n<div>Assume that we think the instrumental arguments in favor of these intuitions (like societies with fewer murders being better off for everyone) are insufficient - we think that the intuitions should hold even if disregarding them had no effect on anything else. Now, many forms of utilitarianism will violate these intuitions, saying that in all cases both of the offered scenarios are equally good or equally bad.</div>\n<div><br /></div>\n<div>The problem is that UFUs ignore the history of the world, looking only at individual states. By analogy to stochastic processes, we could say that UFUs exhibit the <a href=\"http://en.wikipedia.org/wiki/Markov_property\">Markov property</a>: that is to say, the value of a state depends only on that state, not on the sequence of events that preceded it. When deciding whether a possible world at time t+1 is better or worse than the actual world at t, UFUs do not look at any of the earlier times. Actually, UFUs do not really even care about the world at time t: all they do is compare the possible worlds at t+1, and choose the one with the highest happiness (or lowest suffering, or highest preference satisfaction, or&hellip;) as compared to the alternatives. As a result, they do not care about people getting murdered or resurrected, aside for the impact that this has on the general level of happiness (or whatever).</div>\n<div><br /></div>\n<div>We can fix this by incorporating a history to the utility function. Suppose that a person X is born at time T: we enter the fact of \"X was born\" into the utility function's memory. From now, for every future state the UF checks whether or not X is still alive. If yes, good, if not, that state loses one point of utility. Now the UF has a very large \"incentive\" to keep X from getting killed: if X dies, then every future state from that moment on will be a point worse than it would otherwise have been. If we assume the lifetime of the universe to be 10<sup>100</sup> years, say, then with no discounting, X dying means a loss of 10<sup>100</sup> points of utility. If we pick an appropriate value for the \"not alive anymore\" penalty, then it won't be so large as to outweigh all other considerations, but enough that situations with unnecessary death will be evaluated as clearly worse than ones where that death could have been prevented.</div>\n<div><br /></div>\n<div>Similarly, if it becomes possible to resurrect someone from physical death, then that is better than creating an entirely new life, because it will allow us to get rid of the penalty of them being dead.</div>\n<div><br /></div>\n<div>This approach could also be construed to develop yet another attack on the Repugnant Conclusion, though the assumption we need to make for that might be more controversial. Suppose that X has 50 points of well-being at time T, whereas at T+1, X only has 25 points of well-being, but we have created another person Y who also has 25 points of well-being. UFUs would consider this scenario to be equally good as the one with no person Y and where X kept their 50 points. We can block this by maintaining a memory of the amount of peak well-being that anyone has ever had, and if they fall below their past peak well-being, apply the difference as a penalty. So if X used to have 50 points of well-being but now only has 25, then we apply an extra -25 to the utility of that scenario.</div>\n<div><br /></div>\n<div>This captures the popular intuition that, while a larger population can be better, a larger population that comes at the cost of reducing the well-being of people who are currently well off is worse, even if the overall utility was somewhat greater. It's also noteworthy that if X is dead, then their well-being is 0, which is presumably worse than their peak well-being, so there's an eternal penalty applied to the value of future states where X is dead. Thus this approach, of penalizing states by the difference between the current and peak well-being of the people in those states, can be thought of as a generalization of the \"penalize any state in which the people who once lived are dead\" approach.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZTRNmvQGgoYiymYnq": 2, "HAFdXkW4YW4KRe2Gx": 2, "z95PGFXtPpwakqkTA": 1, "E9ihK6bA9YKkmJs2f": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6ushcKgibJH6dQB6q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 1.514404958672406e-06, "legacy": true, "legacyId": "25263", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-15T18:46:29.053Z", "modifiedAt": null, "url": null, "title": "The non-Independence of superficially-Irrelevant Alternatives", "slug": "the-non-independence-of-superficially-irrelevant", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:03.872Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "roystgnr", "createdAt": "2010-10-25T23:10:44.178Z", "isAdmin": false, "displayName": "roystgnr"}, "userId": "hmAApjmpL8eXtfzig", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ReYtXfTHWf6T6FF79/the-non-independence-of-superficially-irrelevant", "pageUrlRelative": "/posts/ReYtXfTHWf6T6FF79/the-non-independence-of-superficially-irrelevant", "linkUrl": "https://www.lesswrong.com/posts/ReYtXfTHWf6T6FF79/the-non-independence-of-superficially-irrelevant", "postedAtFormatted": "Wednesday, January 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20non-Independence%20of%20superficially-Irrelevant%20Alternatives&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20non-Independence%20of%20superficially-Irrelevant%20Alternatives%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FReYtXfTHWf6T6FF79%2Fthe-non-independence-of-superficially-irrelevant%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20non-Independence%20of%20superficially-Irrelevant%20Alternatives%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FReYtXfTHWf6T6FF79%2Fthe-non-independence-of-superficially-irrelevant", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FReYtXfTHWf6T6FF79%2Fthe-non-independence-of-superficially-irrelevant", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p>A <a href=\"http://rsbl.royalsocietypublishing.org/content/10/1/20130935.full\">new article in Biology Letters</a> shows that under some conditions in which animals appear to behave \"irrationally\" (by apparently failing to conform to the <a href=\"https://en.wikipedia.org/wiki/Independence_of_irrelevant_alternatives\">Independence of Irrelevant Alternatives</a> or even the <a href=\"https://en.wikipedia.org/wiki/Expected_utility_hypothesis#The_von_Neumann-Morgenstern_axioms\">Transitivity</a> axioms of decision theory), the animal behavior may in fact resemble utility-maximizing strategies which also appear to violate those axioms.&nbsp; The optimal strategies' current preferences are altered in response to the information conveyed by the current presence or absence of various alternatives.</p>\n<p>A press release about the article is available from <a href=\"http://www.bris.ac.uk/news/2014/10064.html\">the lead author's university</a>, U. Bristol. A news piece summarizing it is at <a href=\"http://www.nature.com/news/why-irrational-choices-can-be-rational-1.14517\">Nature's website</a>.</p>\n<p>These results probably shouldn't surprise the most careful rational thinkers. For instance, \"I prefer A to B when I believe more B is likely to be available later, and B to C when I believe more B is likely to be available later\" clearly does not necessarily entail \"I prefer A to B under all circumstances, and B to C under all circumstances\". But there is clearly a pitfall of tempting model oversimplification here. Although this new paper and most discussion of it is carefully putting scare quotes around \"irrational\", the papers being cited purport to show <a href=\"http://dx.doi.org/10.1007/s002650100346\">\"intransitive preferences\"</a>, <a href=\"http://dx.doi.org/10.1007/s00265-001-0420-8\">\"violations of rational choice\"</a>, <a href=\"http://dx.doi.org/10.1098/rspb.2010.1045\">\"irrational decision-making\"</a>, and <a href=\"http://dx.doi.org/10.1006/anbe.2001.1925\">\"irrational choices\"</a>, without obvious irony.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ReYtXfTHWf6T6FF79", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 1.5152143311098482e-06, "legacy": true, "legacyId": "25267", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-15T20:21:36.011Z", "modifiedAt": null, "url": null, "title": "Recreational Cryonics", "slug": "recreational-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:00.773Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "topynate", "createdAt": "2009-03-05T02:37:28.578Z", "isAdmin": false, "displayName": "topynate"}, "userId": "M7Jasma8TWpbJmAcs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JQPr6qyuqbs8MDCLH/recreational-cryonics", "pageUrlRelative": "/posts/JQPr6qyuqbs8MDCLH/recreational-cryonics", "linkUrl": "https://www.lesswrong.com/posts/JQPr6qyuqbs8MDCLH/recreational-cryonics", "postedAtFormatted": "Wednesday, January 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recreational%20Cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecreational%20Cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJQPr6qyuqbs8MDCLH%2Frecreational-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recreational%20Cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJQPr6qyuqbs8MDCLH%2Frecreational-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJQPr6qyuqbs8MDCLH%2Frecreational-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1070, "htmlBody": "<p>We recently saw a post in Discussion by ChrisHallquist, <a href=\"/lw/jgu/i_will_pay_500_to_anyone_who_can_convince_me_to/\">asking to be talked out of cryonics</a>. It so happened that I'd just read <a href=\"http://subterraneanpress.com/magazine/winter_2014/bit_players_by_greg_egan\">a new short story by Greg Egan</a>&nbsp;which gave me the inspiration to write the following:</p>\n<p>&nbsp;</p>\n<blockquote>It is likely that you would not wish for your brain-state to be available to all-and-sundry, subjecting you to the possibility of being simulated according to their whims. However, you know nothing about the ethics of the society that will exist when the technology to extract and run your brain-state is developed. Thus you are taking a risk of a negative outcome that may be less attractive to you than mere non-existence.</blockquote>\n<p>&nbsp;</p>\n<p>I had little expectation of this actually convincing anyone, but thought it was a fairly novel contribution. When <a href=\"/lw/jgu/i_will_pay_500_to_anyone_who_can_convince_me_to/ad3a\">jowen's plea for a refutation</a>&nbsp;went unanswered, I began attempting one myself. What I ended up with closes the door on the scenario I outlined, but opens one I find rather more disturbing.<a id=\"more\"></a></p>\n<p>I think I'd better start by explaining why I wrote my comment the way that I did.</p>\n<p>Normally, when being simulated is raised as a negative possibility (referred to in SF as being 'deathcubed', and carrying the implication not so much of torture as of arbitrariness), it's in the context of an AI doing so. Now there's a pretty good argument against being deathcubed by an AI, as follows:</p>\n<p>Any AI that would do this is unFriendly. The vast majority of uFAIs have goals incompatible with human life but not in any way concerned with it. Humans are just matter that can be better used for something else; likewise simulations use computational resources better used for something else. Therefore there is little to fear in the way of being tortured by an AI.</p>\n<p>I sidestepped that entire argument (but I'll return to it in a minute) by referring to \"the ethics of the society that will exist\". In other words, without making it explicit, I created the image of a <em>community</em>&nbsp;of agents, probably human-like agents, each with&nbsp;<em>ownership</em>&nbsp;of resources that they could use according to a moral code and subject to some degree of enforcement. I assumed a naturalistic polity, rather than a hegemony.</p>\n<p>With the assumptions behind my scenario laid bare, it should now be apparent that it is no more stable than a world in which everyone owns nuclear weapons. If the resources to do these simulations are dispersed amongst that many people, someone will use them to brute-force an AI which will then hegemonize the universe.</p>\n<p>If you accept the above, then you need only worry about a hegemon that permits such a society to exist. Such a hegemon would probably be classified as uFAI, and so we go back to the already refuted situation of a perversely evil AI.</p>\n<hr />\n<p>Thus far I believe myself to have argued according to what passes for orthodoxy on LW. Note, though, that everything hinges on predictions about uFAI. These tend to be based on Steve Omohundro's <a href=\"http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/\">The Basic AI Drives</a>, which if taken seriously imply that an uFAI would convert the universe to utilons and not give a fig for human beings.</p>\n<p>One of the drives AIs are predicted to have is the desire to be rational. I claim that a key behaviour that is eminently rational in humans has been neglected in considering AIs, particularly uFAIs. Namely, play. We humans take pleasure in play, quite aside from whatever productive gains we get out of it. Nevertheless, intellectual play has contributed to innumerable advances in science, mathematics, philosophy, etc. Dour, incurious people demonstrably fail to satisfy their values in dimensions that ostensibly are completely unrelated to leisure. An AI, you may say, need not play in order to create utility; it can structure its thoughts more rationally than humans, without resorting to undirected intellectual activity. I grant that a hyperrational agent will not allocate as great a proportion of resources to such undirected activity - that follows from holding more accurate beliefs about what it is productive to do. But no agent can have perfect knowledge of this nature. Thus all sufficiently rational agents will devote some proportion of their resources, no matter how small, to play.</p>\n<p>What is the nature of play for a superintelligence? For Friendly AI, by definition it will not involve deathcubing. For an unFriendly AI, this is not the case. We then need to assess <em>how likely</em> it is that play for such an AI would involve such atrocities. Well then, first consider the sum of resources available to a hegemonic AI. Computationally speaking, doing sims of humans, indeed of human civilizations, would be a relative drop in the ocean. In a universe containing billions of stars in a single galaxy, how many tonnes of computronium would it take? Not many, I'd wager. Yet, no matter how easy it is for an AI to deathcube, perhaps it's simply so irrelevant, or uninteresting, an activity as not to occur even in undirected intellectual activity.</p>\n<p>Perhaps. Yet it's rather easy to devise side-projects for an uFAI that are very simple to describe, take minimal resources and include untold human suffering. It begins to strain belief that of all the compactly specified tasks that refer to the universe in which an AI finds itself, it will try none of those which fall into this category.</p>\n<p>An example seems in order. One would be just to do every thought experiment devised by mankind. Some of these would be computationally intractable even for a superintelligence, but those tend to be limited to computer science and number theory. Let's assume that 10 billion human beings record an average of a thousand unique and tractable ideas of note, each requiring about 10^30 operations - numbers deliberately on the high side of plausible. Then it would take 10^43 operations to do all of them. A maximally efficient computer weighing one kilogram and occupying one litre appears capable of <a href=\"http://arxiv.org/pdf/quant-ph/9908043.pdf\">~10^50 operations a second</a>. Thus, a superintelligence that can achieve efficencies of one ten-millionth in all these dimensions would take one second on a computer weighing 10,000 tonnes and occupying 10,000 m<sup>3</sup> (imagine a zeppelin filled with water rather than hydrogen gas) to finish the job. It is at least <em>plausible</em> for an uFAI to do such a thing even before beginning wholescale matter-conversion. In such a context, where humans actually exist, play involving humans looks very natural.</p>\n<p>It hardly needs mentioning that almost everything ever discussed on this site would be included in the example above.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JQPr6qyuqbs8MDCLH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 1, "extendedScore": null, "score": 1.5153185598257015e-06, "legacy": true, "legacyId": "25268", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HxGRCquTQPSJE2k9g"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-15T21:02:26.223Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley meetup: 5-minute exercises", "slug": "meetup-berkeley-meetup-5-minute-exercises", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jHrar7DeGjtR36PRk/meetup-berkeley-meetup-5-minute-exercises", "pageUrlRelative": "/posts/jHrar7DeGjtR36PRk/meetup-berkeley-meetup-5-minute-exercises", "linkUrl": "https://www.lesswrong.com/posts/jHrar7DeGjtR36PRk/meetup-berkeley-meetup-5-minute-exercises", "postedAtFormatted": "Wednesday, January 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20meetup%3A%205-minute%20exercises&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20meetup%3A%205-minute%20exercises%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjHrar7DeGjtR36PRk%2Fmeetup-berkeley-meetup-5-minute-exercises%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20meetup%3A%205-minute%20exercises%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjHrar7DeGjtR36PRk%2Fmeetup-berkeley-meetup-5-minute-exercises", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjHrar7DeGjtR36PRk%2Fmeetup-berkeley-meetup-5-minute-exercises", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vm'>Berkeley meetup: 5-minute exercises</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 January 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2030 Addison, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>At today's meetup, I want to try a kind of exercise I learned / shamelessly stole from User:apophenia . The idea is to try a rationality / cognition exercise that's as valuable as can be while only taking 5 minutes. See this example:</p>\n\n<p><a href=\"http://lesswrong.com/lw/irr/the_best_15_words/\" rel=\"nofollow\">http://lesswrong.com/lw/irr/the_best_15_words/</a></p>\n\n<p>Please arrive between 7pm and 7:30pm today. The exercise will begin at 7:30pm. It won't take very long, and we will hang out afterward. The CFAR office is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p>\n\n<p>Even though this takes place at CFAR, it's not a CFAR-sponsored event.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vm'>Berkeley meetup: 5-minute exercises</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jHrar7DeGjtR36PRk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.5153633130180076e-06, "legacy": true, "legacyId": "25269", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__5_minute_exercises\">Discussion article for the meetup : <a href=\"/meetups/vm\">Berkeley meetup: 5-minute exercises</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 January 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2030 Addison, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>At today's meetup, I want to try a kind of exercise I learned / shamelessly stole from User:apophenia . The idea is to try a rationality / cognition exercise that's as valuable as can be while only taking 5 minutes. See this example:</p>\n\n<p><a href=\"http://lesswrong.com/lw/irr/the_best_15_words/\" rel=\"nofollow\">http://lesswrong.com/lw/irr/the_best_15_words/</a></p>\n\n<p>Please arrive between 7pm and 7:30pm today. The exercise will begin at 7:30pm. It won't take very long, and we will hang out afterward. The CFAR office is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p>\n\n<p>Even though this takes place at CFAR, it's not a CFAR-sponsored event.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__5_minute_exercises1\">Discussion article for the meetup : <a href=\"/meetups/vm\">Berkeley meetup: 5-minute exercises</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley meetup: 5-minute exercises", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__5_minute_exercises", "level": 1}, {"title": "Discussion article for the meetup : Berkeley meetup: 5-minute exercises", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__5_minute_exercises1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3tFmWHD6Zqy7DKEMG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-15T22:29:57.368Z", "modifiedAt": null, "url": null, "title": "Results from MIRI's December workshop", "slug": "results-from-miri-s-december-workshop", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:25.861Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pGLtzd8sEQTYb8sne/results-from-miri-s-december-workshop", "pageUrlRelative": "/posts/pGLtzd8sEQTYb8sne/results-from-miri-s-december-workshop", "linkUrl": "https://www.lesswrong.com/posts/pGLtzd8sEQTYb8sne/results-from-miri-s-december-workshop", "postedAtFormatted": "Wednesday, January 15th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Results%20from%20MIRI's%20December%20workshop&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AResults%20from%20MIRI's%20December%20workshop%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpGLtzd8sEQTYb8sne%2Fresults-from-miri-s-december-workshop%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Results%20from%20MIRI's%20December%20workshop%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpGLtzd8sEQTYb8sne%2Fresults-from-miri-s-december-workshop", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpGLtzd8sEQTYb8sne%2Fresults-from-miri-s-december-workshop", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1762, "htmlBody": "<p>Last week (Dec. 14-20), MIRI ran its <a href=\"http://intelligence.org/workshops/#december-2013\">6th research workshop</a> on logic, probability, and reflection. Writing up mathematical results takes time, and in the past, it's taken quite a while for results from these workshops to become available even in draft form. Because of this, at the December workshop, we tried something new: taking time during and in the days immediately after the workshop to write up results in quick and somewhat dirty form, while they still feel fresh and exciting.</p>\n<p>In total, there are seven short writeups. Here's a list, with short descriptions of each. Before you get started on these writeups, you may want to read <a href=\"http://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/\">John Baez's blog post about the workshop</a>, which gives an introduction to the two main themes of the workshop.</p>\n<h3><a id=\"more\"></a>Theme 1: Scientific induction in mathematics</h3>\n<p>One of the main themes was using Bayesian probability to represent uncertainty about mathematical statements. Like human mathematicians, an AI will be able to outright prove or disprove many mathematical statements, but there will also be many that it will be uncertain about, and the obvious thing to try to get a handle on dealing with such uncertainty is to assign a probability to each such statement. This would mean choosing some sort of prior, and then updating on evidence: For example, if you're not sure whether the twin prime conjecture is true, then each time you discover a new twin prime larger than all that you have seen before, you should ever so slightly increase the probability you assign to the conjecture.</p>\n<p>But what sort of prior should we choose? That's the problem that people at the December workshop tried to make some progress on. Here is an interesting problem which the best previous proposal, due to Abram Demski, fails on: Suppose that Q(x) is a predicate symbol, and suppose that you find evidence that Q(x) is true of exactly 90% of all numbers between x=1 and x=10^100. Then we would expect that if you plug in some arbitrary number in this range, say n = floor(7^7^7^7 * pi) mod 10^100, then the posterior probability of Q(n), after conditioning on your evidence, would be 0.9. But it turns out that in Demski's proposal, the probability of Q(n) would be approximately 0.5.</p>\n<p>To learn more about this, see \"<strong><a href=\"https://intelligence.org/wp-content/uploads/2013/12/scientific-induction-in-probabilistic-mathematics.pdf\">Scientific Induction in Probabilistic Mathematics</a></strong>\", written up by Jeremy Hahn. Jeremy writes: \"At the workshop, after pinning down the above example of undesired behaiviour we turned to other proposals for priors. None of the ideas presented are in a polished enough form to be considered a complete proposal, but we are very happy with the progress that was made.\"</p>\n<h3>Theme 2: The \"procrastination paradox\"</h3>\n<p>The other main theme of the workshop was the <a href=\"/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">L&ouml;bian obstacle to self-modifying AI</a>, and more specifically, something we're calling the \"procrastination paradox\". One approach for constructing a self-modifying AI based on mathematical logic would be to require the AI to prove, whenever it wants to take an action -- be it an external action or making some modification to its own source code -- that this action is \"safe\". Then you would naively expect the AI to be able to reason as follows: \"The following simple self-modification X is safe, because the modified AI will still only take actions that it has proven to be safe, so it will only take safe actions.\" But by L&ouml;b's theorem, a consistent system T which is as strong as Peano Arithmetic cannot go from \"whatever the modified AI does will <em>have been proven</em> (in T) to be safe\" to \"whatever the modified AI does will <em>in fact</em> be safe\". (See <a href=\"http://intelligence.org/2013/08/04/benja-interview/\">here</a> for discussion of why it seems reasonable to tackle self-modifying AI from the perspective of mathematical logic, even though a realistic AI will certainly need to use probability to reason about the physical world.) In particular, we cannot have a consistent system T as strong as PA that proves \"If S is provable in T, then S\" for all sentences S.</p>\n<p>If we can't have a system T that trusts <em>itself</em> in this way, maybe we can have an infinite sequence of consistent systems T_n such that each T_n proves \"If S is provable in T_(n+1), then S\", for all sentences S? Marcello (Herreshoff) has shown that in fact there is such a sequence, explained in <a href=\"http://intelligence.org/files/TilingAgents.pdf\">his paper with Eliezer on the L&ouml;bian obstacle</a>. However, although it is consistent (meaning that it doesn't prove <em>everything</em>; in particular, it doesn't prove 0=1), this system is <em>unsound</em> (meaning that it proves <em>some</em> things which aren't actually true). We now have an intuition for why this is so, and a corresponding formal proof that any sequence like this must be unsound: Imagine a sequence of agents A_n, using the systems T_n, whose goal it is that a particular button gets pressed -- they don't care <em>when</em> it gets pressed, just that it <em>does</em> get pressed, eventually. Each of these agents will try to prove that one of its successors presses the button; if it can't find such a proof, then it will press the button itself. Now A_n reasons as follows (in T_n):</p>\n<p style=\"padding-left: 30px;\">Either my successor, A_(n+1), will press the button, or it won't. If it <em>doesn't</em> press the button, then it must have found a proof that the button gets pressed at some later time. This proof is in T_(n+1), and I know that if something is provable in T_(n+1), then it is true. Thus, either way, the button gets pressed -- by my successor, or at some later time.</p>\n<p>So A_n has found a proof that the button gets pressed, and therefore doesn't press the button itself. But since this is true for every n, the button never gets pressed -- and therefore, all of the systems T_n are unsound, since they prove a statement which isn't true. Metaphorically, every time the AI thinks about pressing the button, it convinces itself that there is some <em>future</em> time when it will stop procrastinating and actually press the button, so <em>today</em> it is surely fine to procrastinate -- and in this way, the AI procrastinates forever. For more, including formal proofs, see \"<a href=\"https://intelligence.org/wp-content/uploads/2013/12/procrastination-paradox.pdf\"><strong>The Procrastination Paradox</strong></a>\", written up by Eliezer.</p>\n<p>However, on the positive side, it turns out that if you modify Marcello's construction so that T_n merely proves that T_(n+1) is <em>consistent</em>, rather than proving that everything T_(n+1) proves is true, then Marcello's proof not only shows that the T_n are all consistent, but also that they are sound. (This is really easy to see: T_n is PA plus the axiom \"psi(n) -&gt; Con(T_(n+1))\", for some formula psi(n). Marcello's proof shows that all of these systems T_n are consistent. But then, since T_(n+1) is consistent, \"psi(n) -&gt; Con(T_(n+1))\" is true, so all axioms of T_n are true and T_n is sound.) Moreover, although there are things that T_n proves that T_(n+1) doesn't -- namely, \"Con(T_(n+1))\", since if T_(n+1) proved that, it would be inconsistent -- you can construct these T_n so that they all have the same <a href=\"http://en.wikipedia.org/wiki/Proof-theoretic_ordinal\">proof-theoretic ordinal</a>, so to the extent that you think that proof-theoretic ordinals are a good measure of the \"mathematical strength\" of a theory, all of the theories T_n can be said to have the same amount of mathematical strength. Finally, although T_n doesn't prove the L&ouml;b schema \"If T_(n+1) proves S, then S is true\" for <em>all</em> sentences S, it does in fact prove it for a particular class of sentences (the Pi_1 sentences). For details, see \"<strong><a href=\"https://intelligence.org/wp-content/uploads/2013/12/consistency-waterfall.pdf\">An in\ffinitely descending sequence of sound theories each proving the next consistent</a></strong>\", written up by Benja Fallenstein, a.k.a. myself.</p>\n<p>Is it enough to have the L&ouml;b schema for Pi_1 sentences? (These are sentences of the form \"forall x. phi(x)\", where phi(x) is such that you can write a computer program that takes x as input and outputs whether phi(x) is true or false.) Actually, there's an argument to be made that it might be <em>exactly right</em> for tiling agents: It seems to be as much as you can get without running into the procrastination paradox, and it's enough to formalize goals like \"The world is not destroyed before time n\" (if the world is computable). Moreover, there's a way to combine this with ideas from a different solution to the tiling agents problem, parametric polymorphism (one version of which is described in the tiling agents paper), in order to also handle goals that don't as obviously fit into the Pi_1 mold; we call this hybrid of ideas \"<a href=\"https://intelligence.org/wp-content/uploads/2013/12/fallensteins-monster.pdf\"><strong>Fallenstein's monster</strong></a>\", written up by Nate Soares a.k.a. <a href=\"/user/So8res/overview/\">So8res</a>, because it looks rather grafted together. (In fact, it was even uglier when we came up with it, because back then we didn't even have the sound version of Marcello's construction yet. I'm hopeful that we will find prettier solutions to the remaining monstrous aspects as well, so that I will soon be able to ask my trusted research assistant, Igor, to dispose of the monster.)</p>\n<p>Finally, we usually talk of a set of equations like \"T_n = PA + psi(n) -&gt; Con(T_(n+1))\" as if they define <em>one particular</em> sequence of theories T_n, but it's not obvious that different ways of making this formal lead to logically equivalent theories. In fact, I'd have guessed that in general they don't, but Will Sawin had the opposite intuition and was right; any recursively enumerable set of equations of this type has a unique solution (in the sense that if PA proves that two r.e. lists of theories U_i and V_i are both solutions, then PA proves that they are equivalent). For details, see \"<strong><a href=\"https://intelligence.org/wp-content/uploads/2013/12/recursively-defined-theories-are-well-defined.pdf\">Recursively-defined logical theories are well-defined</a></strong>\", written up by Nisan Stiennon. Interestingly, this turns out to be an equivalent formulation of L&ouml;b's theorem!</p>\n<h3>Odds and ends</h3>\n<p>In addition to the two main themes, there were some odds and ends: First, it turns out that the <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/1217\">5-and-10 problem</a>, which has been discussed on LW quite a bit, is <em>almost</em> a problem for the formalism in the tiling agents paper; the exact system discussed in the paper narrowly avoids the problem, but a simple and natural variant runs into it. For details, see \"<strong><a href=\"https://intelligence.org/wp-content/uploads/2013/12/tiling-agents-5-and-10.pdf\">The 5-and-10 problem and the tiling agents formalism</a></strong>\", written up by me.</p>\n<p>And both last and least, we considered and disposed of a conjecture that a particular version of parametric polymorphism might be able to avoid the problem of losing mathematical strength when an agent using it rewrites itself. This would have been an interesting result if it had been true, so it seems worth recording, but since it turned out to be false, it's not particularly exciting. See \"<strong><a href=\"https://intelligence.org/wp-content/uploads/2013/12/decreasing-strength-parametric-polymorphism.pdf\">Decreasing mathematical strength in one formalization of parametric polymorphism</a></strong>\", also written up by me.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pGLtzd8sEQTYb8sne", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 71, "extendedScore": null, "score": 0.00014202087785480038, "legacy": true, "legacyId": "25141", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 45, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Last week (Dec. 14-20), MIRI ran its <a href=\"http://intelligence.org/workshops/#december-2013\">6th research workshop</a> on logic, probability, and reflection. Writing up mathematical results takes time, and in the past, it's taken quite a while for results from these workshops to become available even in draft form. Because of this, at the December workshop, we tried something new: taking time during and in the days immediately after the workshop to write up results in quick and somewhat dirty form, while they still feel fresh and exciting.</p>\n<p>In total, there are seven short writeups. Here's a list, with short descriptions of each. Before you get started on these writeups, you may want to read <a href=\"http://johncarlosbaez.wordpress.com/2013/12/26/logic-probability-and-reflection/\">John Baez's blog post about the workshop</a>, which gives an introduction to the two main themes of the workshop.</p>\n<h3 id=\"Theme_1__Scientific_induction_in_mathematics\"><a id=\"more\"></a>Theme 1: Scientific induction in mathematics</h3>\n<p>One of the main themes was using Bayesian probability to represent uncertainty about mathematical statements. Like human mathematicians, an AI will be able to outright prove or disprove many mathematical statements, but there will also be many that it will be uncertain about, and the obvious thing to try to get a handle on dealing with such uncertainty is to assign a probability to each such statement. This would mean choosing some sort of prior, and then updating on evidence: For example, if you're not sure whether the twin prime conjecture is true, then each time you discover a new twin prime larger than all that you have seen before, you should ever so slightly increase the probability you assign to the conjecture.</p>\n<p>But what sort of prior should we choose? That's the problem that people at the December workshop tried to make some progress on. Here is an interesting problem which the best previous proposal, due to Abram Demski, fails on: Suppose that Q(x) is a predicate symbol, and suppose that you find evidence that Q(x) is true of exactly 90% of all numbers between x=1 and x=10^100. Then we would expect that if you plug in some arbitrary number in this range, say n = floor(7^7^7^7 * pi) mod 10^100, then the posterior probability of Q(n), after conditioning on your evidence, would be 0.9. But it turns out that in Demski's proposal, the probability of Q(n) would be approximately 0.5.</p>\n<p>To learn more about this, see \"<strong><a href=\"https://intelligence.org/wp-content/uploads/2013/12/scientific-induction-in-probabilistic-mathematics.pdf\">Scientific Induction in Probabilistic Mathematics</a></strong>\", written up by Jeremy Hahn. Jeremy writes: \"At the workshop, after pinning down the above example of undesired behaiviour we turned to other proposals for priors. None of the ideas presented are in a polished enough form to be considered a complete proposal, but we are very happy with the progress that was made.\"</p>\n<h3 id=\"Theme_2__The__procrastination_paradox_\">Theme 2: The \"procrastination paradox\"</h3>\n<p>The other main theme of the workshop was the <a href=\"/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">L\u00f6bian obstacle to self-modifying AI</a>, and more specifically, something we're calling the \"procrastination paradox\". One approach for constructing a self-modifying AI based on mathematical logic would be to require the AI to prove, whenever it wants to take an action -- be it an external action or making some modification to its own source code -- that this action is \"safe\". Then you would naively expect the AI to be able to reason as follows: \"The following simple self-modification X is safe, because the modified AI will still only take actions that it has proven to be safe, so it will only take safe actions.\" But by L\u00f6b's theorem, a consistent system T which is as strong as Peano Arithmetic cannot go from \"whatever the modified AI does will <em>have been proven</em> (in T) to be safe\" to \"whatever the modified AI does will <em>in fact</em> be safe\". (See <a href=\"http://intelligence.org/2013/08/04/benja-interview/\">here</a> for discussion of why it seems reasonable to tackle self-modifying AI from the perspective of mathematical logic, even though a realistic AI will certainly need to use probability to reason about the physical world.) In particular, we cannot have a consistent system T as strong as PA that proves \"If S is provable in T, then S\" for all sentences S.</p>\n<p>If we can't have a system T that trusts <em>itself</em> in this way, maybe we can have an infinite sequence of consistent systems T_n such that each T_n proves \"If S is provable in T_(n+1), then S\", for all sentences S? Marcello (Herreshoff) has shown that in fact there is such a sequence, explained in <a href=\"http://intelligence.org/files/TilingAgents.pdf\">his paper with Eliezer on the L\u00f6bian obstacle</a>. However, although it is consistent (meaning that it doesn't prove <em>everything</em>; in particular, it doesn't prove 0=1), this system is <em>unsound</em> (meaning that it proves <em>some</em> things which aren't actually true). We now have an intuition for why this is so, and a corresponding formal proof that any sequence like this must be unsound: Imagine a sequence of agents A_n, using the systems T_n, whose goal it is that a particular button gets pressed -- they don't care <em>when</em> it gets pressed, just that it <em>does</em> get pressed, eventually. Each of these agents will try to prove that one of its successors presses the button; if it can't find such a proof, then it will press the button itself. Now A_n reasons as follows (in T_n):</p>\n<p style=\"padding-left: 30px;\">Either my successor, A_(n+1), will press the button, or it won't. If it <em>doesn't</em> press the button, then it must have found a proof that the button gets pressed at some later time. This proof is in T_(n+1), and I know that if something is provable in T_(n+1), then it is true. Thus, either way, the button gets pressed -- by my successor, or at some later time.</p>\n<p>So A_n has found a proof that the button gets pressed, and therefore doesn't press the button itself. But since this is true for every n, the button never gets pressed -- and therefore, all of the systems T_n are unsound, since they prove a statement which isn't true. Metaphorically, every time the AI thinks about pressing the button, it convinces itself that there is some <em>future</em> time when it will stop procrastinating and actually press the button, so <em>today</em> it is surely fine to procrastinate -- and in this way, the AI procrastinates forever. For more, including formal proofs, see \"<a href=\"https://intelligence.org/wp-content/uploads/2013/12/procrastination-paradox.pdf\"><strong>The Procrastination Paradox</strong></a>\", written up by Eliezer.</p>\n<p>However, on the positive side, it turns out that if you modify Marcello's construction so that T_n merely proves that T_(n+1) is <em>consistent</em>, rather than proving that everything T_(n+1) proves is true, then Marcello's proof not only shows that the T_n are all consistent, but also that they are sound. (This is really easy to see: T_n is PA plus the axiom \"psi(n) -&gt; Con(T_(n+1))\", for some formula psi(n). Marcello's proof shows that all of these systems T_n are consistent. But then, since T_(n+1) is consistent, \"psi(n) -&gt; Con(T_(n+1))\" is true, so all axioms of T_n are true and T_n is sound.) Moreover, although there are things that T_n proves that T_(n+1) doesn't -- namely, \"Con(T_(n+1))\", since if T_(n+1) proved that, it would be inconsistent -- you can construct these T_n so that they all have the same <a href=\"http://en.wikipedia.org/wiki/Proof-theoretic_ordinal\">proof-theoretic ordinal</a>, so to the extent that you think that proof-theoretic ordinals are a good measure of the \"mathematical strength\" of a theory, all of the theories T_n can be said to have the same amount of mathematical strength. Finally, although T_n doesn't prove the L\u00f6b schema \"If T_(n+1) proves S, then S is true\" for <em>all</em> sentences S, it does in fact prove it for a particular class of sentences (the Pi_1 sentences). For details, see \"<strong><a href=\"https://intelligence.org/wp-content/uploads/2013/12/consistency-waterfall.pdf\">An in\ffinitely descending sequence of sound theories each proving the next consistent</a></strong>\", written up by Benja Fallenstein, a.k.a. myself.</p>\n<p>Is it enough to have the L\u00f6b schema for Pi_1 sentences? (These are sentences of the form \"forall x. phi(x)\", where phi(x) is such that you can write a computer program that takes x as input and outputs whether phi(x) is true or false.) Actually, there's an argument to be made that it might be <em>exactly right</em> for tiling agents: It seems to be as much as you can get without running into the procrastination paradox, and it's enough to formalize goals like \"The world is not destroyed before time n\" (if the world is computable). Moreover, there's a way to combine this with ideas from a different solution to the tiling agents problem, parametric polymorphism (one version of which is described in the tiling agents paper), in order to also handle goals that don't as obviously fit into the Pi_1 mold; we call this hybrid of ideas \"<a href=\"https://intelligence.org/wp-content/uploads/2013/12/fallensteins-monster.pdf\"><strong>Fallenstein's monster</strong></a>\", written up by Nate Soares a.k.a. <a href=\"/user/So8res/overview/\">So8res</a>, because it looks rather grafted together. (In fact, it was even uglier when we came up with it, because back then we didn't even have the sound version of Marcello's construction yet. I'm hopeful that we will find prettier solutions to the remaining monstrous aspects as well, so that I will soon be able to ask my trusted research assistant, Igor, to dispose of the monster.)</p>\n<p>Finally, we usually talk of a set of equations like \"T_n = PA + psi(n) -&gt; Con(T_(n+1))\" as if they define <em>one particular</em> sequence of theories T_n, but it's not obvious that different ways of making this formal lead to logically equivalent theories. In fact, I'd have guessed that in general they don't, but Will Sawin had the opposite intuition and was right; any recursively enumerable set of equations of this type has a unique solution (in the sense that if PA proves that two r.e. lists of theories U_i and V_i are both solutions, then PA proves that they are equivalent). For details, see \"<strong><a href=\"https://intelligence.org/wp-content/uploads/2013/12/recursively-defined-theories-are-well-defined.pdf\">Recursively-defined logical theories are well-defined</a></strong>\", written up by Nisan Stiennon. Interestingly, this turns out to be an equivalent formulation of L\u00f6b's theorem!</p>\n<h3 id=\"Odds_and_ends\">Odds and ends</h3>\n<p>In addition to the two main themes, there were some odds and ends: First, it turns out that the <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/1217\">5-and-10 problem</a>, which has been discussed on LW quite a bit, is <em>almost</em> a problem for the formalism in the tiling agents paper; the exact system discussed in the paper narrowly avoids the problem, but a simple and natural variant runs into it. For details, see \"<strong><a href=\"https://intelligence.org/wp-content/uploads/2013/12/tiling-agents-5-and-10.pdf\">The 5-and-10 problem and the tiling agents formalism</a></strong>\", written up by me.</p>\n<p>And both last and least, we considered and disposed of a conjecture that a particular version of parametric polymorphism might be able to avoid the problem of losing mathematical strength when an agent using it rewrites itself. This would have been an interesting result if it had been true, so it seems worth recording, but since it turned out to be false, it's not particularly exciting. See \"<strong><a href=\"https://intelligence.org/wp-content/uploads/2013/12/decreasing-strength-parametric-polymorphism.pdf\">Decreasing mathematical strength in one formalization of parametric polymorphism</a></strong>\", also written up by me.</p>", "sections": [{"title": "Theme 1: Scientific induction in mathematics", "anchor": "Theme_1__Scientific_induction_in_mathematics", "level": 1}, {"title": "Theme 2: The \"procrastination paradox\"", "anchor": "Theme_2__The__procrastination_paradox_", "level": 1}, {"title": "Odds and ends", "anchor": "Odds_and_ends", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "43 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gnxDNEtkEo3sfeyPn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-16T01:37:49.048Z", "modifiedAt": null, "url": null, "title": "The first AI probably won't be very smart", "slug": "the-first-ai-probably-won-t-be-very-smart", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:33.382Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jpaulson", "createdAt": "2013-01-17T05:05:45.722Z", "isAdmin": false, "displayName": "Jonathan Paulson"}, "userId": "4NQjysoJPjEwcJ2Sv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hoPYgLJFjynxkYNsW/the-first-ai-probably-won-t-be-very-smart", "pageUrlRelative": "/posts/hoPYgLJFjynxkYNsW/the-first-ai-probably-won-t-be-very-smart", "linkUrl": "https://www.lesswrong.com/posts/hoPYgLJFjynxkYNsW/the-first-ai-probably-won-t-be-very-smart", "postedAtFormatted": "Thursday, January 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20first%20AI%20probably%20won't%20be%20very%20smart&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20first%20AI%20probably%20won't%20be%20very%20smart%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhoPYgLJFjynxkYNsW%2Fthe-first-ai-probably-won-t-be-very-smart%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20first%20AI%20probably%20won't%20be%20very%20smart%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhoPYgLJFjynxkYNsW%2Fthe-first-ai-probably-won-t-be-very-smart", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhoPYgLJFjynxkYNsW%2Fthe-first-ai-probably-won-t-be-very-smart", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 226, "htmlBody": "<p>Claim: The first human-level AIs are not likely to undergo an intelligence explosion.</p>\n<p>1) Brains have a ton of computational power: ~86 billion neurons and trillions of connections between them. Unless there's a \"shortcut\" to intelligence, we won't be able to efficiently simulate a brain for a long time.&nbsp;http://io9.com/this-computer-took-40-minutes-to-simulate-one-second-of-1043288954 describes one of the largest computers in the world simulating 1s of brain activity in 40m (i.e. this \"AI\" would think 2400 times slower than you or me). The first AIs are not likely to be fast thinkers.<br /><br />2) Being able to read your own source code does not mean you can self-modify. You know that you're made of DNA. You can even get your own \"source code\" for a few thousand dollars. No humans have successfully self-modified into an intelligence explosion; the idea seems laughable.<br /><br />3) Self-improvement is not like compound interest: if an AI comes up with an idea to modify it's source code to make it smarter, that doesn't automatically mean it will have a new idea tomorrow. In fact, as it picks off low-hanging fruit, new ideas will probably be harder and harder to think of. There's no guarantee that \"how smart the AI is\" will keep up with \"how hard it is to think of ways to make the AI smarter\"; to me, it seems very unlikely.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hoPYgLJFjynxkYNsW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": -2, "extendedScore": null, "score": 1.5156651640902342e-06, "legacy": true, "legacyId": "25272", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-16T05:51:40.710Z", "modifiedAt": null, "url": null, "title": "LessWrong Help Desk - free paper downloads and more (2014)", "slug": "lesswrong-help-desk-free-paper-downloads-and-more-2014", "viewCount": null, "lastCommentedAt": "2018-12-31T23:01:55.328Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4sAsygakd4oCpbEKs/lesswrong-help-desk-free-paper-downloads-and-more-2014", "pageUrlRelative": "/posts/4sAsygakd4oCpbEKs/lesswrong-help-desk-free-paper-downloads-and-more-2014", "linkUrl": "https://www.lesswrong.com/posts/4sAsygakd4oCpbEKs/lesswrong-help-desk-free-paper-downloads-and-more-2014", "postedAtFormatted": "Thursday, January 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20Help%20Desk%20-%20free%20paper%20downloads%20and%20more%20(2014)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20Help%20Desk%20-%20free%20paper%20downloads%20and%20more%20(2014)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4sAsygakd4oCpbEKs%2Flesswrong-help-desk-free-paper-downloads-and-more-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20Help%20Desk%20-%20free%20paper%20downloads%20and%20more%20(2014)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4sAsygakd4oCpbEKs%2Flesswrong-help-desk-free-paper-downloads-and-more-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4sAsygakd4oCpbEKs%2Flesswrong-help-desk-free-paper-downloads-and-more-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<p>Over the last year, VincentYu, gwern and others have provided many papers for the LessWrong community (87% success rate in 2012) through <a href=\"/r/discussion/tag/help_desk\">previous help desk threads</a>.&nbsp;We originally intended to provide editing, research and general troubleshooting help, but article downloads are by far the most requested service.</p>\n<p>If you're doing a LessWrong relevant project we want to help you. If you need help accessing a journal article or academic book chapter, we can get it for you. If you need some research or writing help, we can help there too.</p>\n<p>Turnaround times for articles published in the last 20 years or so is usually less than a day. Older articles often take a couple days.</p>\n<p>Please make new article requests in the comment section of this thread.</p>\n<p>If you would like to help out with finding papers, please monitor this thread for requests. If you want to monitor via RSS like I do, many RSS readers will give you the comment feed if you give it the URL for this thread (or use&nbsp;<a href=\"/r/discussion/lw/eto/lesswrong_help_desk_free_paper_downloads_and_more/.rss\">this</a>&nbsp;link directly).&nbsp;</p>\n<p>If you have some special skills you want to volunteer, mention them in the comment section.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4sAsygakd4oCpbEKs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 47, "extendedScore": null, "score": 0.000193, "legacy": true, "legacyId": "25275", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 335, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-16T06:35:31.625Z", "modifiedAt": null, "url": null, "title": "Dangers of steelmanning / principle of charity", "slug": "dangers-of-steelmanning-principle-of-charity", "viewCount": null, "lastCommentedAt": "2020-10-16T08:12:43.690Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gothgirl420666", "createdAt": "2013-01-06T19:35:18.030Z", "isAdmin": false, "displayName": "gothgirl420666"}, "userId": "P7J37T964Pxizzp9g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wW9mcj8GP5avS5ovW/dangers-of-steelmanning-principle-of-charity", "pageUrlRelative": "/posts/wW9mcj8GP5avS5ovW/dangers-of-steelmanning-principle-of-charity", "linkUrl": "https://www.lesswrong.com/posts/wW9mcj8GP5avS5ovW/dangers-of-steelmanning-principle-of-charity", "postedAtFormatted": "Thursday, January 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dangers%20of%20steelmanning%20%2F%20principle%20of%20charity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADangers%20of%20steelmanning%20%2F%20principle%20of%20charity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwW9mcj8GP5avS5ovW%2Fdangers-of-steelmanning-principle-of-charity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dangers%20of%20steelmanning%20%2F%20principle%20of%20charity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwW9mcj8GP5avS5ovW%2Fdangers-of-steelmanning-principle-of-charity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwW9mcj8GP5avS5ovW%2Fdangers-of-steelmanning-principle-of-charity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 699, "htmlBody": "<p>As far as I can tell, most people around these parts consider the <a href=\"http://en.wikipedia.org/wiki/Principle_of_charity\">principle of charity</a> and its super saiyan form, <a href=\"http://www.patheos.com/blogs/camelswithhammers/2012/12/the-virtue-of-steelmanning/\">steelmanning</a>, to be Very Good Rationalist Virtues. I basically agree and I in fact operate under these principles more or less automatically now. HOWEVER, no matter how good the rule is, there are always exceptions, which I have found myself increasingly concerned about.</p>\n<p><a href=\"http://habitableworlds.wordpress.com/2013/10/29/the-motives-of-social-policy/\">This blog post that I found in the responses to Yvain's anti-reactionary FAQ</a>&nbsp;argues that even though the ancient Romans had welfare, this policy was motivated not for concern for the poor or for a desire for equality like our modern welfare policies, but instead \"the Roman dole was wrapped up in discourses about a) the might and wealth of Rome and b) goddess worship... The dole was there because it made the emperor more popular and demonstrated the wealth of Rome to the people. What&rsquo;s more, the dole was personified as Annona, a goddess to be worshiped and thanked.\"&nbsp;</p>\n<p>So let's assume this guy is right, and imagine that an ancient Roman travels through time to the present day. He reads an article by some progressive arguing (using the rationale one would typically use) that Obama should increase unemployment benefits. \"This makes no sense,\" the Roman thinks to himself. \"Why would you give money to someone who doesn't work for it? Why would you reward lack of virtue? Also, what's this about equality? Isn't it right that an upper class exists to rule over a lower class?\" Etc.&nbsp;</p>\n<p>But fortunately, between when he hopped out of the time machine and when he found this article, a rationalist found him and explained to him steelmanning and the principle of charity. \"Ah, yes,\" he thinks. \"Now I remember what the rationalist said. I was not being so charitable. I now realize that this position kind of makes sense, if you read between the lines. Giving more unemployment benefits <em>would</em>, now that I think about it, demonstrate the power of America to the people, and certainly Annona would approve. I don't know why whoever wrote this article didn't just come out and say that, though. Maybe they were confused\".&nbsp;</p>\n<p>Hopefully you can see what I'm getting at. When you regularly use the principle of charity and steelmanning, you run the risk of:</p>\n<p>1. Sticking rigidly to a certain worldview/paradigm/established belief set, even as you find yourself willing to consider more and more concrete propositions. The Roman would have done better to really read what the modern progressive's logic was, think about it, and try to see where he was coming from than to automatically filter it through his own worldview. If he consistently does this he will never find himself considering alternative ways of seeing the world that might be better. &nbsp;</p>\n<p>2. Falsely developing the sense that your worldview/paradigm/established belief set is more popular than it is. Pretty much no one today holds the same values that an ancient Roman does, but if the Roman goes around being charitable all the time then he will probably see his own beliefs reflected back at him a fair amount.</p>\n<p>3. Taking arguments more seriously than you possibly should. I feel like I see all the time on rationalist communities people say stuff like \"this argument by A sort of makes sense, you just need to frame it in objective, consequentialist terms like blah blah blah blah blah\" and then follow with what looks to me like a completely original thought that I've never seen before. But why didn't A just frame her argument in objective, consequentialist terms? Do we assume that what she wrote was sort of a telephone-game approximation of what was originally a highly logical consequentialist argument? If so where can I find that argument? And if not, why are we assuming that A is a crypto-consequentialist when she probably isn't? And if we're sure that objective, consequentialist logic is The Way To Go, then shouldn't we be very skeptical of arguments that seem like their basis is in some other reasoning system entirely?&nbsp;</p>\n<p>4. Just having a poor model of people's beliefs in general, which could lead to problems.</p>\n<p>Hopefully this made sense, and I'm sorry if this is something that's been pointed out before.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RE6h98Ziwcfh4EP9T": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wW9mcj8GP5avS5ovW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 111, "baseScore": 150, "extendedScore": null, "score": 0.000382, "legacy": true, "legacyId": "25247", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 150, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 94, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-16T07:27:47.032Z", "modifiedAt": null, "url": null, "title": "Things I Wish They'd Taught Me When I Was Younger: Why Money Is Awesome", "slug": "things-i-wish-they-d-taught-me-when-i-was-younger-why-money", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:40.052Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QyZvL6hrmS6AEXJLF/things-i-wish-they-d-taught-me-when-i-was-younger-why-money", "pageUrlRelative": "/posts/QyZvL6hrmS6AEXJLF/things-i-wish-they-d-taught-me-when-i-was-younger-why-money", "linkUrl": "https://www.lesswrong.com/posts/QyZvL6hrmS6AEXJLF/things-i-wish-they-d-taught-me-when-i-was-younger-why-money", "postedAtFormatted": "Thursday, January 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Things%20I%20Wish%20They'd%20Taught%20Me%20When%20I%20Was%20Younger%3A%20Why%20Money%20Is%20Awesome&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThings%20I%20Wish%20They'd%20Taught%20Me%20When%20I%20Was%20Younger%3A%20Why%20Money%20Is%20Awesome%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQyZvL6hrmS6AEXJLF%2Fthings-i-wish-they-d-taught-me-when-i-was-younger-why-money%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Things%20I%20Wish%20They'd%20Taught%20Me%20When%20I%20Was%20Younger%3A%20Why%20Money%20Is%20Awesome%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQyZvL6hrmS6AEXJLF%2Fthings-i-wish-they-d-taught-me-when-i-was-younger-why-money", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQyZvL6hrmS6AEXJLF%2Fthings-i-wish-they-d-taught-me-when-i-was-younger-why-money", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1884, "htmlBody": "<p>There are some things money can't buy. They are the exceptions that prove the rule.</p>\n<p>For the pedants, to say something is an exception that proves the rule is to say that when you look at the exceptions, they're so unusual that it reinforces the point that the rule is generally valid even though it isn't universally valid. In the case of money, there's a reason people don't say things like \"there are some things hand-knit scarves can't be bartered for\" or \"Hand-knit scarves can't be bartered for happiness.\"</p>\n<p>Eliezer <a href=\"/lw/u2/the_sheer_folly_of_callow_youth/#more\">once described</a> the sequences as the letter he wishes he could have written to his former self. When I think of the letter I wish I could write to my former self, the value of money is at the top of the list of things I'd include.&nbsp;</p>\n<p>You can give a cynical, Hansonian explanation of why we don't tell young people enough about the awesomeness of money, and I suppose there'd be some truth to it. But I'm not sure that was my main problem. Growing up, my dad spent a lot of time urging me to go into a high-paying career, to the point giving me advice on what medical specialty to go into. He just didn't do a great job of selling me on it. It wasn't until I learned some economics that I really came to understand why money is so awesome.</p>\n<p><em><a id=\"more\"></a>(Disclaimer: I don't actually know that much economics, and in fact have never taken an economics course. I just know more than my former self.)</em></p>\n<p>The first thing to understand about money is that the range of things you can get for it is really incredibly huge. Econ bloggers Tyler Cowen and Alex Tabarrok periodically do posts called \"<a href=\"http://marketsineverything.com/\">markets in everything</a>\" where they highlight some of the weirder examples of this, but the weird examples matter less than the obvious examples people just don't think about much. &nbsp;There's a tendency to associate money with a narrow range of things rich people stereotypically spend their money on. Or, in my case growing up in an upper middle-class family, there were the family vacations and boats that my dad seemed to mainly spend his money on, which were nice but didn't seem particularly worth planning my career around.</p>\n<p>Yet not only is the range of things you can get with money huge, even with things you can get without money, spending money on them is often a better way of acquiring them. The reason for this is comparative advantage, a concept that gets discussed a lot in the context of nation-states and why free trade is a good idea, but which also works on an individual level. For example, say you're a lawyer who makes $300 an hour, and you're trying to solve the problem of how to keep your house clean. You could spend a couple hours a week doing it yourself&mdash;or you could work slightly longer hours and hire someone else to do it for $30 an hour.</p>\n<p>The reason this is an example of <em>comparative </em>advantage is it doesn't matter if the people you're paying to clean your house are any better at house-cleaning than you. In fact, it works even if they're slightly worse, as long as the difference in house-cleaning ability is overshadowed by the difference in lawyering ability. In econ jargon, you can have an <em>absolute</em> advantage at both lawyering and house-cleaning, and it will still make sense to pay other people to clean your house if they have a <em>comparative </em>advantage there. Many people who aren't rich probably assume that when rich people hire other people to do basic tasks for them, it's a frivolous expense, but under the right circumstances it can a matter of economic efficiency.</p>\n<p>This point about comparative advantage, when applied to charity, is one of the central insights of the effective altruism movement (<a href=\"http://80000hours.org/earning-to-give\">\"earning to give\"</a>). Suppose instead of talking about a lawyer who wants to keep his house clean, we're instead talking about a lawyer who wants to help the local soup kitchen. He could volunteer to help out there in his spare time, but he could also work a little longer hours, donate the money, and enable the soup kitchen to hire more person-hours of work there. Choosing to volunteer rather than give would suggest the lawyer isn't mainly concerned about helping the soup kitchen, but perhaps with warm fuzzies or being seen doing good.</p>\n<p>And this doesn't just to small-scale decisions about donating some money vs. volunteering a few hours. It also applies to someone trying to decide between, saying, going into a career in medicine and eventually joining Doctors Without Borders vs. going into a career in finance and using the money you make to pay people to distribute bed nets to stricken regions of the world. (That person was me when I was younger, except the second option wasn't even on my radar.)</p>\n<p>Note that while I personally think earning to give is an especially important example of how you can exploit comparative advantage to achieve you're goals, it's also worth emphasizing that it's just a special case of a general principle which can be extremely powerful even if you don't care about making the world a better place.</p>\n<p>Given all this, what of the saying \"if you want something done right do it yourself\"? The answer is that, yes, the difficulty of figuring out who's competent and trustworthy does impose transaction costs on hiring people to do stuff for you, but it's important to remember the costs are finite. When the difference in comparative advantage is large enough, they'll often be worth paying.</p>\n<p>Now there are still things money can't buy, at least not literally. But money tends to make them easier to acquire. Take the classic example of happiness: there's a traditional idea (which I've heard attributed to the Greek philosopher Epicurus, though I can't find the source now) that more money makes you happier up to a certain point since it's hard to be happy if you're starving, but beyond that more money doesn't help. It turns out that it's <a href=\"http://www.brookings.edu/research/interactives/2013/income-well-being\">not clear this is actually true</a>&mdash;some studies have found more money leads to greater happiness up through the highest income levels examined.</p>\n<p>But suppose, in spite of this, that you're an income satisficer, meaning you want to make a certain amount of money and don't care about additional money beyond that. Suppose as long as you have that certain amount of money, you care more about being able to do what you love. And suppose you don't care about being able to make the world a better place through donating to charity. Should you then pursue whatever career you think you'll enjoy the most out of those that pay enough money?</p>\n<p>Not necessarily. The way to think about this is to realize that time spent &nbsp;is, in an important sense, an expensive luxury. In economics, there's a concept called opportunity cost, which is closely related to comparative advantage. Opportunity cost asks: by choosing to do something, what's the next-best alternative you're giving up? So for example, by this standard the biggest cost of college for many people will be not tuition, but the time they spent in college that could've been spent working. Even if you didn't go to summer classes, didn't study all that much, and were only qualified for minimum wage jobs, it still easily adds up to more than the cost of a state school in the US.</p>\n<p>A lot of things turn out to be like this: when you translate the cost in time into a monetary value, time is <em>the </em>biggest component of the cost. Once you start thinking in those terms, it becomes easier to see that just as there are two ways to convert time into a clean house (clean it yourself, or work at a job where you have the comparative advantage and pay someone else to clean it), there are two ways to maximize the amount of time you spend doing things you enjoy: find a job you mostly enjoy, or else find a high paying job you hate and work part-time / take frequent long sabbaticals / work hard when you're young, then retire early.</p>\n<p>People tend not to even consider the second set of options because they've been sold a model of \"work nine to five for fifty weeks a year from college graduation until you qualify for Social Security,\" and you are nudged towards that model somewhat by <em>employers </em>assuming it. But it's not mandatory, and if you acquire in-demands skills that can translate into greater flexibility. I have a friend who's a dev consultant who recently took a month sabbatical from her job and then quit entirely without having another one lined up because (1) she makes enough money she doesn't need to work year-round and (2) her skills are sufficiently in-demand that she's not worried about her ability to get another job when she wants one.</p>\n<p>On the flip side, to understand one of the main problems with the \"get a job you love\" strategy, consider the extreme case: a job you'd do for free. The problem with such jobs is that they tend to be jobs other people are willing to do them for free too. That makes it hard for anyone to get paid. For example, I love writing, and I'm doing it for free right now. But it turns out lots of other people feel the same way, and the internet has made it really easy for all of us to distribute our writing for free, and now it's even harder to get paid as a writer than it was during the age of print.</p>\n<p>This is just one example, but I suspect there's a <em>systematic </em>reason why the \"get a job you love\" strategy tends to produce outcomes you didn't really want: it can make it harder to see what tradeoffs you're <em>really </em>making between money and time spent doing things you want to do for their own sake. In the worst case, you end up getting the worst of both worlds: you become a college professor because you think it will pay okay (if not great), and you'll get to devote all your time to the life of the mind. But you end up adjuncting for what's effectively minimum wage while spending most of your time dealing with undergrads who are just taking the course for the elective and only care about getting an A with as little effort as possible.</p>\n<p>I'm not saying <em>everyone </em>should optimize solely for money in choosing their career. But at the very least, it's worth putting considerable effort into finding out how much you could (perhaps not immediately, but after but in a year or several) if you did optimize for money. That way, you'll at least know the tradeoff you're making when you chose a different career.</p>\n<p>And by the way, if you're reading LessWrong, odds are you're fairly smart, and may be underestimating how monetizable your intelligence is. I'd like to repeat the advice given by other people in the online rationalist community to look into programming as a career choice. I'm currently doing <a href=\"http://www.appacademy.io/#p-home\">App Academy</a>&nbsp;and highly recommend it, if you do apply tell them I sent you. You may also be able to get good information on choosing a career from <a href=\"http://80000hours.org/\">80,000 Hours</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 1, "4kQXps8dYsKJgaayN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QyZvL6hrmS6AEXJLF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 46, "extendedScore": null, "score": 0.000132, "legacy": true, "legacyId": "25190", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 234, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-16T09:28:02.920Z", "modifiedAt": null, "url": null, "title": "Division of cognitive labour in accordance with researchers' ability", "slug": "division-of-cognitive-labour-in-accordance-with-researchers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.447Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stefan_Schubert", "createdAt": "2013-12-26T16:42:04.883Z", "isAdmin": false, "displayName": "Stefan_Schubert"}, "userId": "6omuoq9oQuy3KQzG9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9FWWDoMWA7nbFmh6g/division-of-cognitive-labour-in-accordance-with-researchers", "pageUrlRelative": "/posts/9FWWDoMWA7nbFmh6g/division-of-cognitive-labour-in-accordance-with-researchers", "linkUrl": "https://www.lesswrong.com/posts/9FWWDoMWA7nbFmh6g/division-of-cognitive-labour-in-accordance-with-researchers", "postedAtFormatted": "Thursday, January 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Division%20of%20cognitive%20labour%20in%20accordance%20with%20researchers'%20ability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADivision%20of%20cognitive%20labour%20in%20accordance%20with%20researchers'%20ability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FWWDoMWA7nbFmh6g%2Fdivision-of-cognitive-labour-in-accordance-with-researchers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Division%20of%20cognitive%20labour%20in%20accordance%20with%20researchers'%20ability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FWWDoMWA7nbFmh6g%2Fdivision-of-cognitive-labour-in-accordance-with-researchers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9FWWDoMWA7nbFmh6g%2Fdivision-of-cognitive-labour-in-accordance-with-researchers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1434, "htmlBody": "<p>The variance in productivity between people is much greater in some areas than in others. The most productive cleaner is probably less than twice as productive as the average cleaner, whereas the most productive entrepreneur arguably is many times more productive than the average entrepreneur. One area where this difference in productivity is particularly great is research - especially in its more theoretical parts. A physicist once told me that Einstein put forth four theories worthy of the Nobel Prize in one year alone (1905, his <a href=\"http://en.wikipedia.org/wiki/Annus_Mirabilis_papers\">Annus Mirabilis</a>). That's a level of productivity that, on any reasonable &nbsp;measure, dwarfs that of whole armies of lesser physicists. The same pattern recurs in many disciplines (though it is perhaps not always quite as strong).</p>\n<p>Such a recurring pattern cries out for an explanation. In some areas the reason might be hero-worship&nbsp;(postmodernist philosophy springs to mind), but the pattern is strong in many serious disciplines as well, so that can't be the whole explanation.</p>\n<p>Instead, I would argue that the reason is that science is a social activity in a much deeper sense than cleaning is. A scientific problem is \"hard\" due to the fact that most scientists are not sufficiently talented to solve it given our present knowledge. In most sciences, there will normally at any given point be a lot of such problems which are too hard for most scientists, but solvable by a few really smart people.</p>\n<p>I'm not saying that but for them, there would be no progress. Clearly there would, but we would need more knowledge - clues, as it were - to solve the hard problems. Thus progress would go much slower.</p>\n<p>If this picture - rather an elitist one - is right, what are the consequences for policy? It seems to me that the most obvious consequence is that we need to make sure that the most talented people's time is spent in a rational way. Firstly, it does not seem to me that they should spend time teaching students - their time is far too valuable for that. Overall, it's a bit strange that researchers are expected to teach since they really are two quite different jobs.</p>\n<p>That's not my main point, however. My main point is that they should focus on what what they're best at: producing new ideas. Research today involves so much more - applying for grants, writing up articles that have to conform to all sorts of disciplinary conventions, attending seminars of questionable value, etc, etc.</p>\n<p>It is widely noted that the present incentive structure (\"publish or &nbsp;perish\") in the academia has led to a flood of uninteresting but publishable articles. A less-noted but to my mind more serious problem is that this incentive structure fails to make the most talented people create and (in particular) publish ideas at the pace they're capable of. I know several very talented people whose publication rates are very low even though they are positively brim full of interesting ideas.</p>\n<p>I don't know exactly why they don't publish more but suspect that they're bored by the tedium of the peer review process (who isn't - I have two papers that haven't been reviewed after 1,5 years). Perfectionism is also to blame, I think. Many smart people are fed up with the massive amount of mediocre research that is produced, and therefore (and perhaps for other reasons) they swear to themselves never to publish anything that isn't top-notch and very well worked through. Though I share the annoyance with the massive amount of mediocre research, I think that this reaction is misguided. It would be much better for the research community as a whole if they shared their ideas, even if some of them are a bit half-baked.&nbsp;</p>\n<p>Hence we need to devise a system that makes them produce more publically available research. Reading this blog - and in particular Yudkowsky's excellent posts - it strikes me that they should have a blog where they could write down various ideas. They would need some incentive in order to do that, and hence the academia needs to start recognizing the blog as a proper medium of publication (as is actually happening in some disciplines such as macro economics, if <a href=\"http://krugman.blogs.nytimes.com/2013/12/17/the-facebooking-of-economics/\">Krugman is right</a>).</p>\n<p>Indeed, I think that science would usually be better off if these people left the job of proving their hypotheses, or that of making them more precise, to other people, and hurried on to the next subject. We need a much more thorough division of cognitive labour in the academia: we should not waste the most able people's time (our most precious resource) by having them work on tasks that could be left to less able people. (This principle is explicitly followed in many other organizations - e.g. it is frequently argued that various tasks done by doctors could be done by less educated, and cheaper, employees such as nurses.)</p>\n<p>Of course you do have a fair amount of this in the academia. Professors have Ph.D. students and postdocs to do carry out simpler parts of research for them. Academic stars do tend to work on the \"big problems\" and leave to others to fill in the blanks. By and large, this is good. But we need to go much further in this direction.&nbsp;</p>\n<p>Apart from such obvious problems such as vested interests' resistance to change and sheer inertia (ever-present problems in all institutions), two things obstruct a change in this direction. The first is that people are poor at estimating researchers' skills. They don't recognize that the differences in skills are as large as I have argued here. Different professors earn roughly as much, have roughly the same social standing, etc - how could it possibly be that their productivity levels are vastly different, the seem to think. Also, there is the <a href=\"http://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect\">Dunning-Kruger effect</a>, which says that incompetent people overestimate their skills, whereas competent people underestimate theirs. This contributes to the impression that productivity levels are more equal than they actually are.</p>\n<p>Also, people are not very good at determining who's skilled and who's not. (This goes in particular for incompetent people.) In many cases, they go for superficial characteristics such as whether the researcher uses some respected technique (whether this technique is actually mastered, or even relevant in the case at hand, is often not regarded). To some extent, this is an unescapable problem, but possibly it could be alleviated somewhat by more sophisticated techniques for identifying good texts. Such simple techniques as explicitly counting the number and significance of ideas per page, could go some way towards making it more transparent who is actually an original thinker, and who is not. More advanced techniques could conceivably go much further. This is a topic rationalists should explore, I think.</p>\n<p>Another problem is egalitarianism. The system I am sketching is in many ways more hierarchical than today's academia - some people would focus entirely on the hardest and most prestigious tasks, whereas others would have to do more mundane work. I also think that this could potentially be reflected in greater differences in pay (if nothing else, universities would be more reluctant to waste talented people's time if they had to pay them huge salaries). Hence people would oppose it for egalitarian reasons (most people would probably deny that there are huge differences in ability because it clashes with their egalitarian morality, but I think that some people who admit that there are such big difference in ability would oppose it nevertheless, for egalitarian reasons).</p>\n<p>If you're not an egalitarian, this is obviously not a valid objection, but even if you are, I think that it has less force than most people would think. What is important for egalitarianism is that society <em>as a whole </em>is egalitarian - i.e. that the resources (including not only money but also, e.g. respect) are not too unevenly distributed - not that any <em>particular professio</em>n is egalitarian. Why should it be more objectionable that productive professors earn five times more than an unproductive professor, than that professors earn five times more than cleaners? Now if a less egalitarian academia makes the <a href=\"http://en.wikipedia.org/wiki/Gini_coefficient\">Gini coefficient</a> increase somewhat, this could easily be offset by introducing a bit more progressive taxes.</p>\n<p>I leave the details of how to set up this system at the moment for now even though I think it's very important to pay great attention to the institutional arrangements of the academia, since researchers are self-interested creatures who react to incentives just like everybody else. If anyone has any ideas on this, I'd be interested in that.</p>\n<p>I've got a lot more to say about this but this post is too long already so I leave it at that.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9FWWDoMWA7nbFmh6g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 1.516180841795171e-06, "legacy": true, "legacyId": "25270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-16T11:08:58.150Z", "modifiedAt": null, "url": null, "title": "Meetup : Meet Up: Less Wrong Israel Meetup (Tel Aviv): Uses of Cryptography", "slug": "meetup-meet-up-less-wrong-israel-meetup-tel-aviv-uses-of", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SoftFlare", "createdAt": "2012-11-09T00:22:21.187Z", "isAdmin": false, "displayName": "SoftFlare"}, "userId": "dSdSRiHQPaFuBXhG6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mijk6JNzmzJp4rc7i/meetup-meet-up-less-wrong-israel-meetup-tel-aviv-uses-of", "pageUrlRelative": "/posts/Mijk6JNzmzJp4rc7i/meetup-meet-up-less-wrong-israel-meetup-tel-aviv-uses-of", "linkUrl": "https://www.lesswrong.com/posts/Mijk6JNzmzJp4rc7i/meetup-meet-up-less-wrong-israel-meetup-tel-aviv-uses-of", "postedAtFormatted": "Thursday, January 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meet%20Up%3A%20Less%20Wrong%20Israel%20Meetup%20(Tel%20Aviv)%3A%20Uses%20of%20Cryptography&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meet%20Up%3A%20Less%20Wrong%20Israel%20Meetup%20(Tel%20Aviv)%3A%20Uses%20of%20Cryptography%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMijk6JNzmzJp4rc7i%2Fmeetup-meet-up-less-wrong-israel-meetup-tel-aviv-uses-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meet%20Up%3A%20Less%20Wrong%20Israel%20Meetup%20(Tel%20Aviv)%3A%20Uses%20of%20Cryptography%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMijk6JNzmzJp4rc7i%2Fmeetup-meet-up-less-wrong-israel-meetup-tel-aviv-uses-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMijk6JNzmzJp4rc7i%2Fmeetup-meet-up-less-wrong-israel-meetup-tel-aviv-uses-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 246, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vn'>Meet Up: Less Wrong Israel Meetup (Tel Aviv): Uses of Cryptography</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 January 2014 08:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">7 begin ramat gan</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, Jan 30th at VisionMap's offices, Gibor Sport House, 15th floor, 7 Menachem Begin st., Ramat-Gan.</p>\n\n<p>Our program is:</p>\n\n<p>20:00-20:15: Assembly</p>\n\n<p>20:15-21:00: Main Talk</p>\n\n<p>21:00-22:00: Dinner &amp; Discussion</p>\n\n<p>22:00-23:00: Rump Session (minitalks)</p>\n\n<p>23:00-: End of official programming</p>\n\n<p>Main Talk: Uses of Cryptography / Daniel Armak</p>\n\n<p>How can you use Cryptography as a tool? Most conversations about cryptography deal with the available crypto primitives. This talk will instead focus on what you can do with them and describe the most important and/or interesting things that have been built using crypto as a tool. (The primitives will be briefly described).</p>\n\n<p>The talk will be accessible to people without knowledge of the math behind cryptography.</p>\n\n<p>Backup Talk: TBA</p>\n\n<p>Rump Session: Each participant will give a 4-minute talk (+3 minute encore if we applaud hard enough). Giving a talk isn't mandatory, but it's highly recommended. Not confident that what you have to say is relevant to our interests? Unsure about your public speaking skills? Doesn't matter - in the rump session, anything goes. - Note, you don't have to prepare a talk to come! Speaking at the rump session is completely only if you want to.</p>\n\n<p>Feel free to contact me (Gal Hochberg) at hochbergg@gmail.com or at 0545330678 for any further information</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vn'>Meet Up: Less Wrong Israel Meetup (Tel Aviv): Uses of Cryptography</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mijk6JNzmzJp4rc7i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.516291556429127e-06, "legacy": true, "legacyId": "25276", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meet_Up__Less_Wrong_Israel_Meetup__Tel_Aviv___Uses_of_Cryptography\">Discussion article for the meetup : <a href=\"/meetups/vn\">Meet Up: Less Wrong Israel Meetup (Tel Aviv): Uses of Cryptography</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 January 2014 08:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">7 begin ramat gan</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, Jan 30th at VisionMap's offices, Gibor Sport House, 15th floor, 7 Menachem Begin st., Ramat-Gan.</p>\n\n<p>Our program is:</p>\n\n<p>20:00-20:15: Assembly</p>\n\n<p>20:15-21:00: Main Talk</p>\n\n<p>21:00-22:00: Dinner &amp; Discussion</p>\n\n<p>22:00-23:00: Rump Session (minitalks)</p>\n\n<p>23:00-: End of official programming</p>\n\n<p>Main Talk: Uses of Cryptography / Daniel Armak</p>\n\n<p>How can you use Cryptography as a tool? Most conversations about cryptography deal with the available crypto primitives. This talk will instead focus on what you can do with them and describe the most important and/or interesting things that have been built using crypto as a tool. (The primitives will be briefly described).</p>\n\n<p>The talk will be accessible to people without knowledge of the math behind cryptography.</p>\n\n<p>Backup Talk: TBA</p>\n\n<p>Rump Session: Each participant will give a 4-minute talk (+3 minute encore if we applaud hard enough). Giving a talk isn't mandatory, but it's highly recommended. Not confident that what you have to say is relevant to our interests? Unsure about your public speaking skills? Doesn't matter - in the rump session, anything goes. - Note, you don't have to prepare a talk to come! Speaking at the rump session is completely only if you want to.</p>\n\n<p>Feel free to contact me (Gal Hochberg) at hochbergg@gmail.com or at 0545330678 for any further information</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meet_Up__Less_Wrong_Israel_Meetup__Tel_Aviv___Uses_of_Cryptography1\">Discussion article for the meetup : <a href=\"/meetups/vn\">Meet Up: Less Wrong Israel Meetup (Tel Aviv): Uses of Cryptography</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meet Up: Less Wrong Israel Meetup (Tel Aviv): Uses of Cryptography", "anchor": "Discussion_article_for_the_meetup___Meet_Up__Less_Wrong_Israel_Meetup__Tel_Aviv___Uses_of_Cryptography", "level": 1}, {"title": "Discussion article for the meetup : Meet Up: Less Wrong Israel Meetup (Tel Aviv): Uses of Cryptography", "anchor": "Discussion_article_for_the_meetup___Meet_Up__Less_Wrong_Israel_Meetup__Tel_Aviv___Uses_of_Cryptography1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-16T13:56:38.834Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, January 16-31", "slug": "group-rationality-diary-january-16-31-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:01.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HPeHpsvzLFZxNY8k3/group-rationality-diary-january-16-31-0", "pageUrlRelative": "/posts/HPeHpsvzLFZxNY8k3/group-rationality-diary-january-16-31-0", "linkUrl": "https://www.lesswrong.com/posts/HPeHpsvzLFZxNY8k3/group-rationality-diary-january-16-31-0", "postedAtFormatted": "Thursday, January 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20January%2016-31&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20January%2016-31%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHPeHpsvzLFZxNY8k3%2Fgroup-rationality-diary-january-16-31-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20January%2016-31%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHPeHpsvzLFZxNY8k3%2Fgroup-rationality-diary-january-16-31-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHPeHpsvzLFZxNY8k3%2Fgroup-rationality-diary-january-16-31-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">T<span style=\"line-height: 19px;\">his is the public group instrumental rationality diary for January 16-31. &nbsp;</span></p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\">Immediate past diary: &nbsp;<a href=\"/lw/jf5/group_rationality_diary_january_115/\">January 1-15</a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; text-align: justify; line-height: 19px;\"><a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HPeHpsvzLFZxNY8k3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "25266", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RkHKedWX5nJAC3okm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-16T16:09:07.764Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels: Morality - also cake", "slug": "meetup-brussels-morality-also-cake", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fjHkNkYCKsGLs3czg/meetup-brussels-morality-also-cake", "pageUrlRelative": "/posts/fjHkNkYCKsGLs3czg/meetup-brussels-morality-also-cake", "linkUrl": "https://www.lesswrong.com/posts/fjHkNkYCKsGLs3czg/meetup-brussels-morality-also-cake", "postedAtFormatted": "Thursday, January 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%3A%20Morality%20-%20also%20cake&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%3A%20Morality%20-%20also%20cake%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfjHkNkYCKsGLs3czg%2Fmeetup-brussels-morality-also-cake%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%3A%20Morality%20-%20also%20cake%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfjHkNkYCKsGLs3czg%2Fmeetup-brussels-morality-also-cake", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfjHkNkYCKsGLs3czg%2Fmeetup-brussels-morality-also-cake", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 215, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vo'>Brussels: Morality - also cake</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 February 2014 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>On one train track: three transhuman babies. On the other train track, a utility monster who regularly paints masterpieces. Next to you on the bridge, a very fat blue-spotted giraffe, last of its kind. What's a moral agent to do? And what if the giraffe... was your mother? All these questions and more won't be answered in this month's meetup.</p>\n\n<p>We may have to taboo a lot of words in the process, so recommended reading: <a href=\"http://lesswrong.com/lw/nu/taboo_your_words/\" rel=\"nofollow\">http://lesswrong.com/lw/nu/taboo_your_words/</a></p>\n\n<p>This meetup marks the two-years anniversary of the LW Brussels meetup group. There will be cake. It is all right to eat the cake, as that cake would've grown up to become Hitler. (It is also all right to <em>bring</em> cake.)</p>\n\n<p>We will meet at 1 pm at La Fleur en papier dor\u00e9, close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out this one minute form, to share your contact information: <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform</a></p>\n\n<p>The Brussels meetup group communicates through a Google Group: <a href=\"https://groups.google.com/forum/#!forum/lesswrong-brussels\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/lesswrong-brussels</a></p>\n\n<p>Meetup announcements are also mirrored on: <a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">http://www.meetup.com/LWBrussels/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vo'>Brussels: Morality - also cake</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fjHkNkYCKsGLs3czg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.5166209311663022e-06, "legacy": true, "legacyId": "25279", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels__Morality___also_cake\">Discussion article for the meetup : <a href=\"/meetups/vo\">Brussels: Morality - also cake</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 February 2014 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>On one train track: three transhuman babies. On the other train track, a utility monster who regularly paints masterpieces. Next to you on the bridge, a very fat blue-spotted giraffe, last of its kind. What's a moral agent to do? And what if the giraffe... was your mother? All these questions and more won't be answered in this month's meetup.</p>\n\n<p>We may have to taboo a lot of words in the process, so recommended reading: <a href=\"http://lesswrong.com/lw/nu/taboo_your_words/\" rel=\"nofollow\">http://lesswrong.com/lw/nu/taboo_your_words/</a></p>\n\n<p>This meetup marks the two-years anniversary of the LW Brussels meetup group. There will be cake. It is all right to eat the cake, as that cake would've grown up to become Hitler. (It is also all right to <em>bring</em> cake.)</p>\n\n<p>We will meet at 1 pm at La Fleur en papier dor\u00e9, close to the Brussels Central station. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>If you are coming for the first time, please consider filling out this one minute form, to share your contact information: <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform\" rel=\"nofollow\">https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform</a></p>\n\n<p>The Brussels meetup group communicates through a Google Group: <a href=\"https://groups.google.com/forum/#!forum/lesswrong-brussels\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/lesswrong-brussels</a></p>\n\n<p>Meetup announcements are also mirrored on: <a href=\"http://www.meetup.com/LWBrussels/\" rel=\"nofollow\">http://www.meetup.com/LWBrussels/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels__Morality___also_cake1\">Discussion article for the meetup : <a href=\"/meetups/vo\">Brussels: Morality - also cake</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels: Morality - also cake", "anchor": "Discussion_article_for_the_meetup___Brussels__Morality___also_cake", "level": 1}, {"title": "Discussion article for the meetup : Brussels: Morality - also cake", "anchor": "Discussion_article_for_the_meetup___Brussels__Morality___also_cake1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WBdvyyHLdxZSAMmoz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-16T21:09:29.111Z", "modifiedAt": null, "url": null, "title": "Meetup : Boston - Connection Theory", "slug": "meetup-boston-connection-theory", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JJBnTnC8ZLGFbBCDt/meetup-boston-connection-theory", "pageUrlRelative": "/posts/JJBnTnC8ZLGFbBCDt/meetup-boston-connection-theory", "linkUrl": "https://www.lesswrong.com/posts/JJBnTnC8ZLGFbBCDt/meetup-boston-connection-theory", "postedAtFormatted": "Thursday, January 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boston%20-%20Connection%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boston%20-%20Connection%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJJBnTnC8ZLGFbBCDt%2Fmeetup-boston-connection-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boston%20-%20Connection%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJJBnTnC8ZLGFbBCDt%2Fmeetup-boston-connection-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJJBnTnC8ZLGFbBCDt%2Fmeetup-boston-connection-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 128, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vp'>Boston - Connection Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 January 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">98 Elm St Apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Mike Raimondi is giving a talk on Connection Theory, a theory of human motivation developed by the Leverage Institute, and a theoretical foundation for goal charting.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square), except 2nd and 4th Sunday meetups which are held at MIT.</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 3pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vp'>Boston - Connection Theory</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JJBnTnC8ZLGFbBCDt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5169506472895161e-06, "legacy": true, "legacyId": "25280", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boston___Connection_Theory\">Discussion article for the meetup : <a href=\"/meetups/vp\">Boston - Connection Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 January 2014 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">98 Elm St Apt 1, Somerville, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Mike Raimondi is giving a talk on Connection Theory, a theory of human motivation developed by the Leverage Institute, and a theoretical foundation for goal charting.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm at Citadel (98 Elm St Apt 1 Somerville, near Porter Square), except 2nd and 4th Sunday meetups which are held at MIT.</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation.</p>\n\n<p>\u2014Phase 2: The headline event. This starts promptly at 3pm, and lasts 30-60 minutes.</p>\n\n<p>\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups.</p>\n\n<p>\u2014Phase 4: Dinner.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boston___Connection_Theory1\">Discussion article for the meetup : <a href=\"/meetups/vp\">Boston - Connection Theory</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boston - Connection Theory", "anchor": "Discussion_article_for_the_meetup___Boston___Connection_Theory", "level": 1}, {"title": "Discussion article for the meetup : Boston - Connection Theory", "anchor": "Discussion_article_for_the_meetup___Boston___Connection_Theory1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-16T21:53:52.447Z", "modifiedAt": null, "url": null, "title": "Productivity tool: race!", "slug": "productivity-tool-race", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:03.426Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wwa", "createdAt": "2011-12-12T01:43:43.227Z", "isAdmin": false, "displayName": "wwa"}, "userId": "ceumxcAj9mE4tNZJE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DpQx2FaJAGdfrYNQw/productivity-tool-race", "pageUrlRelative": "/posts/DpQx2FaJAGdfrYNQw/productivity-tool-race", "linkUrl": "https://www.lesswrong.com/posts/DpQx2FaJAGdfrYNQw/productivity-tool-race", "postedAtFormatted": "Thursday, January 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Productivity%20tool%3A%20race!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProductivity%20tool%3A%20race!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDpQx2FaJAGdfrYNQw%2Fproductivity-tool-race%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Productivity%20tool%3A%20race!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDpQx2FaJAGdfrYNQw%2Fproductivity-tool-race", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDpQx2FaJAGdfrYNQw%2Fproductivity-tool-race", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 479, "htmlBody": "<p><em>Inspired by recent batch of productivity posts, I wrote my short story</em><em> down</em><em>. To a reasonable extent, this story is real.</em></p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td>2:00</td>\n<td>As usual, I put my breakfast into the microwave and set it to 2:00.</td>\n</tr>\n<tr>\n<td>1:58</td>\n<td>It took two seconds for a train of though to start:</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"Waste, again. Again!\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"What am I supposed to do this entire time? Stare at the clock?\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"Useless and boring, but can't start any serious work or thought in time-frame this short\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"Why don't I switch to Soylent, hire a maid or just eat the damn thing cold?\"</td>\n</tr>\n<tr>\n<td>1:53</td>\n<td>It took me five seconds to notice and derail that train on the basis of \"Been there, done that, nothing significant changed since\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>But this time something went differently.</td>\n</tr>\n<tr>\n<td>1:52</td>\n<td>\"What do I get to lose if I just try to do something, anything?\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"Food's gonna get cold. Also you can't multitask that much, so no thinking either.\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"So what. If I don't make it in time I'll just reheat that food and at least one other thing will be done already. And didn't I just say I can't think of anything serious that fast anyway?\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>It took me 16 seconds to scan today's TODO for anything I had any chance to accomplish within ninety-something seconds.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"Work. Takes too long to start, hardware limitation\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"Read... what? Would take a while to find something new and short enough. I could prepare next time, not now\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>...</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"A shower. Usually takes me at least 5 minutes... but why?\"</td>\n</tr>\n<tr>\n<td>1:36</td>\n<td>I rushed to the bathroom.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"Skip everything that can be skipped, but nothing important\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>Leaving clothes where I stand.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"No time to fiddle with the faucet. Just turn it on roughly around the point where it's supposed to be\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>A bit too cold. So what.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"No time to select soap/shampoo/gel. Just apply top-to-bottom whatever comes up first.\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>Blargh, hair conditioner. Bad idea.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"Note to self: sort this stuff\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>Top-to-bottom, fast moves, keep accurate.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"Come on, come on, come on! I can't believe the microwave didn't finish yet!\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>Head too. Don't skip anything important, remember? One last jet of water and I grab the towel.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"No need to dry the hair so much, you're not going out anytime soon\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>I throw the towel back on the hanger and run to the kitchen. Did it beep already and I didn't hear it? Did it broke?</td>\n</tr>\n<tr>\n<td>0:42</td>\n<td>\"What?!\"</td>\n</tr>\n<tr>\n<td>0:41</td>\n<td>- What?!</td>\n</tr>\n<tr>\n<td>0:20</td>\n<td>For 21 seconds of my Saved Time I allowed myself to stare at the clock to make sure time flows at the same rate it used to.</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"It took me 54 seconds to take an okay shower. A minute and 40 seconds ago I didn't believe it was possible.\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"Not productive.\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"What <em>else</em> can I do?\"</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>\"Now we're talking!\"</td>\n</tr>\n<tr>\n<td>0:001</td>\n<td>I spent the last 20 seconds to build a mental model of what just happened and store it for later experiments...</td>\n</tr>\n<tr>\n<td>BEEP!</td>\n<td>&nbsp;</td>\n</tr>\n<tr>\n<td>&nbsp;</td>\n<td>... and then it hit me, again. I made breakfast, took a shower and thought of something new and possibly significant, all within the time-frame so short I didn't believe possible. I <em>could</em> multitask that much. And I will do better with training.</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2, "udPbn9RthmgTtHMiG": 2, "7thPfS2WbD2JKizr7": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DpQx2FaJAGdfrYNQw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 43, "extendedScore": null, "score": 0.000126, "legacy": true, "legacyId": "25282", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-16T22:30:34.515Z", "modifiedAt": null, "url": null, "title": "[Link] Humanity in Jeopardy - Max Tegmark on AI risk", "slug": "link-humanity-in-jeopardy-max-tegmark-on-ai-risk", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LdPyT3CiQfCWCQfvn/link-humanity-in-jeopardy-max-tegmark-on-ai-risk", "pageUrlRelative": "/posts/LdPyT3CiQfCWCQfvn/link-humanity-in-jeopardy-max-tegmark-on-ai-risk", "linkUrl": "https://www.lesswrong.com/posts/LdPyT3CiQfCWCQfvn/link-humanity-in-jeopardy-max-tegmark-on-ai-risk", "postedAtFormatted": "Thursday, January 16th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Humanity%20in%20Jeopardy%20-%20Max%20Tegmark%20on%20AI%20risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Humanity%20in%20Jeopardy%20-%20Max%20Tegmark%20on%20AI%20risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLdPyT3CiQfCWCQfvn%2Flink-humanity-in-jeopardy-max-tegmark-on-ai-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Humanity%20in%20Jeopardy%20-%20Max%20Tegmark%20on%20AI%20risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLdPyT3CiQfCWCQfvn%2Flink-humanity-in-jeopardy-max-tegmark-on-ai-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLdPyT3CiQfCWCQfvn%2Flink-humanity-in-jeopardy-max-tegmark-on-ai-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 244, "htmlBody": "<p>Article by Max Tegmark for the 3rd anniversary of \"Jeopardy day\" - <a href=\"http://www.huffingtonpost.com/max-tegmark/humanity-in-jeopardy_b_4586992.html\">Humanity in Jeopardy</a>.</p>\n<blockquote>\n<p>Exactly three years ago, on January 13, 2011, we humans were dethroned by a computer on the quiz show <em>Jeopardy!</em>. A year later, a computer was licensed to drive cars in Nevada after being judged safer than a human. What's next? Will computers eventually beat us at all tasks, developing superhuman intelligence?</p>\n<p>[...] After this, life on Earth would never be the same. Whoever or whatever controls this technology would rapidly become the world's wealthiest and most powerful, outsmarting all financial markets, out-inventing and out-patenting all human researchers, and out-manipulating all human leaders. Even if we humans nominally merge with such machines, we might have no guarantees whatsoever about the ultimate outcome, making it feel less like a merger and more like a hostile corporate takeover.</p>\n<p>In summary, will there be a singularity within our lifetime? And is this something we should work for or against? On one hand, it could potentially solve most of our problems, even mortality. It could also open up space, the final frontier: unshackled by the limitations of our human bodies, such advanced life could rise up and eventually make much of our observable universe come alive. On the other hand, it could destroy life as we know it and everything we care about - there are ample doomsday scenarios that look nothing like the Terminator movies, but are far more terrifying.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LdPyT3CiQfCWCQfvn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "25284", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-17T02:16:08.126Z", "modifiedAt": null, "url": null, "title": "H+ review of James Miller's Singularity Rising [link]", "slug": "h-review-of-james-miller-s-singularity-rising-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:01.082Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BrZECvia4TWbBvcjY/h-review-of-james-miller-s-singularity-rising-link", "pageUrlRelative": "/posts/BrZECvia4TWbBvcjY/h-review-of-james-miller-s-singularity-rising-link", "linkUrl": "https://www.lesswrong.com/posts/BrZECvia4TWbBvcjY/h-review-of-james-miller-s-singularity-rising-link", "postedAtFormatted": "Friday, January 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20H%2B%20review%20of%20James%20Miller's%20Singularity%20Rising%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AH%2B%20review%20of%20James%20Miller's%20Singularity%20Rising%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBrZECvia4TWbBvcjY%2Fh-review-of-james-miller-s-singularity-rising-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=H%2B%20review%20of%20James%20Miller's%20Singularity%20Rising%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBrZECvia4TWbBvcjY%2Fh-review-of-james-miller-s-singularity-rising-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBrZECvia4TWbBvcjY%2Fh-review-of-james-miller-s-singularity-rising-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://hplusmagazine.com/2014/01/16/review-singularity-rising-by-james-d-miller/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BrZECvia4TWbBvcjY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -5, "extendedScore": null, "score": 1.5172874039596803e-06, "legacy": true, "legacyId": "25286", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-17T03:41:39.358Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC: Potatoes", "slug": "meetup-washington-dc-potatoes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:28.244Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yX5QHEvPrryft3arY/meetup-washington-dc-potatoes", "pageUrlRelative": "/posts/yX5QHEvPrryft3arY/meetup-washington-dc-potatoes", "linkUrl": "https://www.lesswrong.com/posts/yX5QHEvPrryft3arY/meetup-washington-dc-potatoes", "postedAtFormatted": "Friday, January 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%3A%20Potatoes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%3A%20Potatoes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyX5QHEvPrryft3arY%2Fmeetup-washington-dc-potatoes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%3A%20Potatoes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyX5QHEvPrryft3arY%2Fmeetup-washington-dc-potatoes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyX5QHEvPrryft3arY%2Fmeetup-washington-dc-potatoes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vq'>Washington DC: Potatoes</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 January 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to fully describe potatoes, then try to find them based on the description.</p>\n\n<p>Yes really.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vq'>Washington DC: Potatoes</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yX5QHEvPrryft3arY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.5173813443594028e-06, "legacy": true, "legacyId": "25287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Potatoes\">Discussion article for the meetup : <a href=\"/meetups/vq\">Washington DC: Potatoes</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 January 2014 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to fully describe potatoes, then try to find them based on the description.</p>\n\n<p>Yes really.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Potatoes1\">Discussion article for the meetup : <a href=\"/meetups/vq\">Washington DC: Potatoes</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC: Potatoes", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Potatoes", "level": 1}, {"title": "Discussion article for the meetup : Washington DC: Potatoes", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Potatoes1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-17T04:01:57.662Z", "modifiedAt": null, "url": null, "title": "Community bias in threat evaluation", "slug": "community-bias-in-threat-evaluation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:05.266Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pianoforte611", "createdAt": "2012-05-01T15:15:46.011Z", "isAdmin": false, "displayName": "pianoforte611"}, "userId": "krJ7Se3Mhp9nTX6Hi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ajarvu9K3KTPz2r3k/community-bias-in-threat-evaluation", "pageUrlRelative": "/posts/ajarvu9K3KTPz2r3k/community-bias-in-threat-evaluation", "linkUrl": "https://www.lesswrong.com/posts/ajarvu9K3KTPz2r3k/community-bias-in-threat-evaluation", "postedAtFormatted": "Friday, January 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Community%20bias%20in%20threat%20evaluation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACommunity%20bias%20in%20threat%20evaluation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fajarvu9K3KTPz2r3k%2Fcommunity-bias-in-threat-evaluation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Community%20bias%20in%20threat%20evaluation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fajarvu9K3KTPz2r3k%2Fcommunity-bias-in-threat-evaluation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fajarvu9K3KTPz2r3k%2Fcommunity-bias-in-threat-evaluation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 172, "htmlBody": "<p>If I were to ask the question \"What threat poses the greatest risk to society/humanity?\" to several communities I would expect to get some answers that follow a predictable trend:</p>\n<p>If I asked the question on an HBD blog I'd probably get one of the answers demographic disaster/dysgenics/immigration.</p>\n<p>If I asked the question to a bunch of environmentalists they'd probably say global warming or pollution.</p>\n<p>If I asked the question on a leftist blog I might get the answer: growing inequality/exploitation of workers.</p>\n<p>If I asked the question to Catholic bishops they might say abortion/sexual immorality.</p>\n<p>And if I were to ask the question on LessWrong (which is heavily populated by Computer scientists and programmers) many would respond with unfriendly AI.</p>\n<p>One of these groups might be right, I don't know. However I would treat all of their claims with caution.</p>\n<p>Edit: This may not be a bad from thing from an instrumental rationality perspective. If you think that the problem you're working on is really important then you're more likely to put a good effort into solving it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ajarvu9K3KTPz2r3k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 9, "extendedScore": null, "score": 1.5174036500453842e-06, "legacy": true, "legacyId": "25288", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-17T12:57:04.455Z", "modifiedAt": null, "url": null, "title": "Write down your basic realizations as a matter of habit. Share this file with the world.", "slug": "write-down-your-basic-realizations-as-a-matter-of-habit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:04.542Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FeepingCreature", "createdAt": "2010-07-26T04:00:23.863Z", "isAdmin": false, "displayName": "FeepingCreature"}, "userId": "XZeHMPK6hNC7uTKxM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hxRsp6Y4kwJppjsKk/write-down-your-basic-realizations-as-a-matter-of-habit", "pageUrlRelative": "/posts/hxRsp6Y4kwJppjsKk/write-down-your-basic-realizations-as-a-matter-of-habit", "linkUrl": "https://www.lesswrong.com/posts/hxRsp6Y4kwJppjsKk/write-down-your-basic-realizations-as-a-matter-of-habit", "postedAtFormatted": "Friday, January 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Write%20down%20your%20basic%20realizations%20as%20a%20matter%20of%20habit.%20Share%20this%20file%20with%20the%20world.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWrite%20down%20your%20basic%20realizations%20as%20a%20matter%20of%20habit.%20Share%20this%20file%20with%20the%20world.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhxRsp6Y4kwJppjsKk%2Fwrite-down-your-basic-realizations-as-a-matter-of-habit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Write%20down%20your%20basic%20realizations%20as%20a%20matter%20of%20habit.%20Share%20this%20file%20with%20the%20world.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhxRsp6Y4kwJppjsKk%2Fwrite-down-your-basic-realizations-as-a-matter-of-habit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhxRsp6Y4kwJppjsKk%2Fwrite-down-your-basic-realizations-as-a-matter-of-habit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 516, "htmlBody": "<p>Say you have just realized something that seems, in hindsight, pretty obvious - but that you were totally unaware of before now. You scratch your head, update your model of the world, and move on.</p>\n<p>Hold on. Update your model of your model. <em>Why</em> were you not aware of it before? If this insight, in retrospect, seems so basic, then why did you not realize it earlier? And how many other people do you think missed it as well?</p>\n<p>There is a common situation in user interface development, where you've written a marvellous, clear, easy to use piece of software - after all, you have no problems with it at all! You've stuck to the practice of dogfooding (\"eat your own dog food\" - use your own tool in day to day life) like a good developer should and are now quite faster in it than in whatever you were using before. So find a random coworker who has never touched your software before and may only be passingly familiar with the subject matter. Put him in front of your shiny user interface and watch him waddle around, dumbstruck and befuddled, as he misses all the <em>obvious</em> interface elements and tries nonworking things you'd never attempted in a hundred years. Resist your urge to correct him - pay attention to what he tries.</p>\n<p>You cannot judge the obviousness of an idea from the inside - you have to find somebody without prior exposure and observe them attempt to come to terms with it, to understand. And it only works once - after he's become familiar with your idioms, he'll never be able to show this bright-eyed naivet&eacute; again.</p>\n<p>If you're trying to write a FAQ for life, it'd be very much helpful to have such a coworker go over your life, re-learn all your lessons, all the insights you now consider obvious, and highlight them so you could add them to the help file. But of course, we cannot relive a life. We can only live it once.</p>\n<p>So the obvious thing to do is when you realize that you've just had a novel realization - no matter how apparently trivial - is <strong>write it down</strong>. The brain is really good at caching - realizations become thought habits frighteningly fast. So write it down, so you don't forget that you once had no idea. And after you've written it down (probably on a file on your PC), why not share this file with the world? Might as well let others profit from your insights.</p>\n<p><br />I think it would be useful to spread this habit. Imagine: somebody just did something really clever that raised your estimate of their competence, and oh hey, his user page has a link to a list of insights, maybe I can skip having to manually learn some of those myself? And look, the first piece of advice - \"write down your insights\", that sounds like a good idea!</p>\n<p>It's not proven that this would be useful. But it costs little to start.</p>\n<p><a title=\"Things I Learnt that I wish somebody had told me sooner\" href=\"http://feephome.no-ip.org/~feep/lop/thingsilearnt.cgi\">So here's mine so far</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hxRsp6Y4kwJppjsKk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 21, "extendedScore": null, "score": 1.5179916965536172e-06, "legacy": true, "legacyId": "25289", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-17T13:26:59.378Z", "modifiedAt": null, "url": null, "title": "Open Thread for January 17 - 23 2014", "slug": "open-thread-for-january-17-23-2014", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:38.569Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "niceguyanon", "createdAt": "2012-08-08T00:54:37.281Z", "isAdmin": false, "displayName": "niceguyanon"}, "userId": "BZmMGsoBwFgEkiJGt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N6qYbLYABGqT8cimm/open-thread-for-january-17-23-2014", "pageUrlRelative": "/posts/N6qYbLYABGqT8cimm/open-thread-for-january-17-23-2014", "linkUrl": "https://www.lesswrong.com/posts/N6qYbLYABGqT8cimm/open-thread-for-january-17-23-2014", "postedAtFormatted": "Friday, January 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%20for%20January%2017%20-%2023%202014&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%20for%20January%2017%20-%2023%202014%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN6qYbLYABGqT8cimm%2Fopen-thread-for-january-17-23-2014%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%20for%20January%2017%20-%2023%202014%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN6qYbLYABGqT8cimm%2Fopen-thread-for-january-17-23-2014", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN6qYbLYABGqT8cimm%2Fopen-thread-for-january-17-23-2014", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<h1 style=\"margin: 0px 0px 0.75em; color: #333333; font-size: 20px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\"><span style=\"color: #000000; font-size: small; line-height: 19px;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></h1>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N6qYbLYABGqT8cimm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.5180245830260025e-06, "legacy": true, "legacyId": "25291", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 192, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-17T16:36:20.461Z", "modifiedAt": null, "url": null, "title": "Cryonics As Untested Medical Procedure", "slug": "cryonics-as-untested-medical-procedure", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.174Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ttMZjehANPeLYk8fP/cryonics-as-untested-medical-procedure", "pageUrlRelative": "/posts/ttMZjehANPeLYk8fP/cryonics-as-untested-medical-procedure", "linkUrl": "https://www.lesswrong.com/posts/ttMZjehANPeLYk8fP/cryonics-as-untested-medical-procedure", "postedAtFormatted": "Friday, January 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryonics%20As%20Untested%20Medical%20Procedure&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryonics%20As%20Untested%20Medical%20Procedure%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FttMZjehANPeLYk8fP%2Fcryonics-as-untested-medical-procedure%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryonics%20As%20Untested%20Medical%20Procedure%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FttMZjehANPeLYk8fP%2Fcryonics-as-untested-medical-procedure", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FttMZjehANPeLYk8fP%2Fcryonics-as-untested-medical-procedure", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 567, "htmlBody": "<p>If you're trying to prevent <a href=\"http://en.wikipedia.org/wiki/Information_theoretical_death\">information-theoretic death</a> by <a href=\"/lw/b93\">preserving the brain</a> it's critical that the information that makes you be \"you\" actually be preserved. If you could freeze the brain in a way that did keep around the necessary information then some future civilization might be able to recover the person or the memories, but if the information is gone it's gone for good. The problem is, this is an untested medical procedure, and it's not something we should expect to get right flying blind.</p>\n<p>In freezing a brain there are obvious things that can go wrong. For example, if you just cool it down to below freezing the water in the cells will turn to sharp little ice crystals, disrupting synapse structure and making a huge mess. We know about this now, though, so since the early 2000s cryonics organizations have used \"cryoprotectants\" which are able to vitrify the brain tissue and reduce [1] ice crystal formation. Beyond these known problems, however, there are many aspects of the brain structure that might or might not be relevant. Is information stored in the positions of proteins within the cells? Are phosphorylation states significant? What scale of preservation is sufficient?</p>\n<p>Our <a href=\"http://en.wikipedia.org/wiki/Scientific_method\">normal approach</a> is to try something, see if it works, fix apparent problems, and try again, each cycle getting us closer to something that does work. With cryonics the \"see if it works\" step isn't there, and there's only \"check for known failures\". So what we should expect is that the current process will be \"good to the best of our knowledge\" and then repeatedly our knowledge will expand about what matters and the process will need to be updated.</p>\n<p>(Situations where current preservation technology fails to preserve something we know is required are actually kind of nice, because they're as close as we get to cryonics as an experimental science. Those are the cases when the process can actually improve because the feedback loop is temporarily closed.)</p>\n<p>Imagine if in the development of <a href=\"http://en.wikipedia.org/wiki/History_of_in_vitro_fertilisation\">In-Vitro Fertilization</a> an inexplicable barrier stopped researchers from continuing any experiments past the \"combine egg and sperm\" stage. Instead they worked out something they thought was as good as they were going to get, documented it, and started freezing hopefully-fertilized eggs. How likely would it be that later we would be able to take these frozen eggs and complete the process? Much more likely would be that something unknown was wrong with the beginning of the process and these eggs would actually not be usable. Given that the brain is so much larger and more complex than these zygotes I expect the odds in the cryonics case are much worse.</p>\n<p>Cryonics depends on a complex medical procedure developed under conditions of minimal feedback. Expectations for success like <a href=\"http://www.overcomingbias.com/2009/03/break-cryonics-down.html\">80%</a> or even <a href=\"http://www.overcomingbias.com/2008/12/we-agree-get-froze.html\">more likely than not</a> seem incredibly optimistic. When you can't test the output of a process because you don't know what counts as correct output it's very unlikely you've got the process right.</p>\n<p><small><em>(I also posted this <a href=\"http://www.jefftk.com/p/cryonics-as-untested-medical-procedure\">on my blog</a>.)</em></small></p>\n<p><br /> [1] I say \"reduce\" instead of \"eliminate\" because as far as I can tell no one has actually taken random samples from a human brain that's been preserved with vitrification. There are ethical reasons why the cryonics organizations would not want to do this, but there being reasons why we don't wish to run a test doesn't mean we can act as if we already know the answer.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ttMZjehANPeLYk8fP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 20, "extendedScore": null, "score": 7.1e-05, "legacy": true, "legacyId": "25292", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nk9928vPqoeAMrTh6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-17T17:00:04.600Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-68", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W26QHc4WXrCJWCDzu/weekly-lw-meetups-68", "pageUrlRelative": "/posts/W26QHc4WXrCJWCDzu/weekly-lw-meetups-68", "linkUrl": "https://www.lesswrong.com/posts/W26QHc4WXrCJWCDzu/weekly-lw-meetups-68", "postedAtFormatted": "Friday, January 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW26QHc4WXrCJWCDzu%2Fweekly-lw-meetups-68%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW26QHc4WXrCJWCDzu%2Fweekly-lw-meetups-68", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW26QHc4WXrCJWCDzu%2Fweekly-lw-meetups-68", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 548, "htmlBody": "<p><strong>This summary was posted to LW main on January 10th. The following week's summary is <a href=\"/lw/jim/weekl_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/v7\">[Portland] I moved to Portland! I want to meet you!:&nbsp;<span class=\"date\">11 January 2014 11:50AM</span></a></li>\n<li><a href=\"/meetups/v5\">Sydney Meetup: January:&nbsp;<span class=\"date\">22 January 2014 06:30PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li> <a href=\"/meetups/v9\">Atlanta, January Meetup &amp; Movie Night!:&nbsp;<span class=\"date\">11 January 2014 07:00PM</span></a></li>\n<li><a href=\"/meetups/v8\">Moscow, First New Year:&nbsp;<span class=\"date\">12 January 2014 04:00PM</span></a></li>\n<li><a href=\"/meetups/vg\">Vancouver Biweekly Sequences Discussion: Politics Is the Mind-killer:&nbsp;<span class=\"date\">12 January 2014 03:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">11 January 2020 01:30PM</span></a></li>\n<li><a href=\"/meetups/vd\">Boston - Macroeconomic Theory (Joe Schneider):&nbsp;<span class=\"date\">12 January 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/uo\">Brussels monthly meetup: Futurology!:&nbsp;<span class=\"date\">11 January 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/vf\">London - The Schelling Point Strategy Game (plus socials):&nbsp;<span class=\"date\">19 January 2014 02:00AM</span></a></li>\n<li><a href=\"/meetups/vb\">Melbourne Social Meetup:&nbsp;<span class=\"date\">17 January 2014 05:42PM</span></a></li>\n<li><a href=\"/meetups/uv\">Vienna:&nbsp;<span class=\"date\">18 January 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W26QHc4WXrCJWCDzu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5182588697436e-06, "legacy": true, "legacyId": "25225", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["X6kxHthFG58kRLuhk", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-17T17:09:17.097Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Montreal - Dissolving Disagreements", "slug": "meetup-less-wrong-montreal-dissolving-disagreements", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:01.564Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bartimaeus", "createdAt": "2013-05-07T17:14:04.389Z", "isAdmin": false, "displayName": "bartimaeus"}, "userId": "mqWrbcZHzhfPLnJqg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2rvmHTHw7PjbmXpLN/meetup-less-wrong-montreal-dissolving-disagreements", "pageUrlRelative": "/posts/2rvmHTHw7PjbmXpLN/meetup-less-wrong-montreal-dissolving-disagreements", "linkUrl": "https://www.lesswrong.com/posts/2rvmHTHw7PjbmXpLN/meetup-less-wrong-montreal-dissolving-disagreements", "postedAtFormatted": "Friday, January 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Montreal%20-%20Dissolving%20Disagreements&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Montreal%20-%20Dissolving%20Disagreements%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2rvmHTHw7PjbmXpLN%2Fmeetup-less-wrong-montreal-dissolving-disagreements%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Montreal%20-%20Dissolving%20Disagreements%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2rvmHTHw7PjbmXpLN%2Fmeetup-less-wrong-montreal-dissolving-disagreements", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2rvmHTHw7PjbmXpLN%2Fmeetup-less-wrong-montreal-dissolving-disagreements", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 312, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vr'>Less Wrong Montreal - Dissolving Disagreements</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 January 2014 12:08:27PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">3459 McTavish, Montr\u00e9al, qc, ca</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There's a question that you may sometimes ask yourselves when you are having an argument with someone: \"Wait a minute...what are we ACTUALLY disagreeing about, exactly?\" Sometimes, we get so lost in a web of ideas that we've lost track of which way points to reality.\nWe'll be looking at some more 5-second-level mental skills, this time aimed at clarifying what you're discussing and clearing up confusion that can be caused by the way we use words. The human brain has a particular method of classifying things using words; although it does its job very well most of the time, sometimes this classification method just doesn't work. We'll try to recognize situations where words might be confusing us, and how we can fix this.\nThe location worked out well last time, so we'll stick with it for this time. I'll confirm which pod once it's reserved. Here are the instructions as posted last time:\nDue to construction, you need to:\n-Enter the McLennan Library (the building on the northeast corner of McTavish and Sherbrooke, street view of the entrance: <a href=\"https://maps.google.com/?ll=45.503599,-73.576131&spn=0.001746,0.004128&t=m&z=19&layer=c&cbll=45.503583,-73.576193&panoid=42_osk2KlEyXwc_eJsOYXQ&cbp=12,143.89,,0,11.22\" rel=\"nofollow\">https://maps.google.com/?ll=45.503599,-73.576131&spn=0.001746,0.004128&t=m&z=19&layer=c&cbll=45.503583,-73.576193&panoid=42_osk2KlEyXwc_eJsOYXQ&cbp=12,143.89,,0,11.22</a>)\n-Head towards the Redpath library (the building directly north of the McLennan library; so you need to turn right twice right after you walk in)\n-On the left, there is a door leading towards some stairs (there should be signs for Cybertheque or something similar). Go down one level.\n-You are now in a library with a bunch of glass \"pods\" in the middle. We are in POD 4 (the names are on the doors).\nThese directions might be very confusing, so don't hesitate to message me and I'll provide my phone number so you can call me.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vr'>Less Wrong Montreal - Dissolving Disagreements</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2rvmHTHw7PjbmXpLN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.518268995535118e-06, "legacy": true, "legacyId": "25295", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Montreal___Dissolving_Disagreements\">Discussion article for the meetup : <a href=\"/meetups/vr\">Less Wrong Montreal - Dissolving Disagreements</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 January 2014 12:08:27PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">3459 McTavish, Montr\u00e9al, qc, ca</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There's a question that you may sometimes ask yourselves when you are having an argument with someone: \"Wait a minute...what are we ACTUALLY disagreeing about, exactly?\" Sometimes, we get so lost in a web of ideas that we've lost track of which way points to reality.\nWe'll be looking at some more 5-second-level mental skills, this time aimed at clarifying what you're discussing and clearing up confusion that can be caused by the way we use words. The human brain has a particular method of classifying things using words; although it does its job very well most of the time, sometimes this classification method just doesn't work. We'll try to recognize situations where words might be confusing us, and how we can fix this.\nThe location worked out well last time, so we'll stick with it for this time. I'll confirm which pod once it's reserved. Here are the instructions as posted last time:\nDue to construction, you need to:\n-Enter the McLennan Library (the building on the northeast corner of McTavish and Sherbrooke, street view of the entrance: <a href=\"https://maps.google.com/?ll=45.503599,-73.576131&amp;spn=0.001746,0.004128&amp;t=m&amp;z=19&amp;layer=c&amp;cbll=45.503583,-73.576193&amp;panoid=42_osk2KlEyXwc_eJsOYXQ&amp;cbp=12,143.89,,0,11.22\" rel=\"nofollow\">https://maps.google.com/?ll=45.503599,-73.576131&amp;spn=0.001746,0.004128&amp;t=m&amp;z=19&amp;layer=c&amp;cbll=45.503583,-73.576193&amp;panoid=42_osk2KlEyXwc_eJsOYXQ&amp;cbp=12,143.89,,0,11.22</a>)\n-Head towards the Redpath library (the building directly north of the McLennan library; so you need to turn right twice right after you walk in)\n-On the left, there is a door leading towards some stairs (there should be signs for Cybertheque or something similar). Go down one level.\n-You are now in a library with a bunch of glass \"pods\" in the middle. We are in POD 4 (the names are on the doors).\nThese directions might be very confusing, so don't hesitate to message me and I'll provide my phone number so you can call me.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Montreal___Dissolving_Disagreements1\">Discussion article for the meetup : <a href=\"/meetups/vr\">Less Wrong Montreal - Dissolving Disagreements</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Montreal - Dissolving Disagreements", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Montreal___Dissolving_Disagreements", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Montreal - Dissolving Disagreements", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Montreal___Dissolving_Disagreements1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-17T20:35:27.562Z", "modifiedAt": null, "url": null, "title": "Meetup : Rationality Meetup Ljubljana", "slug": "meetup-rationality-meetup-ljubljana", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:05.672Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ratcourse", "createdAt": "2012-11-03T10:26:05.641Z", "isAdmin": false, "displayName": "Ratcourse"}, "userId": "qwnfbBpAcbxLT4JBr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9NPCTCfbMvmACSH3S/meetup-rationality-meetup-ljubljana", "pageUrlRelative": "/posts/9NPCTCfbMvmACSH3S/meetup-rationality-meetup-ljubljana", "linkUrl": "https://www.lesswrong.com/posts/9NPCTCfbMvmACSH3S/meetup-rationality-meetup-ljubljana", "postedAtFormatted": "Friday, January 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Rationality%20Meetup%20Ljubljana&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Rationality%20Meetup%20Ljubljana%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9NPCTCfbMvmACSH3S%2Fmeetup-rationality-meetup-ljubljana%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Rationality%20Meetup%20Ljubljana%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9NPCTCfbMvmACSH3S%2Fmeetup-rationality-meetup-ljubljana", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9NPCTCfbMvmACSH3S%2Fmeetup-rationality-meetup-ljubljana", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vs'>Rationality Meetup Ljubljana</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 January 2014 05:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Trubarjeva 44</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting at Kavarna Ma\u010dkon.\nAt least two organizers from the Vienna meetups are joining, and as such the meetup will be conducted in english.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vs'>Rationality Meetup Ljubljana</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9NPCTCfbMvmACSH3S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.5184957455583212e-06, "legacy": true, "legacyId": "25296", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Rationality_Meetup_Ljubljana\">Discussion article for the meetup : <a href=\"/meetups/vs\">Rationality Meetup Ljubljana</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 January 2014 05:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Trubarjeva 44</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting at Kavarna Ma\u010dkon.\nAt least two organizers from the Vienna meetups are joining, and as such the meetup will be conducted in english.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Rationality_Meetup_Ljubljana1\">Discussion article for the meetup : <a href=\"/meetups/vs\">Rationality Meetup Ljubljana</a></h2>", "sections": [{"title": "Discussion article for the meetup : Rationality Meetup Ljubljana", "anchor": "Discussion_article_for_the_meetup___Rationality_Meetup_Ljubljana", "level": 1}, {"title": "Discussion article for the meetup : Rationality Meetup Ljubljana", "anchor": "Discussion_article_for_the_meetup___Rationality_Meetup_Ljubljana1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-17T22:57:22.853Z", "modifiedAt": null, "url": null, "title": "Continuity in Uploading", "slug": "continuity-in-uploading", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:36.511Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Error", "createdAt": "2012-09-14T14:09:41.325Z", "isAdmin": false, "displayName": "Error"}, "userId": "uLxJE9XRTCBrzn8uL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RyaEZiNDtciYsZ3nH/continuity-in-uploading", "pageUrlRelative": "/posts/RyaEZiNDtciYsZ3nH/continuity-in-uploading", "linkUrl": "https://www.lesswrong.com/posts/RyaEZiNDtciYsZ3nH/continuity-in-uploading", "postedAtFormatted": "Friday, January 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Continuity%20in%20Uploading&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AContinuity%20in%20Uploading%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRyaEZiNDtciYsZ3nH%2Fcontinuity-in-uploading%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Continuity%20in%20Uploading%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRyaEZiNDtciYsZ3nH%2Fcontinuity-in-uploading", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRyaEZiNDtciYsZ3nH%2Fcontinuity-in-uploading", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 422, "htmlBody": "<p>I don't acknowledge an upload as \"me\" in any meaningful sense of the term; if I copied my brain to a computer and then my body was destroyed, I still think of that as death and would try to avoid it.</p>\n<p>A thought struck me a few minutes ago that seems like it might get around that, though. Suppose that rather than copying my brain, I adjoined it to some external computer in a kind of reverse-<a href=\"/lw/ps/where_physics_meets_experience/\">Ebborian</a> act; electrically connecting my synapses to a big block of computrons that I can consciously perform I/O to. Over the course of life and improved tech, that block expands until, as a percentage, most of my thought processes are going on in the machine-part of me. Eventually my meat brain dies -- but the silicon part of me lives on. I think I would probably still consider that \"me\" in a meaningful sense. Intuitively I feel like I should treat it as the equivalent of minor brain damage.</p>\n<p>Obviously, one could shorten the period of dual-life arbitrarily and I can't point to a specific line where expanded-then-contracted-consciousness turns into copying-then-death. The line that immediately comes to mind is \"whenever I start to feel like the technological expansion of my mind is no longer an external module, but the main component,\" but that feels like unjustified punting.</p>\n<p>I'm curious what other people think, particularly those that share my position on destructive uploads.</p>\n<p>---</p>\n<p>Edited to add:</p>\n<div id=\"body_t1_aeab\" class=\"comment-content \">Solipsist asked me for the <a href=\"/r/discussion/lw/jip/continuity_in_uploading/aea6\">reasoning behind my position</a> on destructive uploads, which led to this additional train of thought: <br /></div>\n<div class=\"comment-content \">\n<div class=\"md\">\n<p>Compare a destructive upload to non-destructive. Copy my mind to a machine non-destructively, and I still identify with meat-me. You could let machine-me run for a day, or a week, or a year, and only then kill off meat-me. I don't like that option and would be confused by someone who did. Destructive uploads feel like the limit of that case, where the time interval approaches zero and I am killed and copied in the same moment. As with the case outlined above, I don't see a crossed line where it stops being death and starts being transition.</p>\n<p>An expand-contract with interval zero is effectively a destructive upload. So is a copy-kill with interval zero. So the two appear to be mirror images, with a discontinuity at the limit. Approach destructive uploads from the copy-then-kill side, and it feels clearly like death. Approach them from the expand-then-contract side, and it feels like continuous identity. Yet at the limit between them they turn into the same operation.</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RyaEZiNDtciYsZ3nH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 6, "extendedScore": null, "score": 1.5186518652236836e-06, "legacy": true, "legacyId": "25297", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WajiC3YWeJutyAXTn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-17T23:21:27.387Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh Meetup 24 January 2014 06:00 PM", "slug": "meetup-pittsburgh-meetup-24-january-2014-06-00-pm", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:01.705Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Duing", "createdAt": "2009-03-06T07:13:31.573Z", "isAdmin": false, "displayName": "Matt_Duing"}, "userId": "QEv5PFb6cmjkrprtR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KvHoaHDxMcmPDz8Nz/meetup-pittsburgh-meetup-24-january-2014-06-00-pm", "pageUrlRelative": "/posts/KvHoaHDxMcmPDz8Nz/meetup-pittsburgh-meetup-24-january-2014-06-00-pm", "linkUrl": "https://www.lesswrong.com/posts/KvHoaHDxMcmPDz8Nz/meetup-pittsburgh-meetup-24-january-2014-06-00-pm", "postedAtFormatted": "Friday, January 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%20Meetup%2024%20January%202014%2006%3A00%20PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%20Meetup%2024%20January%202014%2006%3A00%20PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKvHoaHDxMcmPDz8Nz%2Fmeetup-pittsburgh-meetup-24-january-2014-06-00-pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%20Meetup%2024%20January%202014%2006%3A00%20PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKvHoaHDxMcmPDz8Nz%2Fmeetup-pittsburgh-meetup-24-january-2014-06-00-pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKvHoaHDxMcmPDz8Nz%2Fmeetup-pittsburgh-meetup-24-january-2014-06-00-pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vt'>Pittsburgh Meetup 24 January 2014 06:00 PM</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 January 2014 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Eat Unique, 305 South Craig Street, Pittsburgh, PA 15213</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting from 6:00 PM - 8:00 PM to to socialize, discuss our goals for 2014, and plan topics for upcoming meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vt'>Pittsburgh Meetup 24 January 2014 06:00 PM</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KvHoaHDxMcmPDz8Nz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5186783521512439e-06, "legacy": true, "legacyId": "25298", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh_Meetup_24_January_2014_06_00_PM\">Discussion article for the meetup : <a href=\"/meetups/vt\">Pittsburgh Meetup 24 January 2014 06:00 PM</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 January 2014 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Eat Unique, 305 South Craig Street, Pittsburgh, PA 15213</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting from 6:00 PM - 8:00 PM to to socialize, discuss our goals for 2014, and plan topics for upcoming meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh_Meetup_24_January_2014_06_00_PM1\">Discussion article for the meetup : <a href=\"/meetups/vt\">Pittsburgh Meetup 24 January 2014 06:00 PM</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh Meetup 24 January 2014 06:00 PM", "anchor": "Discussion_article_for_the_meetup___Pittsburgh_Meetup_24_January_2014_06_00_PM", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh Meetup 24 January 2014 06:00 PM", "anchor": "Discussion_article_for_the_meetup___Pittsburgh_Meetup_24_January_2014_06_00_PM1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-17T23:43:46.585Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: Nomic", "slug": "meetup-urbana-champaign-nomic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:29.455Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8hPPXBoa5K7WNvDQT/meetup-urbana-champaign-nomic", "pageUrlRelative": "/posts/8hPPXBoa5K7WNvDQT/meetup-urbana-champaign-nomic", "linkUrl": "https://www.lesswrong.com/posts/8hPPXBoa5K7WNvDQT/meetup-urbana-champaign-nomic", "postedAtFormatted": "Friday, January 17th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20Nomic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20Nomic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8hPPXBoa5K7WNvDQT%2Fmeetup-urbana-champaign-nomic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20Nomic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8hPPXBoa5K7WNvDQT%2Fmeetup-urbana-champaign-nomic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8hPPXBoa5K7WNvDQT%2Fmeetup-urbana-champaign-nomic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vu'>Urbana-Champaign: Nomic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 January 2014 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">40.109545,-88.227318</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Ever played <a href=\"http://en.wikipedia.org/wiki/Nomic\" rel=\"nofollow\">Nomic</a>? Neither have I. But I am curious to see what LessWrongers will make of it.</p>\n\n<p>Coordinates are: 40.109545,-88.227318\nMeetup will be held in the the Courtyard Cafe in the Illini Union, on the ground floor, at 2PM.</p>\n\n<p>Cross-posted on the <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/Wn1ahCxGSVs\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vu'>Urbana-Champaign: Nomic</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8hPPXBoa5K7WNvDQT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.5187029083871592e-06, "legacy": true, "legacyId": "25299", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Nomic\">Discussion article for the meetup : <a href=\"/meetups/vu\">Urbana-Champaign: Nomic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 January 2014 02:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">40.109545,-88.227318</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Ever played <a href=\"http://en.wikipedia.org/wiki/Nomic\" rel=\"nofollow\">Nomic</a>? Neither have I. But I am curious to see what LessWrongers will make of it.</p>\n\n<p>Coordinates are: 40.109545,-88.227318\nMeetup will be held in the the Courtyard Cafe in the Illini Union, on the ground floor, at 2PM.</p>\n\n<p>Cross-posted on the <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/Wn1ahCxGSVs\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Nomic1\">Discussion article for the meetup : <a href=\"/meetups/vu\">Urbana-Champaign: Nomic</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: Nomic", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Nomic", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: Nomic", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Nomic1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-18T09:46:16.988Z", "modifiedAt": null, "url": null, "title": "Meetup : February Rationality Dojo: Planning Fallacy", "slug": "meetup-february-rationality-dojo-planning-fallacy", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "CeTijoorgtNBeSrZG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BHBFweZZHRiycWARm/meetup-february-rationality-dojo-planning-fallacy", "pageUrlRelative": "/posts/BHBFweZZHRiycWARm/meetup-february-rationality-dojo-planning-fallacy", "linkUrl": "https://www.lesswrong.com/posts/BHBFweZZHRiycWARm/meetup-february-rationality-dojo-planning-fallacy", "postedAtFormatted": "Saturday, January 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20February%20Rationality%20Dojo%3A%20Planning%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20February%20Rationality%20Dojo%3A%20Planning%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHBFweZZHRiycWARm%2Fmeetup-february-rationality-dojo-planning-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20February%20Rationality%20Dojo%3A%20Planning%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHBFweZZHRiycWARm%2Fmeetup-february-rationality-dojo-planning-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHBFweZZHRiycWARm%2Fmeetup-february-rationality-dojo-planning-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vv'>February Rationality Dojo: Planning Fallacy</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 February 2014 03:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Isengard, 5 Caraval Lane, Docklands, VIC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Less Wrong Sunday Rationality Dojos are crafted to be serious self-improvement sessions for those committed to the Art of Rationality and personal growth. Each month a community member will run a session involving a presentation of content, discussion, and exercises.</p>\n\n<p>Following our first immensely successful Sunday Rationality Dojo, Brayden will present in February on the Planning Fallacy.\n<a href=\"http://lesswrong.com/lw/jg/planning_fallacy/\" rel=\"nofollow\">http://lesswrong.com/lw/jg/planning_fallacy/</a></p>\n\n<p>Additionally, we will review the personal goals we committed to at the previous Dojo (I will have done X by the next Dojo). Scott Fowler recorded the commitments, if you didn't make it but would like to add your own goal to the records, send him a message (shokwave.sf@gmail.com).</p>\n\n<p>The Dojo is likely to run for 2-3 hours, after which some people will get dinner together.</p>\n\n<p>If you would like to present at a future Dojo or suggest a topic, please fill it in on the Rationality Dojo Roster: <a href=\"http://is.gd/dojoroster\" rel=\"nofollow\">http://is.gd/dojoroster</a></p>\n\n<p>RSVP on the facebook event: <a href=\"https://www.facebook.com/events/1450425828502790/\" rel=\"nofollow\">https://www.facebook.com/events/1450425828502790/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vv'>February Rationality Dojo: Planning Fallacy</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BHBFweZZHRiycWARm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [], "voteCount": 0, "baseScore": 0, "extendedScore": null, "score": 1.5193660470942205e-06, "legacy": true, "legacyId": "25301", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___February_Rationality_Dojo__Planning_Fallacy\">Discussion article for the meetup : <a href=\"/meetups/vv\">February Rationality Dojo: Planning Fallacy</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 February 2014 03:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Isengard, 5 Caraval Lane, Docklands, VIC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Less Wrong Sunday Rationality Dojos are crafted to be serious self-improvement sessions for those committed to the Art of Rationality and personal growth. Each month a community member will run a session involving a presentation of content, discussion, and exercises.</p>\n\n<p>Following our first immensely successful Sunday Rationality Dojo, Brayden will present in February on the Planning Fallacy.\n<a href=\"http://lesswrong.com/lw/jg/planning_fallacy/\" rel=\"nofollow\">http://lesswrong.com/lw/jg/planning_fallacy/</a></p>\n\n<p>Additionally, we will review the personal goals we committed to at the previous Dojo (I will have done X by the next Dojo). Scott Fowler recorded the commitments, if you didn't make it but would like to add your own goal to the records, send him a message (shokwave.sf@gmail.com).</p>\n\n<p>The Dojo is likely to run for 2-3 hours, after which some people will get dinner together.</p>\n\n<p>If you would like to present at a future Dojo or suggest a topic, please fill it in on the Rationality Dojo Roster: <a href=\"http://is.gd/dojoroster\" rel=\"nofollow\">http://is.gd/dojoroster</a></p>\n\n<p>RSVP on the facebook event: <a href=\"https://www.facebook.com/events/1450425828502790/\" rel=\"nofollow\">https://www.facebook.com/events/1450425828502790/</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___February_Rationality_Dojo__Planning_Fallacy1\">Discussion article for the meetup : <a href=\"/meetups/vv\">February Rationality Dojo: Planning Fallacy</a></h2>", "sections": [{"title": "Discussion article for the meetup : February Rationality Dojo: Planning Fallacy", "anchor": "Discussion_article_for_the_meetup___February_Rationality_Dojo__Planning_Fallacy", "level": 1}, {"title": "Discussion article for the meetup : February Rationality Dojo: Planning Fallacy", "anchor": "Discussion_article_for_the_meetup___February_Rationality_Dojo__Planning_Fallacy1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CPm5LTwHrvBJCa9h5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": null, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-18T13:10:01.806Z", "modifiedAt": null, "url": null, "title": "The Onrushing Wave", "slug": "the-onrushing-wave", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:04.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PS6i9XN7sNiAnJYvW/the-onrushing-wave", "pageUrlRelative": "/posts/PS6i9XN7sNiAnJYvW/the-onrushing-wave", "linkUrl": "https://www.lesswrong.com/posts/PS6i9XN7sNiAnJYvW/the-onrushing-wave", "postedAtFormatted": "Saturday, January 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Onrushing%20Wave&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Onrushing%20Wave%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPS6i9XN7sNiAnJYvW%2Fthe-onrushing-wave%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Onrushing%20Wave%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPS6i9XN7sNiAnJYvW%2Fthe-onrushing-wave", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPS6i9XN7sNiAnJYvW%2Fthe-onrushing-wave", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 205, "htmlBody": "<p>There's a long article in this week's The Economist:</p>\n<h1><a href=\"http://www.economist.com/news/briefing/21594264-previous-technological-innovation-has-always-delivered-more-long-run-employment-not-less\">The onrushing wave</a></h1>\n<p>discussing the effect of changing technology upon the amount of employment available in different sectors of the economy.</p>\n<p>Sample paragraph from it:</p>\n<blockquote>\n<p><span style=\"font-size: 15px; line-height: 23px; color: #4a4a4a; font-family: Arial, sans-serif;\">The case for a highly disruptive period of economic growth is made by Erik Brynjolfsson and Andrew McAfee, professors at MIT, in &ldquo;The Second Machine Age&rdquo;, a book to be published later this month. Like the first great era of industrialisation, they argue, it should deliver enormous benefits&mdash;but not without a period of disorienting and uncomfortable change. Their argument rests on an underappreciated aspect of the exponential growth in chip processing speed, memory capacity and other computer metrics: that the amount of progress computers will make in the next few years is always equal to the progress they have made since the very beginning. Mr Brynjolfsson and Mr McAfee reckon that the main bottleneck on innovation is the time it takes society to sort through the many combinations and permutations of new technologies and business models.</span></p>\n</blockquote>\n<p>(There's a summary online of their previous book:&nbsp;<a href=\"http://ebusiness.mit.edu/research/Briefs/Brynjolfsson_McAfee_Race_Against_the_Machine.pdf\">Race Against The Machine: How the Digital Revolution is Accelerating Innovation, Driving Productivity, and Irreversibly Transforming Employment and the Economy</a>)</p>\n<p>&nbsp;</p>\n<p>What do people think are society's practical options for coping with this change?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PS6i9XN7sNiAnJYvW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 1, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "25303", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-18T20:13:09.414Z", "modifiedAt": null, "url": null, "title": "Tell Culture", "slug": "tell-culture", "viewCount": null, "lastCommentedAt": "2020-11-10T21:07:28.959Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrienneYudkowsky", "createdAt": "2013-05-13T00:07:08.935Z", "isAdmin": false, "displayName": "LoganStrohl"}, "userId": "uuYBzWLiixkbN3s7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rEBXN3x6kXgD4pLxs/tell-culture", "pageUrlRelative": "/posts/rEBXN3x6kXgD4pLxs/tell-culture", "linkUrl": "https://www.lesswrong.com/posts/rEBXN3x6kXgD4pLxs/tell-culture", "postedAtFormatted": "Saturday, January 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tell%20Culture&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATell%20Culture%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrEBXN3x6kXgD4pLxs%2Ftell-culture%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tell%20Culture%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrEBXN3x6kXgD4pLxs%2Ftell-culture", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrEBXN3x6kXgD4pLxs%2Ftell-culture", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 841, "htmlBody": "<p><em><strong>Followup to: <a href=\"/lw/375/ask_and_guess/\">Ask and Guess</a></strong></em></p>\n<p>Ask culture: \"I'll be in town this weekend for a business trip. Is it cool if I crash at your place?\" Response: &ldquo;Yes&ldquo; or &ldquo;no&rdquo;.</p>\n<p>Guess culture: \"Hey, great news! I'll be in town this weekend for a business trip!\" Response: Infer that they might be telling you this because they want something from you, conclude that they might want a place to stay, and offer your hospitality only if you want to. Otherwise, pretend you didn&rsquo;t infer that. <br /><br />The two basic rules of Ask Culture: 1) Ask when you want something. 2) Interpret things as requests and feel free to say \"no\".</p>\n<p>The two basic rules of Guess Culture: 1) Ask for things if, and *only* if, you're confident the person will say \"yes\". 2) Interpret requests as expectations of \"yes\", and, when possible, avoid saying \"no\". <br /><br />Both approaches come with costs and benefits. In the end, I feel pretty strongly that Ask is superior.&nbsp;</p>\n<p>But these are not the only two possibilities!<br /><br />\"I'll be in town this weekend for a business trip. I would like to stay at your place, since it would save me the cost of a hotel, plus I would enjoy seeing you and expect we&rsquo;d have some fun. I'm looking for other options, though, and would rather stay elsewhere than inconvenience you.\" Response: &ldquo;I think I need some space this weekend. But I&rsquo;d love to get a beer or something while you&rsquo;re in town!&rdquo; or &ldquo;You should totally stay with me. I&rsquo;m looking forward to it.&rdquo; <br /><br />There is a third alternative, and I think it's probably what rationalist communities ought to strive for. I call it \"Tell Culture\". <br /><br />The two basic rules of Tell Culture: 1) Tell the other person what's going on in your own mind whenever you suspect you'd both benefit from them knowing. (Do NOT assume others will accurately model your mind without your help, or that it will even occur to them to ask you questions to eliminate their ignorance.) 2) Interpret things people tell you as attempts to create common knowledge for shared benefit, rather than as requests or as presumptions of compliance.</p>\n<p>Suppose you&rsquo;re in a conversation that you&rsquo;re finding aversive, and you can&rsquo;t figure out why. Your goal is to procure a rain check.</p>\n<ul>\n<li>Guess: *You see this annoyed body language? Huh? Look at it! If you don&rsquo;t stop talking soon I swear I&rsquo;ll start tapping my foot.* (Or, possibly, tell a little lie to excuse yourself. &ldquo;Oh, look at the time&hellip;&rdquo;)&nbsp;</li>\n<li>Ask: &ldquo;Can we talk about this another time?&rdquo;</li>\n<li>Tell: \"I'm beginning to find this conversation aversive, and I'm not sure why. I propose we hold off until I've figured that out.\"</li>\n</ul>\n<p>Here are more examples from my own life:</p>\n<ul>\n<li>\"I didn't sleep well last night and am feeling frazzled and irritable today. I apologize if I snap at you during this meeting. It isn&rsquo;t personal.\"&nbsp;</li>\n<li>\"I just realized this interaction will be far more productive if my brain has food. I think we should head toward the kitchen.\"&nbsp;</li>\n<li>\"It would be awfully convenient networking for me to stick around for a bit after our meeting to talk with you and [the next person you're meeting with]. But on a scale of one to ten, it's only about 3 useful to me. If you'd rate the loss of utility for you as two or higher, then I have a strong preference for not sticking around.\"&nbsp;</li>\n</ul>\n<p>The burden of honesty is even greater in Tell culture than in Ask culture. To a Guess culture person, I imagine much of the above sounds passive aggressive or manipulative, much worse than the rude bluntness of mere Ask. It&rsquo;s because Guess people aren&rsquo;t expecting relentless truth-telling, which is exactly what&rsquo;s necessary here.</p>\n<p>If you&rsquo;re occasionally dishonest and tell people you want things you don't actually care about--like their comfort or convenience--they&rsquo;ll learn not to trust you, and the inherent freedom of the system will be lost. They&rsquo;ll learn that you only pretend to care about them to take advantage of their reciprocity instincts, when in fact you&rsquo;ll count them as having defected if they respond by stating a preference for protecting their own interests.</p>\n<p>Tell culture is cooperation with open source codes.</p>\n<p>This kind of trust does not develop overnight. Here is the most useful Tell tactic I know of for developing that trust with a native Ask or Guess. It&rsquo;s saved me sooooo much time and trouble, and I wish I&rsquo;d thought of it earlier.</p>\n<p>\"I'm not asking because I expect you to say &lsquo;yes&rsquo;. I'm asking because I'm having trouble imagining the inside of your head, and I want to understand better. You are completely free to say &lsquo;no&rsquo;, or to tell me what you&rsquo;re thinking right now, and I promise it will be fine.\" It is amazing how often people quickly stop looking shifty and say 'no' after this, or better yet begin to discuss further details.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AADZcNS24mmSfPp2w": 8}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rEBXN3x6kXgD4pLxs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 183, "baseScore": 217, "extendedScore": null, "score": 0.000553, "legacy": true, "legacyId": "25300", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 217, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 228, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vs3kzjLhbdKsndnBy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 14, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-18T20:56:50.724Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA: Surreal Numbers", "slug": "meetup-west-la-surreal-numbers", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k59Rscp9kRMTxCayJ/meetup-west-la-surreal-numbers", "pageUrlRelative": "/posts/k59Rscp9kRMTxCayJ/meetup-west-la-surreal-numbers", "linkUrl": "https://www.lesswrong.com/posts/k59Rscp9kRMTxCayJ/meetup-west-la-surreal-numbers", "postedAtFormatted": "Saturday, January 18th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%3A%20Surreal%20Numbers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%3A%20Surreal%20Numbers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk59Rscp9kRMTxCayJ%2Fmeetup-west-la-surreal-numbers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%3A%20Surreal%20Numbers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk59Rscp9kRMTxCayJ%2Fmeetup-west-la-surreal-numbers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk59Rscp9kRMTxCayJ%2Fmeetup-west-la-surreal-numbers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/vw\">West LA: Surreal Numbers</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">22 January 2014 07:00:00PM (-0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>How to get in: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the Westside Pavillion on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n<p>Parking is free for 3 hours.</p>\n<p>Discussion: 'In the beginning, everything was void, and J. H. W. H. Conway began to create numbers. Conway said, \"Let there be two rules which bring forth all numbers large and small. This shall be the first rule: Every number corresponds to two sets of previously created numbers, such that no member of the left set is greater than or equal to any member of the right set. And the second rule shall be this: One number is less than or equal to another number if and only if no member of the first number's left set is greater than or equal to the second number, and no member of the second number's right set is less than or equal to the first number.\" And Conway examined these two rules he had made, and behold! They were very good.' -D. E. Knuth</p>\n<p>No prior knowledge of or exposure to Less Wrong is necessary; this will be generally accessible.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/vw\">West LA: Surreal Numbers</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k59Rscp9kRMTxCayJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.520104692406317e-06, "legacy": true, "legacyId": "25307", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA__Surreal_Numbers\">Discussion article for the meetup : <a href=\"/meetups/vw\">West LA: Surreal Numbers</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">22 January 2014 07:00:00PM (-0800)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>How to get in: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the Westside Pavillion on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n<p>Parking is free for 3 hours.</p>\n<p>Discussion: 'In the beginning, everything was void, and J. H. W. H. Conway began to create numbers. Conway said, \"Let there be two rules which bring forth all numbers large and small. This shall be the first rule: Every number corresponds to two sets of previously created numbers, such that no member of the left set is greater than or equal to any member of the right set. And the second rule shall be this: One number is less than or equal to another number if and only if no member of the first number's left set is greater than or equal to the second number, and no member of the second number's right set is less than or equal to the first number.\" And Conway examined these two rules he had made, and behold! They were very good.' -D. E. Knuth</p>\n<p>No prior knowledge of or exposure to Less Wrong is necessary; this will be generally accessible.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___West_LA__Surreal_Numbers1\">Discussion article for the meetup : <a href=\"/meetups/vw\">West LA: Surreal Numbers</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA: Surreal Numbers", "anchor": "Discussion_article_for_the_meetup___West_LA__Surreal_Numbers", "level": 1}, {"title": "Discussion article for the meetup : West LA: Surreal Numbers", "anchor": "Discussion_article_for_the_meetup___West_LA__Surreal_Numbers1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-19T02:47:03.011Z", "modifiedAt": "2021-08-12T19:33:45.566Z", "url": null, "title": "Dark Arts of Rationality", "slug": "dark-arts-of-rationality", "viewCount": null, "lastCommentedAt": "2022-01-14T01:57:23.589Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4DBBQkEQvNEWafkek/dark-arts-of-rationality", "pageUrlRelative": "/posts/4DBBQkEQvNEWafkek/dark-arts-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/4DBBQkEQvNEWafkek/dark-arts-of-rationality", "postedAtFormatted": "Sunday, January 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dark%20Arts%20of%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADark%20Arts%20of%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4DBBQkEQvNEWafkek%2Fdark-arts-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dark%20Arts%20of%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4DBBQkEQvNEWafkek%2Fdark-arts-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4DBBQkEQvNEWafkek%2Fdark-arts-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5222, "htmlBody": "<p>Today, we're going to talk about Dark rationalist techniques: productivity tools which seem incoherent, mad, and downright irrational. These techniques include:</p>\n<ol>\n<li>Willful Inconsistency</li>\n<li>Intentional Compartmentalization</li>\n<li>Modifying Terminal Goals</li>\n</ol>\n<p>I expect many of you are already up in arms. It seems obvious that consistency is a virtue, that compartmentalization is a flaw, and that one should <em>never</em> modify their terminal goals.</p>\n<p>I claim that these 'obvious' objections are incorrect, and that all three of these techniques can be instrumentally rational.</p>\n<p>In this article, I'll promote the strategic cultivation of false beliefs and condone mindhacking on the values you hold most dear. Truly, these are Dark Arts. I aim to convince you that sometimes, the benefits are worth the price.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"changingyourterminalgoals\">Changing your Terminal Goals</h1>\n<p>In many games there is no \"absolutely optimal\" strategy. Consider the <a href=\"http://wiki.lesswrong.com/wiki/Prisoner's_dilemma\">Prisoner's Dilemma</a>. The optimal strategy depends entirely upon the strategies of the other players. <em>Entirely.</em></p>\n<p>Intuitively, you may believe that there are some fixed \"rational\" strategies. Perhaps you think that even though complex behavior is dependent upon other players, there are still <em>some</em> constants, like \"Never cooperate with DefectBot\". DefectBot always defects against you, so you should never cooperate with it. Cooperating with DefectBot would be insane. Right?</p>\n<p>Wrong. If you find yourself on a playing field where everyone else is a <a href=\"http://intelligence.org/files/RobustCooperation.pdf\">TrollBot</a> (players who cooperate with you if and only if you cooperate with DefectBot) then you should cooperate with DefectBots and defect against TrollBots.</p>\n<p>Consider that. There are playing fields where you should <em>cooperate with DefectBot</em>, even though that looks completely insane from a na&iuml;ve viewpoint.<em>&nbsp;</em>Optimality is not a feature of the strategy, it is a relationship between the strategy and the playing field.</p>\n<p>Take this lesson to heart: in certain games, there are strange playing fields where the optimal move looks <em>completely irrational</em>.</p>\n<p>I'm here to convince you that <em>life</em> is one of those games, and that you occupy a strange playing field&nbsp;<em>right now</em>.</p>\n<hr />\n<p>Here's a toy example of a strange playing field, which illustrates the fact that even your terminal goals are not sacred:</p>\n<p>Imagine that you are completely self-consistent and have a utility function. For the sake of the thought experiment, pretend that your terminal goals are distinct, exclusive, orthogonal, and clearly labeled. You value your goals being achieved, but you have no preferences about <em>how</em> they are achieved or what happens afterwards (unless the goal explicitly mentions the past/future, in which case achieving the goal puts limits on the past/future). You possess at least two terminal goals, one of which we will call <code>A</code>.</p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Omega\">Omega</a> descends from on high and makes you an offer. Omega will cause your terminal goal <code>A</code> to become achieved over a certain span of time, without any expenditure of resources. As a price of taking the offer, you must switch out terminal goal <code>A</code> for terminal goal <code>B</code>. Omega guarantees that <code>B</code> is orthogonal to <code>A</code> and all your other terminal goals. Omega further guarantees that you will achieve <code>B</code> using less time and resources than you would have spent on <code>A</code>. Any other concerns you have are addressed via similar guarantees.</p>\n<p>Clearly, you should take the offer. One of your terminal goals will be achieved, and while you'll be pursuing a new terminal goal that you (before the offer) don't care about, you'll come out ahead in terms of time and resources which can be spent achieving your other goals.</p>\n<p>So the optimal move, in this scenario, is to change your terminal goals.</p>\n<p><em>There are times when the optimal move of a rational agent is to hack its own terminal goals.</em></p>\n<p>You may find this counter-intuitive. It helps to remember that \"optimality\" depends as much upon the playing field as upon the strategy.</p>\n<p>Next, I claim that such scenarios not restricted to toy games where Omega messes with your head. Humans encounter similar situations on a day-to-day basis.</p>\n<hr />\n<p>Humans often find themselves in a position where they should modify their terminal goals, and the reason is simple: our thoughts do not have direct control over our motivation.</p>\n<p>Unfortunately for us, our \"motivation circuits\" can distinguish between terminal and instrumental goals. It is often easier to put in effort, experience inspiration, and work tirelessly when pursuing a terminal goal as opposed to an instrumental goal. It would be nice if this were not the case, but it's a <em>fact of our hardware</em>: we're going to do X more if we want to do X for its own sake as opposed to when we force X upon ourselves.</p>\n<p>Consider, for example, a young woman who wants to be a rockstar. She wants the fame, the money, and the lifestyle: these are her \"terminal goals\". She lives in some strange world where rockstardom is wholly dependent upon merit (rather than social luck and network effects), and decides that in order to become a rockstar she has to produce really good music.</p>\n<p>But here's the problem: She's a human. Her conscious decisions don't directly affect her motivation.</p>\n<p>In her case, it turns out that she can make better music when \"Make Good Music\" is a terminal goal as opposed to an instrumental goal.</p>\n<p>When \"Make Good Music\" is an instrumental goal, she schedules practice time on a sitar and grinds out the hours. But she doesn't really <em>like</em> it, so she cuts corners whenever akrasia comes knocking. She lacks inspiration and spends her spare hours dreaming of stardom. Her songs are shallow and trite.</p>\n<p>When \"Make Good Music\" is a terminal goal, music pours forth, and she spends every spare hour playing her sitar: not because she knows that she \"should\" practice, but because you couldn't pry her sitar from her cold dead fingers. She's not \"practicing\", she's pouring out her soul, and no power in the 'verse can stop her. Her songs are emotional, deep, and moving.</p>\n<p>It's obvious that she should adopt a new terminal goal.</p>\n<p>Ideally, we would be just as motivated to carry out instrumental goals as we are to carry out terminal goals. In reality, this is not the case. As a human, your motivation system <em>does</em> discriminate between the goals that you feel obligated to achieve and the goals that you pursue as ends unto themselves.</p>\n<p>As such, it is sometimes in your best interest to modify your terminal goals.</p>\n<hr />\n<p>Mind the terminology, here. When I speak of \"terminal goals\" I mean actions that feel like ends unto themselves. I am speaking of the stuff you wish you were doing when you're doing boring stuff, the things you do in your free time just because they are&nbsp;<em>fun</em>, the actions you don't need to justify.</p>\n<p>This seems like the obvious meaning of \"terminal goals\" to me, but some of you may think of \"terminal goals\" more akin to self-endorsed morally sound end-values in some consistent utility function. I'm not talking about those. I'm not even convinced I have any.</p>\n<p>Both types of \"terminal goal\" are susceptible to strange playing fields in which the optimal move is to change your goals, but it is only the former type of goal &mdash; the actions that are simply&nbsp;<em>fun</em>, that need no justification &mdash; which I'm suggesting you tweak for instrumental reasons.</p>\n<hr />\n<p>I've largely refrained from goal-hacking, personally. I bring it up for a few reasons:</p>\n<ol>\n<li>It's the easiest Dark Side technique to justify. It helps break people out of the mindset where they think optimal actions are the ones that look rational in a vacuum. Remember, optimality is a feature of the playing field. Sometimes cooperating with DefectBot is the best strategy!</li>\n<li>Goal hacking segues nicely into the other Dark Side techniques which I use frequently, as you will see shortly.</li>\n<li>I have met many people who would benefit from a solid bout of goal-hacking.</li>\n</ol>\n<p>I've crossed paths with many a confused person who (without any explicit thought on their part) had really silly terminal goals. We've all met people who are acting as if \"Acquire Money\" is a terminal goal, never noticing that money is almost entirely instrumental in nature. When you ask them \"but what would you do if money was no issue and you had a lot of time\", all you get is a blank stare.</p>\n<p>Even the <a href=\"http://wiki.lesswrong.com/wiki/Terminal_value\">LessWrong Wiki entry</a> on terminal values describes a college student for which university is instrumental, and getting a job is terminal. This seems like a clear-cut case of a <a href=\"/lw/le/lost_purposes/\">Lost Purpose</a>: a job seems clearly instrumental. And yet, we've all met people who act as if \"Have a Job\" is a terminal value, and who then seem aimless and undirected after finding employment.</p>\n<p>These people could use some goal hacking. You can argue that Acquire Money and Have a Job aren't \"really\" terminal goals, to which I counter that many people don't know their ass from their elbow when it comes to their own goals. Goal hacking is an important part of becoming a rationalist and/or improving mental health.</p>\n<p>Goal-hacking in the name of consistency isn't really a Dark Side power. This power is only Dark when you use it like the musician in our example, when you adopt terminal goals for instrumental reasons. This form of goal hacking is less common, but can be very effective.</p>\n<p>I recently had a personal conversation with <a href=\"/user/Alexei/\">Alexei</a>, who is earning to give. He noted that he was not entirely satisfied with his day-to-day work, and mused that perhaps goal-hacking (making \"Do Well at Work\" an end unto itself) could make him more effective, generally happier, and more productive in the long run.</p>\n<p>Goal-hacking can be a powerful technique, when correctly applied. Remember, you're not in direct control of your motivation circuits. Sometimes, strange though it seems, the optimal action involves fooling <em>yourself</em>.</p>\n<p>You don't get good at programming by sitting down and forcing yourself to practice for three hours a day. I mean, I suppose you <em>could</em> get good at programming that way. But it's much easier to get good at programming by <em>loving programming</em>, by being the type of person who spends every spare hour tinkering on a project. Because then it doesn't feel like practice, it feels like fun.</p>\n<p>This is the power that you can harness, if you're willing to tamper with your terminal goals for instrumental reasons. As rationalists, we would prefer to dedicate to instrumental goals the same vigor that is reserved for terminal goals. Unfortunately, we find ourselves on a strange playing field where goals that feel justified in their own right win the lion's share of our attention.</p>\n<p>Given this strange playing field, goal-hacking can be optimal.</p>\n<p>You don't have to completely mangle your goal system. Our aspiring musician from earlier doesn't need to destroy her \"Become a Rockstar\" goal in order to adopt the \"Make Good Music\" goal. If you can successfully convince yourself to believe that something instrumental is a means unto itself (e.g. terminal), <em>while still believing that it is instrumental</em>, then more power to you.</p>\n<p>This is, of course, an instance of Intentional Compartmentalization.</p>\n<h1 id=\"intentionalcompartmentalization\">Intentional Compartmentalization</h1>\n<p>As soon as you endorse modifying your own terminal goals, Intentional Compartmentalization starts looking like a pretty good idea. If Omega offers to achieve <code>A </code>at the price of dropping&nbsp;<span style=\"font-family: monospace;\">A </span>and adopting&nbsp;<code>B</code>, the ideal move is to take the offer after finding a way to not <em>actually </em>care about&nbsp;<span style=\"font-family: monospace;\">B</span>.</p>\n<p>A consistent agent cannot do this, but I have good news for you: You're a human. You're not consistent. In fact, you're <em>great</em> at being inconsistent!</p>\n<p>You might expect it to be difficult to add a new terminal goal while still believing that it's instrumental. You may also run into strange situations where holding an instrumental goal as terminal <em>directly contradicts</em>&nbsp;other terminal goals.</p>\n<p>For example, our aspiring musician might find that she makes even <em>better</em> music if \"Become a Rockstar\" is <em>not</em> among her terminal goals.</p>\n<p>This means she's in trouble: She either has to drop \"Become a Rockstar\" and have a better chance at <em>actually becoming a rockstar</em>, or she has to settle for a decreased chance that she'll become a rockstar.</p>\n<p>Or, rather, she would have to settle for one of these choices &mdash; if she wasn't human.</p>\n<p>I have good news! Humans are <em>really really</em> good at being inconsistent, and you can leverage this to your advantage. <a href=\"http://wiki.lesswrong.com/wiki/Compartmentalization\">Compartmentalize</a>! Maintain goals that are \"terminal\" in one compartment, but which you know are \"instrumental\" in another, then simply never let those compartments touch!</p>\n<p>This may sound completely crazy and irrational, but remember: <a href=\"http://prettyrational.com/61/\">you aren't actually in control of your motivation system</a>. You find yourself on a strange playing field, and the optimal move may in fact require mental contortions that make epistemic rationalists shudder.</p>\n<p>Hopefully you never run into this particular problem (holding contradictory goals in \"terminal\" positions), but this illustrates that there are scenarios where compartmentalization works in your favor. Of course we'd <em>prefer</em>&nbsp;to have direct control of our motivation systems, but <em>given that we don't</em>, compartmentalization is a huge asset.</p>\n<p>Take a moment and let this sink in before moving on.</p>\n<p>Once you realize that compartmentalization is OK, you are ready to practice my second Dark Side technique: Intentional Compartmentalization. It has many uses outside the realm of goal-hacking.</p>\n<p>See, motivation is a fickle beast. And, as you'll remember, your conscious choices are not directly attached to your motivation levels. You can't just <em>decide</em> to be more motivated.</p>\n<p>At least, not directly.</p>\n<p>I've found that certain beliefs &mdash; beliefs which I <em>know are wrong</em> &mdash; can make me more productive. (On a related note, remember that <a href=\"/lw/5t/can_humanism_match_religions_output/\">religious organizations are generally more coordinated than rationalist groups</a>.)</p>\n<p>It turns out that, under these false beliefs, I can tap into motivational reserves that are otherwise unavailable. The only problem is, I know that these beliefs are downright false.</p>\n<p>I'm just kidding, that's not actually a problem. Compartmentalization to the rescue!</p>\n<p>Here's a couple example beliefs that I keep locked away in my mental compartments, bound up in chains. Every so often, when I need to be extra productive, I don my protective gear and enter these compartments. I never fully believe these things &mdash; not globally, at least &mdash; but I'm capable of attaining \"local belief\", of acting as if I hold these beliefs. This, it turns out, is enough.</p>\n<h2 id=\"nothingisbeyondmygrasp\">Nothing is Beyond My Grasp</h2>\n<p>We'll start off with a tame belief, something that is soundly rooted in evidence outside of its little compartment.</p>\n<p>I have a global belief, outside all my compartments, that nothing is beyond my grasp.</p>\n<p>Others may understand things easier I do or faster than I do. People smarter than myself grok concepts with less effort than I. It may take me <em>years</em>&nbsp;to wrap my head around things that other people find trivial.&nbsp;However, there is no idea that a human has ever had that I cannot, <em>in principle</em>, grok.</p>\n<p>I believe this with moderately high probability, just based on my own general intelligence and the fact that brains are so tightly clustered in mind-space. It may take me a hundred times the effort to understand something, but I can still understand it eventually. Even things that are beyond the grasp of a meager human mind, I will one day be able to grasp after I upgrade my brain. Even if there are limits imposed by reality, I could <em>in principle</em> overcome them if I had enough computing power. Given any finite idea, I could in theory become powerful enough to understand it.</p>\n<p>This belief, itself, is not compartmentalized. What is compartmentalized is the <em>certainty</em>.</p>\n<p>Inside the compartment, I believe that Nothing is Beyond My Grasp with 100% confidence. Note that this is ridiculous: there's no such thing as 100% confidence. At least, not in my global beliefs. But inside the compartments, while we're in la-la land, it helps to treat Nothing is Beyond My Grasp as raw, immutable <em>fact</em>.</p>\n<p>You might think that it's sufficient to believe Nothing is Beyond My Grasp with very high probability. If that's the case, you haven't been listening: I <em>don't</em> actually believe Nothing is Beyond My Grasp with an extraordinarily high probability. I believe it with moderate probability, and then I&nbsp;<em>have a compartment</em> in which it's a certainty.</p>\n<p>It would be <em>nice</em> if I never needed to use the compartment, if I could face down technical problems and incomprehensible lingo and being really out of my depth with a relatively high confidence that I'm going to be able to make sense of it all. However, I'm not in direct control of my motivation. And it turns out that, through some quirk in my psychology, it's easier to face down the oppressive feeling of being in <em>way over my head</em> if I have this rock-solid \"belief\"&nbsp;that Nothing is Beyond My Grasp.</p>\n<p>This is what the compartments are good for: I don't actually believe the things inside them, but I can still <em>act as if I do</em>. That ability allows me to face down challenges that would be difficult to face down otherwise.</p>\n<p>This compartment was largely constructed with the help of <a href=\"http://en.wikipedia.org/wiki/The_Phantom_Tollbooth\">The Phantom Tollbooth</a>: it taught me that there are certain impossible tasks you can do if you think they're possible. It's not always enough to know that if I believe I can do a thing, then I have a higher probability of being able to do it. I get an extra boost from believing I can do&nbsp;<em>anything</em>.</p>\n<p>You might be surprised about how much you can do when you have a mental compartment in which you are <em>unstoppable</em>.</p>\n<h2 id=\"mywillpowerdoesnotdeplete\">My Willpower Does Not Deplete</h2>\n<p>Here's another: My Willpower Does Not Deplete.</p>\n<p>Ok, so my willpower actually does deplete. I've been writing about how it does, and discussing methods that I use to avoid depletion. <em>Right now</em>, I'm writing about how I've acknowledged the fact that my willpower <em>does deplete</em>.</p>\n<p>But I have this compartment where it doesn't.</p>\n<p>Ego depletion is a funny thing. If you don't believe in ego depletion, you suffer <a href=\"http://pss.sagepub.com/content/early/2010/09/28/0956797610384745\">less ego depletion</a>. This <a href=\"http://www.sciencedirect.com/science/article/pii/S0022103112000509\">does not eliminate ego depletion</a>.</p>\n<p>Knowing this, I have a compartment in which My Willpower Does Not Deplete. I go there often, when I'm studying. It's easy, I think, for one to begin to feel tired, and say \"oh, this must be ego depletion, I can't work anymore.\" Whenever my brain tries to go there, I wheel this bad boy out of his cage. \"Nope\", I respond, \"My Willpower Does Not Deplete\".</p>\n<p>Surprisingly, this often works. I won't force myself to keep working, but I'm pretty good at preventing mental escape attempts via \"phantom akrasia\". I don't allow myself to invoke ego depletion or akrasia to stop being productive, because My Willpower Does Not Deplete. I have to <em>actually be tired out</em>, in a way that doesn't trigger the My Willpower Does Not Deplete safeguards. This doesn't let me keep going forever, but it prevents a lot of false alarms.</p>\n<p>In my experience, the strong version (My Willpower Does Not Deplete) is much more effective than the weak version (My Willpower is Not Depleted Yet), even though it's more wrong. This probably says something about my personality. Your mileage may vary. Keep in mind, though, that the effectiveness of your mental compartments may depend more on the motivational content than on degree of falsehood.</p>\n<h2 id=\"anythingisaplacebo\">Anything is a Placebo</h2>\n<p>Placebos work <a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0015591\">even when you know they are placebos</a>.</p>\n<p>This is the sort of madness I'm talking about, when I say things like \"you're on a strange playing field\".</p>\n<p>Knowing this, you can easily activate the placebo effect manually. Feeling sick? Here's a freebie: drink more water. It will make you feel better.</p>\n<p>No? It's just a placebo, you say? Doesn't matter. Tell yourself that water makes it better. Put that in a nice little compartment, save it for later. It doesn't matter that you know what you're doing: your brain is easily fooled.</p>\n<p>Want to be more productive, be healthier, and exercise more effectively? Try using Anything is a Placebo! Pick something trivial and non-harmful and tell yourself that it helps you perform better. Put the belief in a compartment in which you <em>act as if</em> you believe the thing. Cognitive dissonance doesn't matter! Your brain is <em>great</em> at ignoring cognitive dissonance. You can \"know\" you're wrong in the global case, while \"believing\" you're right locally.</p>\n<p>For bonus points, try combining objectives. Are you constantly underhydrated? Try believing that drinking more water makes you more alert!</p>\n<p>Brains are weird.</p>\n<hr />\n<p>Truly, these are the Dark Arts of instrumental rationality. Epistemic rationalists recoil in horror as I advocate <em>intentionally cultivating false beliefs. </em>It goes without saying that you should use this technique with care. Remember to always audit your compartmentalized beliefs through the lens of your actual beliefs, and be very careful not to let incorrect beliefs leak out of their compartments.</p>\n<p>If you think you can achieve similar benefits without \"fooling yourself\", then by all means, do so. I haven't been able to find effective alternatives. Brains have been honing compartmentalization techniques for <em>eons</em>, so I figure I might as well re-use the hardware.</p>\n<p>It's important to reiterate that these techniques are necessary because&nbsp;<em>you're not actually in control of your own motivation</em>. Sometimes, incorrect beliefs make you more motivated. Intentionally cultivating incorrect beliefs is surely a path to the Dark Side: compartmentalization only mitigates the damage. If you make sure you segregate the bad beliefs and acknowledge them for what they are then you can get much of the benefit without paying the cost, but there is still a cost, and the currency is cognitive dissonance.</p>\n<p>At this point, you should be mildly uncomfortable. After all, I'm advocating something which is completely epistemically irrational. We're not done yet, though.</p>\n<p>I have one more Dark Side technique, and it's worse.</p>\n<h1 id=\"willfulinconsistency\">Willful Inconsistency</h1>\n<p>I use Intentional Compartmentalization to \"locally believe\" things that I don't \"globally believe\", in cases where the local belief makes me more productive. In this case, the beliefs in the compartments are things that I tell myself. They're like mantras that I repeat in my head, at the System 2 level. System 1 is fragmented and compartmentalized, and happily obliges.</p>\n<p>Willful Inconsistency is the grown-up, scary version of Intentional Compartmentalization. It involves convincing System 1 wholly and entirely of something that System 2 does not actually believe. There's no compartmentalization and no fragmentation. There's nowhere to shove the incorrect belief when you're done with it. It's taken over the intuition, and it's always on. Willful Inconsistency is about having gut-level intuitive beliefs that you explicitly disavow.</p>\n<p>Your intuitions run the show whenever you're not paying attention, so if you're willfully inconsistent then you're going to actually <em>act as if</em> these incorrect beliefs are true in your day-to-day life, unless your forcibly override your default actions. Ego depletion and distraction make you vulnerable <em>to yourself</em>.</p>\n<p>Use this technique with caution.</p>\n<p>This may seem insane even to those of you who took the previous suggestions in stride. That you must sometimes alter your terminal goals is a feature of the playing field, not the agent. The fact that you are not in direct control of your motivation system readily implies that tricking yourself is useful, and compartmentalization is an obvious way to mitigate the damage.</p>\n<p>But why would anyone ever try to convince themselves, deep down at the core, of something that they don't actually believe?</p>\n<p>The answer is simple: specialization.</p>\n<p>To illustrate, let me explain how I use willful inconsistency.</p>\n<p>I have invoked Willful Inconsistency on only two occasions, and they were similar in nature. Only one instance of Willful Inconsistency is currently active, and it works like this:</p>\n<p>I have completely and totally convinced my intuitions that unfriendly AI is a problem. A big problem. System 1 operates under the assumption that UFAI will come to pass in the next twenty years with very high probability.</p>\n<p>You can imagine how this is somewhat motivating.</p>\n<p>On the conscious level, within System 2, I'm much less certain. I solidly believe that UFAI is a big problem, and that it's the problem that I should be focusing my efforts on. However, my error bars are <em>far</em> wider, my timespan is quite broad. I acknowledge a decent probability of soft takeoff. I assign moderate probabilities to a number of other existential threats. I think there are a large number of unknown unknowns, and there's a non-zero chance that the status quo continues until I die (and that I can't later be brought back). All this I know.</p>\n<p>But, <em>right now</em>, as I type this, my intuition is screaming at me that the above is all wrong, that my error bars are narrow, and that I don't <em>actually</em> expect the status quo to continue for even thirty years.</p>\n<p>This is just how I like things.</p>\n<p>See, I <em>am</em> convinced that building a friendly AI is the most important problem for me to be working on, <em>even though</em> there is a very real chance that MIRI's research won't turn out to be crucial. Perhaps other existential risks will get to us first. Perhaps we'll get brain uploads and Robin Hanson's emulation economy. Perhaps it's going to take far longer than expected to crack general intelligence. However, after much reflection I have concluded that despite the uncertainty, this is where I should focus my efforts.</p>\n<p>The problem is, it's hard to translate that decision down to System 1.</p>\n<p>Consider a toy scenario, where there are ten problems in the world. Imagine that, in the face of uncertainty and diminishing returns from research effort, I have concluded that the world should allocate 30% of resources to problem A, 25% to problem B, 10% to problem C, and 5% to each of the remaining problems.</p>\n<p>Because specialization leads to massive benefits, it's much more effective to dedicate 30% of researchers to working on problem A rather than having all researchers dedicate 30% of their time to problem A. So presume that, in light of these conclusions, I decide to dedicate myself to problem A.</p>\n<p>Here we have a problem: I'm supposed to specialize in problem A, but at the intuitive level problem A isn't <em>that</em>&nbsp;big a deal. It's only 30% of the problem space, after all, and it's not really that much worse than problem B.</p>\n<p>This would be no issue if I were in control of my own motivation system: I could put the blinders on and focus on problem A, crank the motivation knob to maximum, and trust everyone else to focus on the other problems and do their part.</p>\n<p>But I'm not in control of my motivation system. If my intuitions know that there are a number of other similarly worthy problems that I'm ignoring, if they are distracted by other issues of similar scope, then I'm tempted to work on everything at once. This is bad, because output is maximized if we all specialize.</p>\n<p>Things get especially bad when problem A is highly uncertain and unlikely to affect people for decades if not centuries. It's very hard to convince the monkey brain to care about far-future vagaries, <em>even if</em>&nbsp;I've rationally concluded that those are where I should dedicate my resources.</p>\n<p>I find myself on a strange playing field, where the optimal move is to lie to System 1.</p>\n<p>Allow me to make that more concrete:</p>\n<p>I'm <em>much</em> more motivated to do FAI research when I'm intuitively convinced that we have a hard 15 year timer until UFAI.</p>\n<p>Explicitly, I believe UFAI is one possibility among many and that the timeframe should be measured in decades rather than years. I've concluded that it is my most pressing concern, but I don't <em>actually</em> believe we have a hard 15 year countdown.</p>\n<p>That said, it's hard to understate how useful it is to have a gut-level feeling that there's a short, hard timeline. This \"knowledge\" pushes the monkey brain to go all out, no holds barred. In other words, this is the method by which I convince myself to <em>actually</em>&nbsp;specialize.</p>\n<p>This is how I convince myself to deploy every available resource, to attack the problem as if the stakes were incredibly high. Because the stakes <em>are</em>&nbsp;incredibly high, and I <em>do</em>&nbsp;need to deploy every available resource, even if we don't have a hard 15 year timer.</p>\n<p>In other words, Willful Inconsistency is the technique I use to force my intuition to <em>feel as if</em>&nbsp;the stakes are as high as I've calculated them to be, given that my monkey brain is bad at responding to uncertain vague future problems. Willful Inconsistency is my counter to <a href=\"/lw/hw/scope_insensitivity/\">Scope Insensitivity</a>: my intuition has difficulty believing the results when I <a href=\"http://wiki.lesswrong.com/wiki/Shut_up_and_multiply\">do the multiplication</a>, so I lie to it until it acts with appropriate vigor.</p>\n<p>This is the final secret weapon in my motivational arsenal.</p>\n<p>I don't personally recommend that you try this technique. It can have harsh side effects, including feelings of guilt, intense stress, and massive amounts of cognitive dissonance. I'm able to do this in large part because I'm in a very good headspace. I went into this with full knowledge of what I was doing, and I am confident that I can back out (and actually correct my intuitions) if the need arises.</p>\n<p>That said, I've found that cultivating a gut-level feeling that what you're doing <em>must</em> be done, and must be done <em>quickly</em>, is an extraordinarily good motivator. It's such a strong motivator that I seldom explicitly acknowledge it. I don't need to mentally invoke \"we have to study or the world ends\". Rather, this knowledge lingers in the background. It's not a mantra, it's not something that I repeat and wear thin. Instead, it's this gut-level drive that sits underneath it all, that makes me strive to go faster unless I explicitly try to slow down.</p>\n<p>This monkey-brain tunnel vision, combined with a long habit of productivity, is what keeps me <a href=\"/lw/jh0/deregulating_distraction_moving_towards_the_goal/\">Moving Towards the Goal</a>.</p>\n<hr />\n<p>Those are my Dark Side techniques: Willful Inconsistency, Intentional Compartmentalization, and Terminal Goal Modification.</p>\n<p>I expect that these techniques will be rather controversial. If I may be so bold, I recommend that discussion focus on goal-hacking and intentional compartmentalization. I acknowledge that willful inconsistency is unhealthy and I don't generally recommend that others try it. By contrast, both goal-hacking and intentional compartmentalization are quite sane and, indeed, instrumentally rational.</p>\n<p>These are certainly not techniques that I would recommend CFAR teach to newcomers, and I remind you that \"it is dangerous to be half a rationalist\". You can royally screw you over if you're still figuring out your beliefs as you attempt to compartmentalize false beliefs. I recommend only using them when you're sure of what your goals are and confident about the borders between your actual beliefs and your intentionally false \"beliefs\".</p>\n<p>It may be surprising that changing terminal goals can be an optimal strategy, and that humans should consider adopting incorrect beliefs strategically. At the least, I encourage you to remember that there are no absolutely rational actions.</p>\n<p>Modifying your own goals and cultivating false beliefs are useful because we live in strange, hampered control systems. Your brain was optimized with <a href=\"/lw/l3/thou_art_godshatter/\">no concern for truth</a>, and optimal performance may require <a href=\"/lw/cn/instrumental_vs_epistemic_a_bardic_perspective/\">self deception</a>. I remind the uncomfortable that instrumental rationality is not about being the most consistent or the most correct, it's about <em>winning.&nbsp;</em>There are games where the optimal move requires adopting false beliefs, and if you find yourself playing one of those games, then you should adopt false beliefs. Instrumental rationality and epistemic rationality can be pitted against each other.</p>\n<p>We are fortunate, as humans, to be skilled at compartmentalization: this helps us work around our mental handicaps without sacrificing epistemic rationality. Of course, we'd rather not have the mental handicaps in the first place: but you have to work with what you're given.</p>\n<p>We <em>are</em> weird agents without full control of our own minds. We lack direct control over important aspects of ourselves. For that reason, it's often necessary to take actions that may seem contradictory, crazy, or downright irrational.</p>\n<p>Just remember this, before you condemn these techniques: optimality is as much an aspect of the playing field as of the strategy, and humans occupy a strange playing field indeed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XYHzLjwYiqpeqaf4c": 3, "KWFhr6A2dHEb6wmWJ": 2, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4DBBQkEQvNEWafkek", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 203, "baseScore": 240, "extendedScore": null, "score": 0.000616, "legacy": true, "legacyId": "25264", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 242, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Today, we're going to talk about Dark rationalist techniques: productivity tools which seem incoherent, mad, and downright irrational. These techniques include:</p>\n<ol>\n<li>Willful Inconsistency</li>\n<li>Intentional Compartmentalization</li>\n<li>Modifying Terminal Goals</li>\n</ol>\n<p>I expect many of you are already up in arms. It seems obvious that consistency is a virtue, that compartmentalization is a flaw, and that one should <em>never</em> modify their terminal goals.</p>\n<p>I claim that these 'obvious' objections are incorrect, and that all three of these techniques can be instrumentally rational.</p>\n<p>In this article, I'll promote the strategic cultivation of false beliefs and condone mindhacking on the values you hold most dear. Truly, these are Dark Arts. I aim to convince you that sometimes, the benefits are worth the price.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"Changing_your_Terminal_Goals\">Changing your Terminal Goals</h1>\n<p>In many games there is no \"absolutely optimal\" strategy. Consider the <a href=\"http://wiki.lesswrong.com/wiki/Prisoner's_dilemma\">Prisoner's Dilemma</a>. The optimal strategy depends entirely upon the strategies of the other players. <em>Entirely.</em></p>\n<p>Intuitively, you may believe that there are some fixed \"rational\" strategies. Perhaps you think that even though complex behavior is dependent upon other players, there are still <em>some</em> constants, like \"Never cooperate with DefectBot\". DefectBot always defects against you, so you should never cooperate with it. Cooperating with DefectBot would be insane. Right?</p>\n<p>Wrong. If you find yourself on a playing field where everyone else is a <a href=\"http://intelligence.org/files/RobustCooperation.pdf\">TrollBot</a> (players who cooperate with you if and only if you cooperate with DefectBot) then you should cooperate with DefectBots and defect against TrollBots.</p>\n<p>Consider that. There are playing fields where you should <em>cooperate with DefectBot</em>, even though that looks completely insane from a na\u00efve viewpoint.<em>&nbsp;</em>Optimality is not a feature of the strategy, it is a relationship between the strategy and the playing field.</p>\n<p>Take this lesson to heart: in certain games, there are strange playing fields where the optimal move looks <em>completely irrational</em>.</p>\n<p>I'm here to convince you that <em>life</em> is one of those games, and that you occupy a strange playing field&nbsp;<em>right now</em>.</p>\n<hr>\n<p>Here's a toy example of a strange playing field, which illustrates the fact that even your terminal goals are not sacred:</p>\n<p>Imagine that you are completely self-consistent and have a utility function. For the sake of the thought experiment, pretend that your terminal goals are distinct, exclusive, orthogonal, and clearly labeled. You value your goals being achieved, but you have no preferences about <em>how</em> they are achieved or what happens afterwards (unless the goal explicitly mentions the past/future, in which case achieving the goal puts limits on the past/future). You possess at least two terminal goals, one of which we will call <code>A</code>.</p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Omega\">Omega</a> descends from on high and makes you an offer. Omega will cause your terminal goal <code>A</code> to become achieved over a certain span of time, without any expenditure of resources. As a price of taking the offer, you must switch out terminal goal <code>A</code> for terminal goal <code>B</code>. Omega guarantees that <code>B</code> is orthogonal to <code>A</code> and all your other terminal goals. Omega further guarantees that you will achieve <code>B</code> using less time and resources than you would have spent on <code>A</code>. Any other concerns you have are addressed via similar guarantees.</p>\n<p>Clearly, you should take the offer. One of your terminal goals will be achieved, and while you'll be pursuing a new terminal goal that you (before the offer) don't care about, you'll come out ahead in terms of time and resources which can be spent achieving your other goals.</p>\n<p>So the optimal move, in this scenario, is to change your terminal goals.</p>\n<p><em>There are times when the optimal move of a rational agent is to hack its own terminal goals.</em></p>\n<p>You may find this counter-intuitive. It helps to remember that \"optimality\" depends as much upon the playing field as upon the strategy.</p>\n<p>Next, I claim that such scenarios not restricted to toy games where Omega messes with your head. Humans encounter similar situations on a day-to-day basis.</p>\n<hr>\n<p>Humans often find themselves in a position where they should modify their terminal goals, and the reason is simple: our thoughts do not have direct control over our motivation.</p>\n<p>Unfortunately for us, our \"motivation circuits\" can distinguish between terminal and instrumental goals. It is often easier to put in effort, experience inspiration, and work tirelessly when pursuing a terminal goal as opposed to an instrumental goal. It would be nice if this were not the case, but it's a <em>fact of our hardware</em>: we're going to do X more if we want to do X for its own sake as opposed to when we force X upon ourselves.</p>\n<p>Consider, for example, a young woman who wants to be a rockstar. She wants the fame, the money, and the lifestyle: these are her \"terminal goals\". She lives in some strange world where rockstardom is wholly dependent upon merit (rather than social luck and network effects), and decides that in order to become a rockstar she has to produce really good music.</p>\n<p>But here's the problem: She's a human. Her conscious decisions don't directly affect her motivation.</p>\n<p>In her case, it turns out that she can make better music when \"Make Good Music\" is a terminal goal as opposed to an instrumental goal.</p>\n<p>When \"Make Good Music\" is an instrumental goal, she schedules practice time on a sitar and grinds out the hours. But she doesn't really <em>like</em> it, so she cuts corners whenever akrasia comes knocking. She lacks inspiration and spends her spare hours dreaming of stardom. Her songs are shallow and trite.</p>\n<p>When \"Make Good Music\" is a terminal goal, music pours forth, and she spends every spare hour playing her sitar: not because she knows that she \"should\" practice, but because you couldn't pry her sitar from her cold dead fingers. She's not \"practicing\", she's pouring out her soul, and no power in the 'verse can stop her. Her songs are emotional, deep, and moving.</p>\n<p>It's obvious that she should adopt a new terminal goal.</p>\n<p>Ideally, we would be just as motivated to carry out instrumental goals as we are to carry out terminal goals. In reality, this is not the case. As a human, your motivation system <em>does</em> discriminate between the goals that you feel obligated to achieve and the goals that you pursue as ends unto themselves.</p>\n<p>As such, it is sometimes in your best interest to modify your terminal goals.</p>\n<hr>\n<p>Mind the terminology, here. When I speak of \"terminal goals\" I mean actions that feel like ends unto themselves. I am speaking of the stuff you wish you were doing when you're doing boring stuff, the things you do in your free time just because they are&nbsp;<em>fun</em>, the actions you don't need to justify.</p>\n<p>This seems like the obvious meaning of \"terminal goals\" to me, but some of you may think of \"terminal goals\" more akin to self-endorsed morally sound end-values in some consistent utility function. I'm not talking about those. I'm not even convinced I have any.</p>\n<p>Both types of \"terminal goal\" are susceptible to strange playing fields in which the optimal move is to change your goals, but it is only the former type of goal \u2014 the actions that are simply&nbsp;<em>fun</em>, that need no justification \u2014 which I'm suggesting you tweak for instrumental reasons.</p>\n<hr>\n<p>I've largely refrained from goal-hacking, personally. I bring it up for a few reasons:</p>\n<ol>\n<li>It's the easiest Dark Side technique to justify. It helps break people out of the mindset where they think optimal actions are the ones that look rational in a vacuum. Remember, optimality is a feature of the playing field. Sometimes cooperating with DefectBot is the best strategy!</li>\n<li>Goal hacking segues nicely into the other Dark Side techniques which I use frequently, as you will see shortly.</li>\n<li>I have met many people who would benefit from a solid bout of goal-hacking.</li>\n</ol>\n<p>I've crossed paths with many a confused person who (without any explicit thought on their part) had really silly terminal goals. We've all met people who are acting as if \"Acquire Money\" is a terminal goal, never noticing that money is almost entirely instrumental in nature. When you ask them \"but what would you do if money was no issue and you had a lot of time\", all you get is a blank stare.</p>\n<p>Even the <a href=\"http://wiki.lesswrong.com/wiki/Terminal_value\">LessWrong Wiki entry</a> on terminal values describes a college student for which university is instrumental, and getting a job is terminal. This seems like a clear-cut case of a <a href=\"/lw/le/lost_purposes/\">Lost Purpose</a>: a job seems clearly instrumental. And yet, we've all met people who act as if \"Have a Job\" is a terminal value, and who then seem aimless and undirected after finding employment.</p>\n<p>These people could use some goal hacking. You can argue that Acquire Money and Have a Job aren't \"really\" terminal goals, to which I counter that many people don't know their ass from their elbow when it comes to their own goals. Goal hacking is an important part of becoming a rationalist and/or improving mental health.</p>\n<p>Goal-hacking in the name of consistency isn't really a Dark Side power. This power is only Dark when you use it like the musician in our example, when you adopt terminal goals for instrumental reasons. This form of goal hacking is less common, but can be very effective.</p>\n<p>I recently had a personal conversation with <a href=\"/user/Alexei/\">Alexei</a>, who is earning to give. He noted that he was not entirely satisfied with his day-to-day work, and mused that perhaps goal-hacking (making \"Do Well at Work\" an end unto itself) could make him more effective, generally happier, and more productive in the long run.</p>\n<p>Goal-hacking can be a powerful technique, when correctly applied. Remember, you're not in direct control of your motivation circuits. Sometimes, strange though it seems, the optimal action involves fooling <em>yourself</em>.</p>\n<p>You don't get good at programming by sitting down and forcing yourself to practice for three hours a day. I mean, I suppose you <em>could</em> get good at programming that way. But it's much easier to get good at programming by <em>loving programming</em>, by being the type of person who spends every spare hour tinkering on a project. Because then it doesn't feel like practice, it feels like fun.</p>\n<p>This is the power that you can harness, if you're willing to tamper with your terminal goals for instrumental reasons. As rationalists, we would prefer to dedicate to instrumental goals the same vigor that is reserved for terminal goals. Unfortunately, we find ourselves on a strange playing field where goals that feel justified in their own right win the lion's share of our attention.</p>\n<p>Given this strange playing field, goal-hacking can be optimal.</p>\n<p>You don't have to completely mangle your goal system. Our aspiring musician from earlier doesn't need to destroy her \"Become a Rockstar\" goal in order to adopt the \"Make Good Music\" goal. If you can successfully convince yourself to believe that something instrumental is a means unto itself (e.g. terminal), <em>while still believing that it is instrumental</em>, then more power to you.</p>\n<p>This is, of course, an instance of Intentional Compartmentalization.</p>\n<h1 id=\"Intentional_Compartmentalization\">Intentional Compartmentalization</h1>\n<p>As soon as you endorse modifying your own terminal goals, Intentional Compartmentalization starts looking like a pretty good idea. If Omega offers to achieve <code>A </code>at the price of dropping&nbsp;<span style=\"font-family: monospace;\">A </span>and adopting&nbsp;<code>B</code>, the ideal move is to take the offer after finding a way to not <em>actually </em>care about&nbsp;<span style=\"font-family: monospace;\">B</span>.</p>\n<p>A consistent agent cannot do this, but I have good news for you: You're a human. You're not consistent. In fact, you're <em>great</em> at being inconsistent!</p>\n<p>You might expect it to be difficult to add a new terminal goal while still believing that it's instrumental. You may also run into strange situations where holding an instrumental goal as terminal <em>directly contradicts</em>&nbsp;other terminal goals.</p>\n<p>For example, our aspiring musician might find that she makes even <em>better</em> music if \"Become a Rockstar\" is <em>not</em> among her terminal goals.</p>\n<p>This means she's in trouble: She either has to drop \"Become a Rockstar\" and have a better chance at <em>actually becoming a rockstar</em>, or she has to settle for a decreased chance that she'll become a rockstar.</p>\n<p>Or, rather, she would have to settle for one of these choices \u2014 if she wasn't human.</p>\n<p>I have good news! Humans are <em>really really</em> good at being inconsistent, and you can leverage this to your advantage. <a href=\"http://wiki.lesswrong.com/wiki/Compartmentalization\">Compartmentalize</a>! Maintain goals that are \"terminal\" in one compartment, but which you know are \"instrumental\" in another, then simply never let those compartments touch!</p>\n<p>This may sound completely crazy and irrational, but remember: <a href=\"http://prettyrational.com/61/\">you aren't actually in control of your motivation system</a>. You find yourself on a strange playing field, and the optimal move may in fact require mental contortions that make epistemic rationalists shudder.</p>\n<p>Hopefully you never run into this particular problem (holding contradictory goals in \"terminal\" positions), but this illustrates that there are scenarios where compartmentalization works in your favor. Of course we'd <em>prefer</em>&nbsp;to have direct control of our motivation systems, but <em>given that we don't</em>, compartmentalization is a huge asset.</p>\n<p>Take a moment and let this sink in before moving on.</p>\n<p>Once you realize that compartmentalization is OK, you are ready to practice my second Dark Side technique: Intentional Compartmentalization. It has many uses outside the realm of goal-hacking.</p>\n<p>See, motivation is a fickle beast. And, as you'll remember, your conscious choices are not directly attached to your motivation levels. You can't just <em>decide</em> to be more motivated.</p>\n<p>At least, not directly.</p>\n<p>I've found that certain beliefs \u2014 beliefs which I <em>know are wrong</em> \u2014 can make me more productive. (On a related note, remember that <a href=\"/lw/5t/can_humanism_match_religions_output/\">religious organizations are generally more coordinated than rationalist groups</a>.)</p>\n<p>It turns out that, under these false beliefs, I can tap into motivational reserves that are otherwise unavailable. The only problem is, I know that these beliefs are downright false.</p>\n<p>I'm just kidding, that's not actually a problem. Compartmentalization to the rescue!</p>\n<p>Here's a couple example beliefs that I keep locked away in my mental compartments, bound up in chains. Every so often, when I need to be extra productive, I don my protective gear and enter these compartments. I never fully believe these things \u2014 not globally, at least \u2014 but I'm capable of attaining \"local belief\", of acting as if I hold these beliefs. This, it turns out, is enough.</p>\n<h2 id=\"Nothing_is_Beyond_My_Grasp\">Nothing is Beyond My Grasp</h2>\n<p>We'll start off with a tame belief, something that is soundly rooted in evidence outside of its little compartment.</p>\n<p>I have a global belief, outside all my compartments, that nothing is beyond my grasp.</p>\n<p>Others may understand things easier I do or faster than I do. People smarter than myself grok concepts with less effort than I. It may take me <em>years</em>&nbsp;to wrap my head around things that other people find trivial.&nbsp;However, there is no idea that a human has ever had that I cannot, <em>in principle</em>, grok.</p>\n<p>I believe this with moderately high probability, just based on my own general intelligence and the fact that brains are so tightly clustered in mind-space. It may take me a hundred times the effort to understand something, but I can still understand it eventually. Even things that are beyond the grasp of a meager human mind, I will one day be able to grasp after I upgrade my brain. Even if there are limits imposed by reality, I could <em>in principle</em> overcome them if I had enough computing power. Given any finite idea, I could in theory become powerful enough to understand it.</p>\n<p>This belief, itself, is not compartmentalized. What is compartmentalized is the <em>certainty</em>.</p>\n<p>Inside the compartment, I believe that Nothing is Beyond My Grasp with 100% confidence. Note that this is ridiculous: there's no such thing as 100% confidence. At least, not in my global beliefs. But inside the compartments, while we're in la-la land, it helps to treat Nothing is Beyond My Grasp as raw, immutable <em>fact</em>.</p>\n<p>You might think that it's sufficient to believe Nothing is Beyond My Grasp with very high probability. If that's the case, you haven't been listening: I <em>don't</em> actually believe Nothing is Beyond My Grasp with an extraordinarily high probability. I believe it with moderate probability, and then I&nbsp;<em>have a compartment</em> in which it's a certainty.</p>\n<p>It would be <em>nice</em> if I never needed to use the compartment, if I could face down technical problems and incomprehensible lingo and being really out of my depth with a relatively high confidence that I'm going to be able to make sense of it all. However, I'm not in direct control of my motivation. And it turns out that, through some quirk in my psychology, it's easier to face down the oppressive feeling of being in <em>way over my head</em> if I have this rock-solid \"belief\"&nbsp;that Nothing is Beyond My Grasp.</p>\n<p>This is what the compartments are good for: I don't actually believe the things inside them, but I can still <em>act as if I do</em>. That ability allows me to face down challenges that would be difficult to face down otherwise.</p>\n<p>This compartment was largely constructed with the help of <a href=\"http://en.wikipedia.org/wiki/The_Phantom_Tollbooth\">The Phantom Tollbooth</a>: it taught me that there are certain impossible tasks you can do if you think they're possible. It's not always enough to know that if I believe I can do a thing, then I have a higher probability of being able to do it. I get an extra boost from believing I can do&nbsp;<em>anything</em>.</p>\n<p>You might be surprised about how much you can do when you have a mental compartment in which you are <em>unstoppable</em>.</p>\n<h2 id=\"My_Willpower_Does_Not_Deplete\">My Willpower Does Not Deplete</h2>\n<p>Here's another: My Willpower Does Not Deplete.</p>\n<p>Ok, so my willpower actually does deplete. I've been writing about how it does, and discussing methods that I use to avoid depletion. <em>Right now</em>, I'm writing about how I've acknowledged the fact that my willpower <em>does deplete</em>.</p>\n<p>But I have this compartment where it doesn't.</p>\n<p>Ego depletion is a funny thing. If you don't believe in ego depletion, you suffer <a href=\"http://pss.sagepub.com/content/early/2010/09/28/0956797610384745\">less ego depletion</a>. This <a href=\"http://www.sciencedirect.com/science/article/pii/S0022103112000509\">does not eliminate ego depletion</a>.</p>\n<p>Knowing this, I have a compartment in which My Willpower Does Not Deplete. I go there often, when I'm studying. It's easy, I think, for one to begin to feel tired, and say \"oh, this must be ego depletion, I can't work anymore.\" Whenever my brain tries to go there, I wheel this bad boy out of his cage. \"Nope\", I respond, \"My Willpower Does Not Deplete\".</p>\n<p>Surprisingly, this often works. I won't force myself to keep working, but I'm pretty good at preventing mental escape attempts via \"phantom akrasia\". I don't allow myself to invoke ego depletion or akrasia to stop being productive, because My Willpower Does Not Deplete. I have to <em>actually be tired out</em>, in a way that doesn't trigger the My Willpower Does Not Deplete safeguards. This doesn't let me keep going forever, but it prevents a lot of false alarms.</p>\n<p>In my experience, the strong version (My Willpower Does Not Deplete) is much more effective than the weak version (My Willpower is Not Depleted Yet), even though it's more wrong. This probably says something about my personality. Your mileage may vary. Keep in mind, though, that the effectiveness of your mental compartments may depend more on the motivational content than on degree of falsehood.</p>\n<h2 id=\"Anything_is_a_Placebo\">Anything is a Placebo</h2>\n<p>Placebos work <a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0015591\">even when you know they are placebos</a>.</p>\n<p>This is the sort of madness I'm talking about, when I say things like \"you're on a strange playing field\".</p>\n<p>Knowing this, you can easily activate the placebo effect manually. Feeling sick? Here's a freebie: drink more water. It will make you feel better.</p>\n<p>No? It's just a placebo, you say? Doesn't matter. Tell yourself that water makes it better. Put that in a nice little compartment, save it for later. It doesn't matter that you know what you're doing: your brain is easily fooled.</p>\n<p>Want to be more productive, be healthier, and exercise more effectively? Try using Anything is a Placebo! Pick something trivial and non-harmful and tell yourself that it helps you perform better. Put the belief in a compartment in which you <em>act as if</em> you believe the thing. Cognitive dissonance doesn't matter! Your brain is <em>great</em> at ignoring cognitive dissonance. You can \"know\" you're wrong in the global case, while \"believing\" you're right locally.</p>\n<p>For bonus points, try combining objectives. Are you constantly underhydrated? Try believing that drinking more water makes you more alert!</p>\n<p>Brains are weird.</p>\n<hr>\n<p>Truly, these are the Dark Arts of instrumental rationality. Epistemic rationalists recoil in horror as I advocate <em>intentionally cultivating false beliefs. </em>It goes without saying that you should use this technique with care. Remember to always audit your compartmentalized beliefs through the lens of your actual beliefs, and be very careful not to let incorrect beliefs leak out of their compartments.</p>\n<p>If you think you can achieve similar benefits without \"fooling yourself\", then by all means, do so. I haven't been able to find effective alternatives. Brains have been honing compartmentalization techniques for <em>eons</em>, so I figure I might as well re-use the hardware.</p>\n<p>It's important to reiterate that these techniques are necessary because&nbsp;<em>you're not actually in control of your own motivation</em>. Sometimes, incorrect beliefs make you more motivated. Intentionally cultivating incorrect beliefs is surely a path to the Dark Side: compartmentalization only mitigates the damage. If you make sure you segregate the bad beliefs and acknowledge them for what they are then you can get much of the benefit without paying the cost, but there is still a cost, and the currency is cognitive dissonance.</p>\n<p>At this point, you should be mildly uncomfortable. After all, I'm advocating something which is completely epistemically irrational. We're not done yet, though.</p>\n<p>I have one more Dark Side technique, and it's worse.</p>\n<h1 id=\"Willful_Inconsistency\">Willful Inconsistency</h1>\n<p>I use Intentional Compartmentalization to \"locally believe\" things that I don't \"globally believe\", in cases where the local belief makes me more productive. In this case, the beliefs in the compartments are things that I tell myself. They're like mantras that I repeat in my head, at the System 2 level. System 1 is fragmented and compartmentalized, and happily obliges.</p>\n<p>Willful Inconsistency is the grown-up, scary version of Intentional Compartmentalization. It involves convincing System 1 wholly and entirely of something that System 2 does not actually believe. There's no compartmentalization and no fragmentation. There's nowhere to shove the incorrect belief when you're done with it. It's taken over the intuition, and it's always on. Willful Inconsistency is about having gut-level intuitive beliefs that you explicitly disavow.</p>\n<p>Your intuitions run the show whenever you're not paying attention, so if you're willfully inconsistent then you're going to actually <em>act as if</em> these incorrect beliefs are true in your day-to-day life, unless your forcibly override your default actions. Ego depletion and distraction make you vulnerable <em>to yourself</em>.</p>\n<p>Use this technique with caution.</p>\n<p>This may seem insane even to those of you who took the previous suggestions in stride. That you must sometimes alter your terminal goals is a feature of the playing field, not the agent. The fact that you are not in direct control of your motivation system readily implies that tricking yourself is useful, and compartmentalization is an obvious way to mitigate the damage.</p>\n<p>But why would anyone ever try to convince themselves, deep down at the core, of something that they don't actually believe?</p>\n<p>The answer is simple: specialization.</p>\n<p>To illustrate, let me explain how I use willful inconsistency.</p>\n<p>I have invoked Willful Inconsistency on only two occasions, and they were similar in nature. Only one instance of Willful Inconsistency is currently active, and it works like this:</p>\n<p>I have completely and totally convinced my intuitions that unfriendly AI is a problem. A big problem. System 1 operates under the assumption that UFAI will come to pass in the next twenty years with very high probability.</p>\n<p>You can imagine how this is somewhat motivating.</p>\n<p>On the conscious level, within System 2, I'm much less certain. I solidly believe that UFAI is a big problem, and that it's the problem that I should be focusing my efforts on. However, my error bars are <em>far</em> wider, my timespan is quite broad. I acknowledge a decent probability of soft takeoff. I assign moderate probabilities to a number of other existential threats. I think there are a large number of unknown unknowns, and there's a non-zero chance that the status quo continues until I die (and that I can't later be brought back). All this I know.</p>\n<p>But, <em>right now</em>, as I type this, my intuition is screaming at me that the above is all wrong, that my error bars are narrow, and that I don't <em>actually</em> expect the status quo to continue for even thirty years.</p>\n<p>This is just how I like things.</p>\n<p>See, I <em>am</em> convinced that building a friendly AI is the most important problem for me to be working on, <em>even though</em> there is a very real chance that MIRI's research won't turn out to be crucial. Perhaps other existential risks will get to us first. Perhaps we'll get brain uploads and Robin Hanson's emulation economy. Perhaps it's going to take far longer than expected to crack general intelligence. However, after much reflection I have concluded that despite the uncertainty, this is where I should focus my efforts.</p>\n<p>The problem is, it's hard to translate that decision down to System 1.</p>\n<p>Consider a toy scenario, where there are ten problems in the world. Imagine that, in the face of uncertainty and diminishing returns from research effort, I have concluded that the world should allocate 30% of resources to problem A, 25% to problem B, 10% to problem C, and 5% to each of the remaining problems.</p>\n<p>Because specialization leads to massive benefits, it's much more effective to dedicate 30% of researchers to working on problem A rather than having all researchers dedicate 30% of their time to problem A. So presume that, in light of these conclusions, I decide to dedicate myself to problem A.</p>\n<p>Here we have a problem: I'm supposed to specialize in problem A, but at the intuitive level problem A isn't <em>that</em>&nbsp;big a deal. It's only 30% of the problem space, after all, and it's not really that much worse than problem B.</p>\n<p>This would be no issue if I were in control of my own motivation system: I could put the blinders on and focus on problem A, crank the motivation knob to maximum, and trust everyone else to focus on the other problems and do their part.</p>\n<p>But I'm not in control of my motivation system. If my intuitions know that there are a number of other similarly worthy problems that I'm ignoring, if they are distracted by other issues of similar scope, then I'm tempted to work on everything at once. This is bad, because output is maximized if we all specialize.</p>\n<p>Things get especially bad when problem A is highly uncertain and unlikely to affect people for decades if not centuries. It's very hard to convince the monkey brain to care about far-future vagaries, <em>even if</em>&nbsp;I've rationally concluded that those are where I should dedicate my resources.</p>\n<p>I find myself on a strange playing field, where the optimal move is to lie to System 1.</p>\n<p>Allow me to make that more concrete:</p>\n<p>I'm <em>much</em> more motivated to do FAI research when I'm intuitively convinced that we have a hard 15 year timer until UFAI.</p>\n<p>Explicitly, I believe UFAI is one possibility among many and that the timeframe should be measured in decades rather than years. I've concluded that it is my most pressing concern, but I don't <em>actually</em> believe we have a hard 15 year countdown.</p>\n<p>That said, it's hard to understate how useful it is to have a gut-level feeling that there's a short, hard timeline. This \"knowledge\" pushes the monkey brain to go all out, no holds barred. In other words, this is the method by which I convince myself to <em>actually</em>&nbsp;specialize.</p>\n<p>This is how I convince myself to deploy every available resource, to attack the problem as if the stakes were incredibly high. Because the stakes <em>are</em>&nbsp;incredibly high, and I <em>do</em>&nbsp;need to deploy every available resource, even if we don't have a hard 15 year timer.</p>\n<p>In other words, Willful Inconsistency is the technique I use to force my intuition to <em>feel as if</em>&nbsp;the stakes are as high as I've calculated them to be, given that my monkey brain is bad at responding to uncertain vague future problems. Willful Inconsistency is my counter to <a href=\"/lw/hw/scope_insensitivity/\">Scope Insensitivity</a>: my intuition has difficulty believing the results when I <a href=\"http://wiki.lesswrong.com/wiki/Shut_up_and_multiply\">do the multiplication</a>, so I lie to it until it acts with appropriate vigor.</p>\n<p>This is the final secret weapon in my motivational arsenal.</p>\n<p>I don't personally recommend that you try this technique. It can have harsh side effects, including feelings of guilt, intense stress, and massive amounts of cognitive dissonance. I'm able to do this in large part because I'm in a very good headspace. I went into this with full knowledge of what I was doing, and I am confident that I can back out (and actually correct my intuitions) if the need arises.</p>\n<p>That said, I've found that cultivating a gut-level feeling that what you're doing <em>must</em> be done, and must be done <em>quickly</em>, is an extraordinarily good motivator. It's such a strong motivator that I seldom explicitly acknowledge it. I don't need to mentally invoke \"we have to study or the world ends\". Rather, this knowledge lingers in the background. It's not a mantra, it's not something that I repeat and wear thin. Instead, it's this gut-level drive that sits underneath it all, that makes me strive to go faster unless I explicitly try to slow down.</p>\n<p>This monkey-brain tunnel vision, combined with a long habit of productivity, is what keeps me <a href=\"/lw/jh0/deregulating_distraction_moving_towards_the_goal/\">Moving Towards the Goal</a>.</p>\n<hr>\n<p>Those are my Dark Side techniques: Willful Inconsistency, Intentional Compartmentalization, and Terminal Goal Modification.</p>\n<p>I expect that these techniques will be rather controversial. If I may be so bold, I recommend that discussion focus on goal-hacking and intentional compartmentalization. I acknowledge that willful inconsistency is unhealthy and I don't generally recommend that others try it. By contrast, both goal-hacking and intentional compartmentalization are quite sane and, indeed, instrumentally rational.</p>\n<p>These are certainly not techniques that I would recommend CFAR teach to newcomers, and I remind you that \"it is dangerous to be half a rationalist\". You can royally screw you over if you're still figuring out your beliefs as you attempt to compartmentalize false beliefs. I recommend only using them when you're sure of what your goals are and confident about the borders between your actual beliefs and your intentionally false \"beliefs\".</p>\n<p>It may be surprising that changing terminal goals can be an optimal strategy, and that humans should consider adopting incorrect beliefs strategically. At the least, I encourage you to remember that there are no absolutely rational actions.</p>\n<p>Modifying your own goals and cultivating false beliefs are useful because we live in strange, hampered control systems. Your brain was optimized with <a href=\"/lw/l3/thou_art_godshatter/\">no concern for truth</a>, and optimal performance may require <a href=\"/lw/cn/instrumental_vs_epistemic_a_bardic_perspective/\">self deception</a>. I remind the uncomfortable that instrumental rationality is not about being the most consistent or the most correct, it's about <em>winning.&nbsp;</em>There are games where the optimal move requires adopting false beliefs, and if you find yourself playing one of those games, then you should adopt false beliefs. Instrumental rationality and epistemic rationality can be pitted against each other.</p>\n<p>We are fortunate, as humans, to be skilled at compartmentalization: this helps us work around our mental handicaps without sacrificing epistemic rationality. Of course, we'd rather not have the mental handicaps in the first place: but you have to work with what you're given.</p>\n<p>We <em>are</em> weird agents without full control of our own minds. We lack direct control over important aspects of ourselves. For that reason, it's often necessary to take actions that may seem contradictory, crazy, or downright irrational.</p>\n<p>Just remember this, before you condemn these techniques: optimality is as much an aspect of the playing field as of the strategy, and humans occupy a strange playing field indeed.</p>", "sections": [{"title": "Changing your Terminal Goals", "anchor": "Changing_your_Terminal_Goals", "level": 1}, {"title": "Intentional Compartmentalization", "anchor": "Intentional_Compartmentalization", "level": 1}, {"title": "Nothing is Beyond My Grasp", "anchor": "Nothing_is_Beyond_My_Grasp", "level": 2}, {"title": "My Willpower Does Not Deplete", "anchor": "My_Willpower_Does_Not_Deplete", "level": 2}, {"title": "Anything is a Placebo", "anchor": "Anything_is_a_Placebo", "level": 2}, {"title": "Willful Inconsistency", "anchor": "Willful_Inconsistency", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "193 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 193, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sP2Hg6uPwpfp3jZJN", "3fNL2ssfvRzpApvdN", "2ftJ38y9SRBCBsCzy", "yFALNnscB2qgehnJv", "cSXZpvqpa9vbGGLtG", "AGP9PwnhQcuYMKyMm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-01-19T02:47:03.011Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-19T02:51:57.048Z", "modifiedAt": null, "url": null, "title": "2013 Survey Results", "slug": "2013-survey-results", "viewCount": null, "lastCommentedAt": "2019-06-24T07:27:23.694Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pJJdcZgB6mPNWoSWr/2013-survey-results", "pageUrlRelative": "/posts/pJJdcZgB6mPNWoSWr/2013-survey-results", "linkUrl": "https://www.lesswrong.com/posts/pJJdcZgB6mPNWoSWr/2013-survey-results", "postedAtFormatted": "Sunday, January 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202013%20Survey%20Results&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2013%20Survey%20Results%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpJJdcZgB6mPNWoSWr%2F2013-survey-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2013%20Survey%20Results%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpJJdcZgB6mPNWoSWr%2F2013-survey-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpJJdcZgB6mPNWoSWr%2F2013-survey-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5603, "htmlBody": "<p>Thanks to everyone who took the 2013 Less Wrong Census/Survey. Extra thanks to Ozy, who helped me out with the data processing and statistics work, and to everyone who suggested questions.</p>\n<p>This year's results are below. Some of them may make more sense in the context of the original survey questions, <a href=\"https://docs.google.com/spreadsheet/viewform?usp=drive_web&amp;formkey=dGZ6a1NfZ0V1SV9xdE1ma0pUMTc1S1E6MA#gid=0\">which can be seen here</a>. Please do not try to take the survey as it is over and your results will not be counted.</p>\n<p><a id=\"more\"></a></p>\n<h2>Part I. Population</h2>\n<p>1636 people answered the survey.</p>\n<p>Compare this to 1195 people last year, and 1090 people the year before that. It would seem the site is growing, but we do have to consider that each survey lasted a different amount of time; for example, last survey lasted 23 days, but this survey lasted 40.</p>\n<p>However, almost everyone who takes the survey takes it in the first few weeks it is available. 1506 of the respondents answered within the first 23 days, proving that even if the survey ran the same length as last year's, there would still have been growth.<br />As we will see lower down, growth is smooth across all categories of users (lurkers, commenters, posters) EXCEPT people who have posted to Main, the number of which remains nearly the same from year to year.</p>\n<p>We continue to have very high turnover - only 40% of respondents this year say they also took the survey last year.</p>\n<h2>II. Categorical Data</h2>\n<p><strong>SEX:</strong><br />Female: 161, 9.8%<br />Male: 1453, 88.8%<br />Other: 1, 0.1%<br />Did not answer: 21, 1.3%</p>\n<p><em>[[Ozy is disappointed that we've lost 50% of our intersex readers.]]</em></p>\n<p><strong>GENDER:</strong><br />F (cisgender): 140, 8.6%<br />F (transgender MtF): 20, 1.2%<br />M (cisgender): 1401, 85.6%<br />M (transgender FtM): 5, 0.3%<br />Other: 49, 3%<br />Did not answer: 21, 1.3%</p>\n<p><strong>SEXUAL ORIENTATION:</strong><br />Asexual: 47, 2.9%<br />Bisexual: 188, 12.2%<br />Heterosexual: 1287, 78.7%<br />Homosexual: 45, 2.8%<br />Other: 39, 2.4%<br />Did not answer: 19, 1.2%</p>\n<p><strong>RELATIONSHIP STYLE:</strong><br />Prefer monogamous: 829, 50.7%<br />Prefer polyamorous: 234, 14.3%<br />Other: 32, 2.0%<br />Uncertain/no preference: 520, 31.8%<br />Did not answer: 21, 1.3%</p>\n<p><strong>NUMBER OF CURRENT PARTNERS:</strong><br />0: 797, 48.7%<br />1: 728, 44.5%<br />2: 66, 4.0%<br />3: 21, 1.3%<br />4: 1, .1%<br />6: 3, .2%<br />Did not answer: 20, 1.2%</p>\n<p><strong>RELATIONSHIP STATUS:</strong><br />Married: 304, 18.6%<br />Relationship: 473, 28.9%<br />Single: 840, 51.3%</p>\n<p><strong>RELATIONSHIP GOALS: </strong><br />Looking for more relationship partners: 617, 37.7%<br />Not looking for more relationship partners: 993, 60.7%<br />Did not answer: 26, 1.6%</p>\n<p><strong>HAVE YOU DATED SOMEONE YOU MET THROUGH THE LESS WRONG COMMUNITY?</strong><br />Yes: 53, 3.3%<br />I didn't meet them through the community but they're part of the community now: 66, 4.0%<br />No: 1482, 90.5%<br />Did not answer: 35, 2.1%</p>\n<p><strong>COUNTRY:</strong><br />United States: 895, 54.7%<br />United Kingdom: 144, 8.8%<br />Canada: 107, 6.5%<br />Australia: 69, 4.2%<br />Germany: 68, 4.2%<br />Finland: 35, 2.1%<br />Russia: 22, 1.3%<br />New Zealand: 20, 1.2%<br />Israel: 17, 1.0%<br />France: 16, 1.0%<br />Poland: 16, 1.0%</p>\n<p><strong>LESS WRONGERS PER CAPITA:</strong><br />Finland: 1/154,685.<br />New Zealand: 1/221,650. <br />Canada: 1/325,981. <br />Australia: 1/328,659.<br />United States: 1/350,726 <br />United Kingdom: 1/439,097<br />Israel: 1/465,176.<br />Germany: 1/1,204,264.<br />Poland: 1/2,408,750.<br />France: 1/4,106,250. <br />Russia: 1/6,522,727</p>\n<p><strong>RACE: </strong><br />Asian (East Asian): 60, 3.7%<br />Asian (Indian subcontinent): 37, 2.3%<br />Black: 11, .7%<br />Middle Eastern: 9, .6%<br />White (Hispanic): 73, 4.5%<br />White (non-Hispanic): 1373, 83.9%<br />Other: 51, 3.1%<br />Did not answer: 22, 1.3%</p>\n<p><strong>WORK STATUS:</strong><br />Academics (teaching): 77, 4.7%<br />For-profit work: 552, 33.7%<br />Government work: 55, 3.4%<br />Independently wealthy: 14, .9%<br />Non-profit work: 46, 2.8%<br />Self-employed: 103, 6.3%<br />Student: 661, 40.4%<br />Unemployed: 105, 6.4%<br />Did not answer: 23, 1.4%</p>\n<p><strong>PROFESSION:</strong><br />Art: 27, 1.7%<br />Biology: 26, 1.6%<br />Business: 44, 2.7%<br />Computers (AI): 47, 2.9%<br />Computers (other academic computer science): 107, 6.5%<br />Computers (practical): 505, 30.9%<br />Engineering: 128, 7.8%<br />Finance/economics: 92, 5.6%<br />Law: 36, 2.2%<br />Mathematics: 139, 8.5%<br />Medicine: 31, 1.9%<br />Neuroscience: 13, .8%<br />Philosophy: 41, 2.5%<br />Physics: 92, 5.6%<br />Psychology: 34, 2.1%<br />Statistics: 23, 1.4%<br />Other hard science: 31, 1.9%<br />Other social science: 43, 2.6%<br />Other: 139, 8.5%<br />Did not answer: 38, 2.3%</p>\n<p><strong>DEGREE:</strong><br />None: 84, 5.1%<br />High school: 444, 27.1%<br />2 year degree: 68, 4.2%<br />Bachelor's: 554, 33.9%<br />Master's: 323, 19.7%<br />MD/JD/other professional degree: 31, 2.0%<br />PhD.: 90, 5.5%<br />Other: 22, 1.3%<br />Did not answer: 19, 1.2%</p>\n<p><strong>POLITICAL:</strong><br />Communist: 11, .7%<br />Conservative: 64, 3.9%<br />Liberal: 580, 35.5%<br />Libertarian: 437, 26.7%<br />Socialist: 502, 30.7%<br />Did not answer: 42, 2.6%</p>\n<p><strong>COMPLEX POLITICAL WITH WRITE-IN:</strong><br />Anarchist: 52, 3.2%<br />Conservative: 16, 1.0%<br />Futarchist: 42, 2.6%<br />Left-libertarian: 142, 8.7%<br />Liberal: 5<br />Moderate: 53, 3.2%<br />Pragmatist: 110, 6.7%<br />Progressive: 206, 12.6%<br />Reactionary: 40, 2.4%<br />Social democrat: 154, 9.5%<br />Socialist: 135, 8.2%<br />Did not answer: 26.2%</p>\n<p><em>[[All answers with more than 1% of the Less Wrong population included. Other answers which made Ozy giggle included \"are any of you kings?! why do you CARE?!\", \"Exclusionary: you are entitled to an opinion on nuclear power when you know how much of your power is nuclear\", \"having-well-founded-opinions-is-really-hard-ist\", \"kleptocrat\", \"pirate\", and \"SPECIAL FUCKING SNOWFLAKE.\"]]</em></p>\n<p><strong>AMERICAN PARTY AFFILIATION:</strong><br />Democratic Party: 226, 13.8%<br />Libertarian Party: 31, 1.9%<br />Republican Party: 58, 3.5%<br />Other third party: 19, 1.2%<br />Not registered: 447, 27.3%<br />Did not answer or non-American: 856, 52.3%</p>\n<p><strong>VOTING:</strong><br />Yes: 936, 57.2%<br />No: 450, 27.5%<br />My country doesn't hold elections: 2, 0.1%<br />Did not answer: 249, 15.2%</p>\n<p><strong>RELIGIOUS VIEWS:</strong><br />Agnostic: 165, 10.1%<br />Atheist and not spiritual: 1163, 71.1%<br />Atheist but spiritual: 132, 8.1%<br />Deist/pantheist/etc.: 36, 2.2%<br />Lukewarm theist: 53, 3.2%<br />Committed theist 64, 3.9%</p>\n<p><strong>RELIGIOUS DENOMINATION (IF THEIST):</strong><br />Buddhist: 22, 1.3%<br />Christian (Catholic): 44, 2.7%<br />Christian (Protestant): 56, 3.4%<br />Jewish: 31, 1.9%<br />Mixed/Other: 21, 1.3%<br />Unitarian Universalist or similar: 25, 1.5%</p>\n<p><em>[[This includes all religions with more than 1% of Less Wrongers. Minority religions include Dzogchen, Daoism, various sorts of Paganism, Simulationist, a very confused secular humanist, Kopmist, Discordian, and a Cultus Deorum Romanum practitioner whom Ozy wants to be friends with.]]</em></p>\n<p><strong>FAMILY RELIGION:</strong><br />Agnostic: 129, 11.6%<br />Atheist and not spiritual: 225, 13.8%<br />Atheist but spiritual: 73, 4.5%<br />Committed theist: 423, 25.9%<br />Deist/pantheist, etc.: 42, 2.6%<br />Lukewarm theist: 563, 34.4%<br />Mixed/other: 97, 5.9%<br />Did not answer: 24, 1.5%</p>\n<p><strong>RELIGIOUS BACKGROUND:</strong><br />Bahai: 3, 0.2%<br />Buddhist: 13, .8%<br />Christian (Catholic): 418, 25.6%<br />Christian (Mormon): 38, 2.3%<br />Christian (Protestant): 631, 38.4%<br />Christian (Quaker): 7, 0.4%<br />Christian (Unitarian Universalist or similar): 32, 2.0%<br />Christian (other non-Protestant): 99, 6.1%<br />Christian (unknown): 3, 0.2%<br />Eckankar: 1, 0.1%<br />Hindu: 29, 1.8%<br />Jewish: 136, 8.3%<br />Muslim: 12, 0.7%<br />Native American Spiritualist: 1, 0.1%<br />Mixed/Other: 85, 5.3%<br />Sikhism: 1, 0.1%<br />Traditional Chinese: 11, .7%<br />Wiccan: 1, 0.1%<br />None: 8, 0.4%<br />Did not answer: 107, 6.7%</p>\n<p><strong>MORAL VIEWS:</strong><br />Accept/lean towards consequentialism: 1049, 64.1%<br />Accept/lean towards deontology: 77, 4.7%<br />Accept/lean towards virtue ethics: 197, 12.0%<br />Other/no answer: 276, 16.9%<br />Did not answer: 37, 2.3%</p>\n<p><strong>CHILDREN</strong><br />0: 1414, 86.4%<br />1: 77, 4.7%<br />2: 90, 5.5%<br />3: 25, 1.5%<br />4: 7, 0.4%<br />5: 1, 0.1%<br />6: 2, 0.1%<br />Did not answer: 20, 1.2%</p>\n<p><strong>MORE CHILDREN:</strong><br />Have no children, don't want any: 506, 31.3%<br />Have no children, uncertain if want them: 472, 29.2%<br />Have no children, want children: 431, 26.7%<br />Have no children, didn't answer: 5, 0.3%<br />Have children, don't want more: 124, 7.6%<br />Have children, uncertain if want more: 25, 1.5%<br />Have children, want more: 53, 3.2%</p>\n<p><strong>HANDEDNESS:</strong><br />Right: 1256, 76.6%<br />Left: 145, 9.5%<br />Ambidextrous: 36, 2.2%<br />Not sure: 7, 0.4%<br />Did not answer: 182, 11.1%</p>\n<p><strong>LESS WRONG USE: </strong><br />Lurker (no account): 584, 35.7%<br />Lurker (account) 221, 13.5%<br />Poster (comment, no post): 495, 30.3%<br />Poster (Discussion, not Main): 221, 12.9%<br />Poster (Main): 103, 6.3%</p>\n<p><strong>SEQUENCES:</strong><br />Never knew they existed: 119, 7.3%<br />Knew they existed, didn't look at them: 48, 2.9%<br />~25% of the Sequences: 200, 12.2%<br />~50% of the Sequences: 271, 16.6%<br />~75% of the Sequences: 225, 13.8%<br />All the Sequences: 419, 25.6%<br />Did not answer: 24, 1.5%</p>\n<p><strong>MEETUPS: </strong><br />No: 1134, 69.3%<br />Yes, once or a few times: 307, 18.8%<br />Yes, regularly: 159, 9.7%</p>\n<p><strong>HPMOR:</strong><br />No: 272, 16.6%<br />Started it, haven't finished: 255, 15.6%<br />Yes, all of it: 912, 55.7%</p>\n<p><strong>CFAR WORKSHOP ATTENDANCE:</strong><br />Yes, a full workshop: 105, 6.4%<br />A class but not a full-day workshop: 40, 2.4%<br />No: 1446, 88.3%<br />Did not answer: 46, 2.8%</p>\n<p><strong>PHYSICAL INTERACTION WITH LW COMMUNITY: </strong><br />Yes, all the time: 94, 5.7%<br />Yes, sometimes: 179, 10.9%<br />No: 1316, 80.4%<br />Did not answer: 48, 2.9%</p>\n<p><strong>VEGETARIAN:</strong><br />No: 1201, 73.4%<br />Yes: 213, 13.0%<br />Did not answer: 223, 13.6%</p>\n<p><strong>SPACED REPETITION:</strong><br />Never heard of them: 363, 22.2%<br />No,&nbsp; but I've heard of them: 495, 30.2%<br />Yes, in the past: 328, 20%<br />Yes, currently: 219, 13.4%<br />Did not answer: 232, 14.2%</p>\n<p><strong>HAVE YOU TAKEN PREVIOUS INCARNATIONS OF THE LESS WRONG SURVEY?</strong><br />Yes: 638, 39.0%<br />No: 784, 47.9%<br />Did not answer: 215, 13.1%</p>\n<p><strong>PRIMARY LANGUAGE:</strong><br />English: 1009, 67.8%<br />German: 58, 3.6%<br />Finnish: 29, 1.8%<br />Russian: 25, 1.6%<br />French: 17, 1.0%<br />Dutch: 16, 1.0%<br />Did not answer: 15.2%</p>\n<p><em>[[This includes all answers that more than 1% of respondents chose. Other languages include Urdu, both Czech and Slovakian, Latvian, and Love.]]</em></p>\n<p><strong>ENTREPRENEUR:</strong><br />I don't want to start my own business: 617, 37.7%<br />I am considering starting my own business: 474, 29.0%<br />I plan to start my own business: 113, 6.9%<br />I've already started my own business: 156, 9.5%<br />Did not answer: 277, 16.9%</p>\n<p><strong>EFFECTIVE ALTRUIST:</strong><br />Yes: 468, 28.6%<br />No: 883, 53.9%<br />Did not answer: 286, 17.5%</p>\n<p><strong>WHO ARE YOU LIVING WITH?</strong><br />Alone: 348, 21.3%<br />With family: 420, 25.7%<br />With partner/spouse: 400, 24.4%<br />With roommates: 450, 27.5%<br />Did not answer: 19, 1.3%</p>\n<p><strong>DO YOU GIVE BLOOD?</strong><br />No: 646, 39.5%<br />No, only because I'm not allowed: 157, 9.6%<br />Yes, 609, 37.2%<br />Did not answer: 225, 13.7%</p>\n<p><strong>GLOBAL CATASTROPHIC RISK:</strong><br />Pandemic (bioengineered): 374, 22.8%<br />Environmental collapse including global warming: 251, 15.3%<br />Unfriendly AI: 233, 14.2%<br />Nuclear war: 210, 12.8%<br />Pandemic (natural) 145, 8.8%<br />Economic/political collapse: 175, 1, 10.7%<br />Asteroid strike: 65, 3.9%<br />Nanotech/grey goo: 57, 3.5%<br />Didn't answer: 99, 6.0%</p>\n<p><strong>CRYONICS STATUS:</strong><br />Never thought about it / don't understand it: 69, 4.2%<br />No, and don't want to: 414, 25.3%<br />No, still considering: 636, 38.9%<br />No, would like to: 265, 16.2%<br />No, would like to, but it's unavailable: 119, 7.3%<br />Yes: 66, 4.0%<br />Didn't answer: 68, 4.2%</p>\n<p><strong>NEWCOMB'S PROBLEM: </strong><br />Don't understand/prefer not to answer: 92, 5.6%<br />Not sure: 103, 6.3%<br />One box: 1036, 63.3%<br />Two box: 119, 7.3%<br />Did not answer: 287, 17.5%</p>\n<p><strong>GENOMICS:</strong><br />Yes: 177, 10.8%<br />No: 1219, 74.5%<br />Did not answer: 241, 14.7%</p>\n<p><strong>REFERRAL TYPE:</strong><br />Been here since it started in the Overcoming Bias days: 285, 17.4%<br />Referred by a friend: 241, 14.7%<br />Referred by a search engine: 148, 9.0%<br />Referred by HPMOR: 400, 24.4%<br />Referred by a link on another blog: 373, 22.8%<br />Referred by a school course: 1, .1%<br />Other: 160, 9.8%<br />Did not answer: 29, 1.9%</p>\n<p><strong>REFERRAL SOURCE:</strong><br />Common Sense Atheism: 33<br />Slate Star Codex: 20<br />Hacker News: 18<br />Reddit: 18<br />TVTropes: 13<br />Y Combinator: 11<br />Gwern: 9<br />RationalWiki: 8<br />Marginal Revolution: 7<br />Unequally Yoked: 6<br />Armed and Dangerous: 5<br />Shtetl Optimized: 5<br />Econlog: 4<br />StumbleUpon: 4<br />Yudkowsky.net: 4<br />Accelerating Future: 3<br />Stares at the World: 3<br />xkcd: 3<br />David Brin: 2<br />Freethoughtblogs: 2<br />Felicifia: 2<br />Givewell: 2<br />hatrack.com: 2<br />HPMOR: 2<br />Patri Friedman: 2<br />Popehat: 2<br />Overcoming Bias: 2<br />Scientiststhesis: 2<br />Scott Young: 2<br />Stardestroyer.net: 2<br />TalkOrigins: 2<br />Tumblr: 2</p>\n<p><em>[[This includes all sources with&nbsp; more than one referral; needless to say there was a long tail]]</em></p>\n<h2>III. Numeric Data</h2>\n<p>(in the form mean + stdev (1st quartile, 2nd quartile, 3rd quartile) [n = number responding]))</p>\n<p>Age: 27.4 + 8.5 (22, 25, 31) [n = 1558]<br />Height: 176.6 cm + 16.6 (173, 178, 183) [n = 1267]</p>\n<p>Karma Score: 504 + 2085 (0, 0, 100) [n = 1438]<br />Time in community: 2.62 years + 1.84 (1, 2, 4) [n = 1443]<br />Time on LW: 13.25 minutes/day + 20.97 (2, 10, 15) [n = 1457]</p>\n<p>IQ: 138.2 + 13.6 (130, 138, 145) [n = 506]<br />SAT out of 1600: 1474 + 114 (1410, 1490, 1560) [n = 411]<br />SAT out of 2400: 2207 + 161 (2130, 2240, 2330) [n = 333]<br />ACT out of 36: 32.8 + 2.5 (32, 33, 35) [n = 265]</p>\n<p>P(Aliens in observable universe): 74.3 + 32.7 (60, 90, 99) [n = 1496]<br />P(Aliens in Milky Way): 44.9 + 38.2 (5, 40, 85) [n = 1482]<br />P(Supernatural): 7.7 + 22 (0E-9, .000055, 1) [n = 1484]<br />P(God): 9.1 + 22.9 (0E-11, .01, 3) [n = 1490]<br />P(Religion): 5.6 + 19.6 (0E-11, 0E-11, .5) [n = 1497]<br />P(Cryonics): 22.8 + 28 (2, 10, 33) [n = 1500]&nbsp;&nbsp; <br />P(AntiAgathics): 27.6 + 31.2 (2, 10, 50) [n = 1493]<br />P(Simulation): 24.1 + 28.9 (1, 10, 50) [n = 1400]<br />P(ManyWorlds): 50 + 29.8 (25, 50, 75) [n = 1373]<br />P(Warming): 80.7 + 25.2 (75, 90, 98) [n = 1509]<br />P(Global catastrophic risk): 72.9 + 25.41 (60, 80, 95) [n = 1502]<br />Singularity year: 1.67E +11 + 4.089E+12 (2060, 2090, 2150) [n = 1195]</p>\n<p><em>[[Of course, this question was hopelessly screwed up by people who insisted on filling the whole answer field with 9s, or other such nonsense. I went back and eliminated all outliers - answers with more than 4 digits or answers in the past - which changed the results to: 2150 + 226 (2060, 2089, 2150)]]</em></p>\n<p>Yearly Income: $73,226 +423,310 (10,000, 37,000, 80,000) [n = 910]<br />Yearly Charity: $1181.16 + 6037.77 (0, 50, 400) [n = 1231]<br />Yearly Charity to MIRI/CFAR: $307.18 + 4205.37 (0, 0, 0) [n = 1191]<br />Yearly Charity to X-risk (excluding MIRI or CFAR): $6.34 + 55.89 (0, 0, 0) [n = 1150]<br /><br />Number of Languages: 1.49 + .8 (1, 1, 2) [n = 1345]<br />Older Siblings: 0.5 + 0.9 (0, 0, 1) [n = 1366]<br />Time Online/Week: 42.7 hours + 24.8 (25, 40, 60) [n = 1292]<br />Time Watching TV/Week: 4.2 hours + 5.7 (0, 2, 5) [n = 1316]</p>\n<p><em>[[The next nine questions ask respondents to rate how favorable they are to the political idea or movement above on a scale of 1 to 5, with 1 being \"not at all favorable\" and 5 being \"very favorable\". You can see the exact wordings of the questions on the survey.]]</em></p>\n<p>Abortion: 4.4 + 1 (4, 5, 5) [n = 1350]<br />Immigration: 4.1 + 1 (3, 4, 5) [n = 1322]<br />Basic Income: 3.8 + 1.2 (3, 4, 5) [n = 1289]<br />Taxes: 3.1 + 1.3 (2, 3, 4) [n = 1296]<br />Feminism: 3.8 + 1.2 (3, 4, 5) [n = 1329]<br />Social Justice: 3.6 + 1.3 (3, 4, 5) [n = 1263]<br />Minimum Wage: 3.2 + 1.4 (2, 3, 4) [n = 1290]<br />Great Stagnation: 2.3 + 1 (2, 2, 3) [n = 1273]<br />Human Biodiversity: 2.7 + 1.2 (2, 3, 4) [n = 1305]</p>\n<h2>IV. Bivariate Correlations</h2>\n<p>Ozy ran bivariate correlations between all the numerical data and recorded all correlations that were significant at the .001 level in order to maximize the chance that these are genuine results. The format is variable/variable: Pearson correlation (n). Yvain is not hugely on board with the idea of running correlations between everything and seeing what sticks, but will grudgingly publish the results because of the very high bar for significance (p &lt; .001 on ~800 correlations suggests &lt; 1 spurious result) and because he doesn't want to have to do it himself.</p>\n<p><strong>Less Political:</strong><br />SAT score (1600)/SAT score (2400): .835 (56) <br />Charity/MIRI and CFAR donations: .730 (1193)<br />SAT score out of 2400/ACT score: .673 (111)<br />SAT score out of 1600/ACT score: .544 (102)<br />Number of children/age: .507 (1607) <br />P(Cryonics)/P(AntiAgathics): .489 (1515)<br />SAT score out of 1600/IQ: .369 (173)<br />MIRI and CFAR donations/XRisk donations: .284 (1178)<br />Number of children/ACT score: -.279 (269)<br />Income/charity: .269 (884)<br />Charity/Xrisk charity: .262 (1161)<br />P(Cryonics)/P(Simulation): .256 (1419)<br />P(AntiAgathics)/P(Simulation): .253 (1418)<br />Number of current partners/age: .238 (1607)&nbsp; <br />Number of children/SAT score (2400): -.223 (345)<br />Number of current partners/number of children: .205 (1612)<br />SAT score out of 1600/age: -.194 (422)<br />Charity/age: .175 (1259)<br />Time on Less Wrong/IQ: -.164 (492)<br />P(Warming)/P(GlobalCatastrophicRisk): .156 (1522)<br />Number of current partners/IQ: .155 (521)<br />P(Simulation)/age: -.153 (1420)<br />Immigration/P(ManyWorlds): .150 (1195)<br />Income/age: .150 (930)<br />P(Cryonics)/age: -.148 (1521)<br />Income/children: .145 (931)<br />P(God)/P(Simulation): .142 (1409)<br />Number of children/P(Aliens): .140 (1523)<br />P(AntiAgathics)/Hours Online: .138 (1277)<br />Number of current partners/karma score: .137 (1470)<br />Abortion/P(ManyWorlds): .122 (1215)<br />Feminism/Xrisk charity donations: -.122 (1104)<br />P(AntiAgathics)/P(ManyWorlds) .118 (1381) <br />P(Cryonics)/P(ManyWorlds): .117 (1387)<br />Karma score/Great Stagnation: .114 (1202)<br />Hours online/P(simulation): .114 (1199)<br />P(Cryonics)/Hours Online: .113 (1279)<br />P(AntiAgathics)/Great Stagnation: -.111 (1259)<br />Basic income/hours online: .111 (1200)<br />P(GlobalCatastrophicRisk)/Great Stagnation: -.110 (1270)<br />Age/X risk charity donations: .109 (1176)<br />P(AntiAgathics)/P(GlobalCatastrophicRisk): -.109 (1513)<br />Time on Less Wrong/age: -.108 (1491)<br />P(AntiAgathics)/Human Biodiversity: .104 (1286)<br />Immigration/Hours Online: .104 (1226)<br />P(Simulation)/P(GlobalCatastrophicRisk): -.103 (1421)<br />P(Supernatural)/height: -.101 (1232)<br />P(GlobalCatastrophicRisk)/height: .101 (1249)<br />Number of children/hours online: -.099 (1321)<br />P(AntiAgathics)/age: -.097 (1514)<br />Karma score/time on LW: .096 (1404)</p>\n<p>This year for the first time P(Aliens) and P(Aliens2) are entirely uncorrelated with each other. Time in Community, Time on LW, and IQ are not correlated with anything particularly interesting, suggesting all three fail to change people's views.</p>\n<p>Results we find amusing: high-IQ and high-karma people have more romantic partners, suggesting that those are attractive traits. There is definitely a Cryonics/Antiagathics/Simulation/Many Worlds cluster of weird beliefs, which younger people and people who spend more time online are slightly more likely to have - weirdly, that cluster seems slightly less likely to believe in global catastrophic risk. Older people and people with more children have more romantic partners (it'd be interesting to see if that holds true for the polyamorous). People who believe in anti-agathics and global catastrophic risk are less likely to believe in a great stagnation (presumably because both of the above rely on inventions). People who spend more time on Less Wrong have lower IQs. Height is, bizarrely, correlated with belief in the supernatural and global catastrophic risk.</p>\n<p>All political viewpoints are correlated with each other in pretty much exactly the way one would expect. They are also correlated with one's level of belief in God, the supernatural, and religion. There are minor correlations with some of the beliefs and number of partners (presumably because polyamory), number of children, and number of languages spoken. We are doing terribly at avoiding Blue/Green politics, people.</p>\n<p><strong>More Political:</strong><br />P(Supernatural)/P(God): .736 (1496)<br />P(Supernatural)/P(Religion): .667 (1492)<br />Minimum wage/taxes: .649 (1299)<br />P(God)/P(Religion): .631 (1496)<br />Feminism/social justice: .619 (1293)<br />Social justice/minimum wage: .508 (1262)<br />P(Supernatural)/abortion: -.469 (1309)<br />Taxes/basic income: .463 (1285)<br />P(God)/abortion: -.461 (1310)<br />Social justice/taxes: .456 (1267)<br />P(Religion)/abortion: -.413<br />Basic income/minimum wage: .392 (1283)<br />Feminism/taxes: .391 (1318)<br />Feminism/minimum wage: .391 (1312)<br />Feminism/human biodiversity: -.365 (1331)<br />Immigration/feminism: .355 (1336)<br />P(Warming)/taxes: .340 (1292)<br />Basic income/social justice: .311 (1270)<br />Immigration/social justice: .307 (1275)<br />P(Warming)/feminism: .294 (1323)<br />Immigration/human biodiversity: -.292 (1313)<br />P(Warming)/basic income: .290 (1287)<br />Social justice/human biodiversity: -.289 (1281)<br />Basic income/feminism: .284 (1313)<br />Human biodiversity/minimum wage: -.273 (1293)<br />P(Warming)/social justice: .271 (1261)<br />P(Warming)/minimum wage: .262 (1284)<br />Human biodiversity/taxes: -.251 (1270).<br />Abortion/feminism: .239 (1356)<br />Abortion/social justice: .220 (1292)<br />P(Warming)/immigration: .215 (1315)<br />Abortion/immigration: .211 (1353)<br />P(Warming)/abortion: .192 (1340)<br />Immigration/taxes: .186 (1322)<br />Basic income/taxes: .174 (1249)<br />Abortion/taxes: .170 (1328)<br />Abortion/minimum wage: .169 (1317)<br />P(warming)/human biodiversity: -.168 (1301)<br />Abortion/basic income: .168 (1314)<br />Immigration/Great Stagnation: -.163 (1281)<br />P(God)/feminism: -.159 (1294) <br />P(Supernatural)/feminism: -.158 (1292)<br />Human biodiversity/Great Stagnation: .152 (1287)<br />Social justice/Great Stagnation: -.135 (1242)<br />Number of languages/taxes: -.133 (1242)<br />P(God)/P(Warming): -.132 (1491)<br />P(Supernatural)/immigration: -.131 (1284)<br />P(Religion)immigration: -.129 (1296)<br />P(God)/immigration: -.127 (1286)<br />P(Supernatural)/P(Warming): -.125 (1487)<br />P(Supernatural)/social justice: -.125 (1227)<br />P(God)/taxes: -.145<br />Minimum wage/Great Stagnation: -124 (1269)<br />Immigration/minimum wage: .122 (1308)<br />Great Stagnation/taxes: -.121 (1270)<br />P(Religion)/P(Warming): -.113 (1505)<br />P(Supernatural)/taxes: -.113 (1265)<br />Feminism/Great Stagnation: -.112 (1295) <br />Number of children/abortion: -.112 (1386)<br />P(Religion)/basic income: -.108 (1296)<br />Number of current partners/feminism: .108 (1364)<br />Basic income/human biodiversity: -.106 (1301) <br />P(God)/Basic Income: -.105 (1255)<br />Number of current partners/basic income: .105 (1320)<br />Human biodiversity/number of languages: .103 (1253)<br />Number of children/basic income: -.099 (1322)<br />Number of children/P(Warming): -.091 (1535)</p>\n<h2>V. Hypothesis Testing</h2>\n<p><strong>A. Do people in the effective altruism movement donate more money to charity? Do they donate a higher percent of their income to charity? Are they just generally more altruistic people?</strong></p>\n<p>1265 people told us how much they give to charity; of those, 450 gave nothing. On average, effective altruists (n = 412) donated $2503 to charity, and other people (n = 853) donated $523&nbsp; - obviously a significant result. Effective altruists gave on average $800 to MIRI or CFAR, whereas others gave $53. Effective altruists gave on average $16 to other x-risk related charities; others gave only $2.</p>\n<p>In order to calculate percent donated I divided charity donations by income in the 947&nbsp; people helpful enough to give me both numbers. Of those 947, 602 donated nothing to charity, and so had a percent donated of 0. At the other extreme, three&nbsp; people donated 50% of their (substantial) incomes to charity, and 55 people donated at least 10%. I don't want to draw any conclusions about the community from this because the people who provided both their income numbers and their charity numbers are a highly self-selected sample.</p>\n<p>303 effective altruists donated, on average, 3.5% of their income to charity, compared to 645 others who donated, on average, 1% of their income to charity. A small but significant (p &lt; .001) victory for the effective altruism movement.</p>\n<p>But are they more compassionate people in general? After throwing out the people who said they wanted to give blood but couldn't for one or another reason, I got 1255 survey respondents giving me an unambiguous answer (yes or no) about whether they'd ever given blood. I found that 51% of effective altruists had given blood compared to 47% of others - a difference which did not reach statistical significance.</p>\n<p>Finally, at the end of the survey I had a question offering respondents a chance to cooperate (raising the value of a potential monetary prize to be given out by raffle to a random respondent) or defect (decreasing the value of the prize, but increasing their own chance of winning the raffle). 73% of effective altruists cooperated compared to 70% of others - an insignificant difference.</p>\n<p>Conclusion: effective altruists give more money to charity, both absolutely and as a percent of income, but are no more likely (or perhaps only slightly more likely) to be compassionate in other ways.</p>\n<p><strong>B. Can we finally resolve this IQ controversy that comes up every year?</strong></p>\n<p>The story so far - our first survey in 2009 found an average IQ of 146. Everyone said this was stupid, no community could possibly have that high an average IQ, it was just people lying and/or reporting results from horrible Internet IQ tests.<br />Although IQ fell somewhat the next few years - to 140 in 2011 and 139 in 2012 - people continued to complain. So in 2012 we started asking for SAT and ACT scores, which are known to correlate well with IQ and are much harder to get wrong. These scores confirmed the 139 IQ result on the 2012 test. But people still objected that something must be up.</p>\n<p>This year our IQ has fallen further to 138 (no Flynn Effect for us!) but for the first time we asked people to describe the IQ test they used to get the number. So I took a subset of the people with the most unimpeachable IQ tests - ones taken after the age of 15 (when IQ is more stable), and from a seemingly reputable source. I counted a source as reputable either if it name-dropped a specific scientifically validated IQ test (like WAIS or Raven's Progressive Matrices), if it was performed by a reputable institution (a school, a hospital, or a psychologist), or if it was a Mensa exam proctored by a Mensa official.</p>\n<p>This subgroup of 101 people with very reputable IQ tests had an average IQ of 139 - exactly the same as the average among survey respondents as a whole.</p>\n<p>I don't know for sure that Mensa is on the level, so I tried again deleting everyone who took a Mensa test - leaving just the people who could name-drop a well-known test or who knew it was administered by a psychologist in an official setting. This caused a precipitous drop all the way down to 138.</p>\n<p>The IQ numbers have time and time again answered every challenge raised against them and should be presumed accurate.</p>\n<p><strong>C. Can we predict who does or doesn't cooperate on prisoner's dilemmas?</strong></p>\n<p>As mentioned above, I included a prisoner's dilemma type question in the survey, offering people the chance to make a little money by screwing all the other survey respondents over.</p>\n<p>Tendency to cooperate on the prisoner's dilemma was most highly correlated with items in the general leftist political cluster identified by Ozy above. It was most notable for support for feminism, with which it had a correlation of .15, significant at the p &lt; .01 level, and minimum wage, with which it had a correlation of .09, also significant at p &lt; .01. It was also significantly correlated with belief that other people would cooperate on the same question.</p>\n<p>I compared two possible explanations for this result. First, leftists are starry-eyed idealists who believe everyone can just get along - therefore, they expected other people to cooperate more, which made them want to cooperate more. Or, second, most Less Wrongers are white, male, and upper class, meaning that support for leftist values - which often favor nonwhites, women, and the lower class - is itself a symbol of self-sacrifce and altruism which one would expect to correlate with a question testing self-sacrifice and altruism.</p>\n<p>I tested the \"starry-eyed idealist\" hypothesis by checking whether leftists were more likely to believe other people would cooperate. They were not - the correlation was not significant at any level.</p>\n<p>I tested the \"self-sacrifice\" hypothesis by testing whether the feminism correlation went away in women. For women, supporting feminism is presumably not a sign of willingness to self-sacrifice to help an out-group, so we would expect the correlation to disappear.</p>\n<p>In the all-female sample, the correlation between feminism and PD cooperation shrunk from .15 to a puny .04, whereas the correlation between the minimum wage and PD was previously .09 and stayed exactly the same at .09. This provides some small level of support for the hypothesis that the leftist correlation with PD cooperation represents a willingness to self-sacrifice in a population who are not themselves helped by leftist values.</p>\n<p>(on the other hand, neither leftists nor cooperators were more likely to give money to charity, so if this is true it's a very selective form of self-sacrifice)</p>\n<h2>VI. Monetary Prize</h2>\n<p>1389 people answered the prize question at the bottom. 71.6% of these [n = 995] cooperated; 28.4% [n = 394] defected. <br />The prize goes to a person whose two word phrase begins with \"eponymous\". If this person posts below (or PMs or emails me) the second word in their phrase, I will give them $60 * 71.6%, or about $43. I can pay to a PayPal account, a charity of their choice that takes online donations, or a snail-mail address via check.</p>\n<h2>VII. Calibration Questions</h2>\n<p>The population of Europe, according to designated arbiter Wikipedia, is 739 million people.</p>\n<p>People were really really bad at giving their answers in millions. I got numbers anywhere from 3 (really? three million people in Europe?) to 3 billion (3 million billion people = 3 quadrillion). I assume some people thought they were answering in billions, others in thousands, and other people thought they were giving a straight answer in number of individuals.</p>\n<p>My original plan was to just adjust these to make them fit, but this quickly encountered some pitfalls. Suppose someone wrote 1 million (as one person did). Could I fairly guess they meant 100 million, even though there's really no way to guess that from the text itself? 1 billion? Maybe they just thought there were really one million people in Europe?</p>\n<p>If I was too aggressive correcting these, everyone would get close to the right answer not because they were smart, but because I had corrected their answers. If I wasn't aggressive enough, I would end up with some guy who answered 3 quadrillion Europeans totally distorting the mean.</p>\n<p>I ended up deleting 40 answers that suggested there were less than ten million or more than eight billion Europeans, on the grounds that people probably weren't really that far off so it was probably some kind of data entry error, and correcting everyone who entered a reasonable answer in individuals to answer in millions as the question asked.</p>\n<p>The remaining 1457 people who can either follow simple directions or at least fail to follow them in a predictable way estimated an average European population in millions of 601 + 35.6 (380, 500, 750).</p>\n<p>Respondents were told to aim for within 10% of the real value, which means they wanted between 665 million and 812 million. 18.7% of people [n = 272] got within that window.</p>\n<p>I divided people up into calibration brackets of [0,5], [6,15], [16, 25] and so on. The following are what percent of people in each bracket were right.</p>\n<p>[0,5]: 7.7%<br />[6,15]: 12.4%<br />[16,25]: 15.1%<br />[26,35]: 18.4%<br />[36,45]: 20.6%<br />[46,55]: 15.4%<br />[56,65]: 16.5%<br />[66,75]: 21.2%<br />[76,85]: 36.4%<br />[86,95]: 48.6%<br />[96,100]: 100%<br /><br />Among people who should know better (those who have read all or most of the Sequences and have &gt; 500 karma, a group of 162 people)</p>\n<p>[0,5]: 0<br />[6,15]: 17.4%<br />[16,25]: 25.6%<br />[26,35]: 16.7%<br />[36,45]: 26.7%<br />[46,55]: 25%<br />[56,65]: 0%<br />[66,75]: 8.3%<br />[76,85]: 40%<br />[86,95]: 66.6%<br />[96,100]: 66.6%<br /><br />Clearly, the people who <em>should </em>know better <em>don't</em>.</p>\n<p><img src=\"http://slatestarcodex.com/Stuff/calibration2013.png\" alt=\"\" /></p>\n<p>This graph represents your performance relative to ideal performance. Dipping below the blue ideal line represents overconfidence; rising above it represents underconfidence. With few exceptions you were very overconfident. Note that there were so few \"elite\" LWers at certain levels that the graph becomes very noisy and probably isn't representing much; that huge drop at 60 represents like two or three people. The orange \"typical LWer\" line is much more robust.</p>\n<p>There is one other question that gets at the same idea of overconfidence. 651 people were willing to give valid 90% confidence interval on what percent of people would cooperate (this is my fault; I only added this question about halfway through the survey once I realized it would be interesting to investigate). I deleted four for giving extremely high outliers like 9999% which threw off the results, leaving 647 valid answers. The average confidence interval was [28.3, 72.0], which just BARELY contains the correct answer of 71.6%. Of the 647 of you, only 346 (53.5%) gave 90% confidence intervals that included the correct answer!</p>\n<p>Last year I complained about horrible performance on calibration questions, but we all decided it was probably just a fluke caused by a particularly weird question. This year's results suggest that was no fluke and that we haven't even learned to overcome the one bias that we can measure super-well and which is most easily trained away. Disappointment!</p>\n<h2>VIII. Public Data</h2>\n<p>There's still a lot more to be done with this survey. User:Unnamed has promised to analyze the \"Extra Credit: CFAR Questions\" section (not included in this post), but so far no one has looked at the \"Extra Credit: Questions From Sarah\" section, which I didn't really know what to do with. And of course this is most complete survey yet for seeking classic findings like \"People who disagree with me about politics are stupid and evil\".</p>\n<p>1480 people - over 90% of the total - kindly allowed me to make their survey data public. I have included all their information except the timestamp (which would make tracking pretty easy) including their secret passphrases (by far the most interesting part of this exercise was seeing what unusual two word phrases people could come up with on short notice).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pJJdcZgB6mPNWoSWr", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 78, "baseScore": 109, "extendedScore": null, "score": 0.000251, "legacy": true, "legacyId": "25308", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": "2019-06-24T07:28:07.130Z", "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": true, "hideFrontpageComments": false, "maxBaseScore": 78, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Thanks to everyone who took the 2013 Less Wrong Census/Survey. Extra thanks to Ozy, who helped me out with the data processing and statistics work, and to everyone who suggested questions.</p>\n<p>This year's results are below. Some of them may make more sense in the context of the original survey questions, <a href=\"https://docs.google.com/spreadsheet/viewform?usp=drive_web&amp;formkey=dGZ6a1NfZ0V1SV9xdE1ma0pUMTc1S1E6MA#gid=0\">which can be seen here</a>. Please do not try to take the survey as it is over and your results will not be counted.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Part_I__Population\">Part I. Population</h2>\n<p>1636 people answered the survey.</p>\n<p>Compare this to 1195 people last year, and 1090 people the year before that. It would seem the site is growing, but we do have to consider that each survey lasted a different amount of time; for example, last survey lasted 23 days, but this survey lasted 40.</p>\n<p>However, almost everyone who takes the survey takes it in the first few weeks it is available. 1506 of the respondents answered within the first 23 days, proving that even if the survey ran the same length as last year's, there would still have been growth.<br>As we will see lower down, growth is smooth across all categories of users (lurkers, commenters, posters) EXCEPT people who have posted to Main, the number of which remains nearly the same from year to year.</p>\n<p>We continue to have very high turnover - only 40% of respondents this year say they also took the survey last year.</p>\n<h2 id=\"II__Categorical_Data\">II. Categorical Data</h2>\n<p><strong>SEX:</strong><br>Female: 161, 9.8%<br>Male: 1453, 88.8%<br>Other: 1, 0.1%<br>Did not answer: 21, 1.3%</p>\n<p><em>[[Ozy is disappointed that we've lost 50% of our intersex readers.]]</em></p>\n<p><strong>GENDER:</strong><br>F (cisgender): 140, 8.6%<br>F (transgender MtF): 20, 1.2%<br>M (cisgender): 1401, 85.6%<br>M (transgender FtM): 5, 0.3%<br>Other: 49, 3%<br>Did not answer: 21, 1.3%</p>\n<p><strong>SEXUAL ORIENTATION:</strong><br>Asexual: 47, 2.9%<br>Bisexual: 188, 12.2%<br>Heterosexual: 1287, 78.7%<br>Homosexual: 45, 2.8%<br>Other: 39, 2.4%<br>Did not answer: 19, 1.2%</p>\n<p><strong>RELATIONSHIP STYLE:</strong><br>Prefer monogamous: 829, 50.7%<br>Prefer polyamorous: 234, 14.3%<br>Other: 32, 2.0%<br>Uncertain/no preference: 520, 31.8%<br>Did not answer: 21, 1.3%</p>\n<p><strong>NUMBER OF CURRENT PARTNERS:</strong><br>0: 797, 48.7%<br>1: 728, 44.5%<br>2: 66, 4.0%<br>3: 21, 1.3%<br>4: 1, .1%<br>6: 3, .2%<br>Did not answer: 20, 1.2%</p>\n<p><strong>RELATIONSHIP STATUS:</strong><br>Married: 304, 18.6%<br>Relationship: 473, 28.9%<br>Single: 840, 51.3%</p>\n<p><strong>RELATIONSHIP GOALS: </strong><br>Looking for more relationship partners: 617, 37.7%<br>Not looking for more relationship partners: 993, 60.7%<br>Did not answer: 26, 1.6%</p>\n<p><strong>HAVE YOU DATED SOMEONE YOU MET THROUGH THE LESS WRONG COMMUNITY?</strong><br>Yes: 53, 3.3%<br>I didn't meet them through the community but they're part of the community now: 66, 4.0%<br>No: 1482, 90.5%<br>Did not answer: 35, 2.1%</p>\n<p><strong>COUNTRY:</strong><br>United States: 895, 54.7%<br>United Kingdom: 144, 8.8%<br>Canada: 107, 6.5%<br>Australia: 69, 4.2%<br>Germany: 68, 4.2%<br>Finland: 35, 2.1%<br>Russia: 22, 1.3%<br>New Zealand: 20, 1.2%<br>Israel: 17, 1.0%<br>France: 16, 1.0%<br>Poland: 16, 1.0%</p>\n<p><strong>LESS WRONGERS PER CAPITA:</strong><br>Finland: 1/154,685.<br>New Zealand: 1/221,650. <br>Canada: 1/325,981. <br>Australia: 1/328,659.<br>United States: 1/350,726 <br>United Kingdom: 1/439,097<br>Israel: 1/465,176.<br>Germany: 1/1,204,264.<br>Poland: 1/2,408,750.<br>France: 1/4,106,250. <br>Russia: 1/6,522,727</p>\n<p><strong>RACE: </strong><br>Asian (East Asian): 60, 3.7%<br>Asian (Indian subcontinent): 37, 2.3%<br>Black: 11, .7%<br>Middle Eastern: 9, .6%<br>White (Hispanic): 73, 4.5%<br>White (non-Hispanic): 1373, 83.9%<br>Other: 51, 3.1%<br>Did not answer: 22, 1.3%</p>\n<p><strong>WORK STATUS:</strong><br>Academics (teaching): 77, 4.7%<br>For-profit work: 552, 33.7%<br>Government work: 55, 3.4%<br>Independently wealthy: 14, .9%<br>Non-profit work: 46, 2.8%<br>Self-employed: 103, 6.3%<br>Student: 661, 40.4%<br>Unemployed: 105, 6.4%<br>Did not answer: 23, 1.4%</p>\n<p><strong>PROFESSION:</strong><br>Art: 27, 1.7%<br>Biology: 26, 1.6%<br>Business: 44, 2.7%<br>Computers (AI): 47, 2.9%<br>Computers (other academic computer science): 107, 6.5%<br>Computers (practical): 505, 30.9%<br>Engineering: 128, 7.8%<br>Finance/economics: 92, 5.6%<br>Law: 36, 2.2%<br>Mathematics: 139, 8.5%<br>Medicine: 31, 1.9%<br>Neuroscience: 13, .8%<br>Philosophy: 41, 2.5%<br>Physics: 92, 5.6%<br>Psychology: 34, 2.1%<br>Statistics: 23, 1.4%<br>Other hard science: 31, 1.9%<br>Other social science: 43, 2.6%<br>Other: 139, 8.5%<br>Did not answer: 38, 2.3%</p>\n<p><strong>DEGREE:</strong><br>None: 84, 5.1%<br>High school: 444, 27.1%<br>2 year degree: 68, 4.2%<br>Bachelor's: 554, 33.9%<br>Master's: 323, 19.7%<br>MD/JD/other professional degree: 31, 2.0%<br>PhD.: 90, 5.5%<br>Other: 22, 1.3%<br>Did not answer: 19, 1.2%</p>\n<p><strong>POLITICAL:</strong><br>Communist: 11, .7%<br>Conservative: 64, 3.9%<br>Liberal: 580, 35.5%<br>Libertarian: 437, 26.7%<br>Socialist: 502, 30.7%<br>Did not answer: 42, 2.6%</p>\n<p><strong>COMPLEX POLITICAL WITH WRITE-IN:</strong><br>Anarchist: 52, 3.2%<br>Conservative: 16, 1.0%<br>Futarchist: 42, 2.6%<br>Left-libertarian: 142, 8.7%<br>Liberal: 5<br>Moderate: 53, 3.2%<br>Pragmatist: 110, 6.7%<br>Progressive: 206, 12.6%<br>Reactionary: 40, 2.4%<br>Social democrat: 154, 9.5%<br>Socialist: 135, 8.2%<br>Did not answer: 26.2%</p>\n<p><em>[[All answers with more than 1% of the Less Wrong population included. Other answers which made Ozy giggle included \"are any of you kings?! why do you CARE?!\", \"Exclusionary: you are entitled to an opinion on nuclear power when you know how much of your power is nuclear\", \"having-well-founded-opinions-is-really-hard-ist\", \"kleptocrat\", \"pirate\", and \"SPECIAL FUCKING SNOWFLAKE.\"]]</em></p>\n<p><strong>AMERICAN PARTY AFFILIATION:</strong><br>Democratic Party: 226, 13.8%<br>Libertarian Party: 31, 1.9%<br>Republican Party: 58, 3.5%<br>Other third party: 19, 1.2%<br>Not registered: 447, 27.3%<br>Did not answer or non-American: 856, 52.3%</p>\n<p><strong>VOTING:</strong><br>Yes: 936, 57.2%<br>No: 450, 27.5%<br>My country doesn't hold elections: 2, 0.1%<br>Did not answer: 249, 15.2%</p>\n<p><strong>RELIGIOUS VIEWS:</strong><br>Agnostic: 165, 10.1%<br>Atheist and not spiritual: 1163, 71.1%<br>Atheist but spiritual: 132, 8.1%<br>Deist/pantheist/etc.: 36, 2.2%<br>Lukewarm theist: 53, 3.2%<br>Committed theist 64, 3.9%</p>\n<p><strong>RELIGIOUS DENOMINATION (IF THEIST):</strong><br>Buddhist: 22, 1.3%<br>Christian (Catholic): 44, 2.7%<br>Christian (Protestant): 56, 3.4%<br>Jewish: 31, 1.9%<br>Mixed/Other: 21, 1.3%<br>Unitarian Universalist or similar: 25, 1.5%</p>\n<p><em>[[This includes all religions with more than 1% of Less Wrongers. Minority religions include Dzogchen, Daoism, various sorts of Paganism, Simulationist, a very confused secular humanist, Kopmist, Discordian, and a Cultus Deorum Romanum practitioner whom Ozy wants to be friends with.]]</em></p>\n<p><strong>FAMILY RELIGION:</strong><br>Agnostic: 129, 11.6%<br>Atheist and not spiritual: 225, 13.8%<br>Atheist but spiritual: 73, 4.5%<br>Committed theist: 423, 25.9%<br>Deist/pantheist, etc.: 42, 2.6%<br>Lukewarm theist: 563, 34.4%<br>Mixed/other: 97, 5.9%<br>Did not answer: 24, 1.5%</p>\n<p><strong>RELIGIOUS BACKGROUND:</strong><br>Bahai: 3, 0.2%<br>Buddhist: 13, .8%<br>Christian (Catholic): 418, 25.6%<br>Christian (Mormon): 38, 2.3%<br>Christian (Protestant): 631, 38.4%<br>Christian (Quaker): 7, 0.4%<br>Christian (Unitarian Universalist or similar): 32, 2.0%<br>Christian (other non-Protestant): 99, 6.1%<br>Christian (unknown): 3, 0.2%<br>Eckankar: 1, 0.1%<br>Hindu: 29, 1.8%<br>Jewish: 136, 8.3%<br>Muslim: 12, 0.7%<br>Native American Spiritualist: 1, 0.1%<br>Mixed/Other: 85, 5.3%<br>Sikhism: 1, 0.1%<br>Traditional Chinese: 11, .7%<br>Wiccan: 1, 0.1%<br>None: 8, 0.4%<br>Did not answer: 107, 6.7%</p>\n<p><strong>MORAL VIEWS:</strong><br>Accept/lean towards consequentialism: 1049, 64.1%<br>Accept/lean towards deontology: 77, 4.7%<br>Accept/lean towards virtue ethics: 197, 12.0%<br>Other/no answer: 276, 16.9%<br>Did not answer: 37, 2.3%</p>\n<p><strong>CHILDREN</strong><br>0: 1414, 86.4%<br>1: 77, 4.7%<br>2: 90, 5.5%<br>3: 25, 1.5%<br>4: 7, 0.4%<br>5: 1, 0.1%<br>6: 2, 0.1%<br>Did not answer: 20, 1.2%</p>\n<p><strong>MORE CHILDREN:</strong><br>Have no children, don't want any: 506, 31.3%<br>Have no children, uncertain if want them: 472, 29.2%<br>Have no children, want children: 431, 26.7%<br>Have no children, didn't answer: 5, 0.3%<br>Have children, don't want more: 124, 7.6%<br>Have children, uncertain if want more: 25, 1.5%<br>Have children, want more: 53, 3.2%</p>\n<p><strong>HANDEDNESS:</strong><br>Right: 1256, 76.6%<br>Left: 145, 9.5%<br>Ambidextrous: 36, 2.2%<br>Not sure: 7, 0.4%<br>Did not answer: 182, 11.1%</p>\n<p><strong>LESS WRONG USE: </strong><br>Lurker (no account): 584, 35.7%<br>Lurker (account) 221, 13.5%<br>Poster (comment, no post): 495, 30.3%<br>Poster (Discussion, not Main): 221, 12.9%<br>Poster (Main): 103, 6.3%</p>\n<p><strong>SEQUENCES:</strong><br>Never knew they existed: 119, 7.3%<br>Knew they existed, didn't look at them: 48, 2.9%<br>~25% of the Sequences: 200, 12.2%<br>~50% of the Sequences: 271, 16.6%<br>~75% of the Sequences: 225, 13.8%<br>All the Sequences: 419, 25.6%<br>Did not answer: 24, 1.5%</p>\n<p><strong>MEETUPS: </strong><br>No: 1134, 69.3%<br>Yes, once or a few times: 307, 18.8%<br>Yes, regularly: 159, 9.7%</p>\n<p><strong>HPMOR:</strong><br>No: 272, 16.6%<br>Started it, haven't finished: 255, 15.6%<br>Yes, all of it: 912, 55.7%</p>\n<p><strong>CFAR WORKSHOP ATTENDANCE:</strong><br>Yes, a full workshop: 105, 6.4%<br>A class but not a full-day workshop: 40, 2.4%<br>No: 1446, 88.3%<br>Did not answer: 46, 2.8%</p>\n<p><strong>PHYSICAL INTERACTION WITH LW COMMUNITY: </strong><br>Yes, all the time: 94, 5.7%<br>Yes, sometimes: 179, 10.9%<br>No: 1316, 80.4%<br>Did not answer: 48, 2.9%</p>\n<p><strong>VEGETARIAN:</strong><br>No: 1201, 73.4%<br>Yes: 213, 13.0%<br>Did not answer: 223, 13.6%</p>\n<p><strong>SPACED REPETITION:</strong><br>Never heard of them: 363, 22.2%<br>No,&nbsp; but I've heard of them: 495, 30.2%<br>Yes, in the past: 328, 20%<br>Yes, currently: 219, 13.4%<br>Did not answer: 232, 14.2%</p>\n<p><strong>HAVE YOU TAKEN PREVIOUS INCARNATIONS OF THE LESS WRONG SURVEY?</strong><br>Yes: 638, 39.0%<br>No: 784, 47.9%<br>Did not answer: 215, 13.1%</p>\n<p><strong>PRIMARY LANGUAGE:</strong><br>English: 1009, 67.8%<br>German: 58, 3.6%<br>Finnish: 29, 1.8%<br>Russian: 25, 1.6%<br>French: 17, 1.0%<br>Dutch: 16, 1.0%<br>Did not answer: 15.2%</p>\n<p><em>[[This includes all answers that more than 1% of respondents chose. Other languages include Urdu, both Czech and Slovakian, Latvian, and Love.]]</em></p>\n<p><strong>ENTREPRENEUR:</strong><br>I don't want to start my own business: 617, 37.7%<br>I am considering starting my own business: 474, 29.0%<br>I plan to start my own business: 113, 6.9%<br>I've already started my own business: 156, 9.5%<br>Did not answer: 277, 16.9%</p>\n<p><strong>EFFECTIVE ALTRUIST:</strong><br>Yes: 468, 28.6%<br>No: 883, 53.9%<br>Did not answer: 286, 17.5%</p>\n<p><strong>WHO ARE YOU LIVING WITH?</strong><br>Alone: 348, 21.3%<br>With family: 420, 25.7%<br>With partner/spouse: 400, 24.4%<br>With roommates: 450, 27.5%<br>Did not answer: 19, 1.3%</p>\n<p><strong>DO YOU GIVE BLOOD?</strong><br>No: 646, 39.5%<br>No, only because I'm not allowed: 157, 9.6%<br>Yes, 609, 37.2%<br>Did not answer: 225, 13.7%</p>\n<p><strong>GLOBAL CATASTROPHIC RISK:</strong><br>Pandemic (bioengineered): 374, 22.8%<br>Environmental collapse including global warming: 251, 15.3%<br>Unfriendly AI: 233, 14.2%<br>Nuclear war: 210, 12.8%<br>Pandemic (natural) 145, 8.8%<br>Economic/political collapse: 175, 1, 10.7%<br>Asteroid strike: 65, 3.9%<br>Nanotech/grey goo: 57, 3.5%<br>Didn't answer: 99, 6.0%</p>\n<p><strong>CRYONICS STATUS:</strong><br>Never thought about it / don't understand it: 69, 4.2%<br>No, and don't want to: 414, 25.3%<br>No, still considering: 636, 38.9%<br>No, would like to: 265, 16.2%<br>No, would like to, but it's unavailable: 119, 7.3%<br>Yes: 66, 4.0%<br>Didn't answer: 68, 4.2%</p>\n<p><strong>NEWCOMB'S PROBLEM: </strong><br>Don't understand/prefer not to answer: 92, 5.6%<br>Not sure: 103, 6.3%<br>One box: 1036, 63.3%<br>Two box: 119, 7.3%<br>Did not answer: 287, 17.5%</p>\n<p><strong>GENOMICS:</strong><br>Yes: 177, 10.8%<br>No: 1219, 74.5%<br>Did not answer: 241, 14.7%</p>\n<p><strong>REFERRAL TYPE:</strong><br>Been here since it started in the Overcoming Bias days: 285, 17.4%<br>Referred by a friend: 241, 14.7%<br>Referred by a search engine: 148, 9.0%<br>Referred by HPMOR: 400, 24.4%<br>Referred by a link on another blog: 373, 22.8%<br>Referred by a school course: 1, .1%<br>Other: 160, 9.8%<br>Did not answer: 29, 1.9%</p>\n<p><strong>REFERRAL SOURCE:</strong><br>Common Sense Atheism: 33<br>Slate Star Codex: 20<br>Hacker News: 18<br>Reddit: 18<br>TVTropes: 13<br>Y Combinator: 11<br>Gwern: 9<br>RationalWiki: 8<br>Marginal Revolution: 7<br>Unequally Yoked: 6<br>Armed and Dangerous: 5<br>Shtetl Optimized: 5<br>Econlog: 4<br>StumbleUpon: 4<br>Yudkowsky.net: 4<br>Accelerating Future: 3<br>Stares at the World: 3<br>xkcd: 3<br>David Brin: 2<br>Freethoughtblogs: 2<br>Felicifia: 2<br>Givewell: 2<br>hatrack.com: 2<br>HPMOR: 2<br>Patri Friedman: 2<br>Popehat: 2<br>Overcoming Bias: 2<br>Scientiststhesis: 2<br>Scott Young: 2<br>Stardestroyer.net: 2<br>TalkOrigins: 2<br>Tumblr: 2</p>\n<p><em>[[This includes all sources with&nbsp; more than one referral; needless to say there was a long tail]]</em></p>\n<h2 id=\"III__Numeric_Data\">III. Numeric Data</h2>\n<p>(in the form mean + stdev (1st quartile, 2nd quartile, 3rd quartile) [n = number responding]))</p>\n<p>Age: 27.4 + 8.5 (22, 25, 31) [n = 1558]<br>Height: 176.6 cm + 16.6 (173, 178, 183) [n = 1267]</p>\n<p>Karma Score: 504 + 2085 (0, 0, 100) [n = 1438]<br>Time in community: 2.62 years + 1.84 (1, 2, 4) [n = 1443]<br>Time on LW: 13.25 minutes/day + 20.97 (2, 10, 15) [n = 1457]</p>\n<p>IQ: 138.2 + 13.6 (130, 138, 145) [n = 506]<br>SAT out of 1600: 1474 + 114 (1410, 1490, 1560) [n = 411]<br>SAT out of 2400: 2207 + 161 (2130, 2240, 2330) [n = 333]<br>ACT out of 36: 32.8 + 2.5 (32, 33, 35) [n = 265]</p>\n<p>P(Aliens in observable universe): 74.3 + 32.7 (60, 90, 99) [n = 1496]<br>P(Aliens in Milky Way): 44.9 + 38.2 (5, 40, 85) [n = 1482]<br>P(Supernatural): 7.7 + 22 (0E-9, .000055, 1) [n = 1484]<br>P(God): 9.1 + 22.9 (0E-11, .01, 3) [n = 1490]<br>P(Religion): 5.6 + 19.6 (0E-11, 0E-11, .5) [n = 1497]<br>P(Cryonics): 22.8 + 28 (2, 10, 33) [n = 1500]&nbsp;&nbsp; <br>P(AntiAgathics): 27.6 + 31.2 (2, 10, 50) [n = 1493]<br>P(Simulation): 24.1 + 28.9 (1, 10, 50) [n = 1400]<br>P(ManyWorlds): 50 + 29.8 (25, 50, 75) [n = 1373]<br>P(Warming): 80.7 + 25.2 (75, 90, 98) [n = 1509]<br>P(Global catastrophic risk): 72.9 + 25.41 (60, 80, 95) [n = 1502]<br>Singularity year: 1.67E +11 + 4.089E+12 (2060, 2090, 2150) [n = 1195]</p>\n<p><em>[[Of course, this question was hopelessly screwed up by people who insisted on filling the whole answer field with 9s, or other such nonsense. I went back and eliminated all outliers - answers with more than 4 digits or answers in the past - which changed the results to: 2150 + 226 (2060, 2089, 2150)]]</em></p>\n<p>Yearly Income: $73,226 +423,310 (10,000, 37,000, 80,000) [n = 910]<br>Yearly Charity: $1181.16 + 6037.77 (0, 50, 400) [n = 1231]<br>Yearly Charity to MIRI/CFAR: $307.18 + 4205.37 (0, 0, 0) [n = 1191]<br>Yearly Charity to X-risk (excluding MIRI or CFAR): $6.34 + 55.89 (0, 0, 0) [n = 1150]<br><br>Number of Languages: 1.49 + .8 (1, 1, 2) [n = 1345]<br>Older Siblings: 0.5 + 0.9 (0, 0, 1) [n = 1366]<br>Time Online/Week: 42.7 hours + 24.8 (25, 40, 60) [n = 1292]<br>Time Watching TV/Week: 4.2 hours + 5.7 (0, 2, 5) [n = 1316]</p>\n<p><em>[[The next nine questions ask respondents to rate how favorable they are to the political idea or movement above on a scale of 1 to 5, with 1 being \"not at all favorable\" and 5 being \"very favorable\". You can see the exact wordings of the questions on the survey.]]</em></p>\n<p>Abortion: 4.4 + 1 (4, 5, 5) [n = 1350]<br>Immigration: 4.1 + 1 (3, 4, 5) [n = 1322]<br>Basic Income: 3.8 + 1.2 (3, 4, 5) [n = 1289]<br>Taxes: 3.1 + 1.3 (2, 3, 4) [n = 1296]<br>Feminism: 3.8 + 1.2 (3, 4, 5) [n = 1329]<br>Social Justice: 3.6 + 1.3 (3, 4, 5) [n = 1263]<br>Minimum Wage: 3.2 + 1.4 (2, 3, 4) [n = 1290]<br>Great Stagnation: 2.3 + 1 (2, 2, 3) [n = 1273]<br>Human Biodiversity: 2.7 + 1.2 (2, 3, 4) [n = 1305]</p>\n<h2 id=\"IV__Bivariate_Correlations\">IV. Bivariate Correlations</h2>\n<p>Ozy ran bivariate correlations between all the numerical data and recorded all correlations that were significant at the .001 level in order to maximize the chance that these are genuine results. The format is variable/variable: Pearson correlation (n). Yvain is not hugely on board with the idea of running correlations between everything and seeing what sticks, but will grudgingly publish the results because of the very high bar for significance (p &lt; .001 on ~800 correlations suggests &lt; 1 spurious result) and because he doesn't want to have to do it himself.</p>\n<p><strong>Less Political:</strong><br>SAT score (1600)/SAT score (2400): .835 (56) <br>Charity/MIRI and CFAR donations: .730 (1193)<br>SAT score out of 2400/ACT score: .673 (111)<br>SAT score out of 1600/ACT score: .544 (102)<br>Number of children/age: .507 (1607) <br>P(Cryonics)/P(AntiAgathics): .489 (1515)<br>SAT score out of 1600/IQ: .369 (173)<br>MIRI and CFAR donations/XRisk donations: .284 (1178)<br>Number of children/ACT score: -.279 (269)<br>Income/charity: .269 (884)<br>Charity/Xrisk charity: .262 (1161)<br>P(Cryonics)/P(Simulation): .256 (1419)<br>P(AntiAgathics)/P(Simulation): .253 (1418)<br>Number of current partners/age: .238 (1607)&nbsp; <br>Number of children/SAT score (2400): -.223 (345)<br>Number of current partners/number of children: .205 (1612)<br>SAT score out of 1600/age: -.194 (422)<br>Charity/age: .175 (1259)<br>Time on Less Wrong/IQ: -.164 (492)<br>P(Warming)/P(GlobalCatastrophicRisk): .156 (1522)<br>Number of current partners/IQ: .155 (521)<br>P(Simulation)/age: -.153 (1420)<br>Immigration/P(ManyWorlds): .150 (1195)<br>Income/age: .150 (930)<br>P(Cryonics)/age: -.148 (1521)<br>Income/children: .145 (931)<br>P(God)/P(Simulation): .142 (1409)<br>Number of children/P(Aliens): .140 (1523)<br>P(AntiAgathics)/Hours Online: .138 (1277)<br>Number of current partners/karma score: .137 (1470)<br>Abortion/P(ManyWorlds): .122 (1215)<br>Feminism/Xrisk charity donations: -.122 (1104)<br>P(AntiAgathics)/P(ManyWorlds) .118 (1381) <br>P(Cryonics)/P(ManyWorlds): .117 (1387)<br>Karma score/Great Stagnation: .114 (1202)<br>Hours online/P(simulation): .114 (1199)<br>P(Cryonics)/Hours Online: .113 (1279)<br>P(AntiAgathics)/Great Stagnation: -.111 (1259)<br>Basic income/hours online: .111 (1200)<br>P(GlobalCatastrophicRisk)/Great Stagnation: -.110 (1270)<br>Age/X risk charity donations: .109 (1176)<br>P(AntiAgathics)/P(GlobalCatastrophicRisk): -.109 (1513)<br>Time on Less Wrong/age: -.108 (1491)<br>P(AntiAgathics)/Human Biodiversity: .104 (1286)<br>Immigration/Hours Online: .104 (1226)<br>P(Simulation)/P(GlobalCatastrophicRisk): -.103 (1421)<br>P(Supernatural)/height: -.101 (1232)<br>P(GlobalCatastrophicRisk)/height: .101 (1249)<br>Number of children/hours online: -.099 (1321)<br>P(AntiAgathics)/age: -.097 (1514)<br>Karma score/time on LW: .096 (1404)</p>\n<p>This year for the first time P(Aliens) and P(Aliens2) are entirely uncorrelated with each other. Time in Community, Time on LW, and IQ are not correlated with anything particularly interesting, suggesting all three fail to change people's views.</p>\n<p>Results we find amusing: high-IQ and high-karma people have more romantic partners, suggesting that those are attractive traits. There is definitely a Cryonics/Antiagathics/Simulation/Many Worlds cluster of weird beliefs, which younger people and people who spend more time online are slightly more likely to have - weirdly, that cluster seems slightly less likely to believe in global catastrophic risk. Older people and people with more children have more romantic partners (it'd be interesting to see if that holds true for the polyamorous). People who believe in anti-agathics and global catastrophic risk are less likely to believe in a great stagnation (presumably because both of the above rely on inventions). People who spend more time on Less Wrong have lower IQs. Height is, bizarrely, correlated with belief in the supernatural and global catastrophic risk.</p>\n<p>All political viewpoints are correlated with each other in pretty much exactly the way one would expect. They are also correlated with one's level of belief in God, the supernatural, and religion. There are minor correlations with some of the beliefs and number of partners (presumably because polyamory), number of children, and number of languages spoken. We are doing terribly at avoiding Blue/Green politics, people.</p>\n<p><strong>More Political:</strong><br>P(Supernatural)/P(God): .736 (1496)<br>P(Supernatural)/P(Religion): .667 (1492)<br>Minimum wage/taxes: .649 (1299)<br>P(God)/P(Religion): .631 (1496)<br>Feminism/social justice: .619 (1293)<br>Social justice/minimum wage: .508 (1262)<br>P(Supernatural)/abortion: -.469 (1309)<br>Taxes/basic income: .463 (1285)<br>P(God)/abortion: -.461 (1310)<br>Social justice/taxes: .456 (1267)<br>P(Religion)/abortion: -.413<br>Basic income/minimum wage: .392 (1283)<br>Feminism/taxes: .391 (1318)<br>Feminism/minimum wage: .391 (1312)<br>Feminism/human biodiversity: -.365 (1331)<br>Immigration/feminism: .355 (1336)<br>P(Warming)/taxes: .340 (1292)<br>Basic income/social justice: .311 (1270)<br>Immigration/social justice: .307 (1275)<br>P(Warming)/feminism: .294 (1323)<br>Immigration/human biodiversity: -.292 (1313)<br>P(Warming)/basic income: .290 (1287)<br>Social justice/human biodiversity: -.289 (1281)<br>Basic income/feminism: .284 (1313)<br>Human biodiversity/minimum wage: -.273 (1293)<br>P(Warming)/social justice: .271 (1261)<br>P(Warming)/minimum wage: .262 (1284)<br>Human biodiversity/taxes: -.251 (1270).<br>Abortion/feminism: .239 (1356)<br>Abortion/social justice: .220 (1292)<br>P(Warming)/immigration: .215 (1315)<br>Abortion/immigration: .211 (1353)<br>P(Warming)/abortion: .192 (1340)<br>Immigration/taxes: .186 (1322)<br>Basic income/taxes: .174 (1249)<br>Abortion/taxes: .170 (1328)<br>Abortion/minimum wage: .169 (1317)<br>P(warming)/human biodiversity: -.168 (1301)<br>Abortion/basic income: .168 (1314)<br>Immigration/Great Stagnation: -.163 (1281)<br>P(God)/feminism: -.159 (1294) <br>P(Supernatural)/feminism: -.158 (1292)<br>Human biodiversity/Great Stagnation: .152 (1287)<br>Social justice/Great Stagnation: -.135 (1242)<br>Number of languages/taxes: -.133 (1242)<br>P(God)/P(Warming): -.132 (1491)<br>P(Supernatural)/immigration: -.131 (1284)<br>P(Religion)immigration: -.129 (1296)<br>P(God)/immigration: -.127 (1286)<br>P(Supernatural)/P(Warming): -.125 (1487)<br>P(Supernatural)/social justice: -.125 (1227)<br>P(God)/taxes: -.145<br>Minimum wage/Great Stagnation: -124 (1269)<br>Immigration/minimum wage: .122 (1308)<br>Great Stagnation/taxes: -.121 (1270)<br>P(Religion)/P(Warming): -.113 (1505)<br>P(Supernatural)/taxes: -.113 (1265)<br>Feminism/Great Stagnation: -.112 (1295) <br>Number of children/abortion: -.112 (1386)<br>P(Religion)/basic income: -.108 (1296)<br>Number of current partners/feminism: .108 (1364)<br>Basic income/human biodiversity: -.106 (1301) <br>P(God)/Basic Income: -.105 (1255)<br>Number of current partners/basic income: .105 (1320)<br>Human biodiversity/number of languages: .103 (1253)<br>Number of children/basic income: -.099 (1322)<br>Number of children/P(Warming): -.091 (1535)</p>\n<h2 id=\"V__Hypothesis_Testing\">V. Hypothesis Testing</h2>\n<p><strong id=\"A__Do_people_in_the_effective_altruism_movement_donate_more_money_to_charity__Do_they_donate_a_higher_percent_of_their_income_to_charity__Are_they_just_generally_more_altruistic_people_\">A. Do people in the effective altruism movement donate more money to charity? Do they donate a higher percent of their income to charity? Are they just generally more altruistic people?</strong></p>\n<p>1265 people told us how much they give to charity; of those, 450 gave nothing. On average, effective altruists (n = 412) donated $2503 to charity, and other people (n = 853) donated $523&nbsp; - obviously a significant result. Effective altruists gave on average $800 to MIRI or CFAR, whereas others gave $53. Effective altruists gave on average $16 to other x-risk related charities; others gave only $2.</p>\n<p>In order to calculate percent donated I divided charity donations by income in the 947&nbsp; people helpful enough to give me both numbers. Of those 947, 602 donated nothing to charity, and so had a percent donated of 0. At the other extreme, three&nbsp; people donated 50% of their (substantial) incomes to charity, and 55 people donated at least 10%. I don't want to draw any conclusions about the community from this because the people who provided both their income numbers and their charity numbers are a highly self-selected sample.</p>\n<p>303 effective altruists donated, on average, 3.5% of their income to charity, compared to 645 others who donated, on average, 1% of their income to charity. A small but significant (p &lt; .001) victory for the effective altruism movement.</p>\n<p>But are they more compassionate people in general? After throwing out the people who said they wanted to give blood but couldn't for one or another reason, I got 1255 survey respondents giving me an unambiguous answer (yes or no) about whether they'd ever given blood. I found that 51% of effective altruists had given blood compared to 47% of others - a difference which did not reach statistical significance.</p>\n<p>Finally, at the end of the survey I had a question offering respondents a chance to cooperate (raising the value of a potential monetary prize to be given out by raffle to a random respondent) or defect (decreasing the value of the prize, but increasing their own chance of winning the raffle). 73% of effective altruists cooperated compared to 70% of others - an insignificant difference.</p>\n<p>Conclusion: effective altruists give more money to charity, both absolutely and as a percent of income, but are no more likely (or perhaps only slightly more likely) to be compassionate in other ways.</p>\n<p><strong id=\"B__Can_we_finally_resolve_this_IQ_controversy_that_comes_up_every_year_\">B. Can we finally resolve this IQ controversy that comes up every year?</strong></p>\n<p>The story so far - our first survey in 2009 found an average IQ of 146. Everyone said this was stupid, no community could possibly have that high an average IQ, it was just people lying and/or reporting results from horrible Internet IQ tests.<br>Although IQ fell somewhat the next few years - to 140 in 2011 and 139 in 2012 - people continued to complain. So in 2012 we started asking for SAT and ACT scores, which are known to correlate well with IQ and are much harder to get wrong. These scores confirmed the 139 IQ result on the 2012 test. But people still objected that something must be up.</p>\n<p>This year our IQ has fallen further to 138 (no Flynn Effect for us!) but for the first time we asked people to describe the IQ test they used to get the number. So I took a subset of the people with the most unimpeachable IQ tests - ones taken after the age of 15 (when IQ is more stable), and from a seemingly reputable source. I counted a source as reputable either if it name-dropped a specific scientifically validated IQ test (like WAIS or Raven's Progressive Matrices), if it was performed by a reputable institution (a school, a hospital, or a psychologist), or if it was a Mensa exam proctored by a Mensa official.</p>\n<p>This subgroup of 101 people with very reputable IQ tests had an average IQ of 139 - exactly the same as the average among survey respondents as a whole.</p>\n<p>I don't know for sure that Mensa is on the level, so I tried again deleting everyone who took a Mensa test - leaving just the people who could name-drop a well-known test or who knew it was administered by a psychologist in an official setting. This caused a precipitous drop all the way down to 138.</p>\n<p>The IQ numbers have time and time again answered every challenge raised against them and should be presumed accurate.</p>\n<p><strong id=\"C__Can_we_predict_who_does_or_doesn_t_cooperate_on_prisoner_s_dilemmas_\">C. Can we predict who does or doesn't cooperate on prisoner's dilemmas?</strong></p>\n<p>As mentioned above, I included a prisoner's dilemma type question in the survey, offering people the chance to make a little money by screwing all the other survey respondents over.</p>\n<p>Tendency to cooperate on the prisoner's dilemma was most highly correlated with items in the general leftist political cluster identified by Ozy above. It was most notable for support for feminism, with which it had a correlation of .15, significant at the p &lt; .01 level, and minimum wage, with which it had a correlation of .09, also significant at p &lt; .01. It was also significantly correlated with belief that other people would cooperate on the same question.</p>\n<p>I compared two possible explanations for this result. First, leftists are starry-eyed idealists who believe everyone can just get along - therefore, they expected other people to cooperate more, which made them want to cooperate more. Or, second, most Less Wrongers are white, male, and upper class, meaning that support for leftist values - which often favor nonwhites, women, and the lower class - is itself a symbol of self-sacrifce and altruism which one would expect to correlate with a question testing self-sacrifice and altruism.</p>\n<p>I tested the \"starry-eyed idealist\" hypothesis by checking whether leftists were more likely to believe other people would cooperate. They were not - the correlation was not significant at any level.</p>\n<p>I tested the \"self-sacrifice\" hypothesis by testing whether the feminism correlation went away in women. For women, supporting feminism is presumably not a sign of willingness to self-sacrifice to help an out-group, so we would expect the correlation to disappear.</p>\n<p>In the all-female sample, the correlation between feminism and PD cooperation shrunk from .15 to a puny .04, whereas the correlation between the minimum wage and PD was previously .09 and stayed exactly the same at .09. This provides some small level of support for the hypothesis that the leftist correlation with PD cooperation represents a willingness to self-sacrifice in a population who are not themselves helped by leftist values.</p>\n<p>(on the other hand, neither leftists nor cooperators were more likely to give money to charity, so if this is true it's a very selective form of self-sacrifice)</p>\n<h2 id=\"VI__Monetary_Prize\">VI. Monetary Prize</h2>\n<p>1389 people answered the prize question at the bottom. 71.6% of these [n = 995] cooperated; 28.4% [n = 394] defected. <br>The prize goes to a person whose two word phrase begins with \"eponymous\". If this person posts below (or PMs or emails me) the second word in their phrase, I will give them $60 * 71.6%, or about $43. I can pay to a PayPal account, a charity of their choice that takes online donations, or a snail-mail address via check.</p>\n<h2 id=\"VII__Calibration_Questions\">VII. Calibration Questions</h2>\n<p>The population of Europe, according to designated arbiter Wikipedia, is 739 million people.</p>\n<p>People were really really bad at giving their answers in millions. I got numbers anywhere from 3 (really? three million people in Europe?) to 3 billion (3 million billion people = 3 quadrillion). I assume some people thought they were answering in billions, others in thousands, and other people thought they were giving a straight answer in number of individuals.</p>\n<p>My original plan was to just adjust these to make them fit, but this quickly encountered some pitfalls. Suppose someone wrote 1 million (as one person did). Could I fairly guess they meant 100 million, even though there's really no way to guess that from the text itself? 1 billion? Maybe they just thought there were really one million people in Europe?</p>\n<p>If I was too aggressive correcting these, everyone would get close to the right answer not because they were smart, but because I had corrected their answers. If I wasn't aggressive enough, I would end up with some guy who answered 3 quadrillion Europeans totally distorting the mean.</p>\n<p>I ended up deleting 40 answers that suggested there were less than ten million or more than eight billion Europeans, on the grounds that people probably weren't really that far off so it was probably some kind of data entry error, and correcting everyone who entered a reasonable answer in individuals to answer in millions as the question asked.</p>\n<p>The remaining 1457 people who can either follow simple directions or at least fail to follow them in a predictable way estimated an average European population in millions of 601 + 35.6 (380, 500, 750).</p>\n<p>Respondents were told to aim for within 10% of the real value, which means they wanted between 665 million and 812 million. 18.7% of people [n = 272] got within that window.</p>\n<p>I divided people up into calibration brackets of [0,5], [6,15], [16, 25] and so on. The following are what percent of people in each bracket were right.</p>\n<p>[0,5]: 7.7%<br>[6,15]: 12.4%<br>[16,25]: 15.1%<br>[26,35]: 18.4%<br>[36,45]: 20.6%<br>[46,55]: 15.4%<br>[56,65]: 16.5%<br>[66,75]: 21.2%<br>[76,85]: 36.4%<br>[86,95]: 48.6%<br>[96,100]: 100%<br><br>Among people who should know better (those who have read all or most of the Sequences and have &gt; 500 karma, a group of 162 people)</p>\n<p>[0,5]: 0<br>[6,15]: 17.4%<br>[16,25]: 25.6%<br>[26,35]: 16.7%<br>[36,45]: 26.7%<br>[46,55]: 25%<br>[56,65]: 0%<br>[66,75]: 8.3%<br>[76,85]: 40%<br>[86,95]: 66.6%<br>[96,100]: 66.6%<br><br>Clearly, the people who <em>should </em>know better <em>don't</em>.</p>\n<p><img src=\"http://slatestarcodex.com/Stuff/calibration2013.png\" alt=\"\"></p>\n<p>This graph represents your performance relative to ideal performance. Dipping below the blue ideal line represents overconfidence; rising above it represents underconfidence. With few exceptions you were very overconfident. Note that there were so few \"elite\" LWers at certain levels that the graph becomes very noisy and probably isn't representing much; that huge drop at 60 represents like two or three people. The orange \"typical LWer\" line is much more robust.</p>\n<p>There is one other question that gets at the same idea of overconfidence. 651 people were willing to give valid 90% confidence interval on what percent of people would cooperate (this is my fault; I only added this question about halfway through the survey once I realized it would be interesting to investigate). I deleted four for giving extremely high outliers like 9999% which threw off the results, leaving 647 valid answers. The average confidence interval was [28.3, 72.0], which just BARELY contains the correct answer of 71.6%. Of the 647 of you, only 346 (53.5%) gave 90% confidence intervals that included the correct answer!</p>\n<p>Last year I complained about horrible performance on calibration questions, but we all decided it was probably just a fluke caused by a particularly weird question. This year's results suggest that was no fluke and that we haven't even learned to overcome the one bias that we can measure super-well and which is most easily trained away. Disappointment!</p>\n<h2 id=\"VIII__Public_Data\">VIII. Public Data</h2>\n<p>There's still a lot more to be done with this survey. User:Unnamed has promised to analyze the \"Extra Credit: CFAR Questions\" section (not included in this post), but so far no one has looked at the \"Extra Credit: Questions From Sarah\" section, which I didn't really know what to do with. And of course this is most complete survey yet for seeking classic findings like \"People who disagree with me about politics are stupid and evil\".</p>\n<p>1480 people - over 90% of the total - kindly allowed me to make their survey data public. I have included all their information except the timestamp (which would make tracking pretty easy) including their secret passphrases (by far the most interesting part of this exercise was seeing what unusual two word phrases people could come up with on short notice).</p>", "sections": [{"title": "Part I. Population", "anchor": "Part_I__Population", "level": 1}, {"title": "II. Categorical Data", "anchor": "II__Categorical_Data", "level": 1}, {"title": "III. Numeric Data", "anchor": "III__Numeric_Data", "level": 1}, {"title": "IV. Bivariate Correlations", "anchor": "IV__Bivariate_Correlations", "level": 1}, {"title": "V. Hypothesis Testing", "anchor": "V__Hypothesis_Testing", "level": 1}, {"title": "A. Do people in the effective altruism movement donate more money to charity? Do they donate a higher percent of their income to charity? Are they just generally more altruistic people?", "anchor": "A__Do_people_in_the_effective_altruism_movement_donate_more_money_to_charity__Do_they_donate_a_higher_percent_of_their_income_to_charity__Are_they_just_generally_more_altruistic_people_", "level": 2}, {"title": "B. Can we finally resolve this IQ controversy that comes up every year?", "anchor": "B__Can_we_finally_resolve_this_IQ_controversy_that_comes_up_every_year_", "level": 2}, {"title": "C. Can we predict who does or doesn't cooperate on prisoner's dilemmas?", "anchor": "C__Can_we_predict_who_does_or_doesn_t_cooperate_on_prisoner_s_dilemmas_", "level": 2}, {"title": "VI. Monetary Prize", "anchor": "VI__Monetary_Prize", "level": 1}, {"title": "VII. Calibration Questions", "anchor": "VII__Calibration_Questions", "level": 1}, {"title": "VIII. Public Data", "anchor": "VIII__Public_Data", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "561 comments"}], "headingsCount": 13}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 561, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-19T20:45:58.339Z", "modifiedAt": null, "url": null, "title": "Book Review: How Learning Works", "slug": "book-review-how-learning-works", "viewCount": null, "lastCommentedAt": "2018-05-28T14:52:04.997Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whales", "createdAt": "2014-01-07T06:31:44.661Z", "isAdmin": false, "displayName": "whales"}, "userId": "n7p4iZqwmT3whXoAD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mAdMkFqWzbJRB544m/book-review-how-learning-works", "pageUrlRelative": "/posts/mAdMkFqWzbJRB544m/book-review-how-learning-works", "linkUrl": "https://www.lesswrong.com/posts/mAdMkFqWzbJRB544m/book-review-how-learning-works", "postedAtFormatted": "Sunday, January 19th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Review%3A%20How%20Learning%20Works&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Review%3A%20How%20Learning%20Works%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmAdMkFqWzbJRB544m%2Fbook-review-how-learning-works%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Review%3A%20How%20Learning%20Works%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmAdMkFqWzbJRB544m%2Fbook-review-how-learning-works", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmAdMkFqWzbJRB544m%2Fbook-review-how-learning-works", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5201, "htmlBody": "<p>As <a href=\"/lw/jij/open_thread_for_january_17_23_2014/aeaq\">promised</a>, I review and point-by-point summarize <a href=\"http://www.amazon.com/How-Learning-Works-Research-Based-Principles/dp/0470484101\"><strong>How Learning Works: 7 Research-Based Principles for Smart Teaching</strong></a> by Susan A. Ambrose, Michael W. Bridges, Michele DiPietro, Marsha C. Lovett, and Marie K. Norman (2010), hereafter <em>HLW </em>as I scratch in futility at the sprawling length of this post.</p>\n<h1>Review</h1>\n<p>The authors aim to provide \"a bridge between research and practice\" for teaching and learning, very much in the spirit of <a href=\"/lw/d4/practical_advice_backed_by_deep_theories/\">Practical Advice Backed by Deep Theories</a>. They concentrate on widely-supported results that are independent of subject matter and environment, so while the discussion is directed towards instructors in K-12 and college classrooms, there are also implications for essentially anyone in a teaching or learning role.</p>\n<p>Let me restate that a little more strongly: any student, autodidact or not, would be well-served by internalizing the models and recommendations presented here. Teachers have even less of an excuse not to read the book, which is written very clearly and without sinking to punchy popularization. This is basic stuff, in the best possible way.</p>\n<p>Sure, there are more sophisticated ideas out there; there exist subgenres of domain-specific research (especially for math and physics education); you can find diverse perspectives in homeschooling communities or in philosophy of education. There's even some controversy in the depths of the research on some of the points in this book (though for the most part the scope of disagreements is still contained within the boundaries drawn by the authors). But as far as most people need concern themselves, <em>HLW&nbsp;</em>is an earnest and accurate if not quite comprehensive account of What We Know about learning.</p>\n<p>[I do wish there were a similar account of And How We Think We Know It, looking into common research techniques, metrics of learning outcomes, systematic errors to guard against, reliability of longitudinal studies, statistics about replicability and retractions, and so on, but this isn't it. The book lightly describes methods when it sees fit, and my scattered checks of unfamiliar studies leave me fairly confident that the research does in fact bear the claims the book makes.]</p>\n<p>The book organizes research on teaching and learning into seven principles in order to \"provide instructors with an understanding of student learning that can help them (a) see why certain teaching approaches are or are not supporting students &rsquo; learning, (b) generate or refine teaching approaches and strategies that more effectively foster student learning in specific contexts, and (c) transfer and apply these principles to new courses.\" The principles are</p>\n<blockquote><ol>\n<li>Students' prior knowledge can help or hinder learning.</li>\n<li>How students organize knowledge influences how they learn and how they apply what they know.</li>\n<li>Students' motivation determines, directs, and sustains what they do to learn.</li>\n<li>To develop mastery, students must acquire component skills, practice integrating them, and know when to apply what they have learned.</li>\n<li>Goal-directed practice coupled with targeted feedback enhances the quality of students' learning.</li>\n<li>Students' current level of development interacts with the social, emotional, and intellectual climate of the course to impact learning.</li>\n<li>To become self-directed learners, students must learn to monitor and adjust their approaches to learning.</li>\n</ol></blockquote>\n<p>Hopefully these ideas are not surprising to you. They are not meant to be; they stand mostly to organize diverse research findings into a coherent model (see principle #2). And if many of those research findings are old news to you as well, I also take that to be a point in favor of the book, and I trust that you will understand why.</p>\n<p>Each chapter begins with two stories meant to illustrate the principle, a discussion of the principle itself, a discussion of the research related to that principle, and recommendations that take the principle into account. The chapters are interconnected but stand on their own. If you don't plan to teach, you might get most of your value from Chapters 4, 5, and 7. There's some fluff to the book, but not much. My summary, though long, leaves out the stories and examples, useful repetitions and rephrasings, detailed explanations, and specific recommendations, not to mention descriptions and citations of the relevant studies. I do not consider it a substitute for reading the book, which isn't really that long to begin with.</p>\n<p>Before I summarize <em>HLW</em>, I'll make a couple brief comparisons. <strong>Why Don't Students Like School: A Cognitive Scientist Answers Questions About How the Mind Works and What It Means for the Classroom</strong> by Daniel T. Willingham (2009) looks pretty similar, down to the format in which chapter titles ask questions which are then answered by Principles of Learning, followed by a discussion of the principle, followed by recommendations for the classroom. It's written at a more popular level, with less discussion of actual research and lots more fluff. Only occasionally does it draw connections directly to a study, rather than use that as the chief mode of exposition (as in <em>HLW</em>). Each chapter does have a short annotated bibliography divided into less and more technical texts, which is nice. Willingham comes down strongly in favor of drilling and factual knowledge preceding skill. While that's something I've approvingly polemicized about <a href=\"http://whaaales.com/memorize\">at some length</a>, it needs&nbsp;a mountain of caveats. In general he optimizes (explicitly, in fact) for counterintuitive punchiness, and it's not always clear how well-supported his advice really is. The organization and coverage feels haphazard to me, but where he hits on topics covered by <em>HLW</em>, he seems to agree.</p>\n<p>The <strong><a href=\"http://whaaales.com/25principlesoflearning.pdf\">25 Principles of Learning</a></strong> [pdf] from the University of Memphis learning group is a short document with a similar aim: a few sentences describing each principle, a couple sentences describing the implications, and a couple of references. It covers important points that <em>HLW </em>addresses only indirectly or that it inexplicably leaves out entirely (spaced repetition, testing, and generation effects, for example). It's worth looking over to fill in those gaps. But it's really \"25 Important Findings on Learning\": it doesn't give examples, offer very specific advice, or attempt to organize these principles into a causal model of learning. Consider them exercises for the reader.</p>\n<p><a id=\"more\"></a></p>\n<h1>Summary</h1>\n<h2>1. How Does Students' Prior Knowledge Affect Their Learning?</h2>\n<p>Students link new ideas and information to what they already know. This can hinder learning in the case of inactive, insufficient, inappropriate, or inaccurate knowledge, but it can also be harnessed to enhance learning.</p>\n<p><strong>Research consensus:</strong></p>\n<ul>\n<li>In some ways this is common sense&mdash;for example, in the way a mathematics lecture directly relies on definitions and theorems. A student without sufficient background knowledge might still learn to manipulate the symbols, but with more effort, worse retention, worse transfer, and worse ability to explain. But there are also indirect, non-obvious mechanisms at work, in which background knowledge that is not explicitly prerequisite can help learning (as in general knowledge of soccer enhancing recall of arbitrary soccer-match scores).</li>\n<li><em>Declarative&nbsp;</em>knowledge (object-level concepts) and <em>procedural&nbsp;</em>knowledge (how and when to apply those concepts) do not always go hand in hand. One without the other is a knowledge gap that can be tricky to spot, especially in self-assessment.</li>\n<li>Existing knowledge needs to be <em>active </em>to be effective; activation can be achieved with minor prompts and reminders, as well as questions designed to trigger recall.</li>\n<li>Students may activate existing knowledge that's <em>inappropriate </em>(e.g. the colloquial/intuitive meaning of \"force\" when learning Newtonian physics) or inaccurate. Such activation interferes with learning, leads to incorrect conclusions, and predisposes students to resist conflicting evidence.</li>\n<li>Inaccurate isolated facts can be unlearned through empiricism and explicit refutation. Deeper misconceptions can be extremely persistent, but patient repetition and a long series of small inferential bridges can help.</li>\n</ul>\n<p><strong>Strategies for teachers:</strong></p>\n<ul>\n<li>Determine the extent, quality, and nature (e.g. declarative vs. procedural) of students' prior knowledge: \n<ul>\n<li>Talk to previous instructors</li>\n<li>Use diagnostic tests</li>\n<li>Ask self-assessment questions</li>\n<li>Use brainstorming or concept mapping</li>\n<li>Look for patterns of error</li>\n</ul>\n</li>\n<li>Address gaps in prior knowledge: \n<ul>\n<li>Identify for yourself what knowledge is necessary</li>\n<li>Remediate insufficient knowledge as determined above</li>\n</ul>\n</li>\n<li>Activate relevant prior knowledge \n<ul>\n<li>Explicitly point out connections</li>\n<li>Use analogies and examples</li>\n<li>Use exercises that explicitly ask students to use their prior knowledge</li>\n</ul>\n</li>\n<li>Avoid activating inappropriate prior knowledge: \n<ul>\n<li>Highlight the boundaries of what knowledge is applicable, either explicitly or with rules of thumb</li>\n<li>Explicitly identify discipline-specific conventions</li>\n<li>Show where analogies break down and examples don't generalize</li>\n</ul>\n</li>\n<li>Help students revise inaccurate knowledge: \n<ul>\n<li>Ask students to make and test predictions</li>\n<li>Ask students to justify their reasoning</li>\n<li>Help students practice using knowledge meant to replace misconceptions</li>\n<li>Allow sufficient time</li>\n</ul>\n</li>\n</ul>\n<h2>2. How Does the Way Students Organize Knowledge Affect Their Learning?</h2>\n<p>Developing expertise requires rich connections between various facts, concepts, and procedures, organized around abstract principles and causal relationships. Although an expert does not necessarily build such knowledge networks explicitly or consciously, it is possible for a novice learner to deliberately organize knowledge into expert-style structures, improving learning, performance, and retention.</p>\n<p><strong>Research:</strong></p>\n<ul>\n<li>The optimal organization of knowledge depends on how that knowledge is to be used. Learning physics in a historical framework has advantages and disadvantages when compared with learning the same physics according to physical principles.</li>\n<li>Students whose knowledge networks (graphs with \"pieces of knowledge\" as nodes linked by their relationships) are more densely connected will retrieve their knowledge faster and more reliably, and are more likely to notice inconsistencies and contradictions.</li>\n<li>Experts, as a result of their densely connected knowledge networks, process information in coherent <em>chunks </em>where novices process individual bits of information (as for chess positions and circuit diagrams). Memorization of digit sequences can be greatly boosted by hierarchical&nbsp;chunking of subsequences. These facts are seen as related.</li>\n<li>Expert knowledge networks have more meaningful connections and deeper organizing principles.</li>\n<li>Students learn better when provided with a structure for organizing information. Causal relationships are especially effective organizing principles.</li>\n<li>Studying worked examples, analogies, and contrasting cases helps students organize their knowledge meaningfully.</li>\n</ul>\n<p><strong>Strategies:</strong></p>\n<ul>\n<li>Organize the material: \n<ul>\n<li>Create a concept map for the material to be taught</li>\n<li>Identify the knowledge organization best suited to the purpose of learning</li>\n</ul>\n</li>\n<li>Enhance students' knowledge organization: \n<ul>\n<li>Explicitly describe the organization of material at each level in the hierarchy of presentation&mdash;subject, course, lecture, discussion</li>\n<li>Use contrasting and boundary cases</li>\n<li>Explicitly point out deep similarities and other connections</li>\n<li>Use multiple organizing structures</li>\n</ul>\n</li>\n<li>Expose students' knowledge organization \n<ul>\n<li>Ask them to draw a concept map</li>\n<li>Use a sorting task</li>\n<li>Look for patterns of mistakes</li>\n</ul>\n</li>\n</ul>\n<h2>3. What Factors Motivate Students to Learn?</h2>\n<p>Students are motivated by the subjective <em>value </em>of a goal and by their <em>expectancy </em>of success.&nbsp;[You may be reminded of the <a href=\"/lw/3w3/how_to_beat_procrastination/\">Procrastination Equation</a>, which also describes penalties for impulsiveness and delay.] Students may be guided by different goals, and recognizing this can help you foster their motivation.</p>\n<p><strong>Research:</strong></p>\n<ul>\n<li>Students who pursue <em>learning goals</em>, which emphasize the intrinsic or instrumental value of material,&nbsp;are generally the most motivated and have the best learning outcomes.</li>\n<li>Students may also be guided by <em>performance goals</em>, related to their self-image and reputation. These may themselves be performance-approach or performance-avoidant; the former seems to entail a cognitive framework more conducive to learning.</li>\n<li>Work-avoidant goals (\"do as little work as possible\") can be directly at odds with learning, but are generally context dependent.</li>\n<li>There are, broadly, three broad determinants of subjective value: <em>attainment </em>value (satisfaction from mastery or accomplishment), <em>intrinsic </em>value, and <em>instrumental </em>value. These may mutually reinforce each other.</li>\n<li>To be motivated, a student should expect both their own ability to succeed and for success to bring about a desired outcome.</li>\n<li>Expectancy of success is influenced by the student's past success rate in similar situations, and even more strongly by the reasons the student identifies for their past success or failure. Specifically, attributing success to internal and controllable causes* and failure to controllable but temporary causes increases expectancy. Attributing success to luck and failure to personal inadequacy decreases expectancy. [*Interestingly, the authors make no real distinction here between internal and controllable causes for success, which is a fundamental distinction between the \"fixed\" vs. \"malleable\" (which you may know as \"growth\") mindsets addressed in Chapter 7.]</li>\n<li>Supportive environments also increase motivation.</li>\n</ul>\n<p><strong> Strategies:</strong></p>\n<ul>\n<li>Establish value: \n<ul>\n<li>Connect material to students' interests</li>\n<li>Provide authentic tasks</li>\n<li>Show relevance to students' academic lives</li>\n<li>Show relevance of generalizable skills</li>\n<li>Identify and reward what you (as the instructor) value</li>\n<li>Radiate enthusiasm</li>\n<li>Give students opportunities to reflect on the value of their work</li>\n</ul>\n</li>\n<li>Build expectancy: \n<ul>\n<li>Clarify the course goals and your instruction and assessment strategies</li>\n<li>Identify and set an appropriate level of challenge</li>\n<li>Help students build success spirals with early challenges</li>\n<li>Provide feedback on progress</li>\n<li>Be fair</li>\n<li>Help students attribute success and failure appropriately</li>\n<li>Discuss effective study strategies</li>\n</ul>\n</li>\n<li>Give students flexibility and control in course work to increase both value and expectancy</li>\n</ul>\n<h2>4. How Do Students Develop Mastery?</h2>\n<p>Consider a driver changing lanes, making many small motions, visual checks, and mental evaluations fluently and automatically. An expert performs complex tasks with little conscious awareness of the complexity involved. To approach that level of mastery, a novice must not only learn the component skills, but also integrate the skills and know when to apply them.</p>\n<p><strong>Research:</strong></p>\n<ul>\n<li>Experts do not necessarily make good teachers: they process information in chunks, they employ shortcuts and skip steps, they perform with automaticity, and they overestimate students' competence. Their unconscious mastery leads to so-called <em>expert blind spots</em>.</li>\n<li>Students will perform poorly if their component skills are weak.</li>\n<li>Student performance is greatly improved when instructors identify component skills required for a complex task and target weak ones through practice. A small amount of focused practice on a component skill can have a large impact on performance of the complex task.</li>\n<li>Multitasking degrades performance by way of excess information-processing demands or <em>cognitive load</em>. The same applies to combining skills for a complex task, but much more so for novices than for experts.</li>\n<li>Cognitive load can be reduced when learning a complex task by allowing the student to focus on one component skill at a time. It may also be helpful for the instructor to support other aspects of the task while students do their focused practice. This is known as <em>scaffolding</em>.</li>\n<li>Another instance of scaffolding effect appears when the instructor presents students with worked examples rather than problems, freeing up cognitive resources to think about principles and techniques.</li>\n<li>Results on drilling component skills in isolation, as compared with practicing the overall task with focus on the components, are mixed. Some skills afford isolated practice better than others. A highly complex but easily divisible task can be learned more effectively by initially practicing the components in isolation, and then progressively combining them.</li>\n<li>Mastery also involves knowing when to apply learned skills outside of the learning context. Doing so is referred to as <em>transfer</em>. Transfer occurs rarely and with difficulty, and is worse the more dissimilar the learning and transfer contexts.</li>\n<li>Overspecificity and context-dependence of knowledge hurt transfer; deep understanding of principles and relationships helps transfer. The latter effect can be targeted with structured comparisons and analogical reasoning also help transfer.</li>\n<li>Minor prompts and reminders facilitate transfer, much as they help activate appropriate knowledge (see Chapter 1).</li>\n</ul>\n<p><strong>Strategies:</strong></p>\n<ul>\n<li>Expose component skills: \n<ul>\n<li>Map out your own expert blind spot</li>\n<li>Enlist help from those with mere conscious competence</li>\n<li>Talk to others in your discipline</li>\n<li>Talk to others outside your discipline</li>\n<li>Explore educational materials</li>\n</ul>\n</li>\n<li>Reinforce component skills \n<ul>\n<li>Focus students' attention on the key aspects of the task</li>\n<li>Diagnose weak or missing component skills</li>\n<li>Provide isolated practice of those skills.</li>\n</ul>\n</li>\n<li>Build fluency and facilitate integration of skills \n<ul>\n<li>Give students practice exercises explicitly to increase automaticity</li>\n<li>Temporarily constrain the scope of the task</li>\n<li>Explicitly include integration in performance criteria</li>\n</ul>\n</li>\n<li>Facilitate transfer: \n<ul>\n<li>Discuss conditions of applicability</li>\n<li>Give exercises explicitly about conditions of applicability</li>\n<li>Provide opportunities to practice in diverse contexts</li>\n<li>Use hypothetical scenarios for practice questions</li>\n<li>Ask students to generalize to abstract principles</li>\n<li>Identify deep features using comparisons</li>\n<li>Prompt students to retrieve relevant knowledge</li>\n</ul>\n</li>\n</ul>\n<h2>5. What Kinds of Practice and Feedback Enhance Learning?</h2>\n<p>Practice is often misguided and feedback poorly timed, insufficient, or unfocused. To be effective, practice should be directed by goals and coupled with targeted feedback.</p>\n<p><strong>Research:</strong></p>\n<ul>\n<li>Learning can be predicted by time in <em>deliberate practice</em>, which is marked by being directed toward a specific goal and an appropriate level of challenge. [I've often heard deliberate practice described with an emphasis on mindful attention, in contrast with practice in a flow state (for example in an article by Ericsson himself&mdash;the last paragraph before \"Future Directions\"), but the authors questionably suggest that flow is a sign of appropriate challenge. For motivation, perhaps it is, but I would argue not so for deliberate practice.]</li>\n<li>Clearly specified performance criteria can help direct students' practice.</li>\n<li>Learning is hampered by either insufficient or excessive challenge.</li>\n<li>The success of individual tutoring is largely driven by the ability to tailor challenges to a level appropriate to deliberate practice.</li>\n<li>An instructor can improve learning outcomes with difficult tasks by adding structure and support to bring it within the bounds of the student's competence. This can consist of guidance by the instructor, or of written prompts and checklists. (C.f. \"scaffolding\" in Chapter 4.)</li>\n<li>The benefits of deliberate practice accrue gradually with increasing time spent practicing; both students and teachers underestimate the time needed.</li>\n<li>The effectiveness of feedback is determined by both content and timing. It should communicate progress and direct subsequent effort, and it should be supplied when students can best use it.</li>\n<li>Feedback that identifies specific items that need improvement will aid learning more than will a mere indication of error.</li>\n<li>Unfocused feedback can be counterproductive by overwhelming the student and failing to direct effort well.</li>\n<li>Generally, more frequent and more rapid feedback is better for learning. Delayed feedback can be useful in helping students learn to recognize and correct their own errors.</li>\n</ul>\n<p><strong> Strategies:</strong></p>\n<ul>\n<li>Establish goals: \n<ul>\n<li>Be explicit about course goals, and phrase them in terms of capabilities rather than knowledge</li>\n<li>Use a rubric to communicate performance criteria</li>\n<li>Give contrasting examples of high and low quality work</li>\n<li>Progressively refine goals</li>\n</ul>\n</li>\n<li>Encourage deliberate practice: \n<ul>\n<li>Assess prior knowledge to set an appropriate challenge</li>\n<li>Create many chances to practice</li>\n<li>Build scaffolding into assignments</li>\n<li>Set expectations about practice</li>\n</ul>\n</li>\n<li>Target feedback: \n<ul>\n<li>Look for patterns of errors</li>\n<li>Use prioritized feedback to direct student efforts</li>\n<li>Give feedback on strengths and weaknesses</li>\n<li>Allow frequent opportunities for feedback</li>\n<li>Provide feedback at the group level, potentially in real-time</li>\n<li>Require peer feedback on assignments</li>\n<li>Require students to describe how they incorporated feedback</li>\n</ul>\n</li>\n</ul>\n<h2>6. Why Do Student Development and Course Climate Matter for Student Learning?</h2>\n<p>People vary not just intellectually, but also socially and emotionally. Students' identities may be entangled with the course material and environment in complicated ways that often go unrecognized. A student's entire state&mdash;not just the intellect&mdash;interacts with the social, emotional, and intellectual climate of the course to impact learning, for better or for worse. [When I saw this chapter title, I had a vague worry that it would seem out of place, a perfunctory nod to diversity studies or something. I'm still not entirely comfortable with parts of the treatment here, but the above premise is sound.]</p>\n<p><strong>Research:</strong></p>\n<ul>\n<li>The research involved in this first section is of a different nature from the rest of the text. In the first part, the authors seek to describe student development, and cite a model which characterizes developmental changes into seven dimensions: developing competence, managing emotions, developing autonomy, establishing identity, freeing interpersonal relationships, developing purpose, and developing identity. They then cite research characterizing intellectual developments in terms of stages: duality, multiplicity, relativism, and commitment. Similarly, stages for social development. The point is that people can have a lot of different implicit and explicit beliefs, modes of communication, and ways of processing new information, which they can't just switch off and homogenize when they enter a classroom, and that people have done a lot of work to attempt to enumerate and connect these things. [I think the discussion here is the weakest part of the book, and I'd be interested in better resources on the subject, if they exist.]</li>\n<li>For course climate, they describe a classification in terms of whether an environment is <em>marginalizing&nbsp;</em>or <em>centralizing&nbsp;</em>(describing how the perspectives of groups might be discouraged or welcomed), and whether this occurs&nbsp;<em>implicitly&nbsp;</em>or <em>explicitly</em>. Implicitly marginalizing classrooms are the most common of the four quadrants.</li>\n<li>In implicitly marginalizing environments (i.e. without overt exclusion or hostility towards outgroups), individuals may suffer an accumulation of <em>micro-inequities</em>&nbsp;that over time has a large impact on learning. A number of studies have found that perceptions of a marginalizing climate are negatively correlated with learning and career outcomes. The authors identify four important channels for marginalization: stereotypes, tone, faculty-student interactions, and content.</li>\n<li>The activation (in the sense of Chapter 1) of stereotypes can influence learning, generally impairing performance; this effect is known as <em>stereotype threat</em>. The activation does not have to be a result of explicitly invoking the stereotype; implicit communication of assumptions or apparently innocuous comments also have effects.</li>\n<li>The immediate mechanism for stereotype threat seems to be a disruptive emotional reaction; this as opposed to self-efficacy or self-esteem being depressed or otherwise brought in line with the stereotype. The effect does not require any belief in the stereotype. There are deeper nuances as well as strategies for mitigating the effect in the literature.</li>\n<li>Perceived hostility or expectations of failure in stereotypes can decrease motivation and drive students from a discipline.</li>\n<li>A positive, constructive, and encouraging tone in discussions and syllabi improves student motivation and behavior. (This in contrast to punitive, critical, or demeaning tone.)</li>\n<li>Perceived positive faculty attitudes towards and interactions with undergrads are correlated with higher rates of graduate education and better self-reported learning outcomes. Faculty availability is a major factor in students' academic decisions.</li>\n<li>Course content itself in its orientation towards inclusiveness can have cognitive, motivational, and socio-emotional effects on learning.</li>\n</ul>\n<p><strong> Strategies:</strong></p>\n<ul>\n<li>Promote intellectual development: \n<ul>\n<li>Make uncertainty, ambiguity, and complexity safe</li>\n<li>Resist a single right answer</li>\n<li>Incorporate use of evidence into performance criteria</li>\n</ul>\n</li>\n<li>Promote social development: \n<ul>\n<li>Examine your assumptions about your students</li>\n<li>Be mindful of accidental cues regarding stereotypes</li>\n<li>Do not ask individuals to speak for an entire group</li>\n<li>Recognize students as individuals.</li>\n</ul>\n</li>\n<li>Promote an inclusive climate: \n<ul>\n<li>Be a model for inclusive language, behavior, and attitudes</li>\n<li>Use multiple and diverse examples</li>\n<li>Establish and reinforce ground rules for interaction</li>\n<li>Make sure course content does not marginalize students</li>\n<li>Use the syllabus and first day of class to establish climate</li>\n<li>Set up processes to get feedback on the climate</li>\n<li>Anticipate and prepare for sensitive issues</li>\n<li>Address tensions early</li>\n<li>Turn discord and tension into a learning opportunity</li>\n<li>Facilitate and model active listening.</li>\n</ul>\n</li>\n</ul>\n<h2>7. How Do Students Become Self-Directed Learners?</h2>\n<p>As one progresses in academic and professional life, one takes progressively more responsibility for one's own learning. The jump between high school and college can be especially jarring in this regard. <em>Metacognition</em>, \"the process of reflecting on and directing one's own thinking,\" becomes increasingly important, but falls outside the scope of most instruction. Still, to effectively direct their own learning, students must learn and practice an array of metacognitive skills.</p>\n<p><strong>Research:</strong></p>\n<ul>\n<li>One model represents metacognition as a continuously looping cycle of task assessment, evaluation of strengths and weaknesses, planning, execution and simultaneous monitoring, and reflection; all of these five steps are informed by a student's beliefs about intelligence and learning.</li>\n<li>Assessing the task is not always natural or obvious to students (essay prompts are often ignored; learning goals are not always clear).</li>\n<li>People are poor judges of their own knowledge and skills, tending to overestimate their abilities more the weaker they are.</li>\n<li>Novices spend little time in the planning phase of the cycle relative to experts in physics, math, and writing. Novice plans are often poorly matched to the task.</li>\n<li>Students who naturally and continuously monitor their performance and understanding learn better.</li>\n<li>Students can be taught to self-monitor, and this also improves learning.</li>\n<li>Monitoring alone is not sufficient; novice problem solvers will continue to use a strategy after it has failed (and certainly after it has proven modestly successful and familiar but not optimal).</li>\n<li>Students who believe their intelligence is malleable rather than fixed are more likely to learn and perform well.</li>\n<li>Moreover, the \"malleable\" perspective can be promoted by external influences, still leading to better performance.</li>\n</ul>\n<p><strong> Strategies:</strong></p>\n<ul>\n<li>Promote task assessment: \n<ul>\n<li>Be more explicit about assignments than you think is necessary</li>\n<li>Tell students what you do not want</li>\n<li>Check students' understanding of the task in their own words</li>\n<li>Provide a rubric</li>\n</ul>\n</li>\n<li>Promote self-evaluation: \n<ul>\n<li>Give timely feedback</li>\n<li>Provide opportunities for self-assessment.</li>\n</ul>\n</li>\n<li>Promote planning: \n<ul>\n<li>Have students implement a plan you provide</li>\n<li>Have students implement their own plan</li>\n<li>Make planning the central goal of the assignment.</li>\n</ul>\n</li>\n<li>Promote self-monitoring: \n<ul>\n<li>Provide simple heuristic questions for self-evaluation</li>\n<li>Have students do guided self-assessments</li>\n<li>Require students to reflect on and annotate their own work</li>\n<li>Use peer review</li>\n</ul>\n</li>\n<li>Promote reflection and adjustment: \n<ul>\n<li>Prompt students to reflect on their performance</li>\n<li>Prompt students to analyze effectiveness of study skills</li>\n<li>Present multiple strategies</li>\n<li>Create assignments that focus on strategizing</li>\n</ul>\n</li>\n<li>Promote useful beliefs about intelligence and learning: \n<ul>\n<li>Address these beliefs directly</li>\n<li>Broaden students' understanding of learning</li>\n<li>Help students set realistic expectations</li>\n</ul>\n</li>\n<li>Promote metacognition: \n<ul>\n<li>Model your metacognitive process for your students</li>\n<li>Scaffold students in their metacognitive processes</li>\n</ul>\n</li>\n</ul>\n<h2>Conclusion: Applying the Seven Principles to Ourselves</h2>\n<p>The authors turn their principles inward and discuss learning to teach. For the most part this is a restatement of the principles with no particularly new insights in their application to teaching, but there are interesting comments regarding the first few:</p>\n<ul>\n<li>Many teachers were formerly atypically successful students, and their prior knowledge can lead to distorted expectations; accordingly, many of the recommendations involve gathering data about the students.</li>\n<li>The organization of this book into principles is itself a deliberate application of the second principle.</li>\n<li>For motivation, the authors try to connect the content of the course with what every teacher really cares about: efficiency. They also suggest focusing on one or two aspects of teaching in a given semester, in order to build up small successes in improving teaching.</li>\n<li>In terms of mastery, practice and feedback, climate, and metacognition, teaching is not so different from any other skill.</li>\n</ul>\n<h2>Appendices</h2>\n<p><em>HLW </em>has eight appendices on tools mentioned throughout the book, with a reiteration of their nature and utility, and most importantly, example checklists and worksheets. These are</p>\n<ul>\n<li>Student self-assessment</li>\n<li>Concept maps</li>\n<li>Rubrics</li>\n<li>Learning objectives</li>\n<li>Ground rules [for discussion]</li>\n<li>Exam wrappers [for promoting metacognition on graded exams]</li>\n<li>Checklists</li>\n<li>Reader response/peer review</li>\n</ul>\n<p>These alone would have been an improvement over most teaching materials I grew up with.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 2, "fH8jPjHF2R27sRTTG": 2, "H2q58pKG6xFrv8bPz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mAdMkFqWzbJRB544m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 56, "extendedScore": null, "score": 0.000163, "legacy": true, "legacyId": "25310", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>As <a href=\"/lw/jij/open_thread_for_january_17_23_2014/aeaq\">promised</a>, I review and point-by-point summarize <a href=\"http://www.amazon.com/How-Learning-Works-Research-Based-Principles/dp/0470484101\"><strong>How Learning Works: 7 Research-Based Principles for Smart Teaching</strong></a> by Susan A. Ambrose, Michael W. Bridges, Michele DiPietro, Marsha C. Lovett, and Marie K. Norman (2010), hereafter <em>HLW </em>as I scratch in futility at the sprawling length of this post.</p>\n<h1 id=\"Review\">Review</h1>\n<p>The authors aim to provide \"a bridge between research and practice\" for teaching and learning, very much in the spirit of <a href=\"/lw/d4/practical_advice_backed_by_deep_theories/\">Practical Advice Backed by Deep Theories</a>. They concentrate on widely-supported results that are independent of subject matter and environment, so while the discussion is directed towards instructors in K-12 and college classrooms, there are also implications for essentially anyone in a teaching or learning role.</p>\n<p>Let me restate that a little more strongly: any student, autodidact or not, would be well-served by internalizing the models and recommendations presented here. Teachers have even less of an excuse not to read the book, which is written very clearly and without sinking to punchy popularization. This is basic stuff, in the best possible way.</p>\n<p>Sure, there are more sophisticated ideas out there; there exist subgenres of domain-specific research (especially for math and physics education); you can find diverse perspectives in homeschooling communities or in philosophy of education. There's even some controversy in the depths of the research on some of the points in this book (though for the most part the scope of disagreements is still contained within the boundaries drawn by the authors). But as far as most people need concern themselves, <em>HLW&nbsp;</em>is an earnest and accurate if not quite comprehensive account of What We Know about learning.</p>\n<p>[I do wish there were a similar account of And How We Think We Know It, looking into common research techniques, metrics of learning outcomes, systematic errors to guard against, reliability of longitudinal studies, statistics about replicability and retractions, and so on, but this isn't it. The book lightly describes methods when it sees fit, and my scattered checks of unfamiliar studies leave me fairly confident that the research does in fact bear the claims the book makes.]</p>\n<p>The book organizes research on teaching and learning into seven principles in order to \"provide instructors with an understanding of student learning that can help them (a) see why certain teaching approaches are or are not supporting students \u2019 learning, (b) generate or refine teaching approaches and strategies that more effectively foster student learning in specific contexts, and (c) transfer and apply these principles to new courses.\" The principles are</p>\n<blockquote><ol>\n<li>Students' prior knowledge can help or hinder learning.</li>\n<li>How students organize knowledge influences how they learn and how they apply what they know.</li>\n<li>Students' motivation determines, directs, and sustains what they do to learn.</li>\n<li>To develop mastery, students must acquire component skills, practice integrating them, and know when to apply what they have learned.</li>\n<li>Goal-directed practice coupled with targeted feedback enhances the quality of students' learning.</li>\n<li>Students' current level of development interacts with the social, emotional, and intellectual climate of the course to impact learning.</li>\n<li>To become self-directed learners, students must learn to monitor and adjust their approaches to learning.</li>\n</ol></blockquote>\n<p>Hopefully these ideas are not surprising to you. They are not meant to be; they stand mostly to organize diverse research findings into a coherent model (see principle #2). And if many of those research findings are old news to you as well, I also take that to be a point in favor of the book, and I trust that you will understand why.</p>\n<p>Each chapter begins with two stories meant to illustrate the principle, a discussion of the principle itself, a discussion of the research related to that principle, and recommendations that take the principle into account. The chapters are interconnected but stand on their own. If you don't plan to teach, you might get most of your value from Chapters 4, 5, and 7. There's some fluff to the book, but not much. My summary, though long, leaves out the stories and examples, useful repetitions and rephrasings, detailed explanations, and specific recommendations, not to mention descriptions and citations of the relevant studies. I do not consider it a substitute for reading the book, which isn't really that long to begin with.</p>\n<p>Before I summarize <em>HLW</em>, I'll make a couple brief comparisons. <strong>Why Don't Students Like School: A Cognitive Scientist Answers Questions About How the Mind Works and What It Means for the Classroom</strong> by Daniel T. Willingham (2009) looks pretty similar, down to the format in which chapter titles ask questions which are then answered by Principles of Learning, followed by a discussion of the principle, followed by recommendations for the classroom. It's written at a more popular level, with less discussion of actual research and lots more fluff. Only occasionally does it draw connections directly to a study, rather than use that as the chief mode of exposition (as in <em>HLW</em>). Each chapter does have a short annotated bibliography divided into less and more technical texts, which is nice. Willingham comes down strongly in favor of drilling and factual knowledge preceding skill. While that's something I've approvingly polemicized about <a href=\"http://whaaales.com/memorize\">at some length</a>, it needs&nbsp;a mountain of caveats. In general he optimizes (explicitly, in fact) for counterintuitive punchiness, and it's not always clear how well-supported his advice really is. The organization and coverage feels haphazard to me, but where he hits on topics covered by <em>HLW</em>, he seems to agree.</p>\n<p>The <strong><a href=\"http://whaaales.com/25principlesoflearning.pdf\">25 Principles of Learning</a></strong> [pdf] from the University of Memphis learning group is a short document with a similar aim: a few sentences describing each principle, a couple sentences describing the implications, and a couple of references. It covers important points that <em>HLW </em>addresses only indirectly or that it inexplicably leaves out entirely (spaced repetition, testing, and generation effects, for example). It's worth looking over to fill in those gaps. But it's really \"25 Important Findings on Learning\": it doesn't give examples, offer very specific advice, or attempt to organize these principles into a causal model of learning. Consider them exercises for the reader.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"Summary\">Summary</h1>\n<h2 id=\"1__How_Does_Students__Prior_Knowledge_Affect_Their_Learning_\">1. How Does Students' Prior Knowledge Affect Their Learning?</h2>\n<p>Students link new ideas and information to what they already know. This can hinder learning in the case of inactive, insufficient, inappropriate, or inaccurate knowledge, but it can also be harnessed to enhance learning.</p>\n<p><strong id=\"Research_consensus_\">Research consensus:</strong></p>\n<ul>\n<li>In some ways this is common sense\u2014for example, in the way a mathematics lecture directly relies on definitions and theorems. A student without sufficient background knowledge might still learn to manipulate the symbols, but with more effort, worse retention, worse transfer, and worse ability to explain. But there are also indirect, non-obvious mechanisms at work, in which background knowledge that is not explicitly prerequisite can help learning (as in general knowledge of soccer enhancing recall of arbitrary soccer-match scores).</li>\n<li><em>Declarative&nbsp;</em>knowledge (object-level concepts) and <em>procedural&nbsp;</em>knowledge (how and when to apply those concepts) do not always go hand in hand. One without the other is a knowledge gap that can be tricky to spot, especially in self-assessment.</li>\n<li>Existing knowledge needs to be <em>active </em>to be effective; activation can be achieved with minor prompts and reminders, as well as questions designed to trigger recall.</li>\n<li>Students may activate existing knowledge that's <em>inappropriate </em>(e.g. the colloquial/intuitive meaning of \"force\" when learning Newtonian physics) or inaccurate. Such activation interferes with learning, leads to incorrect conclusions, and predisposes students to resist conflicting evidence.</li>\n<li>Inaccurate isolated facts can be unlearned through empiricism and explicit refutation. Deeper misconceptions can be extremely persistent, but patient repetition and a long series of small inferential bridges can help.</li>\n</ul>\n<p><strong id=\"Strategies_for_teachers_\">Strategies for teachers:</strong></p>\n<ul>\n<li>Determine the extent, quality, and nature (e.g. declarative vs. procedural) of students' prior knowledge: \n<ul>\n<li>Talk to previous instructors</li>\n<li>Use diagnostic tests</li>\n<li>Ask self-assessment questions</li>\n<li>Use brainstorming or concept mapping</li>\n<li>Look for patterns of error</li>\n</ul>\n</li>\n<li>Address gaps in prior knowledge: \n<ul>\n<li>Identify for yourself what knowledge is necessary</li>\n<li>Remediate insufficient knowledge as determined above</li>\n</ul>\n</li>\n<li>Activate relevant prior knowledge \n<ul>\n<li>Explicitly point out connections</li>\n<li>Use analogies and examples</li>\n<li>Use exercises that explicitly ask students to use their prior knowledge</li>\n</ul>\n</li>\n<li>Avoid activating inappropriate prior knowledge: \n<ul>\n<li>Highlight the boundaries of what knowledge is applicable, either explicitly or with rules of thumb</li>\n<li>Explicitly identify discipline-specific conventions</li>\n<li>Show where analogies break down and examples don't generalize</li>\n</ul>\n</li>\n<li>Help students revise inaccurate knowledge: \n<ul>\n<li>Ask students to make and test predictions</li>\n<li>Ask students to justify their reasoning</li>\n<li>Help students practice using knowledge meant to replace misconceptions</li>\n<li>Allow sufficient time</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2__How_Does_the_Way_Students_Organize_Knowledge_Affect_Their_Learning_\">2. How Does the Way Students Organize Knowledge Affect Their Learning?</h2>\n<p>Developing expertise requires rich connections between various facts, concepts, and procedures, organized around abstract principles and causal relationships. Although an expert does not necessarily build such knowledge networks explicitly or consciously, it is possible for a novice learner to deliberately organize knowledge into expert-style structures, improving learning, performance, and retention.</p>\n<p><strong id=\"Research_\">Research:</strong></p>\n<ul>\n<li>The optimal organization of knowledge depends on how that knowledge is to be used. Learning physics in a historical framework has advantages and disadvantages when compared with learning the same physics according to physical principles.</li>\n<li>Students whose knowledge networks (graphs with \"pieces of knowledge\" as nodes linked by their relationships) are more densely connected will retrieve their knowledge faster and more reliably, and are more likely to notice inconsistencies and contradictions.</li>\n<li>Experts, as a result of their densely connected knowledge networks, process information in coherent <em>chunks </em>where novices process individual bits of information (as for chess positions and circuit diagrams). Memorization of digit sequences can be greatly boosted by hierarchical&nbsp;chunking of subsequences. These facts are seen as related.</li>\n<li>Expert knowledge networks have more meaningful connections and deeper organizing principles.</li>\n<li>Students learn better when provided with a structure for organizing information. Causal relationships are especially effective organizing principles.</li>\n<li>Studying worked examples, analogies, and contrasting cases helps students organize their knowledge meaningfully.</li>\n</ul>\n<p><strong id=\"Strategies_\">Strategies:</strong></p>\n<ul>\n<li>Organize the material: \n<ul>\n<li>Create a concept map for the material to be taught</li>\n<li>Identify the knowledge organization best suited to the purpose of learning</li>\n</ul>\n</li>\n<li>Enhance students' knowledge organization: \n<ul>\n<li>Explicitly describe the organization of material at each level in the hierarchy of presentation\u2014subject, course, lecture, discussion</li>\n<li>Use contrasting and boundary cases</li>\n<li>Explicitly point out deep similarities and other connections</li>\n<li>Use multiple organizing structures</li>\n</ul>\n</li>\n<li>Expose students' knowledge organization \n<ul>\n<li>Ask them to draw a concept map</li>\n<li>Use a sorting task</li>\n<li>Look for patterns of mistakes</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3__What_Factors_Motivate_Students_to_Learn_\">3. What Factors Motivate Students to Learn?</h2>\n<p>Students are motivated by the subjective <em>value </em>of a goal and by their <em>expectancy </em>of success.&nbsp;[You may be reminded of the <a href=\"/lw/3w3/how_to_beat_procrastination/\">Procrastination Equation</a>, which also describes penalties for impulsiveness and delay.] Students may be guided by different goals, and recognizing this can help you foster their motivation.</p>\n<p><strong id=\"Research_1\">Research:</strong></p>\n<ul>\n<li>Students who pursue <em>learning goals</em>, which emphasize the intrinsic or instrumental value of material,&nbsp;are generally the most motivated and have the best learning outcomes.</li>\n<li>Students may also be guided by <em>performance goals</em>, related to their self-image and reputation. These may themselves be performance-approach or performance-avoidant; the former seems to entail a cognitive framework more conducive to learning.</li>\n<li>Work-avoidant goals (\"do as little work as possible\") can be directly at odds with learning, but are generally context dependent.</li>\n<li>There are, broadly, three broad determinants of subjective value: <em>attainment </em>value (satisfaction from mastery or accomplishment), <em>intrinsic </em>value, and <em>instrumental </em>value. These may mutually reinforce each other.</li>\n<li>To be motivated, a student should expect both their own ability to succeed and for success to bring about a desired outcome.</li>\n<li>Expectancy of success is influenced by the student's past success rate in similar situations, and even more strongly by the reasons the student identifies for their past success or failure. Specifically, attributing success to internal and controllable causes* and failure to controllable but temporary causes increases expectancy. Attributing success to luck and failure to personal inadequacy decreases expectancy. [*Interestingly, the authors make no real distinction here between internal and controllable causes for success, which is a fundamental distinction between the \"fixed\" vs. \"malleable\" (which you may know as \"growth\") mindsets addressed in Chapter 7.]</li>\n<li>Supportive environments also increase motivation.</li>\n</ul>\n<p><strong id=\"Strategies_1\"> Strategies:</strong></p>\n<ul>\n<li>Establish value: \n<ul>\n<li>Connect material to students' interests</li>\n<li>Provide authentic tasks</li>\n<li>Show relevance to students' academic lives</li>\n<li>Show relevance of generalizable skills</li>\n<li>Identify and reward what you (as the instructor) value</li>\n<li>Radiate enthusiasm</li>\n<li>Give students opportunities to reflect on the value of their work</li>\n</ul>\n</li>\n<li>Build expectancy: \n<ul>\n<li>Clarify the course goals and your instruction and assessment strategies</li>\n<li>Identify and set an appropriate level of challenge</li>\n<li>Help students build success spirals with early challenges</li>\n<li>Provide feedback on progress</li>\n<li>Be fair</li>\n<li>Help students attribute success and failure appropriately</li>\n<li>Discuss effective study strategies</li>\n</ul>\n</li>\n<li>Give students flexibility and control in course work to increase both value and expectancy</li>\n</ul>\n<h2 id=\"4__How_Do_Students_Develop_Mastery_\">4. How Do Students Develop Mastery?</h2>\n<p>Consider a driver changing lanes, making many small motions, visual checks, and mental evaluations fluently and automatically. An expert performs complex tasks with little conscious awareness of the complexity involved. To approach that level of mastery, a novice must not only learn the component skills, but also integrate the skills and know when to apply them.</p>\n<p><strong id=\"Research_2\">Research:</strong></p>\n<ul>\n<li>Experts do not necessarily make good teachers: they process information in chunks, they employ shortcuts and skip steps, they perform with automaticity, and they overestimate students' competence. Their unconscious mastery leads to so-called <em>expert blind spots</em>.</li>\n<li>Students will perform poorly if their component skills are weak.</li>\n<li>Student performance is greatly improved when instructors identify component skills required for a complex task and target weak ones through practice. A small amount of focused practice on a component skill can have a large impact on performance of the complex task.</li>\n<li>Multitasking degrades performance by way of excess information-processing demands or <em>cognitive load</em>. The same applies to combining skills for a complex task, but much more so for novices than for experts.</li>\n<li>Cognitive load can be reduced when learning a complex task by allowing the student to focus on one component skill at a time. It may also be helpful for the instructor to support other aspects of the task while students do their focused practice. This is known as <em>scaffolding</em>.</li>\n<li>Another instance of scaffolding effect appears when the instructor presents students with worked examples rather than problems, freeing up cognitive resources to think about principles and techniques.</li>\n<li>Results on drilling component skills in isolation, as compared with practicing the overall task with focus on the components, are mixed. Some skills afford isolated practice better than others. A highly complex but easily divisible task can be learned more effectively by initially practicing the components in isolation, and then progressively combining them.</li>\n<li>Mastery also involves knowing when to apply learned skills outside of the learning context. Doing so is referred to as <em>transfer</em>. Transfer occurs rarely and with difficulty, and is worse the more dissimilar the learning and transfer contexts.</li>\n<li>Overspecificity and context-dependence of knowledge hurt transfer; deep understanding of principles and relationships helps transfer. The latter effect can be targeted with structured comparisons and analogical reasoning also help transfer.</li>\n<li>Minor prompts and reminders facilitate transfer, much as they help activate appropriate knowledge (see Chapter 1).</li>\n</ul>\n<p><strong id=\"Strategies_2\">Strategies:</strong></p>\n<ul>\n<li>Expose component skills: \n<ul>\n<li>Map out your own expert blind spot</li>\n<li>Enlist help from those with mere conscious competence</li>\n<li>Talk to others in your discipline</li>\n<li>Talk to others outside your discipline</li>\n<li>Explore educational materials</li>\n</ul>\n</li>\n<li>Reinforce component skills \n<ul>\n<li>Focus students' attention on the key aspects of the task</li>\n<li>Diagnose weak or missing component skills</li>\n<li>Provide isolated practice of those skills.</li>\n</ul>\n</li>\n<li>Build fluency and facilitate integration of skills \n<ul>\n<li>Give students practice exercises explicitly to increase automaticity</li>\n<li>Temporarily constrain the scope of the task</li>\n<li>Explicitly include integration in performance criteria</li>\n</ul>\n</li>\n<li>Facilitate transfer: \n<ul>\n<li>Discuss conditions of applicability</li>\n<li>Give exercises explicitly about conditions of applicability</li>\n<li>Provide opportunities to practice in diverse contexts</li>\n<li>Use hypothetical scenarios for practice questions</li>\n<li>Ask students to generalize to abstract principles</li>\n<li>Identify deep features using comparisons</li>\n<li>Prompt students to retrieve relevant knowledge</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"5__What_Kinds_of_Practice_and_Feedback_Enhance_Learning_\">5. What Kinds of Practice and Feedback Enhance Learning?</h2>\n<p>Practice is often misguided and feedback poorly timed, insufficient, or unfocused. To be effective, practice should be directed by goals and coupled with targeted feedback.</p>\n<p><strong id=\"Research_3\">Research:</strong></p>\n<ul>\n<li>Learning can be predicted by time in <em>deliberate practice</em>, which is marked by being directed toward a specific goal and an appropriate level of challenge. [I've often heard deliberate practice described with an emphasis on mindful attention, in contrast with practice in a flow state (for example in an article by Ericsson himself\u2014the last paragraph before \"Future Directions\"), but the authors questionably suggest that flow is a sign of appropriate challenge. For motivation, perhaps it is, but I would argue not so for deliberate practice.]</li>\n<li>Clearly specified performance criteria can help direct students' practice.</li>\n<li>Learning is hampered by either insufficient or excessive challenge.</li>\n<li>The success of individual tutoring is largely driven by the ability to tailor challenges to a level appropriate to deliberate practice.</li>\n<li>An instructor can improve learning outcomes with difficult tasks by adding structure and support to bring it within the bounds of the student's competence. This can consist of guidance by the instructor, or of written prompts and checklists. (C.f. \"scaffolding\" in Chapter 4.)</li>\n<li>The benefits of deliberate practice accrue gradually with increasing time spent practicing; both students and teachers underestimate the time needed.</li>\n<li>The effectiveness of feedback is determined by both content and timing. It should communicate progress and direct subsequent effort, and it should be supplied when students can best use it.</li>\n<li>Feedback that identifies specific items that need improvement will aid learning more than will a mere indication of error.</li>\n<li>Unfocused feedback can be counterproductive by overwhelming the student and failing to direct effort well.</li>\n<li>Generally, more frequent and more rapid feedback is better for learning. Delayed feedback can be useful in helping students learn to recognize and correct their own errors.</li>\n</ul>\n<p><strong id=\"Strategies_3\"> Strategies:</strong></p>\n<ul>\n<li>Establish goals: \n<ul>\n<li>Be explicit about course goals, and phrase them in terms of capabilities rather than knowledge</li>\n<li>Use a rubric to communicate performance criteria</li>\n<li>Give contrasting examples of high and low quality work</li>\n<li>Progressively refine goals</li>\n</ul>\n</li>\n<li>Encourage deliberate practice: \n<ul>\n<li>Assess prior knowledge to set an appropriate challenge</li>\n<li>Create many chances to practice</li>\n<li>Build scaffolding into assignments</li>\n<li>Set expectations about practice</li>\n</ul>\n</li>\n<li>Target feedback: \n<ul>\n<li>Look for patterns of errors</li>\n<li>Use prioritized feedback to direct student efforts</li>\n<li>Give feedback on strengths and weaknesses</li>\n<li>Allow frequent opportunities for feedback</li>\n<li>Provide feedback at the group level, potentially in real-time</li>\n<li>Require peer feedback on assignments</li>\n<li>Require students to describe how they incorporated feedback</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"6__Why_Do_Student_Development_and_Course_Climate_Matter_for_Student_Learning_\">6. Why Do Student Development and Course Climate Matter for Student Learning?</h2>\n<p>People vary not just intellectually, but also socially and emotionally. Students' identities may be entangled with the course material and environment in complicated ways that often go unrecognized. A student's entire state\u2014not just the intellect\u2014interacts with the social, emotional, and intellectual climate of the course to impact learning, for better or for worse. [When I saw this chapter title, I had a vague worry that it would seem out of place, a perfunctory nod to diversity studies or something. I'm still not entirely comfortable with parts of the treatment here, but the above premise is sound.]</p>\n<p><strong id=\"Research_4\">Research:</strong></p>\n<ul>\n<li>The research involved in this first section is of a different nature from the rest of the text. In the first part, the authors seek to describe student development, and cite a model which characterizes developmental changes into seven dimensions: developing competence, managing emotions, developing autonomy, establishing identity, freeing interpersonal relationships, developing purpose, and developing identity. They then cite research characterizing intellectual developments in terms of stages: duality, multiplicity, relativism, and commitment. Similarly, stages for social development. The point is that people can have a lot of different implicit and explicit beliefs, modes of communication, and ways of processing new information, which they can't just switch off and homogenize when they enter a classroom, and that people have done a lot of work to attempt to enumerate and connect these things. [I think the discussion here is the weakest part of the book, and I'd be interested in better resources on the subject, if they exist.]</li>\n<li>For course climate, they describe a classification in terms of whether an environment is <em>marginalizing&nbsp;</em>or <em>centralizing&nbsp;</em>(describing how the perspectives of groups might be discouraged or welcomed), and whether this occurs&nbsp;<em>implicitly&nbsp;</em>or <em>explicitly</em>. Implicitly marginalizing classrooms are the most common of the four quadrants.</li>\n<li>In implicitly marginalizing environments (i.e. without overt exclusion or hostility towards outgroups), individuals may suffer an accumulation of <em>micro-inequities</em>&nbsp;that over time has a large impact on learning. A number of studies have found that perceptions of a marginalizing climate are negatively correlated with learning and career outcomes. The authors identify four important channels for marginalization: stereotypes, tone, faculty-student interactions, and content.</li>\n<li>The activation (in the sense of Chapter 1) of stereotypes can influence learning, generally impairing performance; this effect is known as <em>stereotype threat</em>. The activation does not have to be a result of explicitly invoking the stereotype; implicit communication of assumptions or apparently innocuous comments also have effects.</li>\n<li>The immediate mechanism for stereotype threat seems to be a disruptive emotional reaction; this as opposed to self-efficacy or self-esteem being depressed or otherwise brought in line with the stereotype. The effect does not require any belief in the stereotype. There are deeper nuances as well as strategies for mitigating the effect in the literature.</li>\n<li>Perceived hostility or expectations of failure in stereotypes can decrease motivation and drive students from a discipline.</li>\n<li>A positive, constructive, and encouraging tone in discussions and syllabi improves student motivation and behavior. (This in contrast to punitive, critical, or demeaning tone.)</li>\n<li>Perceived positive faculty attitudes towards and interactions with undergrads are correlated with higher rates of graduate education and better self-reported learning outcomes. Faculty availability is a major factor in students' academic decisions.</li>\n<li>Course content itself in its orientation towards inclusiveness can have cognitive, motivational, and socio-emotional effects on learning.</li>\n</ul>\n<p><strong id=\"Strategies_4\"> Strategies:</strong></p>\n<ul>\n<li>Promote intellectual development: \n<ul>\n<li>Make uncertainty, ambiguity, and complexity safe</li>\n<li>Resist a single right answer</li>\n<li>Incorporate use of evidence into performance criteria</li>\n</ul>\n</li>\n<li>Promote social development: \n<ul>\n<li>Examine your assumptions about your students</li>\n<li>Be mindful of accidental cues regarding stereotypes</li>\n<li>Do not ask individuals to speak for an entire group</li>\n<li>Recognize students as individuals.</li>\n</ul>\n</li>\n<li>Promote an inclusive climate: \n<ul>\n<li>Be a model for inclusive language, behavior, and attitudes</li>\n<li>Use multiple and diverse examples</li>\n<li>Establish and reinforce ground rules for interaction</li>\n<li>Make sure course content does not marginalize students</li>\n<li>Use the syllabus and first day of class to establish climate</li>\n<li>Set up processes to get feedback on the climate</li>\n<li>Anticipate and prepare for sensitive issues</li>\n<li>Address tensions early</li>\n<li>Turn discord and tension into a learning opportunity</li>\n<li>Facilitate and model active listening.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"7__How_Do_Students_Become_Self_Directed_Learners_\">7. How Do Students Become Self-Directed Learners?</h2>\n<p>As one progresses in academic and professional life, one takes progressively more responsibility for one's own learning. The jump between high school and college can be especially jarring in this regard. <em>Metacognition</em>, \"the process of reflecting on and directing one's own thinking,\" becomes increasingly important, but falls outside the scope of most instruction. Still, to effectively direct their own learning, students must learn and practice an array of metacognitive skills.</p>\n<p><strong id=\"Research_5\">Research:</strong></p>\n<ul>\n<li>One model represents metacognition as a continuously looping cycle of task assessment, evaluation of strengths and weaknesses, planning, execution and simultaneous monitoring, and reflection; all of these five steps are informed by a student's beliefs about intelligence and learning.</li>\n<li>Assessing the task is not always natural or obvious to students (essay prompts are often ignored; learning goals are not always clear).</li>\n<li>People are poor judges of their own knowledge and skills, tending to overestimate their abilities more the weaker they are.</li>\n<li>Novices spend little time in the planning phase of the cycle relative to experts in physics, math, and writing. Novice plans are often poorly matched to the task.</li>\n<li>Students who naturally and continuously monitor their performance and understanding learn better.</li>\n<li>Students can be taught to self-monitor, and this also improves learning.</li>\n<li>Monitoring alone is not sufficient; novice problem solvers will continue to use a strategy after it has failed (and certainly after it has proven modestly successful and familiar but not optimal).</li>\n<li>Students who believe their intelligence is malleable rather than fixed are more likely to learn and perform well.</li>\n<li>Moreover, the \"malleable\" perspective can be promoted by external influences, still leading to better performance.</li>\n</ul>\n<p><strong id=\"Strategies_5\"> Strategies:</strong></p>\n<ul>\n<li>Promote task assessment: \n<ul>\n<li>Be more explicit about assignments than you think is necessary</li>\n<li>Tell students what you do not want</li>\n<li>Check students' understanding of the task in their own words</li>\n<li>Provide a rubric</li>\n</ul>\n</li>\n<li>Promote self-evaluation: \n<ul>\n<li>Give timely feedback</li>\n<li>Provide opportunities for self-assessment.</li>\n</ul>\n</li>\n<li>Promote planning: \n<ul>\n<li>Have students implement a plan you provide</li>\n<li>Have students implement their own plan</li>\n<li>Make planning the central goal of the assignment.</li>\n</ul>\n</li>\n<li>Promote self-monitoring: \n<ul>\n<li>Provide simple heuristic questions for self-evaluation</li>\n<li>Have students do guided self-assessments</li>\n<li>Require students to reflect on and annotate their own work</li>\n<li>Use peer review</li>\n</ul>\n</li>\n<li>Promote reflection and adjustment: \n<ul>\n<li>Prompt students to reflect on their performance</li>\n<li>Prompt students to analyze effectiveness of study skills</li>\n<li>Present multiple strategies</li>\n<li>Create assignments that focus on strategizing</li>\n</ul>\n</li>\n<li>Promote useful beliefs about intelligence and learning: \n<ul>\n<li>Address these beliefs directly</li>\n<li>Broaden students' understanding of learning</li>\n<li>Help students set realistic expectations</li>\n</ul>\n</li>\n<li>Promote metacognition: \n<ul>\n<li>Model your metacognitive process for your students</li>\n<li>Scaffold students in their metacognitive processes</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Conclusion__Applying_the_Seven_Principles_to_Ourselves\">Conclusion: Applying the Seven Principles to Ourselves</h2>\n<p>The authors turn their principles inward and discuss learning to teach. For the most part this is a restatement of the principles with no particularly new insights in their application to teaching, but there are interesting comments regarding the first few:</p>\n<ul>\n<li>Many teachers were formerly atypically successful students, and their prior knowledge can lead to distorted expectations; accordingly, many of the recommendations involve gathering data about the students.</li>\n<li>The organization of this book into principles is itself a deliberate application of the second principle.</li>\n<li>For motivation, the authors try to connect the content of the course with what every teacher really cares about: efficiency. They also suggest focusing on one or two aspects of teaching in a given semester, in order to build up small successes in improving teaching.</li>\n<li>In terms of mastery, practice and feedback, climate, and metacognition, teaching is not so different from any other skill.</li>\n</ul>\n<h2 id=\"Appendices\">Appendices</h2>\n<p><em>HLW </em>has eight appendices on tools mentioned throughout the book, with a reiteration of their nature and utility, and most importantly, example checklists and worksheets. These are</p>\n<ul>\n<li>Student self-assessment</li>\n<li>Concept maps</li>\n<li>Rubrics</li>\n<li>Learning objectives</li>\n<li>Ground rules [for discussion]</li>\n<li>Exam wrappers [for promoting metacognition on graded exams]</li>\n<li>Checklists</li>\n<li>Reader response/peer review</li>\n</ul>\n<p>These alone would have been an improvement over most teaching materials I grew up with.</p>", "sections": [{"title": "Review", "anchor": "Review", "level": 1}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "1. How Does Students' Prior Knowledge Affect Their Learning?", "anchor": "1__How_Does_Students__Prior_Knowledge_Affect_Their_Learning_", "level": 2}, {"title": "Research consensus:", "anchor": "Research_consensus_", "level": 3}, {"title": "Strategies for teachers:", "anchor": "Strategies_for_teachers_", "level": 3}, {"title": "2. How Does the Way Students Organize Knowledge Affect Their Learning?", "anchor": "2__How_Does_the_Way_Students_Organize_Knowledge_Affect_Their_Learning_", "level": 2}, {"title": "Research:", "anchor": "Research_", "level": 3}, {"title": "Strategies:", "anchor": "Strategies_", "level": 3}, {"title": "3. What Factors Motivate Students to Learn?", "anchor": "3__What_Factors_Motivate_Students_to_Learn_", "level": 2}, {"title": "Research:", "anchor": "Research_1", "level": 3}, {"title": "Strategies:", "anchor": "Strategies_1", "level": 3}, {"title": "4. How Do Students Develop Mastery?", "anchor": "4__How_Do_Students_Develop_Mastery_", "level": 2}, {"title": "Research:", "anchor": "Research_2", "level": 3}, {"title": "Strategies:", "anchor": "Strategies_2", "level": 3}, {"title": "5. What Kinds of Practice and Feedback Enhance Learning?", "anchor": "5__What_Kinds_of_Practice_and_Feedback_Enhance_Learning_", "level": 2}, {"title": "Research:", "anchor": "Research_3", "level": 3}, {"title": "Strategies:", "anchor": "Strategies_3", "level": 3}, {"title": "6. Why Do Student Development and Course Climate Matter for Student Learning?", "anchor": "6__Why_Do_Student_Development_and_Course_Climate_Matter_for_Student_Learning_", "level": 2}, {"title": "Research:", "anchor": "Research_4", "level": 3}, {"title": "Strategies:", "anchor": "Strategies_4", "level": 3}, {"title": "7. How Do Students Become Self-Directed Learners?", "anchor": "7__How_Do_Students_Become_Self_Directed_Learners_", "level": 2}, {"title": "Research:", "anchor": "Research_5", "level": 3}, {"title": "Strategies:", "anchor": "Strategies_5", "level": 3}, {"title": "Conclusion: Applying the Seven Principles to Ourselves", "anchor": "Conclusion__Applying_the_Seven_Principles_to_Ourselves", "level": 2}, {"title": "Appendices", "anchor": "Appendices", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 27}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LqjKP255fPRY7aMzw", "RWo4LwFzpHNQCTcYt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-20T01:36:13.962Z", "modifiedAt": null, "url": null, "title": "Dealing with a Major Personal Crisis", "slug": "dealing-with-a-major-personal-crisis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.143Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5ZmDJEZjoHwhwTqDs/dealing-with-a-major-personal-crisis", "pageUrlRelative": "/posts/5ZmDJEZjoHwhwTqDs/dealing-with-a-major-personal-crisis", "linkUrl": "https://www.lesswrong.com/posts/5ZmDJEZjoHwhwTqDs/dealing-with-a-major-personal-crisis", "postedAtFormatted": "Monday, January 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dealing%20with%20a%20Major%20Personal%20Crisis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADealing%20with%20a%20Major%20Personal%20Crisis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ZmDJEZjoHwhwTqDs%2Fdealing-with-a-major-personal-crisis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dealing%20with%20a%20Major%20Personal%20Crisis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ZmDJEZjoHwhwTqDs%2Fdealing-with-a-major-personal-crisis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ZmDJEZjoHwhwTqDs%2Fdealing-with-a-major-personal-crisis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<p><strong><a href=\"http://wiki.lesswrong.com/wiki/Dealing_with_a_Major_Personal_Crisis\">This</a></strong>&nbsp;is the <a href=\"/r/discussion/lw/jf2/what_are_you_working_on_january_2014/a9s2\">earlier</a>&nbsp;<a href=\"/lw/jfw/january_monthly_bragging_thread/aavq promised\">promised</a>&nbsp;post about <strong><a href=\"http://wiki.lesswrong.com/wiki/Dealing_with_a_Major_Personal_Crisis\">Dealing with a Major Personal Crisis</a></strong>. Please continue reading there but comment here.</p>\n<p>The reasons for posting it this way are explained at the end of the link. I hope this approach does what I want it to.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5ZmDJEZjoHwhwTqDs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 26, "extendedScore": null, "score": 1.5220015569918952e-06, "legacy": true, "legacyId": "25312", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-20T02:03:36.896Z", "modifiedAt": null, "url": null, "title": "Argue against yourself", "slug": "argue-against-yourself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:02.866Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8BJ7CQXquk3qJRqM7/argue-against-yourself", "pageUrlRelative": "/posts/8BJ7CQXquk3qJRqM7/argue-against-yourself", "linkUrl": "https://www.lesswrong.com/posts/8BJ7CQXquk3qJRqM7/argue-against-yourself", "postedAtFormatted": "Monday, January 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Argue%20against%20yourself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArgue%20against%20yourself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8BJ7CQXquk3qJRqM7%2Fargue-against-yourself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Argue%20against%20yourself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8BJ7CQXquk3qJRqM7%2Fargue-against-yourself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8BJ7CQXquk3qJRqM7%2Fargue-against-yourself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<p>Often times, you see the flaw in someone else's thinking. You then conclude that they're wrong, and that the opposite is true.</p>\n<blockquote>\n<p>People who believe in god are so stupid! They're wrong because X. God definitely doesn't exist.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>But just because their <em>logic</em>&nbsp;was wrong, doesn't mean that their <em>conclusion </em>was also wrong. For example, I could believe that gravity works because the sky is blue. My bad logic doesn't cause gravity to stop working.</p>\n<p>What you should do, is argue against yourself. People often derive their beliefs from arguments that they win against other people. You shouldn't do that. You should only derive beliefs from the arguments that <a href=\"http://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html\">you win against yourself</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8BJ7CQXquk3qJRqM7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": -2, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "25313", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-20T02:36:29.575Z", "modifiedAt": null, "url": null, "title": "Using vs. evaluating (or, Why I don't come around here no more)", "slug": "using-vs-evaluating-or-why-i-don-t-come-around-here-no-more", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:04.302Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DkvgxwF9hJqh59rjd/using-vs-evaluating-or-why-i-don-t-come-around-here-no-more", "pageUrlRelative": "/posts/DkvgxwF9hJqh59rjd/using-vs-evaluating-or-why-i-don-t-come-around-here-no-more", "linkUrl": "https://www.lesswrong.com/posts/DkvgxwF9hJqh59rjd/using-vs-evaluating-or-why-i-don-t-come-around-here-no-more", "postedAtFormatted": "Monday, January 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Using%20vs.%20evaluating%20(or%2C%20Why%20I%20don't%20come%20around%20here%20no%20more)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUsing%20vs.%20evaluating%20(or%2C%20Why%20I%20don't%20come%20around%20here%20no%20more)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDkvgxwF9hJqh59rjd%2Fusing-vs-evaluating-or-why-i-don-t-come-around-here-no-more%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Using%20vs.%20evaluating%20(or%2C%20Why%20I%20don't%20come%20around%20here%20no%20more)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDkvgxwF9hJqh59rjd%2Fusing-vs-evaluating-or-why-i-don-t-come-around-here-no-more", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDkvgxwF9hJqh59rjd%2Fusing-vs-evaluating-or-why-i-don-t-come-around-here-no-more", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 256, "htmlBody": "<p>[Summary: Trying to use new ideas is more productive than trying to evaluate them.]</p>\n<p>I haven't posted to LessWrong in a long time. I have a fan-fiction <a href=\"http://www.fimfiction.net/user/Bad+Horse/blog\">blog</a> where I post theories about writing and literature. Topics don't overlap at all between the two websites (so far), but I prioritize posting there much higher than posting here, because responses seem more productive there.</p>\n<p>The key difference, I think, is that people who read posts on LessWrong ask whether they're \"true\" or \"false\", while the writers who read my posts on writing want to <em>write</em>. If I say something that doesn't ring true to one of them, he's likely to say, \"I don't think that's quite right; try changing X to Y,\" or, \"When I'm in that situation, I find Z more helpful\", or, \"That doesn't cover all the cases, but if we expand your idea in this way...\"</p>\n<p>Whereas on LessWrong a more typical response would be, \"Aha, I've found a case for which your step 7 fails! GOTCHA!\"</p>\n<p>It's always clear from the context of a writing blog why a piece of information might be useful. It often isn't clear how a LessWrong post might be useful. You could blame the author for not providing you with that context. Or, you could be pro-active and provide that context yourself, by thinking as you read a post about how it fits into the bigger framework of questions about rationality, utility, philosophy, ethics, and the future, and thinking about what questions and goals you have that it might be relevant to.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DkvgxwF9hJqh59rjd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 25, "extendedScore": null, "score": 1.5220681132867122e-06, "legacy": true, "legacyId": "25314", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-20T02:59:21.759Z", "modifiedAt": null, "url": null, "title": "Meetup : Lesswrong Boulder CO", "slug": "meetup-lesswrong-boulder-co-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:03.343Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "yakurbe0112", "createdAt": "2012-07-01T23:40:26.855Z", "isAdmin": false, "displayName": "yakurbe0112"}, "userId": "xXr6Jngw57uMXveQ9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZcEMQ44w73XBPDX5i/meetup-lesswrong-boulder-co-0", "pageUrlRelative": "/posts/ZcEMQ44w73XBPDX5i/meetup-lesswrong-boulder-co-0", "linkUrl": "https://www.lesswrong.com/posts/ZcEMQ44w73XBPDX5i/meetup-lesswrong-boulder-co-0", "postedAtFormatted": "Monday, January 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Lesswrong%20Boulder%20CO&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Lesswrong%20Boulder%20CO%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZcEMQ44w73XBPDX5i%2Fmeetup-lesswrong-boulder-co-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Lesswrong%20Boulder%20CO%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZcEMQ44w73XBPDX5i%2Fmeetup-lesswrong-boulder-co-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZcEMQ44w73XBPDX5i%2Fmeetup-lesswrong-boulder-co-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vx'>Lesswrong Boulder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 January 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2690 Baseline Rd, Boulder, CO 80305</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Last week didn't work out so we're doing goal factoring again. This place has really good pizza too.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vx'>Lesswrong Boulder CO</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZcEMQ44w73XBPDX5i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5220933737725152e-06, "legacy": true, "legacyId": "25315", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Lesswrong_Boulder_CO\">Discussion article for the meetup : <a href=\"/meetups/vx\">Lesswrong Boulder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 January 2014 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2690 Baseline Rd, Boulder, CO 80305</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Last week didn't work out so we're doing goal factoring again. This place has really good pizza too.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Lesswrong_Boulder_CO1\">Discussion article for the meetup : <a href=\"/meetups/vx\">Lesswrong Boulder CO</a></h2>", "sections": [{"title": "Discussion article for the meetup : Lesswrong Boulder CO", "anchor": "Discussion_article_for_the_meetup___Lesswrong_Boulder_CO", "level": 1}, {"title": "Discussion article for the meetup : Lesswrong Boulder CO", "anchor": "Discussion_article_for_the_meetup___Lesswrong_Boulder_CO1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-20T07:29:08.244Z", "modifiedAt": null, "url": null, "title": "Why don't more rationalists start startups?", "slug": "why-don-t-more-rationalists-start-startups", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:29.655Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamzerner", "createdAt": "2013-08-12T18:18:47.957Z", "isAdmin": false, "displayName": "adamzerner"}, "userId": "6jLdWqegNefgaabhr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a9LSNgpxjhRnPhCbw/why-don-t-more-rationalists-start-startups", "pageUrlRelative": "/posts/a9LSNgpxjhRnPhCbw/why-don-t-more-rationalists-start-startups", "linkUrl": "https://www.lesswrong.com/posts/a9LSNgpxjhRnPhCbw/why-don-t-more-rationalists-start-startups", "postedAtFormatted": "Monday, January 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20don't%20more%20rationalists%20start%20startups%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20don't%20more%20rationalists%20start%20startups%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa9LSNgpxjhRnPhCbw%2Fwhy-don-t-more-rationalists-start-startups%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20don't%20more%20rationalists%20start%20startups%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa9LSNgpxjhRnPhCbw%2Fwhy-don-t-more-rationalists-start-startups", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa9LSNgpxjhRnPhCbw%2Fwhy-don-t-more-rationalists-start-startups", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1992, "htmlBody": "<p>My motivation behind this post stems from <a href=\"http://wiki.lesswrong.com/wiki/Aumann's_agreement_theorem\">Aumann's agreement theorem</a>. It seems that my opinions on startups differ from most of the rationality community, so I want to share my thoughts, and hear your thoughts, so we could reach a better conclusion.</p>\n<p>I think that if you're smart and hard working, there's a pretty good chance that you achieve financial independence within a decade of the beginning of your journey to start a startup. And that's my conservative estimate.</p>\n<p>\"Achieve financial independence\" only scratches the surface of the benefits of succeeding with a startup. If you're an altruist, you'll get to help a lot of other people too. And making millions of dollars will also allow you the leverage you need to make riskier investments with much higher expected values, allowing you to grow your money quickly so you could do more good.</p>\n<p>A lot of this is predicated on my belief that you have a good chance at succeeding if you're smart and hardworking, so let me explain why I think this.</p>\n<hr />\n<p>&nbsp;</p>\n<p>Along the lines of reductionism, \"success with a startup\" is an outcome (I guess we could define success as a $5-10M exit in under 10 years). And outcomes consist of their components. My argument consists of breaking the main outcome into it's components, and then arguing that the components are all likely enough for the main outcome to be likely.</p>\n<p>I think that the 4 components are:</p>\n<ol>\n<li>Devise an idea for a product that creates demand.</li>\n<li>Build it.</li>\n<li>Market and sell it.</li>\n<li>Things run smoothly (some might call this luck).</li>\n</ol>\n<div><br /></div>\n<p><strong>The Idea</strong></p>\n<p>Your idea has to be for a product or service (I'll just say product to keep things simple) that creates demand, and can be met profitably. In other words, <a href=\"http://www.paulgraham.com/startupideas.html\">make something people want</a> (this article spells it out pretty well).</p>\n<p>What could go wrong?</p>\n<ul>\n<li>Failure to think <a href=\"/lw/bc3/sotw_be_specific/\">specifically</a> about <a href=\"http://www.entrepreneur.com/article/34942\">benefits</a>. These articles explain what I mean by this better than I could.</li>\n<li>Failure to understand customers. To put yourself in their minds and understand what it is that they do and don't want. This is distinct from the first bullet point. You could have a specific benefit in mind, but be wrong about whether it's something your customer really wants (or about how badly they want it).</li>\n<li>Failure to research competitors. Maybe you came up with a great idea, but it turns out that it exists already.</li>\n</ul>\n<div>The big issue here is the first bullet point. As spelled out by Eliezer's article, people are horrible at thinking specifically about the benefits that their idea will bring customers. They're horrible at moving down the ladder of abstraction. They think more along the lines of \"we connect people\" instead of \"we let you talk to your friends\". Even YC applicants (probably the best startup accelerator in the world) suffer from this problem immensely. I think that this problem is the single biggest cause of failure for startups. (They say that 90% of startups fail? Well &gt;99% of people can't think concretely.) However, I think that it's something that could be avoided with willpower, reading the LessWrong <a href=\"http://wiki.lesswrong.com/wiki/Rationality_materials\">sequences</a>, and taking some time to practice your new habit.</div>\n<div><br /></div>\n<div>The second bullet point shouldn't be too hard, once your thinking becomes specific. And the third one is mostly a matter of taking a few days to do some research.</div>\n<div><br /></div>\n<div><br /></div>\n<div><strong>Build It</strong></div>\n<div><br /></div>\n<div>What I mean by 'build it' is pretty straightforward: take that idea you had, and make it real.</div>\n<div><br /></div>\n<div>What could go wrong?</div>\n<div>\n<ul>\n<li>Our society doesn't have the technological or scientific progress necessary to build the product. For example, I have an idea for a machine that teleports you from one place to another. Unfortunately, we as a society aren't at a point where someone could build that.</li>\n<li>You personally don't have the skills to build it.</li>\n<li>You don't work hard enough. Maybe you try, and find that you don't have the willpower. Maybe you try, find that you do have the willpower, but realize that the amount of work it take isn't worth it to you.</li>\n<li>You can't find people with the skills to work on it with you (cofounders).</li>\n<li>You can't raise money from investors to hire people to help you build it.</li>\n<li>The people you work with/hire aren't good enough to build the product you envisioned.</li>\n</ul>\n<div>There's probably other things that could go wrong that I can't think of, but I think this is enough to work with for now.</div>\n<div><br /></div>\n<div>First bullet point: you really just have to avoid unfeasible ideas. Doesn't sound too hard. I guess this could be a problem for someone at the forefront of their field, trying to push the boundaries, but who makes an error in judging what's buildable. However, I think that there's plenty of ideas that don't run you this risk.</div>\n</div>\n<div><br /></div>\n<div>Second bullet point: if you don't have the skills, then get them. There's plenty of resources available to learn. For one, it only takes a couple months to get the skills you'll need to build a decent website. Or you could invest more time to study something like engineering or design, which will increase your options of what ideas you could build.</div>\n<div><br /></div>\n<div>Third bullet point: if you don't have willpower, it'll be pretty tough to succeed. Possible, but pretty tough. I don't recommend trying.</div>\n<div><br /></div>\n<div>Fourth bullet point: thats just another thing that limits the ideas you could build successfully. Some ideas you can't build without a cofounder/cofounders, and some you can. Finding a cofounder shouldn't be too difficult though.</div>\n<div><br /></div>\n<div>Fifth bullet point: this is actually a tough one. A lot of ideas will require at least seed funding (tens/hundreds of thousands of dollars) to build. There are definitely a bunch of ideas that you could build without any investment, but they're the minority. So let's say you have an idea that does require investment, but you're having trouble raising money (which I think would be understandable). Basically, I'd say that you should focus on peeling away the <a href=\"http://pmarchive.com/guide_to_startups_part2.html\">layers of risk</a>. By following doing that, <a href=\"http://venturehacks.com/\">reading up</a> on fundraising and using <a href=\"https://angel.co/\">Angel List</a>, I think you'd have a pretty good shot at raising the money you need. Still though, I think not being able to find an investor is a legitimate risk.</div>\n<div><br /></div>\n<div>Sixth bullet point: I've never hired anyone before, but it doesn't seem that hard. Doing a good job optimizing your hires seems like something you'd have to be skilled at, but satisficing to the point that they could do a sufficient job building the product you envision seems to be something that any reasonable person can do.</div>\n<div><br /></div>\n<div><br /></div>\n<div><strong>Market and Sell It</strong></div>\n<div><br /></div>\n<div>Once you think up your product and build it, you then have to sell it to your customers. This means reaching them, convincing them, and distributing to them.</div>\n<div><br /></div>\n<div>What could go wrong?</div>\n<div>\n<ul>\n<li>You're unable to communicate clearly to your customers what <a href=\"http://www.entrepreneur.com/article/34942\">benefits</a> they'll be receiving if they use your product.</li>\n<li>You're unable to persuade them. (There are other elements to <a href=\"http://www.amazon.com/Influence-Psychology-Persuasion-Business-Essentials/dp/006124189X\">persuasion</a> aside from clear communication).</li>\n<li>You didn't reach enough people. Maybe you didn't advertise enough. Maybe you thought word would spread, and it didn't.</li>\n<li>You're having <a href=\"http://en.wikipedia.org/wiki/Distribution_(business)\">distribution</a> problems (delivering the product to your customer).</li>\n<li>PR problems. Something goes wrong and you obtain a bad reputation.</li>\n</ul>\n<div>First bullet point: see The Idea.</div>\n</div>\n<div><br /></div>\n<div>Second bullet point: First of all, read that book (Influence by Robert Cialdini). I'm no expert on persuasion, but I think taking a little time to read a few books would make you sufficiently good at it. And it's not that hard to persuade people when you've got a product that they love.</div>\n<div><br /></div>\n<div>Third bullet point: I'm no expert on this either. However, I do hear that internet ads nowadays make it pretty easy and affordable to reach a targeted and good sized audience. Also, as always for things you don't know too much about, read up on it and educate yourself. I don't know enough about this to argue it well, and I don't feel too strongly about it, but I get the sense that this is unlikely to prevent success. Doing <a href=\"http://www.quora.com/Startups/What-are-some-techniques-startups-use-to-initially-draw-attention-to-their-product\">this stuff</a> seems like it'd be sufficient.</div>\n<div><br /></div>\n<div>Fourth bullet point: I don't know much about distribution. It seems that distribution is really only a problem for certain types of businesses. For them, I guess that's something you have to take into account before you go forth with an idea. Otherwise, it doesn't seem like to big a deal.</div>\n<div><br /></div>\n<div>Fifth bullet point: I guess this is something that could kill a business. To a reasonable person though, it doesn't seem like too big a risk.</div>\n<div><br /></div>\n<div><br /></div>\n<div><strong>Things Running Smoothly</strong></div>\n<div><br /></div>\n<div>Obviously, crazy things could happen. However, they don't seem too likely.</div>\n<div><br /></div>\n<div>What could go wrong?</div>\n<div>\n<ul>\n<li>Legal issues (current). Maybe you did something illegal and didn't realize it (ex. copyright infringement), and sanctions or a lawsuit killed your startup.</li>\n<li>Legal issues (future). Maybe new laws were enacted that killed your startup.</li>\n<li>Something in your personal life goes wrong that requires you to quit.</li>\n<li>Your competitors innovate and beat you out. Or a big company decides to enter the market, and crushes you.</li>\n<li>Scientific findings lead to your product being obsolete.</li>\n<li>Macroeconomic conditions change, which somehow leads to people not wanting your product.</li>\n<li>Political/social conditions lead to people not wanting your product.</li>\n</ul>\n<div>Most of these seem like they have pretty low probabilities of happening. Low enough where they don't influence the overall likelihood of success too much. Especially if you're doing something that genuinely helps people (if so, it's less likely that things like legal/economic/political/social changes will end up hurting you).</div>\n</div>\n<div><br /></div>\n<div>Regarding competitors beating you out, that's something that sounds like a big risk, but actually doesn't happen as often as you'd think. You'd think that if a startup comes across an innovative idea, that big companies that are hundreds or thousands of times the size of that startup would just copy the idea and execute it themselves, given that the big company has so many resources. Somehow that doesn't happen too often. Big companies just seem slow to adapt. By the time they react, the startup usually has momentum, which often times causes the big company to acquire the startup, or lose market share. So just based off of my understanding of what actually tends to happen, this risk seems to be something to note, but not something to really worry about (see <a href=\"http://paulgraham.com/startuplessons.html\">lesson #4</a>).</div>\n<div><br /></div>\n<div><br /></div>\n<div><strong>Conclusion</strong></div>\n<div><br /></div>\n<div>Given all of this, I think that if you're smart and hard working, you should have *at least* an 80-90% chance at succeeding at a startup. Again... you have to think about what specific benefits your idea provides... you have to map out how it'll be built, and work hard at doing so... and you have to read up on marketing, and work hard at it. As I argue above, the components all seem very doable, and thus the parent outcome seems very achievable.</div>\n<div><br /></div>\n<div>I really mean for this article to be a starting point for discussion. I think that if we outline the components and discuss each one, we'll make a lot of progress in coming to an agreement. So let me know which components you think I omitted, and which components you think I'm mistaken about.</div>\n<div><br /></div>\n<div><br /></div>\n<div>PS: A lot of people seem to disregard startups as something they don't know much about, and aren't too interested in. Why? Success = millions of dollars. Aren't you curious as to how likely that success is? If there's an outcome you desire, shouldn't you be interested in how achievable it is?</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a9LSNgpxjhRnPhCbw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": -7, "extendedScore": null, "score": 1.5223903980178522e-06, "legacy": true, "legacyId": "25316", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 99, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NgtYDP3ZtLJaM248W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-20T16:55:17.133Z", "modifiedAt": null, "url": null, "title": "[link] Why Self-Control Seems (but may not be) Limited", "slug": "link-why-self-control-seems-but-may-not-be-limited", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:08.869Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9SSXcQ92ZJHgdqzDj/link-why-self-control-seems-but-may-not-be-limited", "pageUrlRelative": "/posts/9SSXcQ92ZJHgdqzDj/link-why-self-control-seems-but-may-not-be-limited", "linkUrl": "https://www.lesswrong.com/posts/9SSXcQ92ZJHgdqzDj/link-why-self-control-seems-but-may-not-be-limited", "postedAtFormatted": "Monday, January 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Why%20Self-Control%20Seems%20(but%20may%20not%20be)%20Limited&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Why%20Self-Control%20Seems%20(but%20may%20not%20be)%20Limited%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9SSXcQ92ZJHgdqzDj%2Flink-why-self-control-seems-but-may-not-be-limited%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Why%20Self-Control%20Seems%20(but%20may%20not%20be)%20Limited%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9SSXcQ92ZJHgdqzDj%2Flink-why-self-control-seems-but-may-not-be-limited", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9SSXcQ92ZJHgdqzDj%2Flink-why-self-control-seems-but-may-not-be-limited", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 977, "htmlBody": "<p><a href=\"/lw/jan/kurzban_et_al_on_opportunity_cost_models_of/\">Another</a> attack on the resource-based model of willpower, Michael Inzlicht, Brandon J. Schmeichel and C. Neil Macrae have a paper called \"Why Self-Control Seems (but may not be) Limited\" <a href=\"http://www.sciencedirect.com/science/article/pii/S1364661313002945\">in press</a> in <em>Trends in Cognitive Sciences</em>. <a href=\"http://www.michaelinzlicht.com/wp/wp-content/uploads/downloads/2013/12/Inzlicht-Schmeichel-Macrae-in-press.pdf\">Ungated version here</a>.</p>\n<p>Some of the most interesting points:</p>\n<ul>\n<li>Over 100 studies appear to be consistent with self-control being a limited resource, but generally these studies do not observe resource depletion directly, but infer it from whether or not people's performance declines in a second self-control task.</li>\n<li>The only attempts to directly measure the loss or gain of a resource have been studies measuring blood glucose, but these studies have serious limitations, the most important being an inability to replicate evidence of mental effort actually affecting the level of glucose in the blood.</li>\n<li>Self-control also seems to replenish by things such as \"watching a favorite television program, affirming some core value, or even praying\", which would seem to conflict with the hypothesis inherent resource limitations. The resource-based model also seems evolutionarily implausible.</li>\n</ul>\n<p>The authors offer their own theory of self-control. One-sentence summary (my formulation, not from the paper): \"Our brains don't want to only work, because by doing some play on the side, we may come to discover things that will allow us to do even more valuable work.\"</p>\n<ul>\n<li>Ultimately, self-control limitations are proposed to be an <a href=\"http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/XX.pdf\">exploration-exploitation</a> tradeoff, \"regulating the extent to which the control system favors task engagement (exploitation) versus task disengagement and sampling of other opportunities (exploration)\".</li>\n<li>Research suggests that cognitive effort is inherently aversive, and that after humans have worked on some task for a while, \"ever more resources are needed to counteract the aversiveness of work, or else people will gravitate toward inherently rewarding leisure instead\". According to the model proposed by the authors, this allows the organism to both focus on activities that will provide it with rewards (exploitation), but also to disengage from them and seek activities which may be even more rewarding (exploration). Feelings such as boredom function to stop the organism from getting too fixated on individual tasks, and allow us to spend some time on tasks which might turn out to be even more valuable.</li>\n</ul>\n<p>The explanation of the actual proposed psychological mechanism is good enough that it deserves to be quoted in full:</p>\n<p style=\"padding-left: 30px;\">Based on the tradeoffs identified above, we propose that initial acts of control lead to shifts in motivation away from &ldquo;have-to&rdquo; or &ldquo;ought-to&rdquo; goals and toward &ldquo;want-to&rdquo; goals (see Figure 2). &ldquo;Have-to&rdquo; tasks are carried out through a sense of duty or contractual obligation, while &ldquo;want-to&rdquo; tasks are carried out because they are personally enjoyable and meaningful [41]; as such, &ldquo;want-to&rdquo; tasks feel easy to perform and to maintain in focal attention [41]. The distinction between &ldquo;have-to&rdquo; and &ldquo;want-to,&rdquo; however, is not always clear cut, with some &ldquo;want-to&rdquo; goals (e.g., wanting to lose weight) being more introjected and feeling more like &ldquo;have-to&rdquo; goals because they are adopted out of a sense of duty, societal conformity, or guilt instead of anticipated pleasure [53].<br /><br />According to decades of research on self-determination theory [54], the quality of motivation that people apply to a situation ranges from extrinsic motivation, whereby behavior is performed because of external demand or reward, to intrinsic motivation, whereby behavior is performed because it is inherently enjoyable and rewarding. Thus, when we suggest that depletion leads to a shift from &ldquo;have-to&rdquo; to &ldquo;want-to&rdquo; goals, we are suggesting that prior acts of cognitive effort lead people to prefer activities that they deem enjoyable or gratifying over activities that they feel they ought to do because it corresponds to some external pressure or introjected goal. For example, after initial cognitive exertion, restrained eaters prefer to indulge their sweet tooth rather than adhere to their strict views of what is appropriate to eat [55]. Crucially, this shift from &ldquo;have-to&rdquo; to &ldquo;want-to&rdquo; can be offset when people become (internally or externally) motivated to perform a &ldquo;have-to&rdquo; task [49]. Thus, it is not that people cannot control themselves on some externally mandated task (e.g., name colors, do not read words); it is that they do not feel like controlling themselves, preferring to indulge instead in more inherently enjoyable and easier pursuits (e.g., read words). Like fatigue, the effect is driven by reluctance and not incapability [41] (see Box 2).<br /><br />Research is consistent with this motivational viewpoint. Although working hard at Time 1 tends to lead to less control on &ldquo;have-to&rdquo; tasks at Time 2, this effect is attenuated when participants are motivated to perform the Time 2 task [32], personally invested in the Time 2 task [56], or when they enjoy the Time 1 task [57]. Similarly, although performance tends to falter after continuously performing a task for a long period, it returns to baseline when participants are rewarded for their efforts [58]; and remains stable for participants who have some control over and are thus engaged with the task [59]. Motivation, in short, moderates depletion [60]. We suggest that changes in task motivation also mediate depletion [61].<br /><br />Depletion, however, is not simply less motivation overall. Rather, it is produced by lower motivation to engage in &ldquo;have-to&rdquo; tasks, yet higher motivation to engage in &ldquo;want-to&rdquo; tasks. Depletion stokes desire [62]. Thus, working hard at Time 1 increases approach motivation, as indexed by self-reported states, impulsive responding, and sensitivity to inherently-rewarding, appetitive stimuli [63]. This shift in motivational priorities from &ldquo;have-to&rdquo; to &ldquo;want-to&rdquo; means that depletion can increase the reward value of inherently-rewarding stimuli. For example, when depleted dieters see food cues, they show more activity in the orbitofrontal cortex, a brain area associated with coding reward value, compared to non-depleted dieters [64].</p>\n<p>See also: <a href=\"/lw/jan/kurzban_et_al_on_opportunity_cost_models_of/\">Kurzban et al. on opportunity cost models of mental fatigue and resource-based models of willpower</a>; <a href=\"/lw/jh0/deregulating_distraction_moving_towards_the_goal/\">Deregulating Distraction, Moving Towards the Goal, and Level Hopping</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ksdiAMKfgSyEeKMo6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9SSXcQ92ZJHgdqzDj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 54, "extendedScore": null, "score": 0.000148, "legacy": true, "legacyId": "25317", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HWvqNAc7aeeeWngEh", "yFALNnscB2qgehnJv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-20T21:16:58.328Z", "modifiedAt": null, "url": null, "title": "Someone who knows how, please fix the horribly broken open thread link.", "slug": "someone-who-knows-how-please-fix-the-horribly-broken-open", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:03.547Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JwTFXga3P8qJc7gLQ/someone-who-knows-how-please-fix-the-horribly-broken-open", "pageUrlRelative": "/posts/JwTFXga3P8qJc7gLQ/someone-who-knows-how-please-fix-the-horribly-broken-open", "linkUrl": "https://www.lesswrong.com/posts/JwTFXga3P8qJc7gLQ/someone-who-knows-how-please-fix-the-horribly-broken-open", "postedAtFormatted": "Monday, January 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Someone%20who%20knows%20how%2C%20please%20fix%20the%20horribly%20broken%20open%20thread%20link.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASomeone%20who%20knows%20how%2C%20please%20fix%20the%20horribly%20broken%20open%20thread%20link.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJwTFXga3P8qJc7gLQ%2Fsomeone-who-knows-how-please-fix-the-horribly-broken-open%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Someone%20who%20knows%20how%2C%20please%20fix%20the%20horribly%20broken%20open%20thread%20link.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJwTFXga3P8qJc7gLQ%2Fsomeone-who-knows-how-please-fix-the-horribly-broken-open", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJwTFXga3P8qJc7gLQ%2Fsomeone-who-knows-how-please-fix-the-horribly-broken-open", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 20, "htmlBody": "<p>(No way of posting this in a visible open thread, sorry.) This post will self-destruct once the link is fixed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JwTFXga3P8qJc7gLQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "25318", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-20T23:30:53.521Z", "modifiedAt": null, "url": null, "title": "Propagandizing Meta-Ethics in an Essay Contest", "slug": "propagandizing-meta-ethics-in-an-essay-contest", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:05.441Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "28iKD7fEnHvK8pNNm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rqnb2qysaP8FbAKv5/propagandizing-meta-ethics-in-an-essay-contest", "pageUrlRelative": "/posts/rqnb2qysaP8FbAKv5/propagandizing-meta-ethics-in-an-essay-contest", "linkUrl": "https://www.lesswrong.com/posts/rqnb2qysaP8FbAKv5/propagandizing-meta-ethics-in-an-essay-contest", "postedAtFormatted": "Monday, January 20th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Propagandizing%20Meta-Ethics%20in%20an%20Essay%20Contest&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APropagandizing%20Meta-Ethics%20in%20an%20Essay%20Contest%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frqnb2qysaP8FbAKv5%2Fpropagandizing-meta-ethics-in-an-essay-contest%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Propagandizing%20Meta-Ethics%20in%20an%20Essay%20Contest%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frqnb2qysaP8FbAKv5%2Fpropagandizing-meta-ethics-in-an-essay-contest", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frqnb2qysaP8FbAKv5%2Fpropagandizing-meta-ethics-in-an-essay-contest", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 942, "htmlBody": "<p>So someone I follow on Facebook linked to <a href=\"http://www.fqxi.org/community/essay\">this essay contest</a> on the subject of: <em>How Should Humanity Steer the Future?</em>&nbsp; My urge to make a joke of it immediately kicked in.&nbsp; The impulse to joke turned into an impulse to actually submit an essay when the words <a href=\"/lw/u9/my_naturalistic_awakening/\">\"steering the future\"</a> set off a minor \"ding!\" in my head.</p>\n<p>&nbsp;</p>\n<p>At least regarding AI, many papers and articles have already been published on <em>what that problem is</em>: even well-intentioned people could accidentally create a Completely or Partially Alien Mind-Design that has no care and no sympathy for us or our values.&nbsp; <a href=\"http://yudkowsky.net/singularity/ai-risk\">Should such a thing grow more powerful than us, it would proceed to kill us all.&nbsp; We would be <em>in the way</em>, as a bacterium is to us, and dealt with identically.</a></p>\n<p>&nbsp;</p>\n<p><em>Blah blah blah blah.</em></p>\n<p>&nbsp;</p>\n<p>To me personally, by sheer madness of personal preferences, <em>that is not the interesting part</em>.&nbsp; Danger, even existential danger, seems to me quite passe these days.&nbsp; In only the time I've been alive, we've been under threat of nuclear apocalypse via war between nation-states or nuclear-weapons usage by terrorist groups, global warming and other environmental damages are slowly destroying the only planetary-level habitat we humans have, and in the past five or so years we've been dealing with continental-level economic collapses and stagnation as well (I personally subscribe to the theory that capitalism itself is a human-Unfriendly optimization process, which is partially apropos here).&nbsp; Those are just the apocalypses: <em>then</em> we have to add in all the daily pains, indignities and deaths suffered by the vast majority of the world's people, many of whom are so inured to their suffering that they consider it a normal or even morally appropriate part of their lives.&nbsp; Only at the end of that vast, astronomical summation can we say we have totalled humanity's <em>problems</em>.</p>\n<p>&nbsp;</p>\n<p>All that, only in the time I've been alive, for about 25 years now, even when, statistically speaking, I'm living in a steadily improving golden age (or at least a gilded age that's fighting to become a golden age).&nbsp; No previous generation of humanity was ever so rich, so educated, so able to dream!&nbsp; Why did I constantly feel myself to be well-past the Despair and Cynicism Event Horizons?</p>\n<p>&nbsp;</p>\n<p>Which left the younger me asking: what's the point, then?&nbsp; How can it be that <a href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CDoQFjAB&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D8r1CZTLk-Gk%2F&amp;ei=oqzdUsPaFabV4wTJ7oH4Aw&amp;usg=AFQjCNG6rkfkJSQmKeLmdx4yEPcI-oAR4A&amp;sig2=qKzWeBNy_H2sTCrazjWvxw&amp;bvm=bv.59568121,d.bGE\">\"everything's amazing and nobody's happy\"</a>?&nbsp; What have we failed to do or achieve?&nbsp; Are we failing to find meaning in our lives because <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/jssr.12046/abstract\">we don't serve God</a>?&nbsp; But God is silent!&nbsp; Do our social structures pervert our daily lives away from the <a href=\"http://techcrunch.com/2013/11/22/geeks-for-monarchy/\">older, deeper \"red in tooth and claw\" human nature</a>?&nbsp; But the utility increases are clear and demonstrable (besides which, those ideologies sicken me, personally)!</p>\n<p>&nbsp;</p>\n<p>Enter the project to <em>solve meta-ethics</em>, normally considered the key to building an AI that will not go obviously and dramatically human!<em>wrong</em> in its operation.&nbsp; Or, as I would put it, the project to <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">fully and really describe the set of all wishes we actually want to make</a>, by indirection if necessary.</p>\n<p>&nbsp;</p>\n<p>Which is what's so much more <em>interesting</em> to me than mere AI safety engineering.&nbsp; Not that I <em>want</em> Clippy to win, but I would certainly be happi<em>er</em> with an outcome in which Clippy wins but I <em>know</em> what I was shooting for and what I did wrong, versus an outcome in which Clippy wins but I die ignorant of some of What is Good and Right.&nbsp; Dying in battle for a good cause is better than dying in battle for a bad cause, and actually dying at home as a helpless victim because I couldn't find a good cause is preferable to damaging others for a bad cause!</p>\n<p>&nbsp;</p>\n<p>After all, how can my younger self be anti-utopian, anti-totalitarian and pro-revolutionary?&nbsp; He considers the existing social conditions in need of overthrow and radical improvement, and yet cannot define the end goal of his movement?&nbsp; Goodness, light, and warmth are like pornography: we know them when we see them (<a href=\"/lw/h5u/seq_rerun_purchase_fuzzies_and_utilons_separately/\">and still be told that<em> </em>instinctive warm-fuzzy recognition <em>doesn't even resemble</em> actual goodness?</a>)?&nbsp; How then, might I today recognize these things when I see them, and do so <em>smartly</em>?</p>\n<p>&nbsp;</p>\n<p>Hence my desired essay topic, and hence my asking for advice here.&nbsp; It's easy to recapitulate the Oncoming AI-Induced Omnicide Problem, but I don't want to go at it from that angle.&nbsp; Inherently, we should wish to know what we wish for, especially since we <em>already</em> possess powerful optimization processes (like our economies, as I said above) well below the superintelligent level.&nbsp; If someone wants to ask about steering the future, by now, my instinctive first response is, \"We <em>must</em> know where we <em>actually want</em> the future to go, such that if we really set out for there, we will look forward to our arrival, <em>and</em> such that once we get there, we will be glad we did, <em>and</em> such that we make sure to value our present selves while we're on the journey.&nbsp; OR ELSE.\"&nbsp; I want to use the essay to argue this view, with reference not only to AI but to any <em>other</em> potential application for a partial or complete descriptive theory of meta-ethics.</p>\n<p>&nbsp;</p>\n<p>Questions (or rather, Gondor calling for aid):</p>\n<ul>\n<li>What are resources on these meta-ethical issues <em>other than</em> the Sequences?&nbsp; Official publications of any kind, but especially those suitable for reference in an academic-level paper.&nbsp; \"Archive dive this really smart guy's blog\" is simply not an effective way to propagandize (except with HPMoR readers, and we're a self-selecting bunch).</li>\n<li>What are interesting and effective ways to apply a descriptive theory of ethics <em>other than FAI</em>?&nbsp; What can we <em>do</em> with wishes we <em>know</em> we want to make?</li>\n</ul>\n<p>&nbsp;</p>\n<p>Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rqnb2qysaP8FbAKv5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 2, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "25320", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["75LZMCCePG4Pwj3dB", "4ARaTpNX62uaL86j6", "DzuRwTGYW6ABgrDnN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-21T02:18:17.720Z", "modifiedAt": null, "url": null, "title": "Rationalists Are Less Credulous But Better At Taking Ideas Seriously", "slug": "rationalists-are-less-credulous-but-better-at-taking-ideas", "viewCount": null, "lastCommentedAt": "2021-11-13T10:40:51.006Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gh2qQHrCg3teQen3c/rationalists-are-less-credulous-but-better-at-taking-ideas", "pageUrlRelative": "/posts/Gh2qQHrCg3teQen3c/rationalists-are-less-credulous-but-better-at-taking-ideas", "linkUrl": "https://www.lesswrong.com/posts/Gh2qQHrCg3teQen3c/rationalists-are-less-credulous-but-better-at-taking-ideas", "postedAtFormatted": "Tuesday, January 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalists%20Are%20Less%20Credulous%20But%20Better%20At%20Taking%20Ideas%20Seriously&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalists%20Are%20Less%20Credulous%20But%20Better%20At%20Taking%20Ideas%20Seriously%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGh2qQHrCg3teQen3c%2Frationalists-are-less-credulous-but-better-at-taking-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalists%20Are%20Less%20Credulous%20But%20Better%20At%20Taking%20Ideas%20Seriously%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGh2qQHrCg3teQen3c%2Frationalists-are-less-credulous-but-better-at-taking-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGh2qQHrCg3teQen3c%2Frationalists-are-less-credulous-but-better-at-taking-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1007, "htmlBody": "<p>Consider the following commonly-made argument: cryonics is unlikely to work. Trained rationalists are signed up for cryonics at rates much greater than the general population. Therefore, rationalists must be pretty gullible people, and their claims to be good at evaluating evidence must be exaggerations at best.<br /><br />This argument is wrong, and we can prove it using data from the last two Less Wrong surveys.<br /><br />The question at hand is whether rationalist training - represented here by extensive familiarity with Less Wrong material - makes people more likely to believe in cryonics. <br /><br />We investigate with a cross-sectional study, looking at proto-rationalists versus experienced rationalists. Define proto-rationalists as those respondents to the Less Wrong survey who indicate they have been in the community for less than six months and have zero karma (usually indicative of never having posted a comment). And define experienced rationalists as those respondents to the Less Wrong survey who indicate they have been in the community for over two years and have &gt;1000 karma (usually indicative of having written many well-received posts).<br /><br />By these definitions, there are 93 proto-rationalists, who have been in the community an average of 1.3 months, and 134 experienced rationalists, who have been in the community an average of 4.5 years. Proto-rationalists generally have not read any rationality training material - only 20/93 had read even one-quarter of the Less Wrong Sequences. Experienced rationalists are, well, more experienced: two-thirds of them have read pretty much all the Sequence material.<br /><br />Proto-rationalists thought that, on average, there was a 21% chance of an average cryonically frozen person being revived in the future. Experienced rationalists thought that, on average, there was a 15% chance of same. The difference was marginally significant (p &lt; 0.1).<br /><br />Marginal significance is a copout, but this isn't our only data source. Last year, using the same definitions, proto-rationalists assigned a 15% probability to cryonics working, and experienced rationalists assigned a 12% chance. We see the same pattern.<br /><br />So experienced rationalists are consistently less likely to believe in cryonics than proto-rationalists, and rationalist training probably makes you less likely to believe cryonics will work.<br /><br />On the other hand, 0% of proto-rationalists had signed up for cryonics compared to 13% of experienced rationalists. 48% of proto-rationalists rejected the idea of signing up for cryonics entirely, compared to only 25% of experienced rationalists. So although rationalists are less likely to believe cryonics will work, they are much more likely to sign up for it. Last year's survey shows the same pattern.<br /><br />This is not necessarily surprising. It only indicates that experienced rationalists and proto-rationalists treat their beliefs in different ways. Proto-rationalists form a belief, play with it in their heads, and then do whatever they were going to do anyway -&nbsp; usually some variant on what everyone else does. Experienced rationalists form a belief, examine the consequences, and then act strategically to get what they want.<br /><br />Imagine a lottery run by an incompetent official who accidentally sets it up so that the average payoff is far more than the average ticket price. For example, maybe the lottery sells only ten $1 tickets, but the jackpot is $1 million, so that each $1 ticket gives you a 10% chance of winning $1 million.<br /><br />Goofus hears about the lottery and realizes that his expected gain from playing the lottery is $99,999. \"Huh,\" he says, \"the numbers say I could actually <em>win</em> money by playing this lottery. What an interesting mathematical curiosity!\" Then he goes off and does something else, since everyone knows playing the lottery is what stupid people do.<br /><br />Gallant hears about the lottery, performs the same calculation, and buys up all ten tickets.<br /><br />The relevant difference between Goofus and Gallant is not skill at estimating the chances of winning the lottery. We can even change the problem so that Gallant is more aware of the unlikelihood of winning than Goofus - perhaps Goofus mistakenly believes there are only five tickets, and so Gallant's superior knowledge tells him that winning the lottery is even more unlikely than Goofus thinks. Gallant will still play, and Goofus will still pass.<br /><br />The relevant difference is that Gallant knows how to <a href=\"/lw/2l6/taking_ideas_seriously/ \">take ideas seriously</a>.<br /><br />Taking ideas seriously isn't always smart. If you're the sort of person who <a href=\"http://mathforum.org/dr.math/faq/faq.false.proof.html\">falls for proofs that 1 = 2</a>&nbsp; , then refusing to take ideas seriously is a good way to avoid ending up actually believing that 1 = 2, and a generally excellent life choice.<br /><br />On the other hand, progress depends on someone somewhere taking a new idea seriously, so it's nice to have people who can do that too. Helping people learn this skill and when to apply it is one goal of the rationalist movement.<br /><br />In this case it seems to have been successful. Proto-rationalists think there is a 21% chance of a new technology making them immortal - surely an outcome as desirable as any lottery jackpot - consider it an interesting curiosity, and go do something else because only weirdos sign up for cryonics. <br /><br />Experienced rationalists think there is a lower chance of cryonics working, but some of them decide that even a pretty low chance of immortality sounds pretty good, and act strategically on this belief.<br /><br />This is not to either attack or defend the policy of assigning a non-negligible probability to cryonics working. This is meant to show only that the difference in cryonics status between proto-rationalists and experienced rationalists is based on meta-level cognitive skills in the latter whose desirability is orthogonal to the object-level question about cryonics.</p>\n<p><em>(an earlier version of this article was posted on my blog last year; I have moved it here now that I have replicated the results with a second survey)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 3, "ZnHkaTkxukegSrZqE": 2, "qoTbWwaJtTSKosRCA": 12}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gh2qQHrCg3teQen3c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}], "voteCount": 79, "baseScore": 96, "extendedScore": null, "score": 0.000245, "legacy": true, "legacyId": "25321", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 97, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 286, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q8jyAdRYbieK8PtfT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-21T14:27:35.517Z", "modifiedAt": null, "url": null, "title": "Meetup : Social meetup in Raleigh", "slug": "meetup-social-meetup-in-raleigh", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pugG87GKPmvwednX4/meetup-social-meetup-in-raleigh", "pageUrlRelative": "/posts/pugG87GKPmvwednX4/meetup-social-meetup-in-raleigh", "linkUrl": "https://www.lesswrong.com/posts/pugG87GKPmvwednX4/meetup-social-meetup-in-raleigh", "postedAtFormatted": "Tuesday, January 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Social%20meetup%20in%20Raleigh&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Social%20meetup%20in%20Raleigh%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpugG87GKPmvwednX4%2Fmeetup-social-meetup-in-raleigh%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Social%20meetup%20in%20Raleigh%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpugG87GKPmvwednX4%2Fmeetup-social-meetup-in-raleigh", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpugG87GKPmvwednX4%2Fmeetup-social-meetup-in-raleigh", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/vy'>Social meetup in Raleigh</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 January 2014 07:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">3709 Neil St, Raleigh, NC 27607</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>New location this time!</p>\n\n<p>We'll have a followup to our previous meetup about persuasion techniques and defenses to them from Influence, with a somewhat more low-key social focus. For those that have been eagerly anticipating it, this is also the date and location for our friend Deniz' cider release party (Sourwood Brewing).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/vy'>Social meetup in Raleigh</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pugG87GKPmvwednX4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.5244473403959398e-06, "legacy": true, "legacyId": "25324", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Social_meetup_in_Raleigh\">Discussion article for the meetup : <a href=\"/meetups/vy\">Social meetup in Raleigh</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 January 2014 07:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">3709 Neil St, Raleigh, NC 27607</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>New location this time!</p>\n\n<p>We'll have a followup to our previous meetup about persuasion techniques and defenses to them from Influence, with a somewhat more low-key social focus. For those that have been eagerly anticipating it, this is also the date and location for our friend Deniz' cider release party (Sourwood Brewing).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Social_meetup_in_Raleigh1\">Discussion article for the meetup : <a href=\"/meetups/vy\">Social meetup in Raleigh</a></h2>", "sections": [{"title": "Discussion article for the meetup : Social meetup in Raleigh", "anchor": "Discussion_article_for_the_meetup___Social_meetup_in_Raleigh", "level": 1}, {"title": "Discussion article for the meetup : Social meetup in Raleigh", "anchor": "Discussion_article_for_the_meetup___Social_meetup_in_Raleigh1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-21T21:13:19.363Z", "modifiedAt": null, "url": null, "title": "Decision Auctions aka \"How to fairly assign chores, or decide who gets the last cookie\"", "slug": "decision-auctions-aka-how-to-fairly-assign-chores-or-decide", "viewCount": null, "lastCommentedAt": "2021-04-01T01:24:55.176Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3bjYRDaYR2pvBekNT/decision-auctions-aka-how-to-fairly-assign-chores-or-decide", "pageUrlRelative": "/posts/3bjYRDaYR2pvBekNT/decision-auctions-aka-how-to-fairly-assign-chores-or-decide", "linkUrl": "https://www.lesswrong.com/posts/3bjYRDaYR2pvBekNT/decision-auctions-aka-how-to-fairly-assign-chores-or-decide", "postedAtFormatted": "Tuesday, January 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20Auctions%20aka%20%22How%20to%20fairly%20assign%20chores%2C%20or%20decide%20who%20gets%20the%20last%20cookie%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20Auctions%20aka%20%22How%20to%20fairly%20assign%20chores%2C%20or%20decide%20who%20gets%20the%20last%20cookie%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3bjYRDaYR2pvBekNT%2Fdecision-auctions-aka-how-to-fairly-assign-chores-or-decide%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20Auctions%20aka%20%22How%20to%20fairly%20assign%20chores%2C%20or%20decide%20who%20gets%20the%20last%20cookie%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3bjYRDaYR2pvBekNT%2Fdecision-auctions-aka-how-to-fairly-assign-chores-or-decide", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3bjYRDaYR2pvBekNT%2Fdecision-auctions-aka-how-to-fairly-assign-chores-or-decide", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 974, "htmlBody": "<p>After moving in with my new roomies (Danny and Bethany of <a href=\"https://www.beeminder.com/\">Beeminder</a>), I discovered they have a fair and useful way of auctioning off joint decisions. It helps you figure out how much you value certain chores or activities, and it guarantees that these decisions are worked out in a fair way. They call it \"yootling\", and wrote more about it <a title=\"For Love or Money\" href=\"http://messymatters.com/autonomy/\">here</a>. <br /><br />A quick example (Note: this only works if all participants are of the types of people who consider this sort of thing a Good Idea, and not A Grotesque Parody of Caring or whatnot):</p>\n<p>&nbsp;</p>\n<p><strong>Use Case: Who Picks up the Kids from Grandma's?</strong></p>\n<p>D and B are both busy working, but it's time to pick up the kids from their grandparents house. They decide to yootle for it.<br /><br />B bids $100 (In a regular Normal Person exchange, this would be like saying \"I'm elbows deep in code right now, and don't want to break flow. I'd really rather continue working right now, but of course I'll go if it's needed.\")<br /><br />D bids $15 (In a regular Normal Person exchange this would be like saying \"I don't mind too much, though I do have other things to do now...\")</p>\n<p>So D \"wins\" the bid, and B pays him $15 to go get the kids from their grandma's.</p>\n<p>Of course.... it would be a pain in the butt to constantly be paying each other, so instead they have a 10% chance of paying 10x the amount, and a 90% chance to pay nothing, using a random number generator.</p>\n<p>&nbsp;</p>\n<p>This is made easier by the fact that we have a bot to run this, but before that they would use the high-tech solution of Holding Up Fingers.</p>\n<blockquote>\n<div>We may do this multiple times per day, whenever there&rsquo;s a good that we have shared ownership of and one of us wants to offload their shares onto the other person. The goods can be anything, e.g. the last brownie, but they&rsquo;re more often &ldquo;bads&rdquo; like who will get up in the middle of the night with a vomiting child, or who will book plane tickets for a trip.</div>\n<div><br /></div>\n<div>We find this an elegant means of assigning loathed tasks. The person who minded least winds up doing the chore, but gets compensated for it at a price that by their own estimation was fair.<br /></div>\n</blockquote>\n<div><br /></div>\n<div><br /></div>\n<div>Some other ways it can be implemented:</div>\n<div><br /></div>\n<blockquote>\n<div><strong>Joint purchase auction</strong></div>\n<div><br /></div>\n<div>The decision auction and variants are about allocating shared or partially shared resources to one person or the other, or picking one person to do something. Once in a while you have the opposite problem: deciding on a joint purchase.</div>\n<div><br /></div>\n<div>Suppose Danny thinks we need a new sofa (this is very hypothetical). I think the one we have is just fine thank you. After some discussion I concede that it would be nice to have a sofa that was less doggy. Danny, being terribly excited about getting a new sofa does a bunch of research and finds his ideal sofa. I think it is a bit overpriced considering it is going to be a piece of gymnastics equipment for the kids for the next 6 years. Conflict ensues! I could bluff that I&rsquo;m not interested in a new sofa at all and that he can buy it himself if he wants it that badly. But he probably doesn&rsquo;t want it that bad, and I do want it a little. If only we could buy the sofa conditional on our combined utility for it exceeding the cost, and pay in proportion to our utilities to boot. Well, thanks to separate finances and the magic of mechanism design, we can! We submit sealed bids for the sofa and buy it if the sum of our bids is enough. (And, importantly, commit to not buying it for at least a year otherwise.) Any surplus is redistributed in proportion to our bids. For example, if Danny bid $80 and I bid $40 to buy a hundred dollar sofa, then we&rsquo;d buy it, with Danny chipping in twice as much as me, namely $67 to my $33.</div>\n</blockquote>\n<div><br /><br /></div>\n<blockquote>\n<div><strong>Generosity without sacrificing social efficiency</strong></div>\n<div><br /></div>\n<div>&ldquo;The payments are simply what keep us honest in assessing that.&rdquo;</div>\n<div>If you&rsquo;re thinking &ldquo;how mercenary all this is!&rdquo; then, well, I&rsquo;m unclear how you made it this far into this post. But it&rsquo;s not nearly as cold as it may sound. We do nice things for each other all the time, and frequently use yootling to make sure it&rsquo;s socially efficient to do so. Suppose I invite Danny to a sing-along showing of Once More With Feeling (this may or may not be hypothetical) and Danny doesn&rsquo;t exactly want to go but can see that I have value for his company. He might (quite non-hypothetically) say &ldquo;I&rsquo;ll half-accompany you!&rdquo; by which he means that he&rsquo;ll yootle me for whether he goes or not. In other words, he magnanimously decides to treat his joining me as a 50/50 joint decision. If I have greater value for him coming than he has for not coming, then I&rsquo;ll pay him to come. But if it&rsquo;s the other way around, he will pay me to let him off the hook. We don&rsquo;t actually care much about the payments, though those are necessary for the auction to work. We care about making sure that he comes to the Buffy sing-along if and only if my value for his company exceeds his value for staying home. The payments are simply what keep us honest in assessing that. The increased fairness &mdash; the winner sharing their utility with the loser &mdash; is icing.</div>\n</blockquote>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 1, "b8FHrKqyXuYGWc6vn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3bjYRDaYR2pvBekNT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 55, "extendedScore": null, "score": 0.000145, "legacy": true, "legacyId": "25322", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-21T23:18:23.758Z", "modifiedAt": null, "url": null, "title": "Meetup : First Meetup in Hamburg, Germany", "slug": "meetup-first-meetup-in-hamburg-germany", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:05.747Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i3ksD3Ns4oYESkaYe/meetup-first-meetup-in-hamburg-germany", "pageUrlRelative": "/posts/i3ksD3Ns4oYESkaYe/meetup-first-meetup-in-hamburg-germany", "linkUrl": "https://www.lesswrong.com/posts/i3ksD3Ns4oYESkaYe/meetup-first-meetup-in-hamburg-germany", "postedAtFormatted": "Tuesday, January 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Meetup%20in%20Hamburg%2C%20Germany&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Meetup%20in%20Hamburg%2C%20Germany%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi3ksD3Ns4oYESkaYe%2Fmeetup-first-meetup-in-hamburg-germany%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Meetup%20in%20Hamburg%2C%20Germany%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi3ksD3Ns4oYESkaYe%2Fmeetup-first-meetup-in-hamburg-germany", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi3ksD3Ns4oYESkaYe%2Fmeetup-first-meetup-in-hamburg-germany", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/vz\">First Meetup in Hamburg, Germany</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">07 February 2014 07:00:00PM (+0100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Einstein Bistro, Grindelberg 81-83, 20144 Hamburg, Germany</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The first Meetup in Hamburg!</p>\n<p>We will get to know each other and depending on time and interest do the following:</p>\n<ul>\n<li>Discuss rationality topics (no politics or sports)</li>\n<li>Play Liars Dice aka Bluff</li>\n<li>Play Set (the game)</li>\n<li>Play a calibration game (Wits and Wagers or a special variant)</li>\n</ul>\n<p>For the latter you may bring a demanding guessing question e.g. \"how many cars are registered in Hamburg?\"</p>\n<p>The Meetup is open-ended. You may ask for a ride.</p>\n<p>I'd be happy if you could tell us whether you will participate so I can reserve accordingly. You can ask any questions in this brand-new mailing list: <a rel=\"nofollow\" href=\"https://groups.google.com/forum/#!forum/lesswrong-hamburg\">https://groups.google.com/forum/#!forum/lesswrong-hamburg</a></p>\n<p>The location is directly next to the train and bus station Hoheluft, easily reachable from campus, central station and the homes of some participants. We will sit in the back and have a sign so you can find us. Sorry to say but it being a restaurant you will have to pay for a dish or drink. On later meetups we may choose a different location if that suits you better.</p>\n<p>Einstein Bistro: <a rel=\"nofollow\" href=\"http://www.einstein-bistro.de/index.php?option=com_content&amp;view=article&amp;catid=36%3Akontakt&amp;id=74%3Ahoheluft&amp;Itemid=60\">http://www.einstein-bistro.de/index.php?option=com_content&amp;view=article&amp;catid=36%3Akontakt&amp;id=74%3Ahoheluft&amp;Itemid=60</a></p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/vz\">First Meetup in Hamburg, Germany</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i3ksD3Ns4oYESkaYe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.5250354495559634e-06, "legacy": true, "legacyId": "25326", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Meetup_in_Hamburg__Germany\">Discussion article for the meetup : <a href=\"/meetups/vz\">First Meetup in Hamburg, Germany</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">07 February 2014 07:00:00PM (+0100)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Einstein Bistro, Grindelberg 81-83, 20144 Hamburg, Germany</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The first Meetup in Hamburg!</p>\n<p>We will get to know each other and depending on time and interest do the following:</p>\n<ul>\n<li>Discuss rationality topics (no politics or sports)</li>\n<li>Play Liars Dice aka Bluff</li>\n<li>Play Set (the game)</li>\n<li>Play a calibration game (Wits and Wagers or a special variant)</li>\n</ul>\n<p>For the latter you may bring a demanding guessing question e.g. \"how many cars are registered in Hamburg?\"</p>\n<p>The Meetup is open-ended. You may ask for a ride.</p>\n<p>I'd be happy if you could tell us whether you will participate so I can reserve accordingly. You can ask any questions in this brand-new mailing list: <a rel=\"nofollow\" href=\"https://groups.google.com/forum/#!forum/lesswrong-hamburg\">https://groups.google.com/forum/#!forum/lesswrong-hamburg</a></p>\n<p>The location is directly next to the train and bus station Hoheluft, easily reachable from campus, central station and the homes of some participants. We will sit in the back and have a sign so you can find us. Sorry to say but it being a restaurant you will have to pay for a dish or drink. On later meetups we may choose a different location if that suits you better.</p>\n<p>Einstein Bistro: <a rel=\"nofollow\" href=\"http://www.einstein-bistro.de/index.php?option=com_content&amp;view=article&amp;catid=36%3Akontakt&amp;id=74%3Ahoheluft&amp;Itemid=60\">http://www.einstein-bistro.de/index.php?option=com_content&amp;view=article&amp;catid=36%3Akontakt&amp;id=74%3Ahoheluft&amp;Itemid=60</a></p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___First_Meetup_in_Hamburg__Germany1\">Discussion article for the meetup : <a href=\"/meetups/vz\">First Meetup in Hamburg, Germany</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Meetup in Hamburg, Germany", "anchor": "Discussion_article_for_the_meetup___First_Meetup_in_Hamburg__Germany", "level": 1}, {"title": "Discussion article for the meetup : First Meetup in Hamburg, Germany", "anchor": "Discussion_article_for_the_meetup___First_Meetup_in_Hamburg__Germany1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-21T23:21:45.645Z", "modifiedAt": null, "url": null, "title": "Prescriptive vs. descriptive and objective vs. subjective definitions", "slug": "prescriptive-vs-descriptive-and-objective-vs-subjective", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:07.250Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/THzX9znDv7RsT5GJm/prescriptive-vs-descriptive-and-objective-vs-subjective", "pageUrlRelative": "/posts/THzX9znDv7RsT5GJm/prescriptive-vs-descriptive-and-objective-vs-subjective", "linkUrl": "https://www.lesswrong.com/posts/THzX9znDv7RsT5GJm/prescriptive-vs-descriptive-and-objective-vs-subjective", "postedAtFormatted": "Tuesday, January 21st 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Prescriptive%20vs.%20descriptive%20and%20objective%20vs.%20subjective%20definitions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrescriptive%20vs.%20descriptive%20and%20objective%20vs.%20subjective%20definitions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTHzX9znDv7RsT5GJm%2Fprescriptive-vs-descriptive-and-objective-vs-subjective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Prescriptive%20vs.%20descriptive%20and%20objective%20vs.%20subjective%20definitions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTHzX9znDv7RsT5GJm%2Fprescriptive-vs-descriptive-and-objective-vs-subjective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTHzX9znDv7RsT5GJm%2Fprescriptive-vs-descriptive-and-objective-vs-subjective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1120, "htmlBody": "<p>Imagine you're writing a Field Guide to Boats, and you want to know what you should include in your field guide. Barges? Rafts? These things?</p>\n<p><img src=\"http://www.rockhill.ie/images/imagebank/uploads/Best%202%20water%20walkers.jpg\" alt=\"\" width=\"500\" height=\"333\" /></p>\n<p>You want something like a dictionary definition of boat. A descriptive definition that includes anything people commonly think of as a boat; an objective definition, because you're only writing one book, not a separate version for each reader.</p>\n<p>Now imagine you're stranded on an island, and you open a bottle, and a genie comes out and gives you one wish, and you say, \"I wish for a boat!\", and the genie says, \"Well, what's a boat?\" And you know, because you've read stories, that the genie will take your definition of \"boat\" and try to screw you over. You'd better not read out the dictionary definition, or the genie will give you a toy boat, or a boat with a hole in it, or a kayak too small for you to fit into. You need a prescriptive, subjective definition of a thing that will transport <em>you</em> over water.</p>\n<p><a id=\"more\"></a>Too often I write a story, then read it and say, \"Is that really a story?\" It's easy to write something that fits existing theories of what a story is, and is emotionally powerful, yet seems to lack some integrity, relevance, or, hell, I might as well call it a \"soul\"--some magic stuff that makes a story satisfying.</p>\n<p>I want to figure out what the magic stuff is so I can produce it consistently. So I've been blogging about the question, \"What makes a narrative a story (to person P)?\"</p>\n<p>My blog quickly bogged down in unproductive debates about how I'm using the word \"story\". I'm trying to come up with a category definition that includes things I think are good dramatic stories, excludes things that I think are bad stories, and does not need to include poems, comedies, genre fiction, or meta-fiction. But people keep objecting that that isn't what the word \"story\" means.</p>\n<p>True; it isn't. I'm trying to come up with a prescriptive, subjective definition of a dramatic story that says what a story ought to be in order to satisfy person P, exclusive of certain well-understood or highly-distinctive categories, so that I can write stories that satisfy a large number of people and also myself. But my readers are used to descriptive, objective definitions of story that describe things found in books, and can be used in public discourse&nbsp;without reference to a subject. It would be confusing if I said, \"<em>The 10 Best Dinosaur Porn Stories</em>&nbsp;anthology didn't have any stories in it.\"</p>\n<p>I thought it was obvious from context that I was looking for a prescriptive, subjective definition of story, but it was not. At all.</p>\n<p>(Also, dinosaur porn is a thing now.)</p>\n<p>I've seen other discussions deadlock because people didn't realize that some of them were using prescriptive definitions, some descriptive, some objective, some subjective.</p>\n<p>An atheist makes observations about religion, using examples from Unitarian Universalism, Christianity, and Scientology.&nbsp;A Unitarian responds that Scientology isn't a real religion, because its founder and its chief proponents are cynical, deceptive, and uncharitable. A Christian responds that neither is Unitarian Universalism&nbsp;a real religion, since it's different for every UUist. The atheist thinks the idea of a \"real\" religion doesn't even make sense; religions are what other people point at when they say \"religion\" (descriptive, objective).</p>\n<p>The three people are at an impasse, because the Christian framework requires a prescriptive, objective definition of religion, the Unitarian requires a prescriptive, subjective definition, and the atheist requires a descriptive definition. You could even say that UU is <em>defined</em>&nbsp;as the belief that religion must be defined prescriptively and subjectively.</p>\n<p>And then there's the Hopi Indian who told me, \"As a Hopi I believe the old myths; as a modern person I do not.\" He had a&nbsp;descriptive, subjective definition of \"belief\".</p>\n<p>Perhaps people pay so little attention to these distinctions because Western thought is heavily Platonist, and Platonism says that there are no subjective definitions, and there is no distinction between prescriptive and descriptive definitions--the only world we can describe conforms to prescriptive, objective definitions.</p>\n<p>We run into even more problems when topics are complex enough that we must ask of subjective definitions, \"The subjectivity of whom?\" John Searle's famous Chinese-room argument is a sleight-of-hand that exploits the ambiguity of natural language to swap different subjectivities into the scenario at different times. Keep your eye on the subjectivity, and it falls apart. You could say that once you've accepted the framework of utility theory, ethics is primarily about whose subjectivity to use when optimizing utility.</p>\n<p>To relate this to AI ethics (not my main point, but I said yesterday that giving a motivational example is good):</p>\n<p>Three people are discussing ethics. One believes in natural law, which is prescriptive and objective. Another is an evolutionary biologist who believes that a species plus an environment leads to a set of behaviors that maximize reproductive fitness. That's a prescriptive, subjective definition of ethics, but the subjectivity is that of genes, not of \"individuals\", whatever they are. Another is trying to build a \"Friendly AI\". According to FAI theory, he needs a not-completely-subjective definition, since he's not supposed to build his own subjective definition into the AI and its subjectivity is not pre-determined. According to CEV theory, he needs it to be descriptive, since CEV asks you to extrapolate from observed human morality rather than try to derive ethics from first principles. So he wants a descriptive, semi-objective definition.</p>\n<p>Speaking more strictly, I think that CEV attempts to take a descriptive, subjective definition that covers all subjects S where S is the rational, conscious part of a human mind, and extrapolate it to a&nbsp;descriptive&nbsp;definition for all subjects S where S is the computational closure of the rational and conscious parts of any human mind (which has problems due to the requirement of infinite computational power, but let's set that aside). We usually use the term \"objective definition\" to mean a definition used by all humans, because non-humans on Earth don't talk. But in CEV this distinction between objective and subjectivity(HUMANS) becomes very important, as we must consider intelligences unrelated (evolutionarily) to humans.</p>\n<p>An FAIer defending the FAI+CEV program may begin with the assumption that he's looking for a descriptive, subjectivity(rational_part(conscious_mind(HUMANS))) ethics that doesn't contradict ethics(descriptive, subjectivity(computational_closure(rational_part(conscious_mind(HUMANS))))). He then tries to show how you could design a CEV algorithm that will construct such a system of ethics.</p>\n<p>Whether or not a CEV specification can be devised that can achieve its goal isn't of much interest until you address the prior contentious points of whether we want ethics(descriptive, subjectivity(computational_closure(rational_part(conscious_mind(HUMANS))))), and whether that is even a coherent concept. There are many huge logical leaps in asserting that that's a coherent subjective perspective, and philosophical leaps in claiming that any of us, individually, should care about it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "THzX9znDv7RsT5GJm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 8, "extendedScore": null, "score": 1.5250391788423457e-06, "legacy": true, "legacyId": "25325", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-22T02:22:00.639Z", "modifiedAt": null, "url": null, "title": "Studying and Part-time work/supplementary income", "slug": "studying-and-part-time-work-supplementary-income", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.686Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Goobahman", "createdAt": "2011-01-13T05:09:28.962Z", "isAdmin": false, "displayName": "Goobahman"}, "userId": "cidN68rGuy4wwnvFp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3jWn3FCSwzHq4Ck5S/studying-and-part-time-work-supplementary-income", "pageUrlRelative": "/posts/3jWn3FCSwzHq4Ck5S/studying-and-part-time-work-supplementary-income", "linkUrl": "https://www.lesswrong.com/posts/3jWn3FCSwzHq4Ck5S/studying-and-part-time-work-supplementary-income", "postedAtFormatted": "Wednesday, January 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Studying%20and%20Part-time%20work%2Fsupplementary%20income&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStudying%20and%20Part-time%20work%2Fsupplementary%20income%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3jWn3FCSwzHq4Ck5S%2Fstudying-and-part-time-work-supplementary-income%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Studying%20and%20Part-time%20work%2Fsupplementary%20income%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3jWn3FCSwzHq4Ck5S%2Fstudying-and-part-time-work-supplementary-income", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3jWn3FCSwzHq4Ck5S%2Fstudying-and-part-time-work-supplementary-income", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Hi Less Wrong,</p>\n<p>&nbsp;</p>\n<p>I'd like to draw on you for some advice.</p>\n<p>&nbsp;</p>\n<p>I'm about to undertake studies, but will need some supplementary income to attain my desired standard of living while doing so.</p>\n<p>Part-time work could be attained quite easily, but is likely to take the form of something fairly boring e.g. data entry/bar work.</p>\n<p>&nbsp;</p>\n<p>I was thinking that there might be ways out there for me to learn a particular skill set that would enable me to work from home and at more flexible hours for a source of income, as well as providing me the opportunity to learn something new, given that so many people on here seem to do such things quite successfully.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Given my circumstances below what would you recommend:</p>\n<p>&nbsp;</p>\n<p>I'm fairly intelligent, enjoying learning, and have strong social skills</p>\n<p>I live in Sydney Australia</p>\n<p>I have musical talents</p>\n<p>I have a car</p>\n<p>I lack softward development/programming skills</p>\n<p>I have decent office application skills.</p>\n<p>I'm willing to put the hours in to level up a new ability.</p>\n<p>&nbsp;</p>\n<p>Any suggestions/Tips/criticisms are welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3jWn3FCSwzHq4Ck5S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 1, "extendedScore": null, "score": 1.5252389805314558e-06, "legacy": true, "legacyId": "25327", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-22T14:03:45.990Z", "modifiedAt": null, "url": null, "title": "Why are we not starting to map human values?", "slug": "why-are-we-not-starting-to-map-human-values", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:08.582Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "djm", "createdAt": "2013-07-25T02:09:05.823Z", "isAdmin": false, "displayName": "djm"}, "userId": "n3CQbiPiYEKJhXjas", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zyuXC7suPt2M85Scd/why-are-we-not-starting-to-map-human-values", "pageUrlRelative": "/posts/zyuXC7suPt2M85Scd/why-are-we-not-starting-to-map-human-values", "linkUrl": "https://www.lesswrong.com/posts/zyuXC7suPt2M85Scd/why-are-we-not-starting-to-map-human-values", "postedAtFormatted": "Wednesday, January 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20are%20we%20not%20starting%20to%20map%20human%20values%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20are%20we%20not%20starting%20to%20map%20human%20values%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzyuXC7suPt2M85Scd%2Fwhy-are-we-not-starting-to-map-human-values%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20are%20we%20not%20starting%20to%20map%20human%20values%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzyuXC7suPt2M85Scd%2Fwhy-are-we-not-starting-to-map-human-values", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzyuXC7suPt2M85Scd%2Fwhy-are-we-not-starting-to-map-human-values", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p><span style=\"background-color: transparent; font-family: Arial; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"></span></p>\n<p>I agree that trying to map all human values is extremely complex as articulated here [<a style=\"line-height: 1.15; font-size: small; text-decoration: none;\" href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">http://wiki.lesswrong.com/wiki/Complexity_of_value</a>] , but the problem as I see it, is that we do not really have a choice - there has to be some way of measuring the initial AGI to see how it is handling these concepts. <br />I dont understand why we don&rsquo;t try to prototype a high level ontology of core values for an AGI to adhere to - something that humans can discuss and argue about for many years before we actually build an AGI. &nbsp;</p>\n<p><span style=\"line-height: 1.15;\">Law is a useful example which shows that human values cannot be absolutely quantified into a universal system. The law is constantly abused, misused and corrected so if a similar system were to be put into place for an AGI it could quickly lead to UFAI.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\">One of the interesting things about the law is that for core concepts like murder, the rules are well defined and fairly unambiguous, whereas more trivial things (in terms of risk to humans) like tax laws, parking laws are the bits that have a lot of complexity to them.</p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"></span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zyuXC7suPt2M85Scd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 6, "extendedScore": null, "score": 8.570009216566527e-06, "legacy": true, "legacyId": "25332", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-22T17:00:29.670Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta Meetup, Topic: How to Become Immortal", "slug": "meetup-atlanta-meetup-topic-how-to-become-immortal", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YPj9mZoKD6Kif2YAm/meetup-atlanta-meetup-topic-how-to-become-immortal", "pageUrlRelative": "/posts/YPj9mZoKD6Kif2YAm/meetup-atlanta-meetup-topic-how-to-become-immortal", "linkUrl": "https://www.lesswrong.com/posts/YPj9mZoKD6Kif2YAm/meetup-atlanta-meetup-topic-how-to-become-immortal", "postedAtFormatted": "Wednesday, January 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20Meetup%2C%20Topic%3A%20How%20to%20Become%20Immortal&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20Meetup%2C%20Topic%3A%20How%20to%20Become%20Immortal%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYPj9mZoKD6Kif2YAm%2Fmeetup-atlanta-meetup-topic-how-to-become-immortal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20Meetup%2C%20Topic%3A%20How%20to%20Become%20Immortal%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYPj9mZoKD6Kif2YAm%2Fmeetup-atlanta-meetup-topic-how-to-become-immortal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYPj9mZoKD6Kif2YAm%2Fmeetup-atlanta-meetup-topic-how-to-become-immortal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/w0'>Atlanta Meetup, Topic: How to Become Immortal</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 January 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">491 Lindbergh Place NE Apt 618 Atlanta, GA 30324</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join us!  Our topic this time is how to become immortal.  We'll be bring up the various organizations working towards this goal, cryonics, current research etc.</p>\n\n<p>There will be presentation by Oge Nnadi based on the book \"Ending Aging,\" linked here:  <a href=\"http://www.scribd.com/doc/56456472/Aubrey-de-Grey-Ending-Aging\" rel=\"nofollow\">http://www.scribd.com/doc/56456472/Aubrey-de-Grey-Ending-Aging</a></p>\n\n<p>Please feel free to look over the book if you like, but it's not required!</p>\n\n<p>We'll be doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times!</p>\n\n<p>Check out ATLesswrong's facebook group, if you haven't already: <a href=\"https://www.facebook.com/groups/Atlanta.Lesswrong/\" rel=\"nofollow\">https://www.facebook.com/groups/Atlanta.Lesswrong/</a> where you can connect with Atlanta Lesswrongers, suggest a topics for discussion at this meetup, and join our book club or study group!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/w0'>Atlanta Meetup, Topic: How to Become Immortal</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YPj9mZoKD6Kif2YAm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 1, "extendedScore": null, "score": 1.526213417201743e-06, "legacy": true, "legacyId": "25334", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Meetup__Topic__How_to_Become_Immortal\">Discussion article for the meetup : <a href=\"/meetups/w0\">Atlanta Meetup, Topic: How to Become Immortal</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 January 2014 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">491 Lindbergh Place NE Apt 618 Atlanta, GA 30324</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join us!  Our topic this time is how to become immortal.  We'll be bring up the various organizations working towards this goal, cryonics, current research etc.</p>\n\n<p>There will be presentation by Oge Nnadi based on the book \"Ending Aging,\" linked here:  <a href=\"http://www.scribd.com/doc/56456472/Aubrey-de-Grey-Ending-Aging\" rel=\"nofollow\">http://www.scribd.com/doc/56456472/Aubrey-de-Grey-Ending-Aging</a></p>\n\n<p>Please feel free to look over the book if you like, but it's not required!</p>\n\n<p>We'll be doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times!</p>\n\n<p>Check out ATLesswrong's facebook group, if you haven't already: <a href=\"https://www.facebook.com/groups/Atlanta.Lesswrong/\" rel=\"nofollow\">https://www.facebook.com/groups/Atlanta.Lesswrong/</a> where you can connect with Atlanta Lesswrongers, suggest a topics for discussion at this meetup, and join our book club or study group!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Meetup__Topic__How_to_Become_Immortal1\">Discussion article for the meetup : <a href=\"/meetups/w0\">Atlanta Meetup, Topic: How to Become Immortal</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta Meetup, Topic: How to Become Immortal", "anchor": "Discussion_article_for_the_meetup___Atlanta_Meetup__Topic__How_to_Become_Immortal", "level": 1}, {"title": "Discussion article for the meetup : Atlanta Meetup, Topic: How to Become Immortal", "anchor": "Discussion_article_for_the_meetup___Atlanta_Meetup__Topic__How_to_Become_Immortal1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-22T17:37:26.608Z", "modifiedAt": null, "url": null, "title": "Do we underuse the genetic heuristic?", "slug": "do-we-underuse-the-genetic-heuristic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.205Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stefan_Schubert", "createdAt": "2013-12-26T16:42:04.883Z", "isAdmin": false, "displayName": "Stefan_Schubert"}, "userId": "6omuoq9oQuy3KQzG9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZQn7vFCpCrti2QGCy/do-we-underuse-the-genetic-heuristic", "pageUrlRelative": "/posts/ZQn7vFCpCrti2QGCy/do-we-underuse-the-genetic-heuristic", "linkUrl": "https://www.lesswrong.com/posts/ZQn7vFCpCrti2QGCy/do-we-underuse-the-genetic-heuristic", "postedAtFormatted": "Wednesday, January 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20we%20underuse%20the%20genetic%20heuristic%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20we%20underuse%20the%20genetic%20heuristic%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZQn7vFCpCrti2QGCy%2Fdo-we-underuse-the-genetic-heuristic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20we%20underuse%20the%20genetic%20heuristic%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZQn7vFCpCrti2QGCy%2Fdo-we-underuse-the-genetic-heuristic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZQn7vFCpCrti2QGCy%2Fdo-we-underuse-the-genetic-heuristic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3416, "htmlBody": "<p>Someone, say Anna, has uttered a certain proposition P, say \"Betty is stupid\", and we want to evaluate whether it is true or not. We can do this by investigating P&nbsp;<em>directly - </em>i.e. we disregard the fact that Anna has said that Betty is stupid, but look only at what we know about Betty's behaviour (and possibly, we try to find out more about it). Alternatively, we can do this&nbsp;<em>indirectly</em>, by evaluating Anna's credibility with respect to P. If we know, for instance, that Anna is in general very reliable, then we are likely to infer that Betty is indeed stupid, but if we know that Anna hates Betty and that she frequently bases her beliefs on emotion, we are not.</p>\n<p>The latter kind of arguments are called <em>ad hominem </em>arguments, or, in&nbsp;<a href=\"/lw/s3/the_genetic_fallacy/lls\">Hal Finney's</a>&nbsp;apt phrase, the genetic heuristic (I'm going to use these terms interchangeably here). They are often criticized, not the least within analytical philosophy, where the traditional view is that they are more often than not fallacious. Certainly the genetic heuristic is often applied in fallacious ways, some of which are pointed out in <a href=\"/lw/s3/the_genetic_fallacy/\">Yudkowsky's article on the topic</a>. Moreover, it seems reasonable to assume that such fallacies would be much more common if they weren't so frequently pointed out by people (accussations of <em>ad hominem </em>fallacies are common in all sorts of debates). No doubt, we are biologically disposed to attack the person rather than what he is saying on irrelevant grounds.</p>\n<div>The genetic heuristic is not always fallacious, though. If a reputable scientist tells us that P is true, where P falls under her domain, then we have reason to believe that P is true. Similarly, if we know that Liza is a compulsive liar, then we have reason to believe that P is false if Liza has said P.</div>\n<div><br /></div>\n<div>\n<p>We see that genetic reasoning can be both positive and negative - i.e. it can both be used to confirm, and to disconfirm, P. It should also be noted that negative genetic arguments typically only make sense if we assume that we generally put trust into what other people say - i.e. that we use a genetic argument to the effect that the fact that S having said P makes P more likely to be true. If people don't use such arguments, but only look at P directly to evaluate whether it is true or not, it is unclear what importance arguments that throw doubt on the reliability of S have, since it that case, knowing whether S is reliable or not shouldn't affect our belief in P.</p>\n</div>\n<div><strong>Three kinds of genetic arguments</strong></div>\n<div><br /></div>\n<div>We can differentiate between three kinds of genetic arguments (this list is not intended to be exhaustive):</div>\n<div><br /></div>\n<div>1) Caren is unreliable. Hence we disregard anything she says (e.g. since Caren is three years old).</div>\n<div><br /></div>\n<div>\n<p>2) David says P, and given what we know about P and about David (especially of David's knowledge and attitute to P), we have reason to believe that David is not reliable with respect to P. (For instance, P might be some complicated idea in theoretical physics, and we know that David greatly overestimates his knowledge of theoretical physics.)</p>\n<p>3) Eric's beliefs on a certain topic has a certain <em>pattern</em>. Given what we know of Eric's beliefs and preferences, this pattern is best explained on the hypothesis that he uses some non-rational heuristic (e.g. wishful thinking). Hence we infer that Eric beliefs on this topic are not justified. (E.g. Eric is asked to order different people with respect to friendliness, beauty and intelligence. Eric orders people very similarly on all these criteria - a striking pattern that is best explained, given what we now know of human psychology, by the <a href=\"/lw/lj/the_halo_effect/\">halo effect</a>.)</p>\n<p>(Possibly 3) could be reduced to 2) but the prototypical instances of these categories are sufficiently different to justify listing them as separate.)</p>\n<p>Now I would like to put forward the hypothesis that we underuse the genetic heuristic, possibly to quite a great degree. I'm not completely sure of this, though, which is part of the reason for why I write this post: I'm curious to see what you think. In any case, here is how I'm thinking.</p>\n<p><strong>Direct arguments for the genetic heuristic</strong></p>\n</div>\n<div>\n<p>My first three arguments are direct arguments purporting to show that genetic arguments are extremely useful.</p>\n<p>a) The differences in reliability between different people are vast (<a href=\"/lw/jhy/division_of_cognitive_labour_in_accordance_with/\">as I discuss here</a>; Kaj Sotala gave some <a href=\"/lw/jhy/division_of_cognitive_labour_in_accordance_with/ae8b\">interesting data</a> which backed up my speculations). Not only are the differences between, e.g. Steven Pinker and uneducated people vast, but also, and more interestingly, so are the difference between Steven Pinker and an average academic. If this is true, it makes sense to think that P is more probable conditional on Pinker having said it, compared to if some average academic in his field have said P. But also, and more importantly, it makes sense to <em>read </em>whatever Pinker has written. The main difference between Pinker and the average academic does not concern the probabilities that what they say is true, but in the strikingness of what they are saying. Smart academics say interesting things, and hence it makes sense to read whatever they write, whereas not-so-smart academics generally say dull things. If this is true, then it definitely makes sense to keep a good track of who's reliable and interesting (within a certain area or all-in-all), and who is not.&nbsp;</p>\n<p>b) Psychologists have during the last decades amassed a lot of knowledege of different psychological mechanisms such as the halo effect, the <a href=\"http://en.wikipedia.org/wiki/IKEA_effect\">IKEA effect</a>, the <a href=\"http://en.wikipedia.org/wiki/Just-world_hypothesis\">just world hypothesis</a>, etc. This knowledge was not previously available (even though people did have a hunch of some of them, as pointed out, e.g. by Daniel Kahnemann in <em><a href=\"http://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow\">Thinking Fast and Slow</a></em>). This knowledge gives us a formidable tool for hypothesizing that others' (and, indeed, our own), beliefs are the result of unreliable processes. For instance, there are, I'd say, lots of patterns of beliefs which are suspicious in the same way Eric's are suspicious, and which also are best explained by reference to some non-rational psychological mechanism. (I think a lot of the posts on this site could be seen in these terms - as genetic arguments against certain beliefs or patterns of beliefs, which are based on our knowledge of different psychological mechanisms. I haven't seen anyone phrase this in terms of the genetic heuristic, though.)</p>\n<p>c) As mentioned in the first paragraph, those who only use direct arguments against P disregard some information - i.e. the information that Betty has uttered P. It's a general principle in the philosophy of science and Bayesian reasoning that you should use all the available evidence and not disregard anything unless you have special reasons for doing so. Of course, there might be such reasons, but the burden of proof seems to be on those arguing that we should disregard it.</p>\n<p><strong>Genetic arguments for the genetic heuristic</strong></p>\n<p>My next arguments are genetic arguments (well I should use genetic arguments when arguing for the usefulness of genetic arguments, shouldn't I?) intended to show why we fail to see how useful they are. Now it should be pointed out that I think that we do use them on a massive scale - even though that's too seldom pointed out (and hence it is important to do so). My main point is, however, that we don't do it enough.</p>\n<p>d) There are several psychological mechanisms that block us from seeing the scale of the usefulness of the genetic heuristic. For instance, we have a tendency to \"<a href=\"/lw/k4/do_we_believe_everything_were_told/\">believe everything we read/are told</a>\". Hence it would seem that we do not disregard what poor reasoners (whose statements we shouldn't believe) say to a sufficient degree. Also, there is, <a href=\"/lw/jhy/division_of_cognitive_labour_in_accordance_with/\">as pointed out in my previous post</a>, the <a href=\"http://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect\">Dunning-Kruger effect</a>&nbsp;which says that incompetent people overestimate their level of competence massively, while competent people underestimate their level of competence. This makes the levels of competence to look more similar than they actually are. Also, it is just generally hard to assess reasoning skills, as frequently pointed out here, and in the absence of reliable knowledge people often go for the simple and egalitarian hypothesis that people are roughly equal (I think the Dunning-Kruger effect is partly due to something like this).</p>\n<p>It could be argued that there is at least one other important mechanism that plays in the other direction, namely the <a href=\"http://en.wikipedia.org/wiki/Fundamental_attribution_error\">fundamental attribution error</a> (i.e. we explain others' actions by reference to their character rather than to situational factors). This could lead us to explain poor reasoning by lack of capability, even though the true cause is some situational factor such as fatigue. Now even though you sometimes do see this, my experience is that it is not as common as one would think. It would be interesting to see your take on this.</p>\n<p>Of course people do often classify people who actually are quite reliable and interesting as stupid based on some irrelevant factor, and then use the genetic heuristic to disregard whatever they say. This does not imply that the genetic heuristic is generally useless, though - if you really are good at tracking down reliable and interesting people, it is, in my mind, a wonderful weapon. It does imply that we should be really careful when we classify people, though. Also, it's of course true that if you are absolutely useless at picking out strong reasoners, then you'd better not use the genetic heuristic but have to stick to direct arguments.</p>\n<p>e) Many social institutions are set up in a way which hides the extreme differences in capability between different people (this is also pointed out in my <a href=\"/lw/jhy/division_of_cognitive_labour_in_accordance_with/\">previous post</a>). Professors are paid roughly the same, are given roughly the same speech time in seminars, etc, regardless of their competence. This is partly due to the psychological mechanisms that make us believe people are more cognitively equally than they are, but it also reinforces this idea. How could the differences between different academics be so vast, given that they are treated in roughly the same way by society? We are, as always, impressed by what is immediately visible and have differences understanding that under the surface huge differences in capability are hidden.</p>\n<div>f) Another reason for why these social institutions are set up in this way is egalitarianism: we have a political belief that people should be roughly equally treated, and letting the best professors talk all the time is not compatible with that. This egalitarianism also is, I think, an obstacle to us seeing the vast differences in capability. We engage in wishful thinking to the effect that talent is more equally distributed than it is.&nbsp;</div>\n<div><br /></div>\n<p>g) There are strong social norms against giving <em>ad hominem</em> arguments to someone else's face. These norms are not entirely unjustified:&nbsp;<em>ad hominem </em>arguments do have a tendency to make debates derail into quarrels. In any case, this makes the genetic heuristic invisible, and, again, people tend to go by what they see and hear, so if they don't hear any <em>ad hominem </em>arguments, they'll use them less. I use the genetic heuristic much more often when I <em>think </em>than when I <em>speak</em>&nbsp;and since I suspect that others do likewise, its visibility doesn't match its use nor its usefulness. (More on this below).</p>\n<p>These social norms are also partly due to the history of analytic philosophy. Analytical philosophers were traditionally strongly opposed to <em>ad hominem</em>&nbsp;arguments. This had partly to do with their strong opposition to <a href=\"http://plato.stanford.edu/entries/psychologism/\">\"psychologism\"</a> - a rather vague term which refers to different uses of psychology in philosophy and logic. Genetic arguments typically speculate that this or that belief was due to some non-rational psychological mechanism, and hence it is easy to see how someone who'd like to banish psychology from philosophy (under which argumentation theory was supposed to fall) would be opposed to such arguments.*&nbsp;</p>\n<p>h) Unlike direct arguments, genetic arguments&nbsp;can be seen as \"embarrasing\", in a sense. Starting to question why others, or I myself came to have a certain belief is a rather personal business. (This is of course an important reason why people get upset when someone gives an <em>ad homimen </em>argument against them.) Most people don't want to start question whether they believe in this or that simply because it's in their material interest, for if that turned out to be true, they'd come out as selfish. It seems to me that people who underuse genetic reasoning are generally poor not only at metacognition (thinking about one's own thinking) on a narrow construal -i.e. on thinking of what biases they suffer from - but also are bad at analyzing their own personalities as a whole. If that speculation is true, it incidates that genetic reasoning has an empathic and emotional component that direct reasoning typically lack. I think I've observed many people who are really smart at direct reasoning, but who completely fail at genetic reasoning (e.g. they treat arguments coming from incompetent people as on par with those from competent people). These people tend to lack empathy (i.e. they don't understand other people - or themselves, I would guess).</p>\n<p>i) Another important and related reason for why we underuse <em>ad hominem </em>arguments&nbsp;is, I think, that we wish to avoid negative emotions, and <em>ad hominem</em> reasoning often does give rise to negative feelings (we think we're being judgy). This goes especially for the kind of <em>ad hominem</em> reasoning that classifies people into smart/dumb people <em>in general</em>. Most people have rather egalitarian views and don't like thinking those kinds of thoughts. Indeed when I discuss this idea with people they are visibly uncomfortable with it even though they admit that there is some truth to it. We often avoid <a href=\"/lw/2q6/compartmentalization_in_epistemic_and/\">thinking about ideas that we're not emotionally comfortable with</a>.</p>\n<p>j) Another reason is mostly relevant to the third genetic heuristic and has to do with the fact that many of these patterns might be so complex as to be hard to spot. This is definitely so, but I'm convinced that with training you could be much better at spotting these patterns than most people are today. As stated, <em>ad hominem</em>-arguments aren't held in high regard today, which makes people not so inclined to look for them. In groups where such arguments are seen as important - such as Marxists and Freudians - people come up with intricate <em>ad hominem </em>arguments all the time. True, these are generally invalid, as they postulate psychological mechanisms that simply aren't there, but there's no reason to believe that you couldn't come up with equally complex <em>ad hominem</em>-arguments that track real psychological mechanisms.</p>\n<p><strong>Pragmatic considerations</strong></p>\n<p>It is true, as many have pointed out, that since genetic reasoning are bound to upset, we need to proceed cautiously if we're going to use it against someone we're discussing with. However, there are many situations where the object of our genetic reasoning doesn't know that we're using it, and hence can't get upset. For instance, I'm using it all the time when I'm thinking for myself, and this obviously doesn't upset anyone. Likewise, if I'm discussing someone's views - say Karl Popper's - with a friend and I use genetic arguments against Popper's views, that's unlikely to upset him.</p>\n<p>Also, given the ubiquity of wishful thinking, the halo effect, etc, it seems to me that reasonable people shouldn't get too upset if others hypothesize that they have fallen prey to these biases if the patterns of their beliefs suggest this might be so (such as they do in the case of Eric). Indeed, ideally they should anticipate such hypotheses, or objections, by explicitly showing that the patterns that seem to indicate that they have fallen prey to some bias actually do not do that. At the very least, they should acknowledge that these patterns are bound to raise their discussion partners' suspicoun. I think it would be a great step forward if our debating culture would change so that this would become standard practice.</p>\n<p>In general, it seems to me that we pay too much heed to the arguments given by people who are not actually persuaded by those arguments, but rather have decided what to believe beforehand, and then simply pick whatever arguments support their view (e.g. doctors' arguments for why doctors should be better paid). It is true that such people might sometimes actually come up with good arguments or evidence for their position, but in general their arguments tend to be poor. I certainly often just turn off when I hear that someone is arguing in this way: I have a limited amount of time, and prioritize to listen to people who are genuinely interested in the truth for its own sake.</p>\n<p>Another factor that should be considered is that it is true that genetic reasoning <em>is </em>kind of judgy, elitistic and negative to a certain extent. This is not unproblematic: I consider it important to be generally optimistic and positive, not the least for your own sake. I'm not really sure what to conclude from this, other than that I think genetic reasoning is an indispensable tool in the rationalist's toolbox, and that you thus have to use it frequently even if it would have an emotional cost attached to it.</p>\n<p>In genetic reasoning, you treat what is being said - P - as a \"black box\", more or less: you don't try to analyze P or look at how justified P is directly. Instead, you look at the process of how someone came to believe P. This is obviously especially useful when it's hard or time-consuming to assess P directly, while comparatively easy to assess the reliability of the process that gave rise to the belief in P. I'd say there are many such situations. To take but one example, consider a certain academic discipline - call it \"modernpostism\". We don't know much about the content of modernpostism, since modernpostists use terminology that is hard to penetrate for outsiders. We know, however, how the bigshots of modernpostism tend to behave and think in other areas. On the basis of this, we have inferred that they're intellectually dishonest, prone to all sorts of irrational thinking, and simply not very smart. On the basis of this, we infer that they probably have no justification for what they're saying in their professional life either. (More examples of useful <em>ad hominem </em>arguments are very welcome.)</p>\n</div>\n<div>Psychology is all the time uncovering new data relevant for <em>ad hominem-</em>reasoning - data not only on cognitive biases but also on thought-styles, personality psychology, etc. Indeed, it might even be that brain-scanning could be used for these purposes in the future. In principle it should be possible to do a brainscan on the likes of <a href=\"http://en.wikipedia.org/wiki/Slavoj_%C5%BDi%C5%BEek\">Zizek</a>, <a href=\"http://en.wikipedia.org/wiki/Jacques_Derrida\">Derrida</a> or <a href=\"http://en.wikipedia.org/wiki/Michel_Foucault\">Foucault</a>, observe that there is nothing much going on in the relevant areas of the brain, and conclude that what they say is indeed rubbish. That would be a glorious victory of cold science over empty bullshit indeed...</div>\n<div><br /></div>\n<div>\n<p>I clearly need to learn to write shorter.</p>\n<p>* \"Anti-psychologism\" is a rather absurd position, to my mind. Even though there have of course been misapplications of psychological knowledge in philosophy, a blanket prohibition of the use of psychological knowledge - knowledge of how people typically do reason - in philosophy - which is, at least in part, the study of how we ought to reason - seems to me to be quiet absurd. For an interesting sociological explanation of why this idea became so widespread, see Martin Kusch's&nbsp;<em><a href=\"http://books.google.co.uk/books/about/Psychologism.html?id=roZNtzy3Cf4C&amp;redir_esc=y\">Psychologism: A Case Study in the Sociology of Philosophical Knowledge</a>&nbsp;</em>- in effect a genetic argument against anti-psychologism...</p>\n<p>Another reason was that analytical philosophers revolted against the rather crude genetic arguments often given by Marxists (\"you only say so because you're bourgeois\") and Freudians (\"you only say so because you're sexually repressed\"). Popper's name especially comes to mind here. The problem with their&nbsp;<em>ad hominem&nbsp;</em>arguments was not so much that they were&nbsp;<em>ad hominem</em>, though, but that they were based on flawed theories of how our mind works. We now know much better - the psychological mechanisms discussed here have been validated in countless experiments - and should make use of that knowledge.</p>\n<p>There are also other reasons, such as early analytic philosophy's much too \"individualistic\" picture of human knowledge (a picture which I think comes naturally to us for biological reasons, but which also is an important aspect of Enlightenment thought, starting perhaps with Descartes). They simply underestimated the degree to which we rely on trusting other people in modern society (something discussed, e.g. by Hilary Putnam). I will come back to this theme in a later post but will not go into it further now.</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZQn7vFCpCrti2QGCy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 1.526254426042721e-06, "legacy": true, "legacyId": "25331", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Someone, say Anna, has uttered a certain proposition P, say \"Betty is stupid\", and we want to evaluate whether it is true or not. We can do this by investigating P&nbsp;<em>directly - </em>i.e. we disregard the fact that Anna has said that Betty is stupid, but look only at what we know about Betty's behaviour (and possibly, we try to find out more about it). Alternatively, we can do this&nbsp;<em>indirectly</em>, by evaluating Anna's credibility with respect to P. If we know, for instance, that Anna is in general very reliable, then we are likely to infer that Betty is indeed stupid, but if we know that Anna hates Betty and that she frequently bases her beliefs on emotion, we are not.</p>\n<p>The latter kind of arguments are called <em>ad hominem </em>arguments, or, in&nbsp;<a href=\"/lw/s3/the_genetic_fallacy/lls\">Hal Finney's</a>&nbsp;apt phrase, the genetic heuristic (I'm going to use these terms interchangeably here). They are often criticized, not the least within analytical philosophy, where the traditional view is that they are more often than not fallacious. Certainly the genetic heuristic is often applied in fallacious ways, some of which are pointed out in <a href=\"/lw/s3/the_genetic_fallacy/\">Yudkowsky's article on the topic</a>. Moreover, it seems reasonable to assume that such fallacies would be much more common if they weren't so frequently pointed out by people (accussations of <em>ad hominem </em>fallacies are common in all sorts of debates). No doubt, we are biologically disposed to attack the person rather than what he is saying on irrelevant grounds.</p>\n<div>The genetic heuristic is not always fallacious, though. If a reputable scientist tells us that P is true, where P falls under her domain, then we have reason to believe that P is true. Similarly, if we know that Liza is a compulsive liar, then we have reason to believe that P is false if Liza has said P.</div>\n<div><br></div>\n<div>\n<p>We see that genetic reasoning can be both positive and negative - i.e. it can both be used to confirm, and to disconfirm, P. It should also be noted that negative genetic arguments typically only make sense if we assume that we generally put trust into what other people say - i.e. that we use a genetic argument to the effect that the fact that S having said P makes P more likely to be true. If people don't use such arguments, but only look at P directly to evaluate whether it is true or not, it is unclear what importance arguments that throw doubt on the reliability of S have, since it that case, knowing whether S is reliable or not shouldn't affect our belief in P.</p>\n</div>\n<div><strong>Three kinds of genetic arguments</strong></div>\n<div><br></div>\n<div>We can differentiate between three kinds of genetic arguments (this list is not intended to be exhaustive):</div>\n<div><br></div>\n<div>1) Caren is unreliable. Hence we disregard anything she says (e.g. since Caren is three years old).</div>\n<div><br></div>\n<div>\n<p>2) David says P, and given what we know about P and about David (especially of David's knowledge and attitute to P), we have reason to believe that David is not reliable with respect to P. (For instance, P might be some complicated idea in theoretical physics, and we know that David greatly overestimates his knowledge of theoretical physics.)</p>\n<p>3) Eric's beliefs on a certain topic has a certain <em>pattern</em>. Given what we know of Eric's beliefs and preferences, this pattern is best explained on the hypothesis that he uses some non-rational heuristic (e.g. wishful thinking). Hence we infer that Eric beliefs on this topic are not justified. (E.g. Eric is asked to order different people with respect to friendliness, beauty and intelligence. Eric orders people very similarly on all these criteria - a striking pattern that is best explained, given what we now know of human psychology, by the <a href=\"/lw/lj/the_halo_effect/\">halo effect</a>.)</p>\n<p>(Possibly 3) could be reduced to 2) but the prototypical instances of these categories are sufficiently different to justify listing them as separate.)</p>\n<p>Now I would like to put forward the hypothesis that we underuse the genetic heuristic, possibly to quite a great degree. I'm not completely sure of this, though, which is part of the reason for why I write this post: I'm curious to see what you think. In any case, here is how I'm thinking.</p>\n<p><strong id=\"Direct_arguments_for_the_genetic_heuristic\">Direct arguments for the genetic heuristic</strong></p>\n</div>\n<div>\n<p>My first three arguments are direct arguments purporting to show that genetic arguments are extremely useful.</p>\n<p>a) The differences in reliability between different people are vast (<a href=\"/lw/jhy/division_of_cognitive_labour_in_accordance_with/\">as I discuss here</a>; Kaj Sotala gave some <a href=\"/lw/jhy/division_of_cognitive_labour_in_accordance_with/ae8b\">interesting data</a> which backed up my speculations). Not only are the differences between, e.g. Steven Pinker and uneducated people vast, but also, and more interestingly, so are the difference between Steven Pinker and an average academic. If this is true, it makes sense to think that P is more probable conditional on Pinker having said it, compared to if some average academic in his field have said P. But also, and more importantly, it makes sense to <em>read </em>whatever Pinker has written. The main difference between Pinker and the average academic does not concern the probabilities that what they say is true, but in the strikingness of what they are saying. Smart academics say interesting things, and hence it makes sense to read whatever they write, whereas not-so-smart academics generally say dull things. If this is true, then it definitely makes sense to keep a good track of who's reliable and interesting (within a certain area or all-in-all), and who is not.&nbsp;</p>\n<p>b) Psychologists have during the last decades amassed a lot of knowledege of different psychological mechanisms such as the halo effect, the <a href=\"http://en.wikipedia.org/wiki/IKEA_effect\">IKEA effect</a>, the <a href=\"http://en.wikipedia.org/wiki/Just-world_hypothesis\">just world hypothesis</a>, etc. This knowledge was not previously available (even though people did have a hunch of some of them, as pointed out, e.g. by Daniel Kahnemann in <em><a href=\"http://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow\">Thinking Fast and Slow</a></em>). This knowledge gives us a formidable tool for hypothesizing that others' (and, indeed, our own), beliefs are the result of unreliable processes. For instance, there are, I'd say, lots of patterns of beliefs which are suspicious in the same way Eric's are suspicious, and which also are best explained by reference to some non-rational psychological mechanism. (I think a lot of the posts on this site could be seen in these terms - as genetic arguments against certain beliefs or patterns of beliefs, which are based on our knowledge of different psychological mechanisms. I haven't seen anyone phrase this in terms of the genetic heuristic, though.)</p>\n<p>c) As mentioned in the first paragraph, those who only use direct arguments against P disregard some information - i.e. the information that Betty has uttered P. It's a general principle in the philosophy of science and Bayesian reasoning that you should use all the available evidence and not disregard anything unless you have special reasons for doing so. Of course, there might be such reasons, but the burden of proof seems to be on those arguing that we should disregard it.</p>\n<p><strong id=\"Genetic_arguments_for_the_genetic_heuristic\">Genetic arguments for the genetic heuristic</strong></p>\n<p>My next arguments are genetic arguments (well I should use genetic arguments when arguing for the usefulness of genetic arguments, shouldn't I?) intended to show why we fail to see how useful they are. Now it should be pointed out that I think that we do use them on a massive scale - even though that's too seldom pointed out (and hence it is important to do so). My main point is, however, that we don't do it enough.</p>\n<p>d) There are several psychological mechanisms that block us from seeing the scale of the usefulness of the genetic heuristic. For instance, we have a tendency to \"<a href=\"/lw/k4/do_we_believe_everything_were_told/\">believe everything we read/are told</a>\". Hence it would seem that we do not disregard what poor reasoners (whose statements we shouldn't believe) say to a sufficient degree. Also, there is, <a href=\"/lw/jhy/division_of_cognitive_labour_in_accordance_with/\">as pointed out in my previous post</a>, the <a href=\"http://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect\">Dunning-Kruger effect</a>&nbsp;which says that incompetent people overestimate their level of competence massively, while competent people underestimate their level of competence. This makes the levels of competence to look more similar than they actually are. Also, it is just generally hard to assess reasoning skills, as frequently pointed out here, and in the absence of reliable knowledge people often go for the simple and egalitarian hypothesis that people are roughly equal (I think the Dunning-Kruger effect is partly due to something like this).</p>\n<p>It could be argued that there is at least one other important mechanism that plays in the other direction, namely the <a href=\"http://en.wikipedia.org/wiki/Fundamental_attribution_error\">fundamental attribution error</a> (i.e. we explain others' actions by reference to their character rather than to situational factors). This could lead us to explain poor reasoning by lack of capability, even though the true cause is some situational factor such as fatigue. Now even though you sometimes do see this, my experience is that it is not as common as one would think. It would be interesting to see your take on this.</p>\n<p>Of course people do often classify people who actually are quite reliable and interesting as stupid based on some irrelevant factor, and then use the genetic heuristic to disregard whatever they say. This does not imply that the genetic heuristic is generally useless, though - if you really are good at tracking down reliable and interesting people, it is, in my mind, a wonderful weapon. It does imply that we should be really careful when we classify people, though. Also, it's of course true that if you are absolutely useless at picking out strong reasoners, then you'd better not use the genetic heuristic but have to stick to direct arguments.</p>\n<p>e) Many social institutions are set up in a way which hides the extreme differences in capability between different people (this is also pointed out in my <a href=\"/lw/jhy/division_of_cognitive_labour_in_accordance_with/\">previous post</a>). Professors are paid roughly the same, are given roughly the same speech time in seminars, etc, regardless of their competence. This is partly due to the psychological mechanisms that make us believe people are more cognitively equally than they are, but it also reinforces this idea. How could the differences between different academics be so vast, given that they are treated in roughly the same way by society? We are, as always, impressed by what is immediately visible and have differences understanding that under the surface huge differences in capability are hidden.</p>\n<div>f) Another reason for why these social institutions are set up in this way is egalitarianism: we have a political belief that people should be roughly equally treated, and letting the best professors talk all the time is not compatible with that. This egalitarianism also is, I think, an obstacle to us seeing the vast differences in capability. We engage in wishful thinking to the effect that talent is more equally distributed than it is.&nbsp;</div>\n<div><br></div>\n<p>g) There are strong social norms against giving <em>ad hominem</em> arguments to someone else's face. These norms are not entirely unjustified:&nbsp;<em>ad hominem </em>arguments do have a tendency to make debates derail into quarrels. In any case, this makes the genetic heuristic invisible, and, again, people tend to go by what they see and hear, so if they don't hear any <em>ad hominem </em>arguments, they'll use them less. I use the genetic heuristic much more often when I <em>think </em>than when I <em>speak</em>&nbsp;and since I suspect that others do likewise, its visibility doesn't match its use nor its usefulness. (More on this below).</p>\n<p>These social norms are also partly due to the history of analytic philosophy. Analytical philosophers were traditionally strongly opposed to <em>ad hominem</em>&nbsp;arguments. This had partly to do with their strong opposition to <a href=\"http://plato.stanford.edu/entries/psychologism/\">\"psychologism\"</a> - a rather vague term which refers to different uses of psychology in philosophy and logic. Genetic arguments typically speculate that this or that belief was due to some non-rational psychological mechanism, and hence it is easy to see how someone who'd like to banish psychology from philosophy (under which argumentation theory was supposed to fall) would be opposed to such arguments.*&nbsp;</p>\n<p>h) Unlike direct arguments, genetic arguments&nbsp;can be seen as \"embarrasing\", in a sense. Starting to question why others, or I myself came to have a certain belief is a rather personal business. (This is of course an important reason why people get upset when someone gives an <em>ad homimen </em>argument against them.) Most people don't want to start question whether they believe in this or that simply because it's in their material interest, for if that turned out to be true, they'd come out as selfish. It seems to me that people who underuse genetic reasoning are generally poor not only at metacognition (thinking about one's own thinking) on a narrow construal -i.e. on thinking of what biases they suffer from - but also are bad at analyzing their own personalities as a whole. If that speculation is true, it incidates that genetic reasoning has an empathic and emotional component that direct reasoning typically lack. I think I've observed many people who are really smart at direct reasoning, but who completely fail at genetic reasoning (e.g. they treat arguments coming from incompetent people as on par with those from competent people). These people tend to lack empathy (i.e. they don't understand other people - or themselves, I would guess).</p>\n<p>i) Another important and related reason for why we underuse <em>ad hominem </em>arguments&nbsp;is, I think, that we wish to avoid negative emotions, and <em>ad hominem</em> reasoning often does give rise to negative feelings (we think we're being judgy). This goes especially for the kind of <em>ad hominem</em> reasoning that classifies people into smart/dumb people <em>in general</em>. Most people have rather egalitarian views and don't like thinking those kinds of thoughts. Indeed when I discuss this idea with people they are visibly uncomfortable with it even though they admit that there is some truth to it. We often avoid <a href=\"/lw/2q6/compartmentalization_in_epistemic_and/\">thinking about ideas that we're not emotionally comfortable with</a>.</p>\n<p>j) Another reason is mostly relevant to the third genetic heuristic and has to do with the fact that many of these patterns might be so complex as to be hard to spot. This is definitely so, but I'm convinced that with training you could be much better at spotting these patterns than most people are today. As stated, <em>ad hominem</em>-arguments aren't held in high regard today, which makes people not so inclined to look for them. In groups where such arguments are seen as important - such as Marxists and Freudians - people come up with intricate <em>ad hominem </em>arguments all the time. True, these are generally invalid, as they postulate psychological mechanisms that simply aren't there, but there's no reason to believe that you couldn't come up with equally complex <em>ad hominem</em>-arguments that track real psychological mechanisms.</p>\n<p><strong id=\"Pragmatic_considerations\">Pragmatic considerations</strong></p>\n<p>It is true, as many have pointed out, that since genetic reasoning are bound to upset, we need to proceed cautiously if we're going to use it against someone we're discussing with. However, there are many situations where the object of our genetic reasoning doesn't know that we're using it, and hence can't get upset. For instance, I'm using it all the time when I'm thinking for myself, and this obviously doesn't upset anyone. Likewise, if I'm discussing someone's views - say Karl Popper's - with a friend and I use genetic arguments against Popper's views, that's unlikely to upset him.</p>\n<p>Also, given the ubiquity of wishful thinking, the halo effect, etc, it seems to me that reasonable people shouldn't get too upset if others hypothesize that they have fallen prey to these biases if the patterns of their beliefs suggest this might be so (such as they do in the case of Eric). Indeed, ideally they should anticipate such hypotheses, or objections, by explicitly showing that the patterns that seem to indicate that they have fallen prey to some bias actually do not do that. At the very least, they should acknowledge that these patterns are bound to raise their discussion partners' suspicoun. I think it would be a great step forward if our debating culture would change so that this would become standard practice.</p>\n<p>In general, it seems to me that we pay too much heed to the arguments given by people who are not actually persuaded by those arguments, but rather have decided what to believe beforehand, and then simply pick whatever arguments support their view (e.g. doctors' arguments for why doctors should be better paid). It is true that such people might sometimes actually come up with good arguments or evidence for their position, but in general their arguments tend to be poor. I certainly often just turn off when I hear that someone is arguing in this way: I have a limited amount of time, and prioritize to listen to people who are genuinely interested in the truth for its own sake.</p>\n<p>Another factor that should be considered is that it is true that genetic reasoning <em>is </em>kind of judgy, elitistic and negative to a certain extent. This is not unproblematic: I consider it important to be generally optimistic and positive, not the least for your own sake. I'm not really sure what to conclude from this, other than that I think genetic reasoning is an indispensable tool in the rationalist's toolbox, and that you thus have to use it frequently even if it would have an emotional cost attached to it.</p>\n<p>In genetic reasoning, you treat what is being said - P - as a \"black box\", more or less: you don't try to analyze P or look at how justified P is directly. Instead, you look at the process of how someone came to believe P. This is obviously especially useful when it's hard or time-consuming to assess P directly, while comparatively easy to assess the reliability of the process that gave rise to the belief in P. I'd say there are many such situations. To take but one example, consider a certain academic discipline - call it \"modernpostism\". We don't know much about the content of modernpostism, since modernpostists use terminology that is hard to penetrate for outsiders. We know, however, how the bigshots of modernpostism tend to behave and think in other areas. On the basis of this, we have inferred that they're intellectually dishonest, prone to all sorts of irrational thinking, and simply not very smart. On the basis of this, we infer that they probably have no justification for what they're saying in their professional life either. (More examples of useful <em>ad hominem </em>arguments are very welcome.)</p>\n</div>\n<div>Psychology is all the time uncovering new data relevant for <em>ad hominem-</em>reasoning - data not only on cognitive biases but also on thought-styles, personality psychology, etc. Indeed, it might even be that brain-scanning could be used for these purposes in the future. In principle it should be possible to do a brainscan on the likes of <a href=\"http://en.wikipedia.org/wiki/Slavoj_%C5%BDi%C5%BEek\">Zizek</a>, <a href=\"http://en.wikipedia.org/wiki/Jacques_Derrida\">Derrida</a> or <a href=\"http://en.wikipedia.org/wiki/Michel_Foucault\">Foucault</a>, observe that there is nothing much going on in the relevant areas of the brain, and conclude that what they say is indeed rubbish. That would be a glorious victory of cold science over empty bullshit indeed...</div>\n<div><br></div>\n<div>\n<p>I clearly need to learn to write shorter.</p>\n<p>* \"Anti-psychologism\" is a rather absurd position, to my mind. Even though there have of course been misapplications of psychological knowledge in philosophy, a blanket prohibition of the use of psychological knowledge - knowledge of how people typically do reason - in philosophy - which is, at least in part, the study of how we ought to reason - seems to me to be quiet absurd. For an interesting sociological explanation of why this idea became so widespread, see Martin Kusch's&nbsp;<em><a href=\"http://books.google.co.uk/books/about/Psychologism.html?id=roZNtzy3Cf4C&amp;redir_esc=y\">Psychologism: A Case Study in the Sociology of Philosophical Knowledge</a>&nbsp;</em>- in effect a genetic argument against anti-psychologism...</p>\n<p>Another reason was that analytical philosophers revolted against the rather crude genetic arguments often given by Marxists (\"you only say so because you're bourgeois\") and Freudians (\"you only say so because you're sexually repressed\"). Popper's name especially comes to mind here. The problem with their&nbsp;<em>ad hominem&nbsp;</em>arguments was not so much that they were&nbsp;<em>ad hominem</em>, though, but that they were based on flawed theories of how our mind works. We now know much better - the psychological mechanisms discussed here have been validated in countless experiments - and should make use of that knowledge.</p>\n<p>There are also other reasons, such as early analytic philosophy's much too \"individualistic\" picture of human knowledge (a picture which I think comes naturally to us for biological reasons, but which also is an important aspect of Enlightenment thought, starting perhaps with Descartes). They simply underestimated the degree to which we rely on trusting other people in modern society (something discussed, e.g. by Hilary Putnam). I will come back to this theme in a later post but will not go into it further now.</p>\n</div>", "sections": [{"title": "Direct arguments for the genetic heuristic", "anchor": "Direct_arguments_for_the_genetic_heuristic", "level": 1}, {"title": "Genetic arguments for the genetic heuristic", "anchor": "Genetic_arguments_for_the_genetic_heuristic", "level": 1}, {"title": "Pragmatic considerations", "anchor": "Pragmatic_considerations", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KZLa74SzyKhSJ3M55", "ACGeaAk6KButv2xwQ", "9FWWDoMWA7nbFmh6g", "TiDGXt3WrQwtCdDj3", "N99KgncSXewWqkzMA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-22T18:16:38.107Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley: Weekly goals", "slug": "meetup-berkeley-weekly-goals", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3Ej2v66Ttwr8XAdjN/meetup-berkeley-weekly-goals", "pageUrlRelative": "/posts/3Ej2v66Ttwr8XAdjN/meetup-berkeley-weekly-goals", "linkUrl": "https://www.lesswrong.com/posts/3Ej2v66Ttwr8XAdjN/meetup-berkeley-weekly-goals", "postedAtFormatted": "Wednesday, January 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%3A%20Weekly%20goals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%3A%20Weekly%20goals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3Ej2v66Ttwr8XAdjN%2Fmeetup-berkeley-weekly-goals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%3A%20Weekly%20goals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3Ej2v66Ttwr8XAdjN%2Fmeetup-berkeley-weekly-goals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3Ej2v66Ttwr8XAdjN%2Fmeetup-berkeley-weekly-goals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/w1'>Berkeley: Weekly goals</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 January 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2030 Addison, 3rd Floor, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello all, at today's meetup I'd like to start experimenting with an accountability practice other meetup groups have tried: Keeping a list of personal goals for the next week. If you're up for it, you'll describe your goal and how you plan to achieve it, and we'll enter it into the list this week. Next week, we'll report whether we achieved our goals, what worked, and what didn't.</p>\n\n<p>Please arrive between 7pm and 7:30pm today. The exercise will begin at 7:30pm. It won't take very long, and we will hang out afterward. The CFAR office is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p>\n\n<p>Even though this takes place at CFAR, it's not a CFAR-sponsored event.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/w1'>Berkeley: Weekly goals</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3Ej2v66Ttwr8XAdjN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.5262979261397145e-06, "legacy": true, "legacyId": "25335", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Weekly_goals\">Discussion article for the meetup : <a href=\"/meetups/w1\">Berkeley: Weekly goals</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 January 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2030 Addison, 3rd Floor, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello all, at today's meetup I'd like to start experimenting with an accountability practice other meetup groups have tried: Keeping a list of personal goals for the next week. If you're up for it, you'll describe your goal and how you plan to achieve it, and we'll enter it into the list this week. Next week, we'll report whether we achieved our goals, what worked, and what didn't.</p>\n\n<p>Please arrive between 7pm and 7:30pm today. The exercise will begin at 7:30pm. It won't take very long, and we will hang out afterward. The CFAR office is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p>\n\n<p>Even though this takes place at CFAR, it's not a CFAR-sponsored event.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Weekly_goals1\">Discussion article for the meetup : <a href=\"/meetups/w1\">Berkeley: Weekly goals</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley: Weekly goals", "anchor": "Discussion_article_for_the_meetup___Berkeley__Weekly_goals", "level": 1}, {"title": "Discussion article for the meetup : Berkeley: Weekly goals", "anchor": "Discussion_article_for_the_meetup___Berkeley__Weekly_goals1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-22T19:05:34.704Z", "modifiedAt": null, "url": null, "title": "Meetup : Frankfurt meetup", "slug": "meetup-frankfurt-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kendra", "createdAt": "2012-02-29T23:10:44.583Z", "isAdmin": false, "displayName": "Kendra"}, "userId": "BPB6kHkfZwFLrhcbG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3WmgdXwhswwADFaGQ/meetup-frankfurt-meetup", "pageUrlRelative": "/posts/3WmgdXwhswwADFaGQ/meetup-frankfurt-meetup", "linkUrl": "https://www.lesswrong.com/posts/3WmgdXwhswwADFaGQ/meetup-frankfurt-meetup", "postedAtFormatted": "Wednesday, January 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Frankfurt%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Frankfurt%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3WmgdXwhswwADFaGQ%2Fmeetup-frankfurt-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Frankfurt%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3WmgdXwhswwADFaGQ%2Fmeetup-frankfurt-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3WmgdXwhswwADFaGQ%2Fmeetup-frankfurt-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/w2'>Frankfurt meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 February 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Frankfurt</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting again! If you have any special requirements to attend or need some help, please try me in advance, we'll try to accommodate your needs.</p>\n\n<p>Location will be published in time.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/w2'>Frankfurt meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3WmgdXwhswwADFaGQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.5263522529789618e-06, "legacy": true, "legacyId": "25336", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Frankfurt_meetup\">Discussion article for the meetup : <a href=\"/meetups/w2\">Frankfurt meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 February 2014 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Frankfurt</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are meeting again! If you have any special requirements to attend or need some help, please try me in advance, we'll try to accommodate your needs.</p>\n\n<p>Location will be published in time.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Frankfurt_meetup1\">Discussion article for the meetup : <a href=\"/meetups/w2\">Frankfurt meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Frankfurt meetup", "anchor": "Discussion_article_for_the_meetup___Frankfurt_meetup", "level": 1}, {"title": "Discussion article for the meetup : Frankfurt meetup", "anchor": "Discussion_article_for_the_meetup___Frankfurt_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-22T21:14:58.174Z", "modifiedAt": null, "url": null, "title": "Polling Thread", "slug": "polling-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:38.459Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sqeXx4dPiRixdRxdb/polling-thread", "pageUrlRelative": "/posts/sqeXx4dPiRixdRxdb/polling-thread", "linkUrl": "https://www.lesswrong.com/posts/sqeXx4dPiRixdRxdb/polling-thread", "postedAtFormatted": "Wednesday, January 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Polling%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolling%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsqeXx4dPiRixdRxdb%2Fpolling-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Polling%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsqeXx4dPiRixdRxdb%2Fpolling-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsqeXx4dPiRixdRxdb%2Fpolling-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 250, "htmlBody": "<p>This is an experiment to use polls to tap into the crowd knowledge probably present on LW.</p>\n<p>This is your chance to ask your multiple choice question you always wanted to throw in. Get qualified numeric feedback to your comments. Post fun polls.</p>\n<p>There are some rules:</p>\n<ol>\n<li>Each poll goes into its own top level comment and may be commented there.</li>\n<li>You must at least vote all polls that were posted earlier than you own. This ensures participation in all polls and also limits the total number of polls. You may of course vote without posting a poll.</li>\n<li>Your poll should include a 'don't know' option (to avoid conflict with 2). I don't know whether we need to add a troll catch option here but we will see.</li>\n</ol>\n<p>If you don't know how to make a poll in a comment look at the <a href=\"http://wiki.lesswrong.com/wiki/Comment_formatting#Polls\">Poll Markup Help</a>.</p>\n<hr />\n<p>This being an experiment I do not announce it to be regular. If it is successful I may. Or you may. In that case I recommend the following to make this potentially more usable:</p>\n<ul>\n<li>Use \"Polling Thread\" in the title.</li>\n<li>Copy the rules.</li>\n<li>Add the tag \"poll\".</li>\n<li>Link to this Thread or a previous Thread.</li>\n<li>Create a top-level comment saying 'Discussion of this thread goes here; all other to-level comments should be polls or similar'</li>\n<li>Add a second top-level comment with an initial poll to start participation.</li>\n</ul>\n<p>EDIT: Added recommendations from KnaveOfAllTrades.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sqeXx4dPiRixdRxdb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 1.5264958931222573e-06, "legacy": true, "legacyId": "25338", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 120, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-22T23:34:05.971Z", "modifiedAt": null, "url": null, "title": "An additional problem with Solomonoff induction", "slug": "an-additional-problem-with-solomonoff-induction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:09.695Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gedymin", "createdAt": "2011-02-19T16:22:18.343Z", "isAdmin": false, "displayName": "gedymin"}, "userId": "xPg56rQ4KieN3rPtQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YTZRhY3nBj8p3ojTP/an-additional-problem-with-solomonoff-induction", "pageUrlRelative": "/posts/YTZRhY3nBj8p3ojTP/an-additional-problem-with-solomonoff-induction", "linkUrl": "https://www.lesswrong.com/posts/YTZRhY3nBj8p3ojTP/an-additional-problem-with-solomonoff-induction", "postedAtFormatted": "Wednesday, January 22nd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20additional%20problem%20with%20Solomonoff%20induction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20additional%20problem%20with%20Solomonoff%20induction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTZRhY3nBj8p3ojTP%2Fan-additional-problem-with-solomonoff-induction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20additional%20problem%20with%20Solomonoff%20induction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTZRhY3nBj8p3ojTP%2Fan-additional-problem-with-solomonoff-induction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTZRhY3nBj8p3ojTP%2Fan-additional-problem-with-solomonoff-induction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1079, "htmlBody": "<p><!-- P { margin-bottom: 0.21cm; } -->Let's continue from my <a href=\"/r/discussion/lw/jhm/understanding_and_justifying_solomonoff_induction/\">previous post</a> and look how Solomonoff induction fails to adequately deal with hypercomputation.</p>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">You may have heard of the Physical Church-Turing thesis. It's the idea that the Universe can, in a perfect level of detail, be simulated on a Turing machine. (No problem if the Universe turns out to be infinite - the thesis requires only that each finite portion of it can be simulated.) A corollary to the Physical CTT is the idea that there are no physically realizable uncomputable processes. We can talk about hypercomputers as mathematical abstractions, but we'll never be able to build (or see) one.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">We don't have a very strong scientific evidence for the Physical CTT thesis yet - no one has built the perfect simulator yet, for example. On the other hand, we do have something - all known laws of physics (including quantum physics) allow arbitrary precision simulations on a computer. Even though the complete unified theory of all fundamental forces isn't there yet, the Standard model already makes pretty accurate predictions for almost all environments. (Singularities being the only known exception, as their neighborhoods are the only locations where the role of quantum gravity is not negligible.)</p>\n<p>&nbsp;</p>\n<p>So the Physical CTT does not contradict any known laws of physics. Of course, these laws are not contradicted by a multitude of alternative hypotheses as well; all the hypotheses which suggest that the universe <em>cannot</em> be simulated on a Turing machine. We prefer the Physical CTT solely because it's the simplest one; because Occam's razor says so.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">There are multiple levels and kinds of uncomputability; none of the problems placed on either of these levels are uncomputable in the absolute sense; all of them can computed by <em>some</em> hypothetical devices. And all of these devices are called hypercomputers. A corollary to the \"Universe is uncomputable\" position is that we someday may be able to, in fact, build a hypercomputer that is embedded in the physical world, or at least access some of the Nature's mystical, uncomputable processes as a black box.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">Now, the \"standard\" Solomonoff induction uses the Universal prior, which in turn is related to Kolmogorov complexity (KC). An uncomputable process formally has undefined Kolmogorov complexity. Informally, the KC of such as process is infinitely large, as it must be larger than the KC of any <em>computable</em> process.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">As discussed in the comments to the previous post, Solomonoff induction is by no means restricted to the Universal prior; it can use other priors, including a prior (i.e. probability distribution) defined over an universal hypercomputer. An example of such a hypercomputer is the combination Universal Turing machine + Halting problem oracle. Another example is the combination Universal Turing machine + a true random number oracle. An upgraded form of Solomonoff induction which uses a prior defined over the first kind of universal hypercomputer is going treat the halting problem as a very simple, computable process. An upgraded form of Solomonoff induction over the second kind of universal hypercomputer is going to treat random number generation as a very simple, computable process. And so on.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">Now here's the problem. Mathematically, a Universal Turing machine equipped with the Halting oracle, and a Universal Turing machine equipped with a Random Number oracle are in the same class: they are both universal hypercomputers. Physically and practically, they are miles away from each another.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">A Random Number oracle is just that: something that gives you random numbers. Their statistical properties won't even be particularly better than the properties a good pseudorandom number generator. They simply are, in a sense, \"true\"; therefore uncomputable. However, quantum physics suggests that Random Number oracles, in fact, might be <em>real</em>; i.e. that there are sources of true randomness in the Universe. This is QM interpretation-dependent, of course, but any deterministic, non-random interpretation of quantum mechanics involves things like faster-than light interaction etc., which frankly are much less intuitive.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">A Halting oracle, in contrast, can solve infinite number of hugely important problems. It's magic. In my beliefs, the <em>a priori </em>probability that Universe contains some sort of Halting oracle is tiny. Only a huge amount of proper scientific tests could convince me to change this conclusion.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">On the other hand, <em>mathematically</em> the two hypercomputers are siblings. Both can be approximated by a Turing machine. Both can even be computed by a deterministic algorithm (I think?), if the Turing machine that does the computation is allowed to work <em>forever</em>.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">There <em>is </em>one significant mathematical difference between the two oracles. (Nevertheless, Solomonoff induction fails to take into account this difference.) The Halting oracle has a power on its own; it can be used to solve problems even when it comes without the accompanying Turing machine. The Random Number oracle cannot be used for anything but random number generation. (To solve any computable decision problem P with the Halting oracle, we can reformulate it as program source code:<em> \"if P, then halt; otherwise: loop forever\"</em> and feed this program to the Halting oracle. In this way the Halting oracle can be used to tell that 3&lt;7 is true - the program halts -, while 10&lt;5 is false - it loops forever.)</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">The Solomonoff induction can be fixed, if we assume that the input tape of the Universal prior's Turing machine contains <em>infinite</em> number of random bits. However, this idea needs an explicit justification, and its implications are not at all obvious. Does this mean that Occam's razor should be \"prefer the simplest hypothesis, together with an infinite source of random numbers\", instead of \"prefer the simplest hypothesis\"?</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">So to sum up, the problem is:</p>\n<ul>\n<li>Intuitively, the probability that we are living in a Universe that includes True Random numbers is much larger than the probability that we are living in a Universe that allows Halting problem oracles;</li>\n<li>Solomonoff induction cannot reliably distinguish between these two cases.</li>\n</ul>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">The consequences? When you hear someone claiming - again - that \"computers are not capable of true creativity/consciousness/whatever, because creativity/consciousness/whatever requires human intuition, which is uncomputable, etc.\" - remember that it might be a bad idea to respond with an appeal to Solomonoff induction.</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">Interestingly, quite a few people praise their intuition and view it as almost a mystical power, but no one is surprised by their ability to name a few random numbers :)</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm\">&nbsp;</p>\n<hr />\n<p>A related question: how does finite, bounded universe fit into this? Does it make sense to use the Universal Turing machine as a generator for Kolmogorov complexity, when the actual model of computation required to simulate the universe is much simpler?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bTeiZr6YAEaSPQTC8": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YTZRhY3nBj8p3ojTP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 1, "extendedScore": null, "score": 1.5266503712222062e-06, "legacy": true, "legacyId": "25339", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BWNh2GctNzA26TqQx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-23T17:42:51.726Z", "modifiedAt": null, "url": null, "title": "Plan while your ugh field is down", "slug": "plan-while-your-ugh-field-is-down", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:07.128Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AX5zvmj6EXa2Fmn9f/plan-while-your-ugh-field-is-down", "pageUrlRelative": "/posts/AX5zvmj6EXa2Fmn9f/plan-while-your-ugh-field-is-down", "linkUrl": "https://www.lesswrong.com/posts/AX5zvmj6EXa2Fmn9f/plan-while-your-ugh-field-is-down", "postedAtFormatted": "Thursday, January 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Plan%20while%20your%20ugh%20field%20is%20down&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlan%20while%20your%20ugh%20field%20is%20down%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAX5zvmj6EXa2Fmn9f%2Fplan-while-your-ugh-field-is-down%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Plan%20while%20your%20ugh%20field%20is%20down%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAX5zvmj6EXa2Fmn9f%2Fplan-while-your-ugh-field-is-down", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAX5zvmj6EXa2Fmn9f%2Fplan-while-your-ugh-field-is-down", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 235, "htmlBody": "<p>Here's an example of a mental manoeuvre I accidentally found, and thought might be generally useful (<a href=\"http://wiki.lesswrong.com/wiki/Typical_mind_fallacy\">typical</a> <a href=\"/lw/dr/generalizing_from_one_example/\">caveats</a> apply).</p>\n<p>I've had a manageable-but-important <a href=\"/lw/1ai/the_first_step_is_to_admit_that_you_have_a_problem/\">Problem</a> for a few months now (financial in kind, details neither relevant nor interesting), of moderate complexity and relatively minor importance <em>unless I leave it&nbsp;unsolved just a little longer</em>.</p>\n<p>Unfortunately, this seems to be the precise combination of things that triggers one of my <a href=\"http://wiki.lesswrong.com/wiki/Ugh_field\">ugh fields</a>, which manifests subjectively as a fuzzy blank inability to maintain focus. Several times last week, it occurred to me that I should really Solve The Problem, but I wasn't able to get myself to spend any time thinking about it. Like, at all.</p>\n<p>On Saturday, the Problem found itself top of mind once again. How irritating that I couldn't solve the Problem because it was the weekend, and when it wasn't the weekend, maybe Tuesday when work wasn't busy and the Bureau was open, I should really email Dr. Somebody and call Mrs. Administrator for the ...</p>\n<p>*blink*</p>\n<p>I had a solution, and a plan. What the what?</p>\n<p>My working theory is that when there's no chance of actually Doing Something, this particular ugh field deactivates.&nbsp;</p>\n<p>To me, this suggests a strategy (of uncertain generalizability): when an ugh field is preventing thought about something important, find a time when action is impossible and use it to generate a plan.</p>\n<p>I would feel better about this advice if it had a <a href=\"/lw/d4/practical_advice_backed_by_deep_theories/\">deep theoretical backer</a>. Anybody?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"EuDw6uxQW2ZBRFhMo": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AX5zvmj6EXa2Fmn9f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 33, "extendedScore": null, "score": 1.527860196694777e-06, "legacy": true, "legacyId": "25342", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ", "Pmfk7ruhWaHj9diyv", "LqjKP255fPRY7aMzw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-23T18:23:12.555Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Another Meet up", "slug": "meetup-moscow-another-meet-up", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a3KwxnLpurXAQc9FN/meetup-moscow-another-meet-up", "pageUrlRelative": "/posts/a3KwxnLpurXAQc9FN/meetup-moscow-another-meet-up", "linkUrl": "https://www.lesswrong.com/posts/a3KwxnLpurXAQc9FN/meetup-moscow-another-meet-up", "postedAtFormatted": "Thursday, January 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Another%20Meet%20up&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Another%20Meet%20up%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3KwxnLpurXAQc9FN%2Fmeetup-moscow-another-meet-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Another%20Meet%20up%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3KwxnLpurXAQc9FN%2Fmeetup-moscow-another-meet-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3KwxnLpurXAQc9FN%2Fmeetup-moscow-another-meet-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/w3'>Moscow, Another Meet up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 January 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at the same second entrance, but we will go to the new room at 16:00. So please do not be late.</p>\n\n<p>Main topics and activities:</p>\n\n<ul>\n<li><p>Confounding, short presentation</p></li>\n<li><p>Discussions</p></li>\n</ul>\n\n<p><strong>If you are going for the first time:</strong></p>\n\n<p>We gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the second entrance and then move to the new room.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/w3'>Moscow, Another Meet up</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a3KwxnLpurXAQc9FN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5279050627463012e-06, "legacy": true, "legacyId": "25343", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Another_Meet_up\">Discussion article for the meetup : <a href=\"/meetups/w3\">Moscow, Another Meet up</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 January 2014 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will gather at the same second entrance, but we will go to the new room at 16:00. So please do not be late.</p>\n\n<p>Main topics and activities:</p>\n\n<ul>\n<li><p>Confounding, short presentation</p></li>\n<li><p>Discussions</p></li>\n</ul>\n\n<p><strong id=\"If_you_are_going_for_the_first_time_\">If you are going for the first time:</strong></p>\n\n<p>We gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/lCz9ZSG.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.\nYou can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night. Please pay attention that we only gather near the second entrance and then move to the new room.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Another_Meet_up1\">Discussion article for the meetup : <a href=\"/meetups/w3\">Moscow, Another Meet up</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Another Meet up", "anchor": "Discussion_article_for_the_meetup___Moscow__Another_Meet_up", "level": 1}, {"title": "If you are going for the first time:", "anchor": "If_you_are_going_for_the_first_time_", "level": 2}, {"title": "Discussion article for the meetup : Moscow, Another Meet up", "anchor": "Discussion_article_for_the_meetup___Moscow__Another_Meet_up1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-23T18:58:09.717Z", "modifiedAt": null, "url": null, "title": "[Link] YCombinator-Backed Non-Profit Startup Attempting HIV/AIDS Vaccine", "slug": "link-ycombinator-backed-non-profit-startup-attempting-hiv", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.252Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gavin", "createdAt": "2009-02-27T05:00:57.191Z", "isAdmin": false, "displayName": "Gavin"}, "userId": "9gMQSKRMgpPFYTNFY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jgFif26fQAvxuhodY/link-ycombinator-backed-non-profit-startup-attempting-hiv", "pageUrlRelative": "/posts/jgFif26fQAvxuhodY/link-ycombinator-backed-non-profit-startup-attempting-hiv", "linkUrl": "https://www.lesswrong.com/posts/jgFif26fQAvxuhodY/link-ycombinator-backed-non-profit-startup-attempting-hiv", "postedAtFormatted": "Thursday, January 23rd 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20YCombinator-Backed%20Non-Profit%20Startup%20Attempting%20HIV%2FAIDS%20Vaccine&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20YCombinator-Backed%20Non-Profit%20Startup%20Attempting%20HIV%2FAIDS%20Vaccine%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjgFif26fQAvxuhodY%2Flink-ycombinator-backed-non-profit-startup-attempting-hiv%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20YCombinator-Backed%20Non-Profit%20Startup%20Attempting%20HIV%2FAIDS%20Vaccine%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjgFif26fQAvxuhodY%2Flink-ycombinator-backed-non-profit-startup-attempting-hiv", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjgFif26fQAvxuhodY%2Flink-ycombinator-backed-non-profit-startup-attempting-hiv", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<p>The short story is that animal trials of a vaccine have been completed and they're looking for funding to go further.</p>\n<p>https://pledge.immunityproject.org/the-free-hiv-aids-vaccine</p>\n<p>Discussion on Hacker News here:&nbsp;https://news.ycombinator.com/item?id=7108684</p>\n<p>&nbsp;</p>\n<p>A few personal notes:</p>\n<p>My first thought on seeing this was \"I want to donate, but only if it's actually going to pan out. Otherwise it'll be a waste of my money.\" But that's exactly the wrong way to think about science. You fund it because it has a worthwhile probability of success. Even if the attempt fails, I can still believe that I've made a \"correct\" decision.</p>\n<p>I've basically outsourced my vetting of the worthiness of the endeavor to Paul Graham, Sam Altman, and the rest of the YCombinator team. They have the time and the resources to examine and consider this more thoroughly than I could possibly do.</p>\n<p>Another reason that I donated to this was to encourage the general idea of crowdfunded science.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jgFif26fQAvxuhodY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 1.5279439320300033e-06, "legacy": true, "legacyId": "25344", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-24T00:02:31.864Z", "modifiedAt": null, "url": null, "title": "Meetup : Sydney Meetup: February", "slug": "meetup-sydney-meetup-february-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:00.321Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taryneast", "createdAt": "2010-11-29T20:51:06.328Z", "isAdmin": false, "displayName": "taryneast"}, "userId": "xD8wjhiTvwbXdKirW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ahC4Qa2jjjk94gkod/meetup-sydney-meetup-february-0", "pageUrlRelative": "/posts/ahC4Qa2jjjk94gkod/meetup-sydney-meetup-february-0", "linkUrl": "https://www.lesswrong.com/posts/ahC4Qa2jjjk94gkod/meetup-sydney-meetup-february-0", "postedAtFormatted": "Friday, January 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sydney%20Meetup%3A%20February&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sydney%20Meetup%3A%20February%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FahC4Qa2jjjk94gkod%2Fmeetup-sydney-meetup-february-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sydney%20Meetup%3A%20February%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FahC4Qa2jjjk94gkod%2Fmeetup-sydney-meetup-february-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FahC4Qa2jjjk94gkod%2Fmeetup-sydney-meetup-february-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 205, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/w4'>Sydney Meetup: February</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 February 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">City of Sydney RSL, 565 George St, Sydney NSW</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our last meetup was a great success, so we're doing it all again :)</p>\n\n<p>Please note: there are many levels of the RSL club.</p>\n\n<p>I've booked an ordinary long table under the name \"less wrong\" for 15 starting from 6:30 (which is when I should be able to get there). If we want to retire somewhere else afterwards, we can do that later.</p>\n\n<p>So we will be on whichever level the restaurant is located (but not level 4) under the booking \"less wrong\".</p>\n\n<p>I will verify the location within the next couple of weeks and update this meetup notification.</p>\n\n<p>I'll put up a sheet of paper with \"LessWrong\" and a picture of a paperclip so you can find us. I'll also tell the staff - so ask them to point out the \"less wrong meetup\" if you can't find me.</p>\n\n<p>The meetup will run from roughly 6:30pm to 9:30pm. We'll start with some socialising while people are still arriving, then begin with discussions at around 7PM while we eat dinner - then proceed with some kind of rationality exercise.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/w4'>Sydney Meetup: February</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ahC4Qa2jjjk94gkod", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.528282481135533e-06, "legacy": true, "legacyId": "25345", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sydney_Meetup__February\">Discussion article for the meetup : <a href=\"/meetups/w4\">Sydney Meetup: February</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 February 2014 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">City of Sydney RSL, 565 George St, Sydney NSW</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our last meetup was a great success, so we're doing it all again :)</p>\n\n<p>Please note: there are many levels of the RSL club.</p>\n\n<p>I've booked an ordinary long table under the name \"less wrong\" for 15 starting from 6:30 (which is when I should be able to get there). If we want to retire somewhere else afterwards, we can do that later.</p>\n\n<p>So we will be on whichever level the restaurant is located (but not level 4) under the booking \"less wrong\".</p>\n\n<p>I will verify the location within the next couple of weeks and update this meetup notification.</p>\n\n<p>I'll put up a sheet of paper with \"LessWrong\" and a picture of a paperclip so you can find us. I'll also tell the staff - so ask them to point out the \"less wrong meetup\" if you can't find me.</p>\n\n<p>The meetup will run from roughly 6:30pm to 9:30pm. We'll start with some socialising while people are still arriving, then begin with discussions at around 7PM while we eat dinner - then proceed with some kind of rationality exercise.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sydney_Meetup__February1\">Discussion article for the meetup : <a href=\"/meetups/w4\">Sydney Meetup: February</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sydney Meetup: February", "anchor": "Discussion_article_for_the_meetup___Sydney_Meetup__February", "level": 1}, {"title": "Discussion article for the meetup : Sydney Meetup: February", "anchor": "Discussion_article_for_the_meetup___Sydney_Meetup__February1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-24T02:27:38.851Z", "modifiedAt": null, "url": null, "title": "Find a study partner", "slug": "find-a-study-partner", "viewCount": null, "lastCommentedAt": "2020-06-15T22:30:27.620Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MathieuRoy", "createdAt": "2013-06-16T02:41:27.071Z", "isAdmin": false, "displayName": "Mati_Roy"}, "userId": "Tw9etd8rMnHLeSQ9q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MxxQt9jCZB68SBWGW/find-a-study-partner", "pageUrlRelative": "/posts/MxxQt9jCZB68SBWGW/find-a-study-partner", "linkUrl": "https://www.lesswrong.com/posts/MxxQt9jCZB68SBWGW/find-a-study-partner", "postedAtFormatted": "Friday, January 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Find%20a%20study%20partner&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFind%20a%20study%20partner%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMxxQt9jCZB68SBWGW%2Ffind-a-study-partner%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Find%20a%20study%20partner%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMxxQt9jCZB68SBWGW%2Ffind-a-study-partner", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMxxQt9jCZB68SBWGW%2Ffind-a-study-partner", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 119, "htmlBody": "<p>For reasons mentioned in <a href=\"/lw/j10/on_learning_difficult_things/\">So8res article</a> as well as for other reasons: studying with a partner can be very good. In November, Adele_L had posted an <a href=\"/lw/j1b/miri_course_list_study_pairs/\">article</a> for people wanting to find a study partner. It got 17 comments, but only 1 since November 16th. So I thought we (I) should make a monthly thread on this instead of constantly going back to an old article which people might (seem to) forget about. If people seem to agree with that, I will make a post about it every month.</p>\n<p>So if you're looking for a study partner for an online course or reading a manual (whether it's in the <a href=\"http://intelligence.org/courses/\">MIRI course list</a> or not) tell others in the comment section.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MxxQt9jCZB68SBWGW", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 29, "extendedScore": null, "score": 8.7e-05, "legacy": true, "legacyId": "25337", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w5F4w8tNZc6LcBKRP", "Zfvng53hsYtrPjX5d"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2014-01-24T02:27:38.851Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-24T03:57:50.244Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley: Talk on communication", "slug": "meetup-berkeley-talk-on-communication", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sYYCWeuWPyueNH5BJ/meetup-berkeley-talk-on-communication", "pageUrlRelative": "/posts/sYYCWeuWPyueNH5BJ/meetup-berkeley-talk-on-communication", "linkUrl": "https://www.lesswrong.com/posts/sYYCWeuWPyueNH5BJ/meetup-berkeley-talk-on-communication", "postedAtFormatted": "Friday, January 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%3A%20Talk%20on%20communication&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%3A%20Talk%20on%20communication%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsYYCWeuWPyueNH5BJ%2Fmeetup-berkeley-talk-on-communication%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%3A%20Talk%20on%20communication%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsYYCWeuWPyueNH5BJ%2Fmeetup-berkeley-talk-on-communication", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsYYCWeuWPyueNH5BJ%2Fmeetup-berkeley-talk-on-communication", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/w5'>Berkeley: Talk on communication</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 January 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2030 Addison, 3rd floor, Berkeley</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Next week's meetup will feature a presentation by M. about communication, with a computer science / cognitive science approach.</p>\n\n<p>Please arrive between 7pm and 7:30pm on Wednesday. At 7:30pm we'll review our weekly goals and record goals for the coming week. That should take about 15 minutes, and immediately after Mike will give his talk. You're welcome to stay after and discuss.</p>\n\n<p>The CFAR office is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p>\n\n<p>Even though this takes place at CFAR, it's not a CFAR-sponsored event.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/w5'>Berkeley: Talk on communication</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sYYCWeuWPyueNH5BJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.5285443028075263e-06, "legacy": true, "legacyId": "25346", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Talk_on_communication\">Discussion article for the meetup : <a href=\"/meetups/w5\">Berkeley: Talk on communication</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 January 2014 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2030 Addison, 3rd floor, Berkeley</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Next week's meetup will feature a presentation by M. about communication, with a computer science / cognitive science approach.</p>\n\n<p>Please arrive between 7pm and 7:30pm on Wednesday. At 7:30pm we'll review our weekly goals and record goals for the coming week. That should take about 15 minutes, and immediately after Mike will give his talk. You're welcome to stay after and discuss.</p>\n\n<p>The CFAR office is at 2030 Addison, 3rd floor, Berkeley, near the Downtown Berkeley BART. If you find yourself locked out, text me at</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p>\n\n<p>Even though this takes place at CFAR, it's not a CFAR-sponsored event.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Talk_on_communication1\">Discussion article for the meetup : <a href=\"/meetups/w5\">Berkeley: Talk on communication</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley: Talk on communication", "anchor": "Discussion_article_for_the_meetup___Berkeley__Talk_on_communication", "level": 1}, {"title": "Discussion article for the meetup : Berkeley: Talk on communication", "anchor": "Discussion_article_for_the_meetup___Berkeley__Talk_on_communication1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-24T07:42:05.108Z", "modifiedAt": null, "url": null, "title": "What should superrational players do in asymmetric games?", "slug": "what-should-superrational-players-do-in-asymmetric-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:07.366Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Apy6B9ahRqFMRt9sx/what-should-superrational-players-do-in-asymmetric-games", "pageUrlRelative": "/posts/Apy6B9ahRqFMRt9sx/what-should-superrational-players-do-in-asymmetric-games", "linkUrl": "https://www.lesswrong.com/posts/Apy6B9ahRqFMRt9sx/what-should-superrational-players-do-in-asymmetric-games", "postedAtFormatted": "Friday, January 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20should%20superrational%20players%20do%20in%20asymmetric%20games%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20should%20superrational%20players%20do%20in%20asymmetric%20games%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApy6B9ahRqFMRt9sx%2Fwhat-should-superrational-players-do-in-asymmetric-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20should%20superrational%20players%20do%20in%20asymmetric%20games%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApy6B9ahRqFMRt9sx%2Fwhat-should-superrational-players-do-in-asymmetric-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApy6B9ahRqFMRt9sx%2Fwhat-should-superrational-players-do-in-asymmetric-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1173, "htmlBody": "<p>Rereading Hofstadter's <a href=\"http://www.gwern.net/docs/1985-hofstadter\">essays on superrationality</a> prompted me to wonder what strategies superrational agents would want to commit to in asymmetric games. In symmetric games, everyone can agree on outcome they'd like to jointly achieve, leaving the decision-theoretic question of whether the players can commit or not. In asymmetric games, life becomes murkier. There are typically many Pareto-efficient outcomes, and we enter the wilds of <a href=\"http://en.wikipedia.org/wiki/Cooperative_game\">cooperative game theory</a> and <a href=\"http://en.wikipedia.org/wiki/Bargaining_problem#Bargaining_solutions\">bargaining solutions</a> trying to identify the right one. While, say, the Nash bargaining solution is appealing on many levels, I have a hard time connecting the logic of superrationality to any particular solution. Recently though, I found some insight in \"Cooperation in Strategic Games Revisited\" by Adam Kalai and Ehud Kalai (<a href=\"http://research.microsoft.com/en-us/um/people/adum/publications/2012-coco-qje.pdf\">working paper version</a> and <a href=\"http://www.sigecom.org/exchanges/volume_10/1/KALAI.pdf\">three-page summary version</a>) for the special case of two-player games with side transfers.</p>\n<p>Just to make sure everyone's on common ground, the prototypical game examined in the argument for superrationality is the prisoners' dilemma:</p>\n<table border=\"0\">\n<tbody>\n<tr class=\"odd\">\n<td align=\"right\">Alice / Bob</td>\n<td align=\"center\"><em>&nbsp; Cooperate &nbsp;</em></td>\n<td align=\"center\"><em>&nbsp; &nbsp; Defect &nbsp; &nbsp;</em></td>\n</tr>\n<tr class=\"even\">\n<td align=\"right\"><em>Cooperate</em></td>\n<td align=\"center\">10 / 10</td>\n<td align=\"center\">0 / 12</td>\n</tr>\n<tr class=\"odd\">\n<td align=\"right\"><em>Defect</em></td>\n<td align=\"center\">12 / 0</td>\n<td align=\"center\">4 / 4</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>The unique dominant-strategy equilibrium is <em>(Defect, Defect)</em>. However, Hofstadter argues that \"superrational\" players would recognize the symmetry in reasoning processes between each other and thus conclude that cooperating is in their interest. The argument is not in favor of unconditional cooperation. Instead, the reasoning is closer to \"I cooperate if and only I expect you to cooperate if and only if I cooperate\". Many bits have been devoted to formalizing this reasoning in <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">timeless decision theory</a> and other variants.</p>\n<p>The symmetry in the prisoners' dilemma makes it easy to pick out <em>(Cooperate, Cooperate)</em> as the action profile each player ideally wants to see happen. Consider instead the following skewed prisoners' dilemma:</p>\n<table border=\"0\">\n<tbody>\n<tr class=\"odd\">\n<td align=\"right\">Alice/Bob</td>\n<td align=\"center\"><em>&nbsp; Cooperate &nbsp;</em></td>\n<td align=\"center\"><em>&nbsp; &nbsp; Defect &nbsp; &nbsp;</em></td>\n</tr>\n<tr class=\"even\">\n<td align=\"right\"><em>Cooperate</em></td>\n<td align=\"center\">2 / 18</td>\n<td align=\"center\">&nbsp;0 / 12</td>\n</tr>\n<tr class=\"odd\">\n<td align=\"right\"><em>Defect</em></td>\n<td align=\"center\">12 / 0</td>\n<td align=\"center\">4 / 4</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>The <em>(Cooperate, Cooperate)</em> outcome still has the highest total benefit, but <em>(Defect, Defect)</em> is also Pareto-efficient. With this asymmetry, it seems reasonable for Alice to <em>Defect, </em>even as someone who would cooperate in the original prisoners' dilemma. Suppose however players can also agree to transfer utility between themselves on a 1-to-1 basis (like if they value cash equally and can make side-payments). Then, <em>(Cooperate, Cooperate)</em> with a transfer between 2 and 14 from Bob to Alice dominates <em>(Defect, Defect)</em>. The size of the transfer is still up in the air, although a transfer of 8 (leaving both with a payoff of 10) is appealing since it takes us back to the original symmetric game. I feel confident suggesting this as an outcome the players should commit to if possible.</p>\n<p>While the former game could be symmetrized in a nice way, what about more general games where payoffs could look even more askew or strategy sets could be completely different?</p>\n<p>Let <span class=\"math\"><em>A</em></span> be the payoff matrix for Alice and <span class=\"math\"><em>B</em></span> be the payoff matrix for Bob in any given game. Kalai and Kalai point out that the game <span class=\"math\">(<em>A</em>,\u2006<em>B</em></span>) can be decomposed into the sum of two games:</p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?(A,B)=\\left(\\frac{A+B}{2},\\frac{A+B}{2}\\right)+\\left(\\frac{A-B}{2},\\frac{B-A}{2}\\right),\" alt=\"\" /></p>\n<p>where payoffs are identical in the first game (the <em>team game</em>) and zero-sum in the second (the <em>advantage game</em>). Consider playing these games separately. In the team game, Alice and Bob both agree on the action profile that maximizes their payoff with no controversy. In the advantage game, preferences are exactly opposed, so each can play their <a href=\"http://en.wikipedia.org/wiki/Minimax\">maximin</a> strategy, again with no controversy. Of course, the rub is the team game strategy profile could be very different from the advantage game strategy profile.</p>\n<p>Suppose Alice and Bob could commit to playing each game separately. Kalai and Kalai define the payoffs each gets between the two games as</p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?\\textrm{coco-value}(A,B)=\\textrm{maxmax}\\left(\\frac{A+B}{2},\\frac{A+B}{2}\\right)\\;+\\;\\textrm{maximin}\\left(\\frac{A-B}{2},\\frac{B-A}{2}\\right)\" alt=\"\" /></p>\n<p>where <em>coco</em> stands for <em>cooperative/competitive</em>. We don't actually have two games to be played separately, so the way to achieve these payoffs is for Alice and Bob to actually play the team game actions and hypothetically play the advantage game. Transfers then even out the gains from the team game results and add in the hypothetical advantage game results. Even though the original game might be asymmetric, this simple decomposition allows players to cooperate exactly where interests are aligned and compete exactly where interests are opposed.</p>\n<p>For example, consider two hot dog vendors. There are 40 potential customers at the airport and 100 at the beach. If both choose the same location, they split the customers there evenly. Otherwise, the vendor at each location sells to everyone at that place. Alice turns a profit of $2 per customer, while Bob turns a profit of $1 per customer. Overall this yields the payoffs:</p>\n<table border=\"0\">\n<tbody>\n<tr class=\"odd\">\n<td align=\"right\">Alice / Bob</td>\n<td align=\"center\"><em>&nbsp; &nbsp; &nbsp;Airport &nbsp; &nbsp;&nbsp;</em></td>\n<td align=\"center\"><em>&nbsp; &nbsp; &nbsp; Beach &nbsp; &nbsp; &nbsp;</em></td>\n</tr>\n<tr class=\"even\">\n<td align=\"right\"><em>Airport</em></td>\n<td align=\"center\">40 / 20</td>\n<td align=\"center\">80 / 100</td>\n</tr>\n<tr class=\"odd\">\n<td align=\"right\"><em>Beach</em></td>\n<td align=\"center\">200 / 40</td>\n<td align=\"center\">100 / 50</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>The game decomposes into the team game:</p>\n<table border=\"0\">\n<tbody>\n<tr class=\"odd\">\n<td align=\"right\">Alice / Bob</td>\n<td align=\"center\"><em>&nbsp; &nbsp; &nbsp; Airport &nbsp; &nbsp;&nbsp;</em></td>\n<td align=\"center\"><em>&nbsp; &nbsp; &nbsp; Beach &nbsp; &nbsp; &nbsp;</em></td>\n</tr>\n<tr class=\"even\">\n<td align=\"right\"><em>Airport</em></td>\n<td align=\"center\">30 / 30</td>\n<td align=\"center\">90 / 90</td>\n</tr>\n<tr class=\"odd\">\n<td align=\"right\"><em>Beach</em></td>\n<td align=\"center\">120 / 120</td>\n<td align=\"center\">75 / 75</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>and the advantage game:</p>\n<table border=\"0\">\n<tbody>\n<tr class=\"odd\">\n<td align=\"right\">Alice / Bob</td>\n<td align=\"center\"><em>&nbsp; &nbsp; &nbsp; Airport &nbsp; &nbsp;&nbsp;</em></td>\n<td align=\"center\"><em>&nbsp; &nbsp; &nbsp; Beach &nbsp; &nbsp; &nbsp;</em></td>\n</tr>\n<tr class=\"even\">\n<td align=\"right\"><em>Airport</em></td>\n<td align=\"center\">10 / -10</td>\n<td align=\"center\">-10 / 10</td>\n</tr>\n<tr class=\"odd\">\n<td align=\"right\"><em>Beach</em></td>\n<td align=\"center\">80 / -80</td>\n<td align=\"center\">25 / -25</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>The maximizing strategy profile for the team game is <em>(Beach, Airport)</em> with payoffs <em>(120, 120)</em>. The maximin strategy profile for the advantage game is <em>(Beach, Beach)</em> with payoffs <em>(25, -25)</em>. In total, this game has a coco-value of <em>(145, 95)</em>, which would be realized by Alice selling at the beach, Bob selling at the airport, and Alice transferring <em>55</em> to Bob. Alice generates most of the profits in this situation, but Bob has to be compensated for his credible threat to start selling at the beach too.</p>\n<p>The bulk of the Kalai and Kalai article is extending the coco-value to incomplete information settings. For instance, each vendor might have some private information about the weather tomorrow, which will affect the number of customers at the airport and the beach. The Kalais prove that being able to publicly observe the payoffs for the chosen actions is sufficient for agents to commit themselves to the coco-value ex-ante (before receiving any private information) and that being able to publicly observe all hypothetical payoffs from alternative action profiles is sufficient for commitment even after agents have private information.</p>\n<p>The Kalais provide an axiomatization of the coco-value, showing it is the payoff pair that uniquely satisfies all of the following:</p>\n<ol style=\"list-style-type: decimal\">\n<li><em>Pareto optimality</em>: The sum of the values is maximal.</li>\n<li><em>Shift invariance</em>: Increasing a player's payoff by a constant amount in each cell increases their value by the same amount.</li>\n<li><em>Payoff dominance</em>: If one player always gets more than the other in each cell, that player can't get a smaller value for the game.</li>\n<li><em>Invariance to redundant strategies</em>: Adding a new action that is a convex combination of the payoffs of two other actions can't change the value.</li>\n<li><em>Monotonicity in actions</em>: Removing an action from a player can't increase their value for the game.</li>\n<li><em>Monotonicity in information</em>: Giving a player strictly less information can't increase their value for the game.</li>\n</ol>\n<p>The coco-value is also easily computable, unlike Nash equilibria in general. I'm hard-pressed to think of any more I could want from it (aside from easy extensions to bigger classes of games). Given its simplicity, I'm surprised it wasn't hit upon earlier.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Apy6B9ahRqFMRt9sx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 18, "extendedScore": null, "score": 1.528793893607501e-06, "legacy": true, "legacyId": "25347", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-24T15:53:23.664Z", "modifiedAt": null, "url": null, "title": "Meetup : Community Weekend in Berlin", "slug": "meetup-community-weekend-in-berlin", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YA663qNrbrq26xjwK/meetup-community-weekend-in-berlin", "pageUrlRelative": "/posts/YA663qNrbrq26xjwK/meetup-community-weekend-in-berlin", "linkUrl": "https://www.lesswrong.com/posts/YA663qNrbrq26xjwK/meetup-community-weekend-in-berlin", "postedAtFormatted": "Friday, January 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Community%20Weekend%20in%20Berlin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Community%20Weekend%20in%20Berlin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYA663qNrbrq26xjwK%2Fmeetup-community-weekend-in-berlin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Community%20Weekend%20in%20Berlin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYA663qNrbrq26xjwK%2Fmeetup-community-weekend-in-berlin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYA663qNrbrq26xjwK%2Fmeetup-community-weekend-in-berlin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/w6'>Community Weekend in Berlin</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 April 2014 04:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gr\u00fcnberger Str. 23, 10243 Berlin </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We usually don't announce individual meetups here, but this is an exception! Check <a href=\"http://lesswrong.com/lw/jjw/community_weekend_in_berlin/\">the full announcement</a> for details on how to sign up.</p>\n\n<p>Please join our <a href=\"http://groups.google.com/group/lw-berlin\" rel=\"nofollow\">mailing list</a> if you're interested in our regular meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/w6'>Community Weekend in Berlin</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YA663qNrbrq26xjwK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.5293409796222332e-06, "legacy": true, "legacyId": "25353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Community_Weekend_in_Berlin\">Discussion article for the meetup : <a href=\"/meetups/w6\">Community Weekend in Berlin</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 April 2014 04:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gr\u00fcnberger Str. 23, 10243 Berlin </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We usually don't announce individual meetups here, but this is an exception! Check <a href=\"http://lesswrong.com/lw/jjw/community_weekend_in_berlin/\">the full announcement</a> for details on how to sign up.</p>\n\n<p>Please join our <a href=\"http://groups.google.com/group/lw-berlin\" rel=\"nofollow\">mailing list</a> if you're interested in our regular meetups.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Community_Weekend_in_Berlin1\">Discussion article for the meetup : <a href=\"/meetups/w6\">Community Weekend in Berlin</a></h2>", "sections": [{"title": "Discussion article for the meetup : Community Weekend in Berlin", "anchor": "Discussion_article_for_the_meetup___Community_Weekend_in_Berlin", "level": 1}, {"title": "Discussion article for the meetup : Community Weekend in Berlin", "anchor": "Discussion_article_for_the_meetup___Community_Weekend_in_Berlin1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jSCEaE5TbYnbFCQYB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2014-01-24T16:45:17.661Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-70", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X6kxHthFG58kRLuhk/weekly-lw-meetups-70", "pageUrlRelative": "/posts/X6kxHthFG58kRLuhk/weekly-lw-meetups-70", "linkUrl": "https://www.lesswrong.com/posts/X6kxHthFG58kRLuhk/weekly-lw-meetups-70", "postedAtFormatted": "Friday, January 24th 2014", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX6kxHthFG58kRLuhk%2Fweekly-lw-meetups-70%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX6kxHthFG58kRLuhk%2Fweekly-lw-meetups-70", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX6kxHthFG58kRLuhk%2Fweekly-lw-meetups-70", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 517, "htmlBody": "<p><strong>This summary was posted to LW Main on January 17th. The following week's summary is <a href=\"/lw/jka/new_lw_meetups_hamburg_ljubljana/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/v5\">Sydney Meetup: January:&nbsp;<span class=\"date\">22 January 2014 06:30PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li> <a href=\"/meetups/vk\">Bratislava Meetup IX.:&nbsp;<span class=\"date\">20 January 2014 06:00PM</span></a></li>\n<li><a href=\"/meetups/vn\">Tel Aviv: Uses of Cryptography:&nbsp;<span class=\"date\">30 January 2014 08:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">18 January 2020 01:30PM</span></a></li>\n<li><a href=\"/meetups/vp\">Boston - Connection Theory:&nbsp;<span class=\"date\">19 January 2014 02:00PM</span></a></li>\n<li><a href=\"/meetups/vo\">Brussels: Morality - also cake:&nbsp;<span class=\"date\">08 February 2014 01:00PM</span></a></li>\n<li><a href=\"/meetups/vf\">London - The Schelling Point Strategy Game (plus socials):&nbsp;<span class=\"date\">19 January 2014 02:00AM</span></a></li>\n<li><a href=\"/meetups/uv\">Vienna:&nbsp;<span class=\"date\">18 January 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/vq\">Washington DC: Potatoes:&nbsp;<span class=\"date\">19 January 2014 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\">Berlin</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Brussels.2C_Belgium\">Brussels</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Philadelphia.2C_PA\">Philadelphia</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview is posted on the front page every Friday. These are an attempt to collect information on all the meetups happening in upcoming weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll also have the benefit of having your meetup mentioned in a weekly overview. These overview posts are moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X6kxHthFG58kRLuhk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.5293987918279744e-06, "legacy": true, "legacyId": "25294", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mZRDDoygm35Nq3kmL", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}