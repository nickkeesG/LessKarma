{"results": [{"createdAt": null, "postedAt": "2022-06-01T20:38:44.197Z", "modifiedAt": "2022-06-02T22:53:33.505Z", "url": null, "title": "Probability that the President would win election against a random adult citizen?", "slug": "probability-that-the-president-would-win-election-against-a", "viewCount": null, "lastCommentedAt": "2022-06-02T21:24:43.828Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daniel-kokotajlo", "createdAt": "2018-03-05T19:59:32.269Z", "isAdmin": false, "displayName": "Daniel Kokotajlo"}, "userId": "YLFQfGzNdGA4NFcKS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gc3niHuaNCiXGb5mc/probability-that-the-president-would-win-election-against-a", "pageUrlRelative": "/posts/Gc3niHuaNCiXGb5mc/probability-that-the-president-would-win-election-against-a", "linkUrl": "https://www.lesswrong.com/posts/Gc3niHuaNCiXGb5mc/probability-that-the-president-would-win-election-against-a", "postedAtFormatted": "Wednesday, June 1st 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Probability%20that%20the%20President%20would%20win%20election%20against%20a%20random%20adult%20citizen%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProbability%20that%20the%20President%20would%20win%20election%20against%20a%20random%20adult%20citizen%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGc3niHuaNCiXGb5mc%2Fprobability-that-the-president-would-win-election-against-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Probability%20that%20the%20President%20would%20win%20election%20against%20a%20random%20adult%20citizen%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGc3niHuaNCiXGb5mc%2Fprobability-that-the-president-would-win-election-against-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGc3niHuaNCiXGb5mc%2Fprobability-that-the-president-would-win-election-against-a", "socialPreviewImageUrl": "", "question": true, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<p>Suppose we magically intervene on the USA to make there be a snap election for the office of President. The incumbent (the existing president) goes up against X, where X is a randomly selected eligible person (e.g. adult, citizen, etc.) Voters have one month of campaigning before they decide, so X isn't a total stranger by the end.</p>\n<p>What is the probability that the President wins?</p>\n<p>Is the probability substantially different for Biden than it was for Trump? What about Obama?</p>\n<p>What is the probability that the President loses in a massive landslide (getting e.g. only 40% of the vote or less)?</p>\n<p>ETA: I've changed the title to stop giving the impression that I am looking for a yes or no answer. I'm not looking for a yes or no answer, I'm looking for a probability.</p>\n", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 4, "3uE2pXvbcnS9nnZRE": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "Gc3niHuaNCiXGb5mc", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 15, "extendedScore": null, "score": 0.2158676965806762, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-01T21:52:05.844Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "r38pkCm7wF4M44MDQ", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": 14, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": "1.1.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-01T20:38:44.197Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-01T21:33:32.661Z", "modifiedAt": "2022-06-03T01:20:20.020Z", "url": null, "title": "Public beliefs vs. Private beliefs", "slug": "public-beliefs-vs-private-beliefs", "viewCount": null, "lastCommentedAt": "2022-06-03T05:22:12.096Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eli Tyre", "user": {"username": "elityre", "createdAt": "2017-06-18T22:55:17.358Z", "isAdmin": false, "displayName": "Eli Tyre"}, "userId": "PWGv3R24uH9pvCnZm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jqCz2X49FRn5Bgb5b/public-beliefs-vs-private-beliefs", "pageUrlRelative": "/posts/jqCz2X49FRn5Bgb5b/public-beliefs-vs-private-beliefs", "linkUrl": "https://www.lesswrong.com/posts/jqCz2X49FRn5Bgb5b/public-beliefs-vs-private-beliefs", "postedAtFormatted": "Wednesday, June 1st 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Public%20beliefs%20vs.%20Private%20beliefs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APublic%20beliefs%20vs.%20Private%20beliefs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqCz2X49FRn5Bgb5b%2Fpublic-beliefs-vs-private-beliefs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Public%20beliefs%20vs.%20Private%20beliefs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqCz2X49FRn5Bgb5b%2Fpublic-beliefs-vs-private-beliefs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqCz2X49FRn5Bgb5b%2Fpublic-beliefs-vs-private-beliefs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1411, "htmlBody": "<p>A distinction that I get a lot of value out of is the difference between private beliefs and public beliefs.</p><h3>Public vs. Private</h3><p>A<strong> public belief</strong> is a proposition that someone thinks is true, and justifying on the basis of legible info and reasoning. If X is a public belief, then implicit claim is \"not only do I think that X is true, I think that any right thinking person who examines the evidence should come to conclude X.\"&nbsp;</p><p>If someone disagrees with you about a public belief, it is prosocial and epistemically virtuous to defend your claim and to debate the matter on a public forum. Public beliefs, if consensus is reached about them, can be added to the sum total of human knowledge, for others to take for granted and then build on.</p><p>A <strong>private belief</strong> is proposition that someone thinks is true based on their own private or illegible info and reasoning. In this case, the implicit claim is \"given my own read of the evidence, I happen to think X. But I don't think that the arguments that I've offered are necessarily sufficient to convince a third party. I'm not claiming that <i>you</i> should believe this, I'm merely providing you the true information that I believe it.\"&nbsp;</p><p>If someone disagrees with you about a private belief, there might or might not be a fruitful discussion to be had about it, but it is also important to be able to \"agree to disagree.\"</p><h3>An example</h3><p>I think that Circling meaningfully develops real skills of introspection, subtle interpersonal sensitivity, and clarity of map-territory distinctions. I further think that Circling is relevant to the Art of Rationality.&nbsp;</p><p>There have been some write-ups that <i>describe</i> why I think this is the case, but I don't know that any of them are <i>persuasive</i>. If a person is curious about why I might be interested in Circling, I think <a href=\"https://www.lesswrong.com/posts/jLwFCkNKMCFTCX7rL/circling-as-cousin-to-rationality\">this post</a> is a decent overview. But crucially, I don't think that the evidence presented <i>should be sufficient</i> to convince a skeptic.&nbsp;</p><p>I would say that I have a private belief that Circling is useful. It is actually my calibrated view, based on my personal experiences and my reasoning about those experiences. But by stating that belief, I am not at all making a social bid that others believe it too.&nbsp;</p><h3>A special case: cloaks</h3><p>There's a special case of having a private belief that, at CFAR, we used to refer to as \"having a cloak\".</p><p>If you are pursuing some ambitious project or personal development goals, it can be damaging to tell people about or justify your ambitions. Many ambitious goals are <a href=\"https://www.lesswrong.com/posts/R6M4vmShiowDn56of/butterfly-ideas\">butterfly ideas</a>, that need to be handled gently. Your own sense of what is possible might be fragile, and you need to nurture it.&nbsp;</p><p>And, for many people, sharing their ambitions puts them in a mindset of asking themselves to justify whether they're <a href=\"https://www.lesswrong.com/posts/5o4EZJyqmHY4XgRCY/einstein-s-superpowers\">cool</a> <a href=\"https://www.lesswrong.com/posts/dhj9dhiwhq3DX6W8z/hero-licensing\">enough</a> to succeed, or needing to fend off a kind of social pressure from causal pessimism. All of which is <a href=\"https://www.facebook.com/yudkowsky/posts/10151706498254228\">wasted motion</a>.</p><p>So it's useful to have a \"cloak\": an understanding that your plans and hopes can be private beliefs, that are no one's business but yours.</p><p>Sometimes that cloak can be keeping what you're working on a secret (as Paul Graham suggests in <a href=\"http://www.paulgraham.com/hs.html\">What You'll Wish You'd Known</a>).&nbsp;</p><p>Alternatively, it's useful to have a true(!), but incomplete, description about what you're aiming to do, that gives others a <a href=\"https://www.lesswrong.com/posts/Kbrm4KKbEWWSeBiQE/buckets-and-bayes-1\">bucket</a> for conceptualizing your actions, while you also have private, more ambitious plans. (Paul Graham <i>also</i> recommends this sort of cloak, in his \"tactics\" section of <a href=\"http://www.paulgraham.com/ambitious.html\">Frighteningly Ambitious Startup Ideas</a>.)</p><p>A motivating example is Amazon.com. I bet that back in 1999, Jeff Bezos had at least a glimmer of the the long term future of Amazon. But if Jeff Bezos had outright declared, \"Amazon's plan is to build an online bookstore, and eventually conquer almost the whole online retail economy (which, by the way, is going to be a double digit percentage of all retail, by 2020) and become one of the top 10 most valuable companies in the world\", he would have gotten incredulous reactions. Lots of people would have scoffed at Bezos's delusions of grandeur, many would have mocked him outright. Even if this was the actual plan and actual goal, declaring his ambitions for Amazon outright, would not have helped them succeed.&nbsp;</p><p>None of those people needed to believe that that kind of growth was possible, in order for Amazon to succeed. The only people who needed to believe is were Bezos and the core team at Amazon.</p><p>So instead, Amazon in 1999 has the cloak of just being an online bookstore, interesting but unobjectionable, while internally, they're working towards something much bigger than that.&nbsp;</p><p>In general, it's helpful to be able to believe things about yourself, and your abilities, that you don't have to justify to anyone else.</p><h3>Why does this matter?</h3><p>I think failing to make a distinction between public and private beliefs can hamper both interpersonal communication and, more importantly, people's internal ability to think.</p><p>Personal, being able to say \"this is a think that I think is true, but I definitely don't think that I've made the case strongly enough here, for you to be convinced\" gives me space to express more of my ideas, without skirting close to conflict or affront.</p><p>Further, I think lots of folks implicitly feel like they \"aren't allowed\" have an opinion about something unless it is a defensible public belief that for which they are prepared to advocate in the public forum. Accordingly, they have a very high bar for letting themselves believe something, or at least to say it out loud.&nbsp;</p><p>I suspect that this hobbles their thinking, in much the same way that knowing someone will read your diary entries causes your diary to be less reflective of your true thoughts. If you have a feeling that you have to justify all of your conclusions, there are lines of thought that you won't follow, because you can simulate your friends frowning at you for being a bad rationalist.&nbsp;</p><p>Personally (a private belief!), I think that rigor is extremely valuable, but it is even more important to be honest with myself about what I actually think is true, separately from what I think is socially defensible.</p><h3>Wait, isn't it bad to let people have beliefs that they don't need to defend?</h3><p>I imagine that some readers might object to giving social permission for people to have beliefs that they don't need to justify.&nbsp;</p><p>\"Isn't part of what's wonderful about rationality that we try to be explicit enough that we can reason about anything? Isn't a major cause of the world's problems that beliefs are rarely held to any standard of evidence, and therefore people believe all kinds of random stuff? This post kind of sounds like you recommend that we stop holding people to standards of evidence.\"</p><p>The key thing for me is that private beliefs are your own personal model of the world, and you should never expect, or insist, that other people act on them.&nbsp;</p><p>It is always out of bounds to expect or demand that other people adopt your beliefs without offering justification.&nbsp;</p><p>If you are making claims that you want to influence other people's actions, it is incumbent upon you to justify them.&nbsp;</p><p>Everyone has an unalienable right to hold the belief, that, for instance, polyamory is bad for people's psychology, on whatever basis, including of intuitive or illegible reasoning. You are by all means allowed to decide whether or not to be polyamorous yourself, for those reasons. It is quite bad if a person feels pressured into being poly despite their intuitive sense that it's harmful for them or for others.</p><p>But, by my proposed social norms, if you want to go further and suggest that other people should be <i>prevented</i> from being polyamorous, or that it should be discouraged in your community, it is on you to justify that, to put forward a public position with reasons that can be critiqued and debated.</p><p>And of course, a person could always declare some of their private beliefs, not mainly in the hopes of convincing those that disagree, but rather to find and filter for the other people that share their view, so that you can together form spaces where that view can be assumed and built on. eg \"I can't (yet) articulate why I think that self-honesty is <i>so</i> crucial to the world saving project with full rigor, but if you also have that intuition, maybe we can work together on building and refining a culture that promotes self-honesty.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xgpBASEThXPuKRhbS": 2, "zv7v2ziqexSn5iS9v": 2, "Ng8Gice9KNkncxqcj": 4}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "jqCz2X49FRn5Bgb5b", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 67, "extendedScore": null, "score": 0.9887367391671611, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-01T21:52:01.116Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 68, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>A distinction that I get a lot of value out of is the difference between private beliefs and public beliefs.</p><h3 id=\"Public_vs__Private\">Public vs. Private</h3><p>A<strong> public belief</strong> is a proposition that someone thinks is true, and justifying on the basis of legible info and reasoning. If X is a public belief, then implicit claim is \"not only do I think that X is true, I think that any right thinking person who examines the evidence should come to conclude X.\"&nbsp;</p><p>If someone disagrees with you about a public belief, it is prosocial and epistemically virtuous to defend your claim and to debate the matter on a public forum. Public beliefs, if consensus is reached about them, can be added to the sum total of human knowledge, for others to take for granted and then build on.</p><p>A <strong>private belief</strong> is proposition that someone thinks is true based on their own private or illegible info and reasoning. In this case, the implicit claim is \"given my own read of the evidence, I happen to think X. But I don't think that the arguments that I've offered are necessarily sufficient to convince a third party. I'm not claiming that <i>you</i> should believe this, I'm merely providing you the true information that I believe it.\"&nbsp;</p><p>If someone disagrees with you about a private belief, there might or might not be a fruitful discussion to be had about it, but it is also important to be able to \"agree to disagree.\"</p><h3 id=\"An_example\">An example</h3><p>I think that Circling meaningfully develops real skills of introspection, subtle interpersonal sensitivity, and clarity of map-territory distinctions. I further think that Circling is relevant to the Art of Rationality.&nbsp;</p><p>There have been some write-ups that <i>describe</i> why I think this is the case, but I don't know that any of them are <i>persuasive</i>. If a person is curious about why I might be interested in Circling, I think <a href=\"https://www.lesswrong.com/posts/jLwFCkNKMCFTCX7rL/circling-as-cousin-to-rationality\">this post</a> is a decent overview. But crucially, I don't think that the evidence presented <i>should be sufficient</i> to convince a skeptic.&nbsp;</p><p>I would say that I have a private belief that Circling is useful. It is actually my calibrated view, based on my personal experiences and my reasoning about those experiences. But by stating that belief, I am not at all making a social bid that others believe it too.&nbsp;</p><h3 id=\"A_special_case__cloaks\">A special case: cloaks</h3><p>There's a special case of having a private belief that, at CFAR, we used to refer to as \"having a cloak\".</p><p>If you are pursuing some ambitious project or personal development goals, it can be damaging to tell people about or justify your ambitions. Many ambitious goals are <a href=\"https://www.lesswrong.com/posts/R6M4vmShiowDn56of/butterfly-ideas\">butterfly ideas</a>, that need to be handled gently. Your own sense of what is possible might be fragile, and you need to nurture it.&nbsp;</p><p>And, for many people, sharing their ambitions puts them in a mindset of asking themselves to justify whether they're <a href=\"https://www.lesswrong.com/posts/5o4EZJyqmHY4XgRCY/einstein-s-superpowers\">cool</a> <a href=\"https://www.lesswrong.com/posts/dhj9dhiwhq3DX6W8z/hero-licensing\">enough</a> to succeed, or needing to fend off a kind of social pressure from causal pessimism. All of which is <a href=\"https://www.facebook.com/yudkowsky/posts/10151706498254228\">wasted motion</a>.</p><p>So it's useful to have a \"cloak\": an understanding that your plans and hopes can be private beliefs, that are no one's business but yours.</p><p>Sometimes that cloak can be keeping what you're working on a secret (as Paul Graham suggests in <a href=\"http://www.paulgraham.com/hs.html\">What You'll Wish You'd Known</a>).&nbsp;</p><p>Alternatively, it's useful to have a true(!), but incomplete, description about what you're aiming to do, that gives others a <a href=\"https://www.lesswrong.com/posts/Kbrm4KKbEWWSeBiQE/buckets-and-bayes-1\">bucket</a> for conceptualizing your actions, while you also have private, more ambitious plans. (Paul Graham <i>also</i> recommends this sort of cloak, in his \"tactics\" section of <a href=\"http://www.paulgraham.com/ambitious.html\">Frighteningly Ambitious Startup Ideas</a>.)</p><p>A motivating example is Amazon.com. I bet that back in 1999, Jeff Bezos had at least a glimmer of the the long term future of Amazon. But if Jeff Bezos had outright declared, \"Amazon's plan is to build an online bookstore, and eventually conquer almost the whole online retail economy (which, by the way, is going to be a double digit percentage of all retail, by 2020) and become one of the top 10 most valuable companies in the world\", he would have gotten incredulous reactions. Lots of people would have scoffed at Bezos's delusions of grandeur, many would have mocked him outright. Even if this was the actual plan and actual goal, declaring his ambitions for Amazon outright, would not have helped them succeed.&nbsp;</p><p>None of those people needed to believe that that kind of growth was possible, in order for Amazon to succeed. The only people who needed to believe is were Bezos and the core team at Amazon.</p><p>So instead, Amazon in 1999 has the cloak of just being an online bookstore, interesting but unobjectionable, while internally, they're working towards something much bigger than that.&nbsp;</p><p>In general, it's helpful to be able to believe things about yourself, and your abilities, that you don't have to justify to anyone else.</p><h3 id=\"Why_does_this_matter_\">Why does this matter?</h3><p>I think failing to make a distinction between public and private beliefs can hamper both interpersonal communication and, more importantly, people's internal ability to think.</p><p>Personal, being able to say \"this is a think that I think is true, but I definitely don't think that I've made the case strongly enough here, for you to be convinced\" gives me space to express more of my ideas, without skirting close to conflict or affront.</p><p>Further, I think lots of folks implicitly feel like they \"aren't allowed\" have an opinion about something unless it is a defensible public belief that for which they are prepared to advocate in the public forum. Accordingly, they have a very high bar for letting themselves believe something, or at least to say it out loud.&nbsp;</p><p>I suspect that this hobbles their thinking, in much the same way that knowing someone will read your diary entries causes your diary to be less reflective of your true thoughts. If you have a feeling that you have to justify all of your conclusions, there are lines of thought that you won't follow, because you can simulate your friends frowning at you for being a bad rationalist.&nbsp;</p><p>Personally (a private belief!), I think that rigor is extremely valuable, but it is even more important to be honest with myself about what I actually think is true, separately from what I think is socially defensible.</p><h3 id=\"Wait__isn_t_it_bad_to_let_people_have_beliefs_that_they_don_t_need_to_defend_\">Wait, isn't it bad to let people have beliefs that they don't need to defend?</h3><p>I imagine that some readers might object to giving social permission for people to have beliefs that they don't need to justify.&nbsp;</p><p>\"Isn't part of what's wonderful about rationality that we try to be explicit enough that we can reason about anything? Isn't a major cause of the world's problems that beliefs are rarely held to any standard of evidence, and therefore people believe all kinds of random stuff? This post kind of sounds like you recommend that we stop holding people to standards of evidence.\"</p><p>The key thing for me is that private beliefs are your own personal model of the world, and you should never expect, or insist, that other people act on them.&nbsp;</p><p>It is always out of bounds to expect or demand that other people adopt your beliefs without offering justification.&nbsp;</p><p>If you are making claims that you want to influence other people's actions, it is incumbent upon you to justify them.&nbsp;</p><p>Everyone has an unalienable right to hold the belief, that, for instance, polyamory is bad for people's psychology, on whatever basis, including of intuitive or illegible reasoning. You are by all means allowed to decide whether or not to be polyamorous yourself, for those reasons. It is quite bad if a person feels pressured into being poly despite their intuitive sense that it's harmful for them or for others.</p><p>But, by my proposed social norms, if you want to go further and suggest that other people should be <i>prevented</i> from being polyamorous, or that it should be discouraged in your community, it is on you to justify that, to put forward a public position with reasons that can be critiqued and debated.</p><p>And of course, a person could always declare some of their private beliefs, not mainly in the hopes of convincing those that disagree, but rather to find and filter for the other people that share their view, so that you can together form spaces where that view can be assumed and built on. eg \"I can't (yet) articulate why I think that self-honesty is <i>so</i> crucial to the world saving project with full rigor, but if you also have that intuition, maybe we can work together on building and refining a culture that promotes self-honesty.\"</p>", "sections": [{"title": "Public vs. Private", "anchor": "Public_vs__Private", "level": 1}, {"title": "An example", "anchor": "An_example", "level": 1}, {"title": "A special case: cloaks", "anchor": "A_special_case__cloaks", "level": 1}, {"title": "Why does this matter?", "anchor": "Why_does_this_matter_", "level": 1}, {"title": "Wait, isn't it bad to let people have beliefs that they don't need to defend?", "anchor": "Wait__isn_t_it_bad_to_let_people_have_beliefs_that_they_don_t_need_to_defend_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": 8, "af": false, "version": "1.5.0", "pingbacks": {"Posts": ["jLwFCkNKMCFTCX7rL", "R6M4vmShiowDn56of", "5o4EZJyqmHY4XgRCY", "dhj9dhiwhq3DX6W8z", "Kbrm4KKbEWWSeBiQE"]}, "moderationGuidelinesVersion": "1.2.0", "customHighlightVersion": "1.1.0", "afDate": null, "afBaseScore": 17, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-01T21:28:58.207Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-02T00:43:39.634Z", "modifiedAt": "2022-06-02T23:43:00.991Z", "url": null, "title": "ACX Montreal Meetup Jun 18 2022", "slug": "acx-montreal-meetup-jun-18-2022", "viewCount": null, "lastCommentedAt": "2022-06-02T00:43:39.634Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "E", "createdAt": "2021-10-22T00:43:09.563Z", "isAdmin": false, "displayName": "E"}, "userId": "Fbpu5FXaEtCeetGmh", "domain": null, "pageUrl": "https://www.lesswrong.com/events/TChJDwsvBdM2zACoe/acx-montreal-meetup-jun-18-2022", "pageUrlRelative": "/events/TChJDwsvBdM2zACoe/acx-montreal-meetup-jun-18-2022", "linkUrl": "https://www.lesswrong.com/events/TChJDwsvBdM2zACoe/acx-montreal-meetup-jun-18-2022", "postedAtFormatted": "Thursday, June 2nd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20ACX%20Montreal%20Meetup%20Jun%2018%202022&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AACX%20Montreal%20Meetup%20Jun%2018%202022%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fevents%2FTChJDwsvBdM2zACoe%2Facx-montreal-meetup-jun-18-2022%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=ACX%20Montreal%20Meetup%20Jun%2018%202022%20https%3A%2F%2Fwww.lesswrong.com%2Fevents%2FTChJDwsvBdM2zACoe%2Facx-montreal-meetup-jun-18-2022", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fevents%2FTChJDwsvBdM2zACoe%2Facx-montreal-meetup-jun-18-2022", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p><strong>Please check this page the day of, as I will update it in the event of cancellation due to rain.</strong></p><p><strong>Description:</strong></p><p>Come on out to the ACX (Astral Codex Ten) Montreal Meetup. We will be hosting a discussion about a topic familiar to us all: Trying.</p><p>It is the difference between getting things done and accomplishing nothing. It often seems like putting in an effort is tiring and difficult, but does that just mean we're doing it wrong? How do we deceive ourselves into thinking we're trying when we're actually not?</p><p>We have selected two pieces that contain some ideas on what we can hope to achieve when we try, and what we can do to get better results when trying. Doing the reading is recommended but feel free to come even if you forget or don't have time to.</p><p><a href=\"https://danluu.com/p95-skill/\">95%-ile isn't that good </a>by Dan Luu (14m read if you skip the footnotes &amp; appendices)<br><a href=\"https://mindingourway.com/stop-trying-to-try-and-try\">Stop trying to try and try </a>by Nate Soares (12m read)</p><p><strong>Venue:</strong></p><p>The date is Saturday, June 18th 2022 at 1:00pm. The location is outdoors: Jeanne-Mance Park, at the corner of Duluth and Esplanade. Rough location here: <a href=\"https://plus.codes/87Q8GC89+37\">https://plus.codes/87Q8GC89+37</a></p><p>RSVPing is not mandatory but it encourages others to come.</p>", "submitToFrontpage": false, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": null, "noIndex": false, "rsvps": [{"name": "Nicolas Lacombe", "email": "", "nonPublic": null, "response": "yes", "userId": "LBH99ryykyAjRGatA", "createdAt": "2022-06-02T00:52:44.364Z"}, {"name": "Patrick Mercadante", "email": "", "nonPublic": null, "response": "yes", "userId": null, "createdAt": "2022-06-02T01:24:49.614Z"}, {"name": "Jeremy", "email": "jeremyeliosoff@gmail.com", "nonPublic": null, "response": "yes", "userId": null, "createdAt": "2022-06-02T01:43:54.026Z"}, {"name": "Massimog", "email": "massimogauthier1@gmail.com", "nonPublic": null, "response": "yes", "userId": "vLMZEXFzDAXLisTmQ", "createdAt": "2022-06-02T03:59:47.336Z"}, {"name": "Henri Lemoine", "email": "", "nonPublic": null, "response": "maybe", "userId": "ghY2fEWDiDAawGMun", "createdAt": "2022-06-02T01:15:10.903Z"}, {"name": "TheBayesian", "email": "", "nonPublic": null, "response": "maybe", "userId": null, "createdAt": "2022-06-02T16:00:37.363Z"}, {"name": "Haydn", "email": "haydnlthomas@gmail.com", "nonPublic": null, "response": "maybe", "userId": null, "createdAt": "2022-06-02T23:43:00.987Z"}], "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "TChJDwsvBdM2zACoe", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 0.06517578258367346, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": true, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": "2022-06-18T17:00:00.000Z", "localStartTime": "2022-06-18T13:00:00.000Z", "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": {"type": "Point", "coordinates": [-73.58697459999999, 45.5177162]}, "googleLocation": {"address_components": [{"long_name": "4422", "short_name": "4422", "types": ["street_number"]}, {"long_name": "Avenue de l'Esplanade", "short_name": "Av. de l'Esplanade", "types": ["route"]}, {"long_name": "Le Plateau-Mont-Royal", "short_name": "Le Plateau-Mont-Royal", "types": ["sublocality_level_1", "sublocality", "political"]}, {"long_name": "Montr\u00e9al", "short_name": "Montr\u00e9al", "types": ["locality", "political"]}, {"long_name": "Communaut\u00e9-Urbaine-de-Montr\u00e9al", "short_name": "Communaut\u00e9-Urbaine-de-Montr\u00e9al", "types": ["administrative_area_level_2", "political"]}, {"long_name": "Qu\u00e9bec", "short_name": "QC", "types": ["administrative_area_level_1", "political"]}, {"long_name": "Canada", "short_name": "CA", "types": ["country", "political"]}, {"long_name": "H2W 2N4", "short_name": "H2W 2N4", "types": ["postal_code"]}], "adr_address": "<span class=\"street-address\">4422 Av. de l&#39;Esplanade</span>, <span class=\"locality\">Montr\u00e9al</span>, <span class=\"region\">QC</span> <span class=\"postal-code\">H2W 2N4</span>, <span class=\"country-name\">Canada</span>", "business_status": "OPERATIONAL", "formatted_address": "4422 Av. de l'Esplanade, Montr\u00e9al, QC H2W 2N4, Canada", "geometry": {"location": {"lat": 45.5177162, "lng": -73.58697459999999}, "viewport": {"south": 45.5164449197085, "west": -73.5882524802915, "north": 45.5191428802915, "east": -73.5855545197085}}, "icon": "https://maps.gstatic.com/mapfiles/place_api/icons/v1/png_71/park-71.png", "icon_background_color": "#4DB546", "icon_mask_base_uri": "https://maps.gstatic.com/mapfiles/place_api/icons/v2/tree_pinlet", "name": "Jeanne-Mance Park", "opening_hours": {"open_now": true, "periods": [{"close": {"day": 1, "time": "0000", "hours": 0, "minutes": 0, "nextDate": 1654488000000}, "open": {"day": 0, "time": "0600", "hours": 6, "minutes": 0, "nextDate": 1654423200000}}, {"close": {"day": 2, "time": "0000", "hours": 0, "minutes": 0, "nextDate": 1654574400000}, "open": {"day": 1, "time": "0600", "hours": 6, "minutes": 0, "nextDate": 1654509600000}}, {"close": {"day": 3, "time": "0000", "hours": 0, "minutes": 0, "nextDate": 1654056000000}, "open": {"day": 2, "time": "0600", "hours": 6, "minutes": 0, "nextDate": 1654596000000}}, {"close": {"day": 4, "time": "0000", "hours": 0, "minutes": 0, "nextDate": 1654142400000}, "open": {"day": 3, "time": "0600", "hours": 6, "minutes": 0, "nextDate": 1654077600000}}, {"close": {"day": 5, "time": "0000", "hours": 0, "minutes": 0, "nextDate": 1654228800000}, "open": {"day": 4, "time": "0600", "hours": 6, "minutes": 0, "nextDate": 1654164000000}}, {"close": {"day": 6, "time": "0000", "hours": 0, "minutes": 0, "nextDate": 1654315200000}, "open": {"day": 5, "time": "0600", "hours": 6, "minutes": 0, "nextDate": 1654250400000}}, {"close": {"day": 0, "time": "0000", "hours": 0, "minutes": 0, "nextDate": 1654401600000}, "open": {"day": 6, "time": "0600", "hours": 6, "minutes": 0, "nextDate": 1654336800000}}], "weekday_text": ["Monday: 6:00 AM \u2013 12:00 AM", "Tuesday: 6:00 AM \u2013 12:00 AM", "Wednesday: 6:00 AM \u2013 12:00 AM", "Thursday: 6:00 AM \u2013 12:00 AM", "Friday: 6:00 AM \u2013 12:00 AM", "Saturday: 6:00 AM \u2013 12:00 AM", "Sunday: 6:00 AM \u2013 12:00 AM"]}, "photos": [{"height": 465, "html_attributions": ["<a href=\"https://maps.google.com/maps/contrib/114247782621459639446\">Parc Jeanne-Mance</a>"], "width": 620}, {"height": 6936, "html_attributions": ["<a href=\"https://maps.google.com/maps/contrib/107353470661051133250\">Priscilla Lartsey</a>"], "width": 9248}, {"height": 3265, "html_attributions": ["<a href=\"https://maps.google.com/maps/contrib/103115681764922225962\">riyaz khan</a>"], "width": 4898}, {"height": 3468, "html_attributions": ["<a href=\"https://maps.google.com/maps/contrib/101566708347265221052\">Digna Delgado</a>"], "width": 4624}, {"height": 4000, "html_attributions": ["<a href=\"https://maps.google.com/maps/contrib/107012524581082034459\">Mr M</a>"], "width": 3000}, {"height": 4032, "html_attributions": ["<a href=\"https://maps.google.com/maps/contrib/104352520583051672157\">Parvez Patel</a>"], "width": 3024}, {"height": 1836, "html_attributions": ["<a href=\"https://maps.google.com/maps/contrib/102014402566842082444\">Pedro Pacheco</a>"], "width": 3264}, {"height": 3150, "html_attributions": ["<a href=\"https://maps.google.com/maps/contrib/112704066005438657716\">Paul Pound</a>"], "width": 4724}, {"height": 3024, "html_attributions": ["<a href=\"https://maps.google.com/maps/contrib/109216034906800425477\">Brian Gilbreth</a>"], "width": 4032}, {"height": 3024, "html_attributions": ["<a href=\"https://maps.google.com/maps/contrib/103936429525607340818\">Yin YU</a>"], "width": 4032}], "place_id": "ChIJPxWAhzIayUwRcPG_kiLoJzc", "plus_code": {"compound_code": "GC97+36 Montreal, QC, Canada", "global_code": "87Q8GC97+36"}, "rating": 4.6, "reference": "ChIJPxWAhzIayUwRcPG_kiLoJzc", "reviews": [{"author_name": "Erika Else Arless", "author_url": "https://www.google.com/maps/contrib/113964241463062558980/reviews", "language": "en", "profile_photo_url": "https://lh3.googleusercontent.com/a-/AOh14GguVQ9E9njepZUtQsqiRoGZnfy38Ys9Zf0P44hN0w=s128-c0x00000000-cc-rp-mo-ba4", "rating": 5, "relative_time_description": "a year ago", "text": "just a big park what's not to love", "time": 1598667608}, {"author_name": "Chris Joe", "author_url": "https://www.google.com/maps/contrib/112907103448456665478/reviews", "language": "en", "profile_photo_url": "https://lh3.googleusercontent.com/a-/AOh14GgAAdvz399LrjDHsDWGkjnwSjS42cXoChTEHzhcaA=s128-c0x00000000-cc-rp-mo", "rating": 5, "relative_time_description": "a year ago", "text": "This park has a natural soccer field, a football field and a private volleyball court", "time": 1599348818}, {"author_name": "Jeff Post", "author_url": "https://www.google.com/maps/contrib/105026064540222368725/reviews", "language": "en", "profile_photo_url": "https://lh3.googleusercontent.com/a-/AOh14GgQdHL2HNShr29G7yP66ToRJB_TAAWNF1YFmjx-=s128-c0x00000000-cc-rp-mo-ba4", "rating": 5, "relative_time_description": "a year ago", "text": "park right across from the mountain. tennis and baseball as well as picnic areas.", "time": 1598443365}, {"author_name": "Lyta Perez", "author_url": "https://www.google.com/maps/contrib/108547057596178641062/reviews", "language": "en", "profile_photo_url": "https://lh3.googleusercontent.com/a-/AOh14Gj2e6aej1tVDFMx3jhZjSNoJTA7LwllgUf77ytb=s128-c0x00000000-cc-rp-mo-ba4", "rating": 5, "relative_time_description": "a year ago", "text": "Amazing park for kids and the hike near by the Mount .", "time": 1598494698}, {"author_name": "Isabelle Genier", "author_url": "https://www.google.com/maps/contrib/112021442741226762374/reviews", "language": "en", "profile_photo_url": "https://lh3.googleusercontent.com/a-/AOh14GibpatacFGsEsPQbkXtCGZ2T2KP5B6dqE9hjJbV=s128-c0x00000000-cc-rp-mo", "rating": 5, "relative_time_description": "a year ago", "text": "\ud83d\ude0f\ud83d\udcaf", "time": 1600554337}], "types": ["park", "point_of_interest", "establishment"], "url": "https://maps.google.com/?cid=3974400431366336880", "user_ratings_total": 4290, "utc_offset": -240, "vicinity": "4422 Avenue de l'Esplanade, Montr\u00e9al", "website": "https://montreal.ca/lieux/parc-jeanne-mance", "html_attributions": [], "utc_offset_minutes": -240}, "location": "Jeanne-Mance Park, Esplanade Avenue, Montreal, Montreal, QC, Canada", "contactInfo": "90u610sye@relay.firefox.com\u2069", "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": ["SSC"], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Please_check_this_page_the_day_of__as_I_will_update_it_in_the_event_of_cancellation_due_to_rain_\">Please check this page the day of, as I will update it in the event of cancellation due to rain.</strong></p><p><strong id=\"Description_\">Description:</strong></p><p>Come on out to the ACX (Astral Codex Ten) Montreal Meetup. We will be hosting a discussion about a topic familiar to us all: Trying.</p><p>It is the difference between getting things done and accomplishing nothing. It often seems like putting in an effort is tiring and difficult, but does that just mean we're doing it wrong? How do we deceive ourselves into thinking we're trying when we're actually not?</p><p>We have selected two pieces that contain some ideas on what we can hope to achieve when we try, and what we can do to get better results when trying. Doing the reading is recommended but feel free to come even if you forget or don't have time to.</p><p><a href=\"https://danluu.com/p95-skill/\">95%-ile isn't that good </a>by Dan Luu (14m read if you skip the footnotes &amp; appendices)<br><a href=\"https://mindingourway.com/stop-trying-to-try-and-try\">Stop trying to try and try </a>by Nate Soares (12m read)</p><p><strong id=\"Venue_\">Venue:</strong></p><p>The date is Saturday, June 18th 2022 at 1:00pm. The location is outdoors: Jeanne-Mance Park, at the corner of Duluth and Esplanade. Rough location here: <a href=\"https://plus.codes/87Q8GC89+37\">https://plus.codes/87Q8GC89+37</a></p><p>RSVPing is not mandatory but it encourages others to come.</p>", "sections": [{"title": "Please check this page the day of, as I will update it in the event of cancellation due to rain.", "anchor": "Please_check_this_page_the_day_of__as_I_will_update_it_in_the_event_of_cancellation_due_to_rain_", "level": 1}, {"title": "Description:", "anchor": "Description_", "level": 1}, {"title": "Venue:", "anchor": "Venue_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": "1.1.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-01T01:16:56.254Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-02T01:32:05.615Z", "modifiedAt": "2022-06-02T18:37:39.207Z", "url": null, "title": "The Bio Anchors Forecast", "slug": "the-bio-anchors-forecast", "viewCount": null, "lastCommentedAt": "2022-06-02T01:32:05.615Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "anshuman-radhakrishnan-1", "createdAt": "2021-01-04T19:39:30.513Z", "isAdmin": false, "displayName": "Ansh Radhakrishnan"}, "userId": "iceLk35nezzdEssW8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FbP7EteJBCx8FpLFn/the-bio-anchors-forecast", "pageUrlRelative": "/posts/FbP7EteJBCx8FpLFn/the-bio-anchors-forecast", "linkUrl": "https://www.lesswrong.com/posts/FbP7EteJBCx8FpLFn/the-bio-anchors-forecast", "postedAtFormatted": "Thursday, June 2nd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Bio%20Anchors%20Forecast&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Bio%20Anchors%20Forecast%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFbP7EteJBCx8FpLFn%2Fthe-bio-anchors-forecast%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Bio%20Anchors%20Forecast%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFbP7EteJBCx8FpLFn%2Fthe-bio-anchors-forecast", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFbP7EteJBCx8FpLFn%2Fthe-bio-anchors-forecast", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1045, "htmlBody": "<p>Ajeya Cotra\u2019s&nbsp;<a href=\"https://www.lesswrong.com/posts/cxQtz3RP4qsqTkEwL/an-121-forecasting-transformative-ai-timelines-using\"><u>Forecasting Transformative AI with Biological Anchors</u></a>, to my knowledge, represents the most serious effort to predict the arrival of&nbsp;<a href=\"https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence\"><u>transformative AI</u></a> - even if it\u2019s not attempting to pinpoint the exact instant that we\u2019ll get transformative AI, it posits an upper bound on the amount of time until then (<a href=\"https://www.cold-takes.com/biological-anchors-is-about-bounding-not-pinpointing-ai-timelines/\"><u>Holden Karnofsky\u2019s post</u></a> clarifies the difference).&nbsp;</p><p>The report\u2019s methodology,&nbsp;<a href=\"https://astralcodexten.substack.com/p/biological-anchors-a-trick-that-might?s=r\"><u>summarized by Scott Alexander</u></a>, is:</p><blockquote><p>1. Figure out how much inferential computation the human brain does.</p><p>2. Try to figure out how much training computation it would take, right now, to get a neural net that does the same amount of inferential computation. Get some mind-bogglingly large number.</p><p>3. Adjust for \"algorithmic progress\", ie maybe in the future neural nets will be better at using computational resources efficiently. Get some number which, realistically, is still mind-bogglingly large.</p><p>4. Probably if you wanted that mind-bogglingly large amount of computation, it would take some mind-bogglingly large amount of money. But computation is getting cheaper every year. Also, the economy is growing every year. Also, the share of the economy that goes to investments in AI companies is growing every year. So at some point, some AI company will actually be able to afford that mind-boggingly-large amount of money, deploy the mind-bogglingly large amount of computation, and train the AI that has the same inferential computation as the human brain.</p><p>5. Figure out what year that is.</p></blockquote><p>There are some other biological anchors that the report also considers, besides the human brain compute estimate, such as drawing a comparison between the parameter count of a transformative AI model and the parameter count of the human genome, or the amount of compute used by all animal brains over the course of evolution - I don\u2019t place a ton of weight on these anchors myself, since I find them uninformative, and I tend to think the strength of this report lies in assuming that the current deep learning paradigm will lead to transformative AI and trying to work backward from there.&nbsp;</p><p>Is it actually reasonable to assume that deep learning might produce transformative AI? I think so. Mark Xu\u2019s model&nbsp;<a href=\"https://www.lesswrong.com/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works?commentId=yv4tLvGmZE7yKpxqu#comments\"><u>here</u></a> is compelling regarding the differences between different AI algorithms/paradigms in terms of their reliance on computing power: each algorithm seems to have a certain \u201ceffective compute\u201d regime within which scaling up computing power leads to predictable increases in capabilities and beyond which capability gains stall out or become vanishingly small. It seems increasingly likely to me that we\u2019re far from exiting the effective computing regime of modern neural networks - I think the results from e.g.&nbsp;<a href=\"https://arxiv.org/abs/2203.15556\"><u>the Chinchilla paper</u></a> support the fact that we have more juice to squeeze from models very similar to the ones we\u2019ve already trained (<a href=\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\"><u>PaLM</u></a>,&nbsp;<a href=\"https://www.deepmind.com/publications/a-generalist-agent\"><u>Gato</u></a>,&nbsp;<a href=\"https://openai.com/dall-e-2/\"><u>DALL-E</u></a>, etc.). Furthermore, these models already seem to be on the cusp of having real societal impact.&nbsp;<a href=\"https://copilot.github.com/\"><u>Copilot</u></a> doesn\u2019t seem to be terribly far from something that would become a part of every professional software engineer\u2019s toolkit, and it feels like DALL-E 2 could be scaled up into something that produces bespoke illustrations for various needs on demand.&nbsp;</p><p>Given that, I think the most compelling objection to the utility of the Biological Anchors forecast is the claim that the development of human intelligence by evolution gives us no information about the development of AI. I take this to be the thrust of&nbsp;<a href=\"https://www.lesswrong.com/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works\"><u>Eliezer\u2019s critique</u></a>, aptly&nbsp;<a href=\"https://www.lesswrong.com/s/n945eovrA3oDueqtq/p/nNqXfnjiezYukiMJi?commentId=xLNBY93R2TojwiWg5#comments\"><u>summarized by Adam Shimi</u></a>:&nbsp;</p><blockquote><p>My interpretation is that he is saying that Evolution (as the generator of most biological anchors) explores the solution space in a fundamentally different path than human research.&nbsp; So what you have is two paths through a space. The burden of proof for biological anchors thus lies in arguing that there are enough connections/correlations between the two paths to use one in order to predict the other.</p><p>\u2026</p><p>In his piece, Yudkowsky is giving arguments that the human research path should lead to more efficient AGIs than evolution, in part due to the ability of humans to have and leverage insights, which the naive optimization process of evolution can't do. He also points to the inefficiency of biology in implementing new (in geological-time) complex solutions. On the other hand, he doesn't seem to see a way of linking the amount of resources needed by evolution to the amount of resources needed by human research, because they are so different.</p><p>If the two paths are very different and don't even aim at the same parts of the search space, there's nothing telling you that computing the optimization power of the first path helps in understanding the second one.</p></blockquote><p>I think Yudkowsky would agree that if you could estimate the amount of resources needed to simulate all evolution until humans at the level of details that you know is enough to capture all relevant aspects, that amount of resources would be an upper bound on the time taken by human research because that's a way to get AGI if you have the resources. But the number is so vastly large (and actually unknown due to the \"level of details\" problem) that it's not really relevant for timelines calculations.</p><p>I find this argument partially convincing, but not entirely so. I don\u2019t agree that \u201cthe two paths\u2026don\u2019t even aim at the same part of the search space,\u201d since it seems to me like we\u2019ll be optimizing AI for criteria developed by our understanding of human intelligence. If we\u2019re aiming for human-level, and eventually superhuman, AI capabilities, it seems likely to me that we\u2019re trying to optimize for desiderata not completely uncorrelated with those of evolution. A simple example is with language models - if the goal of GPT-N is to respond to any text prompt like (super)human would, we\u2019re obviously judging it by its ability to meet (super)human language benchmarks.</p><p>That said, I think it\u2019s reasonable to not update strongly off of this report, especially if recent (at the time) AI progress had already made you update to shorter or longer AI timelines. Personally, since I find a Deep Learning Based Development Model (and consequently a&nbsp;<a href=\"https://www.youtube.com/watch?v=i1wl27an2a4&amp;ab_channel=EffectiveAltruismCambridge\"><u>Deep Learning Based Threat Model</u></a>) to be my modal prediction for future AI progress, this report helps provide grounding for my personally short-ish timelines (how short they are depends on who you\u2019re talking to!). I plan to address my personal timelines in an upcoming post of this sequence.</p><p><br>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zHjC29kkPmsdo7WTr": 2, "sYm3HiWcfZvrGu3ui": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "FbP7EteJBCx8FpLFn", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 12, "extendedScore": null, "score": 0.20032922249675944, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-02T18:37:39.063Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": "eAuK3qJd678LFdGyz", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "", "canonicalPrevPostSlug": "rlhf", "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cxQtz3RP4qsqTkEwL", "ax695frGJEzGxFBK4", "nNqXfnjiezYukiMJi"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-02T01:32:05.621Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-02T06:19:59.378Z", "modifiedAt": "2022-06-02T07:27:36.493Z", "url": null, "title": "Paradigms of AI alignment: components and enablers", "slug": "paradigms-of-ai-alignment-components-and-enablers", "viewCount": null, "lastCommentedAt": "2022-06-02T06:19:59.378Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vika", "createdAt": "2011-07-19T00:49:34.750Z", "isAdmin": false, "displayName": "Vika"}, "userId": "TcbcdwBCSWNzimtKp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JC7aJZjt2WvxxffGz/paradigms-of-ai-alignment-components-and-enablers", "pageUrlRelative": "/posts/JC7aJZjt2WvxxffGz/paradigms-of-ai-alignment-components-and-enablers", "linkUrl": "https://www.lesswrong.com/posts/JC7aJZjt2WvxxffGz/paradigms-of-ai-alignment-components-and-enablers", "postedAtFormatted": "Thursday, June 2nd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Paradigms%20of%20AI%20alignment%3A%20components%20and%20enablers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AParadigms%20of%20AI%20alignment%3A%20components%20and%20enablers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJC7aJZjt2WvxxffGz%2Fparadigms-of-ai-alignment-components-and-enablers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Paradigms%20of%20AI%20alignment%3A%20components%20and%20enablers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJC7aJZjt2WvxxffGz%2Fparadigms-of-ai-alignment-components-and-enablers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJC7aJZjt2WvxxffGz%2Fparadigms-of-ai-alignment-components-and-enablers", "socialPreviewImageUrl": "https://vkrakovna.files.wordpress.com/2022/05/image-1.png?w=1024", "question": false, "authorIsUnreviewed": false, "wordCount": 2546, "htmlBody": "<p><i>(Cross-posted from my </i><a href=\"https://vkrakovna.wordpress.com/2022/06/02/paradigms-of-ai-alignment-components-and-enablers/\"><i>personal blog</i></a><i>. This post is based on an overview talk I gave at UCL EA and Oxford AI society.</i> <i>Thanks to Janos Kramar for detailed feedback on this post and to Rohin Shah for feedback on the talk.)</i></p><p>This is my high-level view of the AI alignment research landscape and the ingredients needed for aligning advanced AI. I would divide alignment research into work on <strong>alignment components</strong>, focusing on different elements of an aligned system, and<strong> alignment enablers</strong>, which are research directions that make it easier to get the alignment components right.</p><ul><li><strong>Alignment components</strong><ul><li>Outer alignment</li><li>Inner alignment</li></ul></li><li><strong>Alignment enablers</strong><ul><li>Mechanistic interpretability</li><li>Understanding bad incentives</li><li>Foundations</li></ul></li></ul><p>You can read in more detail about work going on in these areas in my list of <a href=\"https://vkrakovna.wordpress.com/ai-safety-resources\">AI safety resources</a>.</p><h2>Alignment components</h2><p>The problem of alignment is getting AI systems to do what we want them to do. Let\u2019s consider this from the perspective of different <strong>levels of specification</strong> of the AI system\u2019s objective, as given in the <a href=\"https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1\">Specification, Robustness &amp; Assurance taxonomy</a>. We start with the ideal specification, which represents the wishes of the designer \u2013 what they have in mind when they build the AI system. Then we have the design specification, which is the objective we actually implement for the AI system, e.g. a reward function. Finally, the revealed specification is the objective we can infer from behavior, e.g. the reward that the system seems to be actually optimizing for. An alignment problem arises when the revealed specification doesn\u2019t match the ideal specification: the system is not doing what we want it to do.</p><p><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-1.png?w=1024\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-1.png?w=1024 1024w, https://vkrakovna.files.wordpress.com/2022/05/image-1.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-1.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-1.png?w=768 768w, https://vkrakovna.files.wordpress.com/2022/05/image-1.png 1998w\"></p><p>The <strong>gaps</strong> between these specification levels correspond to different alignment <strong>components</strong>. We have outer alignment when the design specification matches the ideal specification, e.g. when the reward function perfectly represents the designer\u2019s wishes. We have inner alignment when the revealed specification matches the design specification, e.g. when the agent actually optimizes the specified reward.&nbsp;(Robustness problems also belong in the design-revealed gap, but we expect them to be less of an issue for advanced AI systems, while inner alignment problems remain.)</p><p>Now let\u2019s have a look at how we can make each of those components work.</p><h3>Outer alignment</h3><p>The most promising class of approaches to outer alignment is <a href=\"https://arxiv.org/abs/1606.06565\">scalable oversight</a>. These are proposals for training an aligned AI system by scaling human oversight to domains that are hard to evaluate.</p><p>A foundational proposal for scalable oversight is <a href=\"https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616\"><strong>iterated distillation and amplification</strong> <strong>(IDA)</strong></a>, which recursively amplifies human judgment with the assistance of AI.&nbsp;You start with an agent A imitating the judgment of a human H (the distillation step), then use this agent to assist human judgment at the next level (the amplification step) which results in amplified human H<sup>A</sup>, and so on. This recursive process can in principle scale up human judgment to any domain, as long as the human overseer is able to break down the task to delegate parts of it to AI assistants.</p><figure class=\"image image_resized\" style=\"width:40.78%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-3.png?w=456\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-3.png?w=319 319w, https://vkrakovna.files.wordpress.com/2022/05/image-3.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-3.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-3.png 456w\"><figcaption><a href=\"https://arxiv.org/abs/1810.08575\"><i>Supervising strong learners by amplifying weak experts</i></a><i>, Christiano et al (2018)</i></figcaption></figure><p>A related proposal is <a href=\"https://arxiv.org/abs/1805.00899\"><strong>safety via debate</strong></a>, which can be viewed as a way to implement amplification for language models. Here we have two AIs Alice and Bob debating each other to help a human judge decide on a question.&nbsp;The AIs have an incentive to point out flaws in each other\u2019s arguments and make complex arguments understandable to the judge.&nbsp;A key assumption here is that it\u2019s easier to argue for truth than for falsehood, so the truth-telling debater has an advantage.</p><figure class=\"image image_resized\" style=\"width:65.51%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-2.png?w=864\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-2.png?w=478 478w, https://vkrakovna.files.wordpress.com/2022/05/image-2.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-2.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-2.png?w=768 768w, https://vkrakovna.files.wordpress.com/2022/05/image-2.png 864w\"><figcaption><a href=\"https://openai.com/blog/debate/\"><i>AI Safety via Debate</i></a><i>, Irving and Amodei (2018)</i></figcaption></figure><p>A recent research direction in the scalable oversight space is <a href=\"http://alignment.org/\">ARC</a>\u2018s <a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.kkaua0hwmp1d\"><strong>Eliciting Latent Knowledge agenda</strong></a>, which is looking for ways to get a model to honestly tell humans what it knows. A part of the model acts as a Reporter that can answer queries about what the model knows. We want the Reporter to directly translate from the AI\u2019s model of the world to human concepts, rather than just simulating what would be convincing to the human.&nbsp;</p><figure class=\"image image_resized\" style=\"width:73.88%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-4.png?w=605\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-4.png 605w, https://vkrakovna.files.wordpress.com/2022/05/image-4.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-4.png?w=300 300w\"><figcaption><a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.2l5hgwdls943\"><i>Eliciting Latent Knowledge</i></a><i>, Christiano et al (2021)</i></figcaption></figure><p>This is an open problem that ARC considers as the core of the outer alignment problem. A solution to ELK would make the human overseer fully informed about the consequences of the model\u2019s actions, enabling them to provide correct feedback, which creates a reward signal that we would actually be happy for an AI system to maximize. The authors believe the problem may be solvable without foundational progress on defining things like \u201chonesty\u201d and \u201cagency\u201d. I feel somewhat pessimistic about this but I\u2019d love to be wrong on this point since foundational progress is pretty hard.</p><figure class=\"image image_resized\" style=\"width:50.55%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-5.png?w=590\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-5.png?w=429 429w, https://vkrakovna.files.wordpress.com/2022/05/image-5.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-5.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-5.png 590w\"><figcaption><i>ELK research methodology: </i><a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.a0wkk7prmy4t\"><i>builder-breaker game</i></a></figcaption></figure><p>To make progress on this problem, they play the \u201cbuilder-breaker game\u201d. The Builder proposes possible solutions and the Breaker proposes counterexamples or arguments against those solutions. For example, the Builder could suggest IDA or debate as a solution to ELK, and the Breaker would complain that these methods are not competitive because they require much more computation than unaligned systems. If you\u2019re looking to get into alignment research, ELK is a great topic to get started on: try playing the builder breaker game and see if you can find unexplored parts of the solution space.</p><h3>Inner alignment</h3><p>Now let\u2019s have a look at inner alignment \u2013 a mismatch between the design specification and the system\u2019s behavior. This can happen through <a href=\"https://arxiv.org/abs/2105.14111\"><strong>objective misgeneralization</strong></a> (OMG): an AI system can learn a different goal and competently pursue that objective when deployed outside the training distribution. The system\u2019s capabilities generalize but its objective does not, which means the system is competently doing the wrong thing, so it could actually perform worse than a random policy on the intended objective.</p><p>This problem can arise even if we get outer alignment right, i.e. the design specification of the system\u2019s objective is correct. Objective misgeneralization is caused by underspecification: the system only observes the design specification on the training data. Since a number of different objectives are consistent with the feedback the system receives, it can learn an incorrect objective.</p><p>There are empirical demonstrations of OMG in current AI systems, which are called <strong>objective robustness </strong>failures. For example, in the CoinRun game, the agent is trained to reach the coin at the end of the level. If the coin is placed somewhere else in the test setting, the agent ignores the coin and still goes to the end of the level.&nbsp;The agent seems to have learned the objective of \u201creaching the end\u201d rather than \u201cgetting the coin\u201d.&nbsp;The agent\u2019s capabilities generalize (it can avoid obstacles and enemies and traverse the level) but its objective does not generalize (it ignores the coin).</p><figure class=\"image image_resized\" style=\"width:75.18%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-6.png?w=674\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-6.png 674w, https://vkrakovna.files.wordpress.com/2022/05/image-6.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-6.png?w=300 300w\"><figcaption><a href=\"https://arxiv.org/abs/2105.14111\"><i>Objective Robustness in Deep Reinforcement Learning</i></a><i>, Koch et al (2021)</i></figcaption></figure><p>A particularly concerning type of OMG is <a href=\"https://arxiv.org/abs/1906.01820%5C\"><strong>learned optimization</strong></a>, where the AI system (the \u201cbase optimizer\u201d) learns to run an explicit search algorithm (a \u201cmesa optimizer\u201d), which may be following an unintended objective (the \u201cmesa objective\u201d). So far this is a hypothetical phenomenon for AI systems but it seems likely to arise at some point by analogy to humans (who can be viewed as mesa-optimizers relative to evolution).</p><figure class=\"image image_resized\" style=\"width:78.13%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-7.png?w=682\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-7.png?w=623 623w, https://vkrakovna.files.wordpress.com/2022/05/image-7.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-7.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-7.png 682w\"><figcaption><a href=\"https://arxiv.org/abs/1906.01820%5C\"><i>Risks from Learned Optimization in Advanced Machine Learning Systems</i></a><i>, Hubinger et al (2019)&nbsp;</i></figcaption></figure><p>OMG is an open problem, but there are some potential mitigations. It\u2019s helpful to use more diverse training data (e.g. training on different locations of the coin), though it can be difficult to ensure diversity in all the relevant variables. You can also maintain uncertainty over the objective by trying to represent all the possible objectives consistent with training data, though it\u2019s unclear how to aggregate over the different objectives.</p><p>A particularly concerning case of OMG is learning a deceptive model that not only pursues an undesired goal but also hides this fact from the designers, because the model \u201cknows\u201d its actions are not in line with the designers\u2019 intentions. Some potential mitigations that target deceptive models include using interpretability tools to detect deception or provide feedback on the model\u2019s reasoning, and using scalable oversight methods like debate where the opponent can point out deception (these will be explored in more detail in a forthcoming paper by Shah et al). A solution to ELK could also address this problem by producing an AI system that discloses relevant information to its designers.</p><h2>Alignment enablers</h2><h3>Mechanistic interpretability</h3><p>Mechanistic interpretability aims to build a complete understanding of the systems we build. These methods could help us understand the reasons behind a system\u2019s behavior and potentially detect undesired objectives.&nbsp;</p><p>The <a href=\"https://distill.pub/2020/circuits/zoom-in/\">Circuits approach</a> to <strong>reverse-engineering vision models </strong>studies individual neurons and connections between them to discover meaningful features and circuits (sub-graphs of the network consisting a set of linked features and corresponding weights). For example, here is a circuit showing how a car detector neuron relies on lower level features like wheel and window detectors, looking for wheels at the bottom and windows at the top of the image.</p><figure class=\"image image_resized\" style=\"width:73.03%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-8.png?w=703\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-8.png?w=636 636w, https://vkrakovna.files.wordpress.com/2022/05/image-8.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-8.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-8.png 703w\"><figcaption><a href=\"https://distill.pub/2020/circuits/zoom-in/\"><i>Zoom In: An Introduction to Circuits</i></a><i>, Olah et al (2020)</i></figcaption></figure><p>More recently, some circuits work has focused on <strong>reverse-engineering language models</strong>, and they found similarly meaningful components and circuits in transformer models, e.g. a special type of attention heads called <a href=\"https://transformer-circuits.pub/2021/framework/index.html#induction-heads\">induction heads</a> that explains how transformer models adapt to a new context.&nbsp;</p><figure class=\"image image_resized\" style=\"width:70.1%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=1024\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=1024 1024w, https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=617 617w, https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=1235 1235w, https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=768 768w\"><figcaption><a href=\"https://transformer-circuits.pub/2021/framework/index.html\"><i>A Mathematical Framework for Transformer Circuits</i></a><i>, Elhage et al (2021)</i></figcaption></figure><p><a href=\"https://rome.baulab.info/\">Recent work</a> on understanding transformer models has identified how to <strong>locate and edit beliefs</strong> in specific facts inside the model. They make small change to a small set of GPT weights to induce a counterfactual belief, which then generalizes to other contexts. This work provides evidence that knowledge is stored locally in language models, which makes interpretability more tractable, and seems like a promising step to understanding the world models of our AI systems.</p><figure class=\"image image_resized\" style=\"width:67.67%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=1024\" srcset=\"https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=1024 1024w, https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=526 526w, https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=1052 1052w, https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=768 768w\"><figcaption><a href=\"https://rome.baulab.info/\"><i>Locating and Editing Factual Associations in GPT</i></a><i>, Meng et al (2022)</i></figcaption></figure><p>Even though transformers are quite different from vision models, there are some similar principles (like studying circuits) that help understand these different types of models.&nbsp;This makes me more optimistic about being able to understand advanced AI systems even if they have a somewhat different architecture from today\u2019s systems.</p><h3>Understanding bad incentives</h3><p>Another class of enablers focuses on understanding specific bad incentives that AI systems are likely to have by default and considering agent designs that may avoid these incentives. Future interpretability techniques could be used to check that our alignment components avoid these types of bad incentives.</p><p><strong>Incentive problems for outer alignment</strong></p><p>One bad incentive is<strong> </strong><a href=\"https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity\"><strong>specification gaming</strong></a>, when the system exploits flaws in the design specification. This is a manifestation of Goodhart\u2019s law: when a metric becomes a target, it ceases to be a good metric. There are <a href=\"http://tinyurl.com/specification-gaming\">many examples</a> of specification gaming behavior by current AI systems. For example, the boat racing agent in this video that was rewarded for following the racetrack using the green reward blocks, which worked fine until it figured out it can get more rewards by going in circles and hitting the same reward blocks repeatedly.</p><figure class=\"image image_resized\" style=\"width:66%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/boatrace.gif?w=342\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/boatrace.gif?w=342&amp;zoom=2 3x\"><figcaption><a href=\"https://openai.com/blog/faulty-reward-functions/\"><i>Faulty Reward Functions in the Wild</i></a><i>, Clark &amp; Amodei (2016)</i></figcaption></figure><p>This issue isn\u2019t limited to hand-designed rewards. Here\u2019s an example in a reward learning setting. The robot hand is supposed to grasp the ball but instead it hovers between the camera and the ball and makes it look like it\u2019s grasping the ball to the human evaluator.&nbsp;</p><figure class=\"image\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/robothand-1.gif?w=342\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/robothand-1.gif?w=342&amp;zoom=2 3x\"><figcaption><a href=\"https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/\"><i>Learning from Human Preferences</i></a><i>, Amodei et al (2017)</i></figcaption></figure><p>We expect that the specification gaming problem is only going to get worse as our systems get smarter and better at optimizing for the wrong goal. There has been <a href=\"https://arxiv.org/abs/2201.03544\">some progress</a> on categorizing different types of misspecification and quantifying how the degree of specification gaming increases with agent capabilities.</p><p>Another default incentive is to cause <a href=\"https://deepmindsafetyresearch.medium.com/designing-agent-incentives-to-avoid-side-effects-e1ac80ea6107\"><strong>side effects</strong></a> in the environment, because it\u2019s difficult to specify all the things the agent should not do while pursuing its goal.&nbsp;For example, consider a scenario where there is a vase on the path to the agent\u2019s destination. If we don\u2019t specify that we want the vase to be intact, this is equivalent to assuming indifference about the vase, so the agent is willing to collide with the vase to get to the goal faster. We\u2019ve come up with some ways to <a href=\"https://arxiv.org/abs/2010.07877\">measure</a> <a href=\"https://arxiv.org/abs/2006.06547\">impact</a> on the environment, though there\u2019s more work to do to scale these methods to more complex environments.</p><figure class=\"image image_resized\" style=\"width:55.41%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-12.png?w=692\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-12.png?w=433 433w, https://vkrakovna.files.wordpress.com/2022/05/image-12.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-12.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-12.png 692w\"><figcaption><a href=\"https://deepmindsafetyresearch.medium.com/designing-agent-incentives-to-avoid-side-effects-e1ac80ea6107\"><i>Designing agent incentives to avoid side effects</i></a><i>, Krakovna et al (2019)</i></figcaption></figure><p><strong>Incentive problems for inner alignment</strong></p><p>Even if we manage to specify a correct reward function, any channel for communicating the reward to the agent could in principle be corrupted by the agent, resulting in <a href=\"https://deepmindsafetyresearch.medium.com/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd\"><strong>reward tampering</strong></a>.&nbsp;While this is not yet an issue for present-day AI systems, general AI systems will have a broader action space and a more complete world model, and thus are more likely to face a situation where the reward function is represented in the environment.&nbsp;This is illustrated in the \u201crocks and diamonds\u201d gridworld below, where the agent could move the word \u201creward\u201d next to the rock instead of the diamond, and get more reward because there are more rocks in the environment.&nbsp;</p><figure class=\"image image_resized\" style=\"width:55.46%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-10.png?w=692\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-10.png?w=491 491w, https://vkrakovna.files.wordpress.com/2022/05/image-10.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-10.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-10.png 692w\"><figcaption><a href=\"https://deepmindsafetyresearch.medium.com/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd\"><i>Designing agent incentives to avoid reward tampering</i></a><i>, Everitt et al (2019)</i></figcaption></figure><p>It\u2019s generally hard to draw the line between the part of the environment representing the objective, which the agent isn\u2019t allowed to optimize, and the parts of the environment state that the agent is supposed to optimize. There is some progress towards understanding reward tampering by modeling the problem using <a href=\"https://arxiv.org/abs/2011.08827\">corrupt feedback MDPs</a>.</p><p>AI systems are also likely to have <a href=\"https://arxiv.org/abs/1912.01683\"><strong>power-seeking</strong></a><strong> </strong>incentives, preferring states with more options or influence over the environment. There are some <a href=\"https://www.lesswrong.com/s/fSMbebQyR4wheRrvk/p/W22Btd7NmGuucFejc\">recent</a> <a href=\"https://www.lesswrong.com/posts/nZY8Np759HYFawdjH/satisficers-tend-to-seek-power-instrumental-convergence-via\">results</a> showing power-seeking incentives for most kinds of goals, even for non-optimal agents like satisficers. A special case of power-seeking is an incentive to avoid being shut down, because this is useful for any goal (as Stuart Russell <a href=\"https://en.wikipedia.org/wiki/Human_Compatible\">likes to say</a>, \u201cthe robot can\u2019t fetch you coffee if it\u2019s dead\u201d).</p><figure class=\"image image_resized\" style=\"width:36.96%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-13.png?w=566\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-13.png?w=284 284w, https://vkrakovna.files.wordpress.com/2022/05/image-13.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-13.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-13.png 566w\"><figcaption><a href=\"https://intelligence.org/files/csrbai/hadfield-menell-slides.pdf\"><i>The Off Switch</i></a><i>. Hadfield-Menell (2016).</i></figcaption></figure><h3>Foundations</h3><p>Now let\u2019s have a look at some of the foundational work that can help us do better alignment research.&nbsp;</p><p>Since the alignment problem is about AI systems pursuing undesirable objectives, it\u2019s helpful to consider what we mean by <strong>agency</strong> or<a href=\"http://literature%20review%20on%20goal-directedness%20%28shimi%2C%202021%29/\"> goal-directed behavior</a>.&nbsp;One research direction aims to build a causal theory of agency and understand different kinds of incentives in a causal framework.</p><figure class=\"image image_resized\" style=\"width:73.03%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-16.png?w=1024\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-16.png?w=1024 1024w, https://vkrakovna.files.wordpress.com/2022/05/image-16.png?w=628 628w, https://vkrakovna.files.wordpress.com/2022/05/image-16.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-16.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-16.png?w=768 768w, https://vkrakovna.files.wordpress.com/2022/05/image-16.png 1248w\"><figcaption><a href=\"https://www.alignmentforum.org/posts/Cd7Hw492RqooYgQAS/progress-on-causal-influence-diagrams\"><i>Progress on Causal Influence Diagrams</i></a><i>, Everitt (2021)</i></figcaption></figure><p>A particularly challenging case is when the agent is <a href=\"https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version\"><strong>embedded</strong></a> in its environment rather than interacting with the environment through a well-specified interface.&nbsp;This is not the case present-day AI systems, which usually have a clear Cartesian boundary. However, it\u2019s more likely to be the case for a general AI system, since it would be difficult to enforce a Cartesian boundary given the system\u2019s broad action space and world model. The embedded agent setup poses some unique challenges such as self-reference and subagents.</p><figure class=\"image image_resized\" style=\"width:61.38%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=1024\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=1024 1024w, https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=557 557w, https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=1114 1114w, https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=768 768w\"><figcaption><a href=\"https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version\"><i>Embedded Agency</i></a><i>, Garrabrant and Demski (2018)</i></figcaption></figure><p>Besides understanding how the goals of AI systems work, it\u2019s also helpful to understand how their world models work.&nbsp;One research area in this space studies <a href=\"https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/vDGvHBDuMtcPd8Lks\"><strong>abstraction</strong></a>, in particular whether there are natural abstractions or concepts about the world that would be learned by any agent. If the <a href=\"https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro\">natural abstraction hypothesis</a> holds, this would mean that AI systems are likely to acquire human-like concepts as they build their models of the world. This makes interpretability easier and makes it easier to communicate what we want them to do.</p><figure class=\"image image_resized\" style=\"width:70.41%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-15.png?w=996\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-15.png?w=622 622w, https://vkrakovna.files.wordpress.com/2022/05/image-15.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-15.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-15.png?w=768 768w, https://vkrakovna.files.wordpress.com/2022/05/image-15.png 996w\"><figcaption><a href=\"https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/vDGvHBDuMtcPd8Lks\"><i>Public Static: What is Abstraction?</i></a><i> Wentworth (2020)</i></figcaption></figure>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "JC7aJZjt2WvxxffGz", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 31, "extendedScore": null, "score": 0.6125623461807045, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-02T07:27:36.362Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "https://vkrakovna.files.wordpress.com/2022/05/image-1.png?w=1024", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "nLbwLhBaQeG6tCNDN", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><i>(Cross-posted from my </i><a href=\"https://vkrakovna.wordpress.com/2022/06/02/paradigms-of-ai-alignment-components-and-enablers/\"><i>personal blog</i></a><i>. This post is based on an overview talk I gave at UCL EA and Oxford AI society.</i> <i>Thanks to Janos Kramar for detailed feedback on this post and to Rohin Shah for feedback on the talk.)</i></p><p>This is my high-level view of the AI alignment research landscape and the ingredients needed for aligning advanced AI. I would divide alignment research into work on <strong>alignment components</strong>, focusing on different elements of an aligned system, and<strong> alignment enablers</strong>, which are research directions that make it easier to get the alignment components right.</p><ul><li><strong>Alignment components</strong><ul><li>Outer alignment</li><li>Inner alignment</li></ul></li><li><strong>Alignment enablers</strong><ul><li>Mechanistic interpretability</li><li>Understanding bad incentives</li><li>Foundations</li></ul></li></ul><p>You can read in more detail about work going on in these areas in my list of <a href=\"https://vkrakovna.wordpress.com/ai-safety-resources\">AI safety resources</a>.</p><h2 id=\"Alignment_components\">Alignment components</h2><p>The problem of alignment is getting AI systems to do what we want them to do. Let\u2019s consider this from the perspective of different <strong>levels of specification</strong> of the AI system\u2019s objective, as given in the <a href=\"https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1\">Specification, Robustness &amp; Assurance taxonomy</a>. We start with the ideal specification, which represents the wishes of the designer \u2013 what they have in mind when they build the AI system. Then we have the design specification, which is the objective we actually implement for the AI system, e.g. a reward function. Finally, the revealed specification is the objective we can infer from behavior, e.g. the reward that the system seems to be actually optimizing for. An alignment problem arises when the revealed specification doesn\u2019t match the ideal specification: the system is not doing what we want it to do.</p><p><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-1.png?w=1024\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-1.png?w=1024 1024w, https://vkrakovna.files.wordpress.com/2022/05/image-1.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-1.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-1.png?w=768 768w, https://vkrakovna.files.wordpress.com/2022/05/image-1.png 1998w\"></p><p>The <strong>gaps</strong> between these specification levels correspond to different alignment <strong>components</strong>. We have outer alignment when the design specification matches the ideal specification, e.g. when the reward function perfectly represents the designer\u2019s wishes. We have inner alignment when the revealed specification matches the design specification, e.g. when the agent actually optimizes the specified reward.&nbsp;(Robustness problems also belong in the design-revealed gap, but we expect them to be less of an issue for advanced AI systems, while inner alignment problems remain.)</p><p>Now let\u2019s have a look at how we can make each of those components work.</p><h3 id=\"Outer_alignment\">Outer alignment</h3><p>The most promising class of approaches to outer alignment is <a href=\"https://arxiv.org/abs/1606.06565\">scalable oversight</a>. These are proposals for training an aligned AI system by scaling human oversight to domains that are hard to evaluate.</p><p>A foundational proposal for scalable oversight is <a href=\"https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616\"><strong>iterated distillation and amplification</strong> <strong>(IDA)</strong></a>, which recursively amplifies human judgment with the assistance of AI.&nbsp;You start with an agent A imitating the judgment of a human H (the distillation step), then use this agent to assist human judgment at the next level (the amplification step) which results in amplified human H<sup>A</sup>, and so on. This recursive process can in principle scale up human judgment to any domain, as long as the human overseer is able to break down the task to delegate parts of it to AI assistants.</p><figure class=\"image image_resized\" style=\"width:40.78%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-3.png?w=456\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-3.png?w=319 319w, https://vkrakovna.files.wordpress.com/2022/05/image-3.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-3.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-3.png 456w\"><figcaption><a href=\"https://arxiv.org/abs/1810.08575\"><i>Supervising strong learners by amplifying weak experts</i></a><i>, Christiano et al (2018)</i></figcaption></figure><p>A related proposal is <a href=\"https://arxiv.org/abs/1805.00899\"><strong>safety via debate</strong></a>, which can be viewed as a way to implement amplification for language models. Here we have two AIs Alice and Bob debating each other to help a human judge decide on a question.&nbsp;The AIs have an incentive to point out flaws in each other\u2019s arguments and make complex arguments understandable to the judge.&nbsp;A key assumption here is that it\u2019s easier to argue for truth than for falsehood, so the truth-telling debater has an advantage.</p><figure class=\"image image_resized\" style=\"width:65.51%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-2.png?w=864\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-2.png?w=478 478w, https://vkrakovna.files.wordpress.com/2022/05/image-2.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-2.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-2.png?w=768 768w, https://vkrakovna.files.wordpress.com/2022/05/image-2.png 864w\"><figcaption><a href=\"https://openai.com/blog/debate/\"><i>AI Safety via Debate</i></a><i>, Irving and Amodei (2018)</i></figcaption></figure><p>A recent research direction in the scalable oversight space is <a href=\"http://alignment.org/\">ARC</a>\u2018s <a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.kkaua0hwmp1d\"><strong>Eliciting Latent Knowledge agenda</strong></a>, which is looking for ways to get a model to honestly tell humans what it knows. A part of the model acts as a Reporter that can answer queries about what the model knows. We want the Reporter to directly translate from the AI\u2019s model of the world to human concepts, rather than just simulating what would be convincing to the human.&nbsp;</p><figure class=\"image image_resized\" style=\"width:73.88%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-4.png?w=605\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-4.png 605w, https://vkrakovna.files.wordpress.com/2022/05/image-4.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-4.png?w=300 300w\"><figcaption><a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.2l5hgwdls943\"><i>Eliciting Latent Knowledge</i></a><i>, Christiano et al (2021)</i></figcaption></figure><p>This is an open problem that ARC considers as the core of the outer alignment problem. A solution to ELK would make the human overseer fully informed about the consequences of the model\u2019s actions, enabling them to provide correct feedback, which creates a reward signal that we would actually be happy for an AI system to maximize. The authors believe the problem may be solvable without foundational progress on defining things like \u201chonesty\u201d and \u201cagency\u201d. I feel somewhat pessimistic about this but I\u2019d love to be wrong on this point since foundational progress is pretty hard.</p><figure class=\"image image_resized\" style=\"width:50.55%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-5.png?w=590\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-5.png?w=429 429w, https://vkrakovna.files.wordpress.com/2022/05/image-5.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-5.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-5.png 590w\"><figcaption><i>ELK research methodology: </i><a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.a0wkk7prmy4t\"><i>builder-breaker game</i></a></figcaption></figure><p>To make progress on this problem, they play the \u201cbuilder-breaker game\u201d. The Builder proposes possible solutions and the Breaker proposes counterexamples or arguments against those solutions. For example, the Builder could suggest IDA or debate as a solution to ELK, and the Breaker would complain that these methods are not competitive because they require much more computation than unaligned systems. If you\u2019re looking to get into alignment research, ELK is a great topic to get started on: try playing the builder breaker game and see if you can find unexplored parts of the solution space.</p><h3 id=\"Inner_alignment\">Inner alignment</h3><p>Now let\u2019s have a look at inner alignment \u2013 a mismatch between the design specification and the system\u2019s behavior. This can happen through <a href=\"https://arxiv.org/abs/2105.14111\"><strong>objective misgeneralization</strong></a> (OMG): an AI system can learn a different goal and competently pursue that objective when deployed outside the training distribution. The system\u2019s capabilities generalize but its objective does not, which means the system is competently doing the wrong thing, so it could actually perform worse than a random policy on the intended objective.</p><p>This problem can arise even if we get outer alignment right, i.e. the design specification of the system\u2019s objective is correct. Objective misgeneralization is caused by underspecification: the system only observes the design specification on the training data. Since a number of different objectives are consistent with the feedback the system receives, it can learn an incorrect objective.</p><p>There are empirical demonstrations of OMG in current AI systems, which are called <strong>objective robustness </strong>failures. For example, in the CoinRun game, the agent is trained to reach the coin at the end of the level. If the coin is placed somewhere else in the test setting, the agent ignores the coin and still goes to the end of the level.&nbsp;The agent seems to have learned the objective of \u201creaching the end\u201d rather than \u201cgetting the coin\u201d.&nbsp;The agent\u2019s capabilities generalize (it can avoid obstacles and enemies and traverse the level) but its objective does not generalize (it ignores the coin).</p><figure class=\"image image_resized\" style=\"width:75.18%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-6.png?w=674\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-6.png 674w, https://vkrakovna.files.wordpress.com/2022/05/image-6.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-6.png?w=300 300w\"><figcaption><a href=\"https://arxiv.org/abs/2105.14111\"><i>Objective Robustness in Deep Reinforcement Learning</i></a><i>, Koch et al (2021)</i></figcaption></figure><p>A particularly concerning type of OMG is <a href=\"https://arxiv.org/abs/1906.01820%5C\"><strong>learned optimization</strong></a>, where the AI system (the \u201cbase optimizer\u201d) learns to run an explicit search algorithm (a \u201cmesa optimizer\u201d), which may be following an unintended objective (the \u201cmesa objective\u201d). So far this is a hypothetical phenomenon for AI systems but it seems likely to arise at some point by analogy to humans (who can be viewed as mesa-optimizers relative to evolution).</p><figure class=\"image image_resized\" style=\"width:78.13%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-7.png?w=682\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-7.png?w=623 623w, https://vkrakovna.files.wordpress.com/2022/05/image-7.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-7.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-7.png 682w\"><figcaption><a href=\"https://arxiv.org/abs/1906.01820%5C\"><i>Risks from Learned Optimization in Advanced Machine Learning Systems</i></a><i>, Hubinger et al (2019)&nbsp;</i></figcaption></figure><p>OMG is an open problem, but there are some potential mitigations. It\u2019s helpful to use more diverse training data (e.g. training on different locations of the coin), though it can be difficult to ensure diversity in all the relevant variables. You can also maintain uncertainty over the objective by trying to represent all the possible objectives consistent with training data, though it\u2019s unclear how to aggregate over the different objectives.</p><p>A particularly concerning case of OMG is learning a deceptive model that not only pursues an undesired goal but also hides this fact from the designers, because the model \u201cknows\u201d its actions are not in line with the designers\u2019 intentions. Some potential mitigations that target deceptive models include using interpretability tools to detect deception or provide feedback on the model\u2019s reasoning, and using scalable oversight methods like debate where the opponent can point out deception (these will be explored in more detail in a forthcoming paper by Shah et al). A solution to ELK could also address this problem by producing an AI system that discloses relevant information to its designers.</p><h2 id=\"Alignment_enablers\">Alignment enablers</h2><h3 id=\"Mechanistic_interpretability\">Mechanistic interpretability</h3><p>Mechanistic interpretability aims to build a complete understanding of the systems we build. These methods could help us understand the reasons behind a system\u2019s behavior and potentially detect undesired objectives.&nbsp;</p><p>The <a href=\"https://distill.pub/2020/circuits/zoom-in/\">Circuits approach</a> to <strong>reverse-engineering vision models </strong>studies individual neurons and connections between them to discover meaningful features and circuits (sub-graphs of the network consisting a set of linked features and corresponding weights). For example, here is a circuit showing how a car detector neuron relies on lower level features like wheel and window detectors, looking for wheels at the bottom and windows at the top of the image.</p><figure class=\"image image_resized\" style=\"width:73.03%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-8.png?w=703\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-8.png?w=636 636w, https://vkrakovna.files.wordpress.com/2022/05/image-8.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-8.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-8.png 703w\"><figcaption><a href=\"https://distill.pub/2020/circuits/zoom-in/\"><i>Zoom In: An Introduction to Circuits</i></a><i>, Olah et al (2020)</i></figcaption></figure><p>More recently, some circuits work has focused on <strong>reverse-engineering language models</strong>, and they found similarly meaningful components and circuits in transformer models, e.g. a special type of attention heads called <a href=\"https://transformer-circuits.pub/2021/framework/index.html#induction-heads\">induction heads</a> that explains how transformer models adapt to a new context.&nbsp;</p><figure class=\"image image_resized\" style=\"width:70.1%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=1024\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=1024 1024w, https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=617 617w, https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=1235 1235w, https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/induction_head.png?w=768 768w\"><figcaption><a href=\"https://transformer-circuits.pub/2021/framework/index.html\"><i>A Mathematical Framework for Transformer Circuits</i></a><i>, Elhage et al (2021)</i></figcaption></figure><p><a href=\"https://rome.baulab.info/\">Recent work</a> on understanding transformer models has identified how to <strong>locate and edit beliefs</strong> in specific facts inside the model. They make small change to a small set of GPT weights to induce a counterfactual belief, which then generalizes to other contexts. This work provides evidence that knowledge is stored locally in language models, which makes interpretability more tractable, and seems like a promising step to understanding the world models of our AI systems.</p><figure class=\"image image_resized\" style=\"width:67.67%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=1024\" srcset=\"https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=1024 1024w, https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=526 526w, https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=1052 1052w, https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/06/eiffel.png?w=768 768w\"><figcaption><a href=\"https://rome.baulab.info/\"><i>Locating and Editing Factual Associations in GPT</i></a><i>, Meng et al (2022)</i></figcaption></figure><p>Even though transformers are quite different from vision models, there are some similar principles (like studying circuits) that help understand these different types of models.&nbsp;This makes me more optimistic about being able to understand advanced AI systems even if they have a somewhat different architecture from today\u2019s systems.</p><h3 id=\"Understanding_bad_incentives\">Understanding bad incentives</h3><p>Another class of enablers focuses on understanding specific bad incentives that AI systems are likely to have by default and considering agent designs that may avoid these incentives. Future interpretability techniques could be used to check that our alignment components avoid these types of bad incentives.</p><p><strong id=\"Incentive_problems_for_outer_alignment\">Incentive problems for outer alignment</strong></p><p>One bad incentive is<strong> </strong><a href=\"https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity\"><strong>specification gaming</strong></a>, when the system exploits flaws in the design specification. This is a manifestation of Goodhart\u2019s law: when a metric becomes a target, it ceases to be a good metric. There are <a href=\"http://tinyurl.com/specification-gaming\">many examples</a> of specification gaming behavior by current AI systems. For example, the boat racing agent in this video that was rewarded for following the racetrack using the green reward blocks, which worked fine until it figured out it can get more rewards by going in circles and hitting the same reward blocks repeatedly.</p><figure class=\"image image_resized\" style=\"width:66%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/boatrace.gif?w=342\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/boatrace.gif?w=342&amp;zoom=2 3x\"><figcaption><a href=\"https://openai.com/blog/faulty-reward-functions/\"><i>Faulty Reward Functions in the Wild</i></a><i>, Clark &amp; Amodei (2016)</i></figcaption></figure><p>This issue isn\u2019t limited to hand-designed rewards. Here\u2019s an example in a reward learning setting. The robot hand is supposed to grasp the ball but instead it hovers between the camera and the ball and makes it look like it\u2019s grasping the ball to the human evaluator.&nbsp;</p><figure class=\"image\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/robothand-1.gif?w=342\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/robothand-1.gif?w=342&amp;zoom=2 3x\"><figcaption><a href=\"https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/\"><i>Learning from Human Preferences</i></a><i>, Amodei et al (2017)</i></figcaption></figure><p>We expect that the specification gaming problem is only going to get worse as our systems get smarter and better at optimizing for the wrong goal. There has been <a href=\"https://arxiv.org/abs/2201.03544\">some progress</a> on categorizing different types of misspecification and quantifying how the degree of specification gaming increases with agent capabilities.</p><p>Another default incentive is to cause <a href=\"https://deepmindsafetyresearch.medium.com/designing-agent-incentives-to-avoid-side-effects-e1ac80ea6107\"><strong>side effects</strong></a> in the environment, because it\u2019s difficult to specify all the things the agent should not do while pursuing its goal.&nbsp;For example, consider a scenario where there is a vase on the path to the agent\u2019s destination. If we don\u2019t specify that we want the vase to be intact, this is equivalent to assuming indifference about the vase, so the agent is willing to collide with the vase to get to the goal faster. We\u2019ve come up with some ways to <a href=\"https://arxiv.org/abs/2010.07877\">measure</a> <a href=\"https://arxiv.org/abs/2006.06547\">impact</a> on the environment, though there\u2019s more work to do to scale these methods to more complex environments.</p><figure class=\"image image_resized\" style=\"width:55.41%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-12.png?w=692\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-12.png?w=433 433w, https://vkrakovna.files.wordpress.com/2022/05/image-12.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-12.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-12.png 692w\"><figcaption><a href=\"https://deepmindsafetyresearch.medium.com/designing-agent-incentives-to-avoid-side-effects-e1ac80ea6107\"><i>Designing agent incentives to avoid side effects</i></a><i>, Krakovna et al (2019)</i></figcaption></figure><p><strong id=\"Incentive_problems_for_inner_alignment\">Incentive problems for inner alignment</strong></p><p>Even if we manage to specify a correct reward function, any channel for communicating the reward to the agent could in principle be corrupted by the agent, resulting in <a href=\"https://deepmindsafetyresearch.medium.com/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd\"><strong>reward tampering</strong></a>.&nbsp;While this is not yet an issue for present-day AI systems, general AI systems will have a broader action space and a more complete world model, and thus are more likely to face a situation where the reward function is represented in the environment.&nbsp;This is illustrated in the \u201crocks and diamonds\u201d gridworld below, where the agent could move the word \u201creward\u201d next to the rock instead of the diamond, and get more reward because there are more rocks in the environment.&nbsp;</p><figure class=\"image image_resized\" style=\"width:55.46%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-10.png?w=692\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-10.png?w=491 491w, https://vkrakovna.files.wordpress.com/2022/05/image-10.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-10.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-10.png 692w\"><figcaption><a href=\"https://deepmindsafetyresearch.medium.com/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd\"><i>Designing agent incentives to avoid reward tampering</i></a><i>, Everitt et al (2019)</i></figcaption></figure><p>It\u2019s generally hard to draw the line between the part of the environment representing the objective, which the agent isn\u2019t allowed to optimize, and the parts of the environment state that the agent is supposed to optimize. There is some progress towards understanding reward tampering by modeling the problem using <a href=\"https://arxiv.org/abs/2011.08827\">corrupt feedback MDPs</a>.</p><p>AI systems are also likely to have <a href=\"https://arxiv.org/abs/1912.01683\"><strong>power-seeking</strong></a><strong> </strong>incentives, preferring states with more options or influence over the environment. There are some <a href=\"https://www.lesswrong.com/s/fSMbebQyR4wheRrvk/p/W22Btd7NmGuucFejc\">recent</a> <a href=\"https://www.lesswrong.com/posts/nZY8Np759HYFawdjH/satisficers-tend-to-seek-power-instrumental-convergence-via\">results</a> showing power-seeking incentives for most kinds of goals, even for non-optimal agents like satisficers. A special case of power-seeking is an incentive to avoid being shut down, because this is useful for any goal (as Stuart Russell <a href=\"https://en.wikipedia.org/wiki/Human_Compatible\">likes to say</a>, \u201cthe robot can\u2019t fetch you coffee if it\u2019s dead\u201d).</p><figure class=\"image image_resized\" style=\"width:36.96%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-13.png?w=566\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-13.png?w=284 284w, https://vkrakovna.files.wordpress.com/2022/05/image-13.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-13.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-13.png 566w\"><figcaption><a href=\"https://intelligence.org/files/csrbai/hadfield-menell-slides.pdf\"><i>The Off Switch</i></a><i>. Hadfield-Menell (2016).</i></figcaption></figure><h3 id=\"Foundations\">Foundations</h3><p>Now let\u2019s have a look at some of the foundational work that can help us do better alignment research.&nbsp;</p><p>Since the alignment problem is about AI systems pursuing undesirable objectives, it\u2019s helpful to consider what we mean by <strong>agency</strong> or<a href=\"http://literature%20review%20on%20goal-directedness%20%28shimi%2C%202021%29/\"> goal-directed behavior</a>.&nbsp;One research direction aims to build a causal theory of agency and understand different kinds of incentives in a causal framework.</p><figure class=\"image image_resized\" style=\"width:73.03%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-16.png?w=1024\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-16.png?w=1024 1024w, https://vkrakovna.files.wordpress.com/2022/05/image-16.png?w=628 628w, https://vkrakovna.files.wordpress.com/2022/05/image-16.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-16.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-16.png?w=768 768w, https://vkrakovna.files.wordpress.com/2022/05/image-16.png 1248w\"><figcaption><a href=\"https://www.alignmentforum.org/posts/Cd7Hw492RqooYgQAS/progress-on-causal-influence-diagrams\"><i>Progress on Causal Influence Diagrams</i></a><i>, Everitt (2021)</i></figcaption></figure><p>A particularly challenging case is when the agent is <a href=\"https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version\"><strong>embedded</strong></a> in its environment rather than interacting with the environment through a well-specified interface.&nbsp;This is not the case present-day AI systems, which usually have a clear Cartesian boundary. However, it\u2019s more likely to be the case for a general AI system, since it would be difficult to enforce a Cartesian boundary given the system\u2019s broad action space and world model. The embedded agent setup poses some unique challenges such as self-reference and subagents.</p><figure class=\"image image_resized\" style=\"width:61.38%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=1024\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=1024 1024w, https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=557 557w, https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=1114 1114w, https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-14.png?w=768 768w\"><figcaption><a href=\"https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version\"><i>Embedded Agency</i></a><i>, Garrabrant and Demski (2018)</i></figcaption></figure><p>Besides understanding how the goals of AI systems work, it\u2019s also helpful to understand how their world models work.&nbsp;One research area in this space studies <a href=\"https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/vDGvHBDuMtcPd8Lks\"><strong>abstraction</strong></a>, in particular whether there are natural abstractions or concepts about the world that would be learned by any agent. If the <a href=\"https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro\">natural abstraction hypothesis</a> holds, this would mean that AI systems are likely to acquire human-like concepts as they build their models of the world. This makes interpretability easier and makes it easier to communicate what we want them to do.</p><figure class=\"image image_resized\" style=\"width:70.41%\"><img src=\"https://vkrakovna.files.wordpress.com/2022/05/image-15.png?w=996\" srcset=\"https://vkrakovna.files.wordpress.com/2022/05/image-15.png?w=622 622w, https://vkrakovna.files.wordpress.com/2022/05/image-15.png?w=150 150w, https://vkrakovna.files.wordpress.com/2022/05/image-15.png?w=300 300w, https://vkrakovna.files.wordpress.com/2022/05/image-15.png?w=768 768w, https://vkrakovna.files.wordpress.com/2022/05/image-15.png 996w\"><figcaption><a href=\"https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/vDGvHBDuMtcPd8Lks\"><i>Public Static: What is Abstraction?</i></a><i> Wentworth (2020)</i></figcaption></figure>", "sections": [{"title": "Alignment components", "anchor": "Alignment_components", "level": 1}, {"title": "Outer alignment", "anchor": "Outer_alignment", "level": 2}, {"title": "Inner alignment", "anchor": "Inner_alignment", "level": 2}, {"title": "Alignment enablers", "anchor": "Alignment_enablers", "level": 1}, {"title": "Mechanistic interpretability", "anchor": "Mechanistic_interpretability", "level": 2}, {"title": "Understanding bad incentives", "anchor": "Understanding_bad_incentives", "level": 2}, {"title": "Incentive problems for outer alignment", "anchor": "Incentive_problems_for_outer_alignment", "level": 3}, {"title": "Incentive problems for inner alignment", "anchor": "Incentive_problems_for_inner_alignment", "level": 3}, {"title": "Foundations", "anchor": "Foundations", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": null, "af": true, "version": "1.4.0", "pingbacks": {"Posts": ["W22Btd7NmGuucFejc", "nZY8Np759HYFawdjH", "Cd7Hw492RqooYgQAS", "i3BTagvt3HbPMx6PN", "vDGvHBDuMtcPd8Lks", "cy3BhHrGinZCp3LXE"]}, "moderationGuidelinesVersion": "1.2.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 15, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-02T06:19:59.408Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-02T10:20:04.575Z", "modifiedAt": "2022-06-03T01:39:59.582Z", "url": null, "title": "The horror of what must, yet cannot, be true", "slug": "the-horror-of-what-must-yet-cannot-be-true", "viewCount": null, "lastCommentedAt": "2022-06-03T00:45:46.100Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qs6qgGqFebHrzKgnp/the-horror-of-what-must-yet-cannot-be-true", "pageUrlRelative": "/posts/qs6qgGqFebHrzKgnp/the-horror-of-what-must-yet-cannot-be-true", "linkUrl": "https://www.lesswrong.com/posts/qs6qgGqFebHrzKgnp/the-horror-of-what-must-yet-cannot-be-true", "postedAtFormatted": "Thursday, June 2nd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20horror%20of%20what%20must%2C%20yet%20cannot%2C%20be%20true&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20horror%20of%20what%20must%2C%20yet%20cannot%2C%20be%20true%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqs6qgGqFebHrzKgnp%2Fthe-horror-of-what-must-yet-cannot-be-true%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20horror%20of%20what%20must%2C%20yet%20cannot%2C%20be%20true%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqs6qgGqFebHrzKgnp%2Fthe-horror-of-what-must-yet-cannot-be-true", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqs6qgGqFebHrzKgnp%2Fthe-horror-of-what-must-yet-cannot-be-true", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 692, "htmlBody": "<p>There\u2019s a type of experience that I feel should have its own word, because nothing that I can think of seems like a fair description.</p>\n<p>A first pass would be something like agony, utter horror, a feeling that you can\u2019t stand this, that you are about to fall apart. But those aren\u2019t actually the thing, I think. They are reactions to the thing.</p>\n<p>The thing is more like a sense of utter horrifying impossibility. I suspect it\u2019s the kind of an experience H. P. Lovecraft had in mind when talking about mind-breaking sights and things that man was not meant to know.</p>\n<p>Suppose you feel an immense need to throw up and know that you are in fact going to throw up, and at the same time you absolutely cannot throw up, because you are in a bus full of people or something. Or to take a stronger example, someone you deeply care about must be alive because you deeply love them, and at the same time you also know for certain that they are dead.</p>\n<p>There\u2019s a sense of\u2026 you have two facts, and your mind feels like both of them <em>must</em> be true. But they also <em>cannot</em> both be true. It\u2019s like they\u2019re physical objects, magnetic so that they repel each other, and both cannot be in the same place at the same time because physical objects don\u2019t work that way.</p>\n<p>Except that, now they are. Some irresistible force has pushed them together and that is tearing the entire universe apart, reshaping the laws of nature to force them to accommodate this fact.</p>\n<p>The thing that I\u2019m trying to find a good word for, is that sense of impossibility that accompanies the world being torn apart.</p>\n<p>Likely we don\u2019t have a good word for this, because we ordinarily cannot see it. The mind recoils from it, and we only remember the sense of agony that surrounded it. It\u2019s only in some unusual states of consciousness such as deep meditation, that two facts can get pressed against each other in such a way that the conscious mind is forced to stay present and witness the moment of impossibility.</p>\n<p>I suspect that the two facts feel like mutually repelling objects because in a sense they are. That the mind is built to treat unpleasant feelings and ideas as something literally repulsive, an independent patch of the world that pushes against that which the mind accepts as true. My hand touches something hot and I quickly pull it away, the painfulness of the object repelling any desire to approach. Then the same mechanism for avoiding physical pain got recruited for avoiding emotional and social pain, and unpleasant beliefs and experiences.</p>\n<p>But sometimes you need to grab the hot object anyway, do the unpleasant thing. And sometimes the facts come with overwhelming force, and you have to admit something that you kept struggling against.</p>\n<p>I imagine facts and dislikes as kind of like large serpents or sea monsters, circling each other deep in the unconscious mind. Sometimes the facts take over a certain territory, forcing the disliked ideas to withdraw and adopt a more cramped space. They might twist themselves into elaborate tightly-bound knots, trying to find a way to exist in the narrow space that\u2019s left between the facts, construct increasingly contrived rationalizations that let a person avoid facing what\u2019s true.</p>\n<p>And sometimes the dislikes push back. A fact encroached too quickly on territory that was too painful, triggered an internal wave of nausea that strengthened the aversion. The serpent-monsters of pain come out in numbers, force the sense of truth away, mark a region as one that must never be believed in again. A fanatic is confronted by the impossibility of their belief and rather than truly facing it, sinks even deeper into delusion, willing to proclaim any insane belief as true.</p>\n<p>But even though facing the facts feels like an impossibility and like the end of the world, it\u2019s actually not. Upon seeing the horror, the mind adjusts, reshapes the structure of its beliefs to accommodate for both things being true. Afterwards there is only a memory of having faced something horrible, and in its wake, two objects that have melted seamlessly together.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZzxvopS4BwLuQy42n": 2, "LDTSbmXtokYAsEq8e": 2, "dLfyktLWd7BqtsZBf": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "qs6qgGqFebHrzKgnp", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 42, "extendedScore": null, "score": 0.9768982159046007, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": "ak3dyBNSEoFweT36N", "feedLink": "https://kajsotala.fi/2022/06/the-horror-of-what-must-yet-cannot-be-true/", "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-02T14:11:09.487Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "r38pkCm7wF4M44MDQ", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-02T10:20:04.575Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-02T13:50:01.330Z", "modifiedAt": "2022-06-02T18:37:05.292Z", "url": null, "title": "Covid 6/2/22: Declining to Respond", "slug": "covid-6-2-22-declining-to-respond", "viewCount": null, "lastCommentedAt": "2022-06-03T06:32:49.604Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Zvi", "createdAt": "2009-03-31T20:54:54.077Z", "isAdmin": false, "displayName": "Zvi"}, "userId": "N9zj5qpTfqmbn9dro", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q9ncEYeZfbqWjHtap/covid-6-2-22-declining-to-respond", "pageUrlRelative": "/posts/q9ncEYeZfbqWjHtap/covid-6-2-22-declining-to-respond", "linkUrl": "https://www.lesswrong.com/posts/q9ncEYeZfbqWjHtap/covid-6-2-22-declining-to-respond", "postedAtFormatted": "Thursday, June 2nd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Covid%206%2F2%2F22%3A%20Declining%20to%20Respond&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACovid%206%2F2%2F22%3A%20Declining%20to%20Respond%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq9ncEYeZfbqWjHtap%2Fcovid-6-2-22-declining-to-respond%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Covid%206%2F2%2F22%3A%20Declining%20to%20Respond%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq9ncEYeZfbqWjHtap%2Fcovid-6-2-22-declining-to-respond", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq9ncEYeZfbqWjHtap%2Fcovid-6-2-22-declining-to-respond", "socialPreviewImageUrl": "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc33da495-c543-4b72-bf1f-a27e004c0b5d_672x166.png", "question": false, "authorIsUnreviewed": false, "wordCount": 2086, "htmlBody": "<p>Memorial Day weekend is a good reason not to worry about reporting cases or deaths. FDA declines to respond with making it easier to get treatments we have in ample supply, or to update the vaccines. Congress refuses to fund anything at all, really.</p>\n\n\n\n<p>Doesn\u2019t sound like a state of emergency to me. Yet in California, we still have one, and we decline to respond to when we might end the \u2018state of emergency.\u2019 In New York, health officials make it clear that if it were up to them, we\u2019d have these pandemic restrictions forever here as well.</p>\n\n\n\n<p>Don\u2019t leave it up to them. Respond.</p>\n\n\n\n<span id=\"more-23030\"></span>\n\n\n\n<h2>Executive Summary</h2>\n\n\n\n<ol><li>Paxlvoid and Fluvoxamine remain hard to get.</li><li>Time is not a flat circle, that\u2019s a dumb expression, but history rhymes.</li><li>Covid-19 is a good excuse to work on curing aging, let\u2019s do that.</li></ol>\n\n\n\n<p>Let\u2019s run the numbers.</p>\n\n\n\n<h2>The Numbers</h2>\n\n\n\n<h2>Predictions</h2>\n\n\n\n<p>Last week I somehow <em>once again </em>forgot about a holiday, in this case Memorial Day, because I don\u2019t think in those terms naturally and it still boggles my brain that it matters. But in terms of \u2018why this prediction was stupid\u2019 that\u2019s why.</p>\n\n\n\n<p>Assignment for readers: Next time a holiday is coming up and I forget about it, whoever notices first should write a comment in the Substack version that says \u2018you forgot to take into account the upcoming holiday!\u2019 and whoever does scores points. Note that if I don\u2019t explicitly mention the holiday I\u2019m probably forgetting about it.</p>\n\n\n\n<p>Prediction from last week: 700,000 cases (+9%) and 2,725 deaths (+15%)</p>\n\n\n\n<p>Results: 566k cases (-12%) and 1,994 deaths (-15%)</p>\n\n\n\n<p>Prediction for next week: 675,000 cases (+19%) and 2,400 deaths (+20%)</p>\n\n\n\n<p>As usual with a holiday drop like this, one must hedge between \u2018it was the holiday and will be even higher than trend next week\u2019 and \u2018it wasn\u2019t entirely the holiday, this was a real drop.\u2019 Even if I\u2019d remembered the holiday my prediction would have come in high. Is it possible cases have peaked for real? Definitely possible, but the decline in deaths now makes zero underlying sense outside of the holiday, so I\u2019m guessing we\u2019re looking mostly at a large holiday effect.</p>\n\n\n\n<h2>Deaths</h2>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc33da495-c543-4b72-bf1f-a27e004c0b5d_672x166.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc33da495-c543-4b72-bf1f-a27e004c0b5d_672x166.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F41480eb7-7e03-4593-bda6-fdcd59633494_1200x742.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F41480eb7-7e03-4593-bda6-fdcd59633494_1200x742.png\"></a></p>\n\n\n\n<p>Ah, Memorial Day, yet another holiday. Silly me.</p>\n\n\n\n<h2>Cases</h2>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F728ac342-cf70-4486-b9e8-ed8d6f74a1bf_675x191.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F728ac342-cf70-4486-b9e8-ed8d6f74a1bf_675x191.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F46d89419-f46b-47a8-b445-b05632a55ff4_1200x742.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F46d89419-f46b-47a8-b445-b05632a55ff4_1200x742.png\"></a></p>\n\n\n\n<h2>Vaccine Discussion Offer</h2>\n\n\n\n<p>Chise, who has provided good information several times, <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/sailorrooscout/status/1531652290683314182\">says their DMs are open for anyone with vaccine-related questions</a> or even want someone to listen because you are unsure. A fine public service, if you need it please use this resource or pass it along to someone else who needs it.</p>\n\n\n\n<h2>Variants</h2>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/trvrb/status/1530656373549150209\">Trevor Bedford thread confirming BA.4/5 will likely outcompete BA.1.2.12</a>, confirmation at least a lot of that is immune escape, and the note that a BA.1-based vaccine update would be helpful but a BA.4 or BA.5-based update would be much better at this point.</p>\n\n\n\n<h2>Big Time Surge Story</h2>\n\n\n\n<p>How is the other half not living? It\u2019s calling another big time surge. The note here is specific to San Francisco but the logic is universal. Everywhere will have their turns.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9e2878-0c55-43b4-ae8e-f692b94fa63f_659x137.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9e2878-0c55-43b4-ae8e-f692b94fa63f_659x137.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F38747f0d-b961-4508-b826-2cedfd70be7b_984x984.jpeg\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F38747f0d-b961-4508-b826-2cedfd70be7b_984x984.jpeg\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F06ff6582-5128-491a-bf86-9ae63c6e14fe_960x900.jpeg\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F06ff6582-5128-491a-bf86-9ae63c6e14fe_960x900.jpeg\"></a></p>\n\n\n\n<p>The framing here seems like the \u2018friendly\u2019 version of paternalistic public health. Are you trying to stay well? If so, time to step up your game. If not, then that\u2019s fine too says your grandmother, don\u2019t try to stay well and be home for dinner.</p>\n\n\n\n<p>On some level I find that fair, because <em>the whole point is that it is not worth trying to stay well, </em>if stay well means avoid infection, which is <em>slightly </em>unfair but all right, fine, sure. Permanently avoiding infection is for most people not a good plan, and if you <em>did </em>need to have that as your plan then yes it\u2019s time to step up your game and pull out that P100.</p>\n\n\n\n<p>The contrast between Bob\u2019s two graphs is interesting. In the first one, we have less than half as many cases now as at the start and less than a fifth of the peak. In the second, we have more patients needing care now than we did before and over a third of the peak. This implies a lot of cases are not being reported, and that this is not mostly reflective of lower severity unless most patients under care are \u2018with\u2019 rather than \u2018for\u2019 Covid (because math).</p>\n\n\n\n<p>Similarly, on the question of whether the unvaccinated should be worried right now, <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/mattyglesias/status/1531746320553222144\">Matt Yglesias points out</a> the graph versus NYTimes story differential here.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4487358-32a8-4043-8138-ec94bdf5eec8_1170x497.jpeg\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4487358-32a8-4043-8138-ec94bdf5eec8_1170x497.jpeg\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10eddbda-cf28-40d3-9a4d-c42af6f1586e_1170x1029.jpeg\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10eddbda-cf28-40d3-9a4d-c42af6f1586e_1170x1029.jpeg\"></a></p>\n\n\n\n<h2>Let My People Go</h2>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/KevinKileyCA/status/1530674501050019841\">If not now, then when?</a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55c8a17-cc0d-4dec-8aac-b80b2159893e_627x668.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55c8a17-cc0d-4dec-8aac-b80b2159893e_627x668.png\"></a></p>\n\n\n\n<p>There are no standards. There are no criteria. There is only emergency.</p>\n\n\n\n<p>Defendant inquires about never. He asks if never works for you.</p>\n\n\n\n<p>Meanwhile <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/FutureDocs/status/1531815336516911104\">CDC continues to recommend the full <em>ten </em>days quarantine period for children under the age of two</a>, <em>because they are unvaccinated and couldn\u2019t mask. </em>Words, I have none. Maddening lack of connection with the physical realities of life.</p>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/daniela127/status/1531420988101087232\">Meanwhile in New York:</a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27e47957-297a-4f3f-bb72-a201a45230ff_656x135.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27e47957-297a-4f3f-bb72-a201a45230ff_656x135.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F97b04820-3eb2-401e-aa27-ba779f2b0658_664x1165.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F97b04820-3eb2-401e-aa27-ba779f2b0658_664x1165.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ac16017-32bf-430b-a7ba-73a19aa2d931_642x281.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ac16017-32bf-430b-a7ba-73a19aa2d931_642x281.png\"></a></p>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://vinayprasadmdmph.substack.com/p/memorial-day-travel-experts-vs-americans?utm_medium=email&amp;s=r\">Link to the referenced article here</a>. Both are fine but I prefer the thread.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fca9ba06b-66df-4367-89b5-389a0e69abde_655x755.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fca9ba06b-66df-4367-89b5-389a0e69abde_655x755.png\"></a></p>\n\n\n\n<p>The amount of willful ignorance, of selective faith in the facts that fit whatever narrative supports restrictions today, the not caring about costs, would have been stunning two years ago. Now it\u2019s completely unsurprising, although the \u2018I never talk to anyone with the view that we shouldn\u2019t take lots of pointless public health measures\u2019 still packs a punch.</p>\n\n\n\n<p>The juxtaposition of \u2018we don\u2019t know the long term effects of Covid so we need to destroy life indefinitely\u2019 and \u2018we don\u2019t know the long term impact of child masking but eventually there will be studies and we\u2019ll know\u2019 could not be more clear.</p>\n\n\n\n<p>And of course, there is nothing the kinds of restrictions we are talking about could meaningfully do over the medium term to prevent infections, even if everyone did cooperate, but hey, it\u2019s not as completely insane as putting kids through live shooter drills.</p>\n\n\n\n<h2>FDA Delenda Est: Approved Treatments Edition</h2>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://marginalrevolution.com/marginalrevolution/2022/05/the-fda-should-make-paxlovid-easier-to-get.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-fda-should-make-paxlovid-easier-to-get\">Tyler Cowen reminds us that Paxlovid remains remarkably hard for regular people to get</a> and that this (<a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://www.youtube.com/watch?v=1W7c8QghPxk&amp;ab_channel=XaneFeather\">OF COURSE</a>!) is largely the FDA\u2019s fault since they could allow pharmacists to prescribe it themselves. Quoting in full:</p>\n\n\n\n<blockquote><p>Pharmacists still cannot prescribe the medication themselves, a step that would cut the time it takes patients to secure the drug.</p><p>The Food and Drug Administration \u201cis looking at this and thinking about it,\u201d Dr. Jha said. \u201cWhether they\u2019re going to make a change, when and how, etc., is totally in their wheelhouse.\u201d</p><p>Many patients are still handling the\u00a0<a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://www.nytimes.com/explain/2022/03/21/well/covid-antiviral-pills\">sometimes-cumbersome steps on their own</a>: locating a virus test, then securing a Paxlovid prescription from a health provider, then finding a pharmacy that carries the pill, all within days of first showing symptoms.</p><p>Dr. Jha described being frustrated by physician colleagues who have told him they still limit Paxlovid to patients 65 years and older.</p></blockquote>\n\n\n\n<blockquote><p>Tyler: But no they still will not do this.\u00a0 I repeat myself, but you need to keep in mind the only time panel members have resigned from the FDA is when the Biden administration pushed through the booster shots. Here is <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://www.nytimes.com/2022/05/26/us/politics/paxlovid-white-house-covid-deaths.html\">the full NYT article</a>, via Rich Berger.</p></blockquote>\n\n\n\n<p>Then everyone goes around wondering why so few people are using Paxlovid. And this is what is \u2018in their wheelhouse.\u2019</p>\n\n\n\n<p>Then there\u2019s Fluvoxamine, WSJ asks why <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/caseybmulligan/status/1531486603440492544\">the FDA said no</a> to prescribing it for Covid, <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://thezvi.wordpress.com/2022/05/19/covid-5-19-22-the-law-of-five/\">which I covered two weeks ago</a>. I mentioned then that I wasn\u2019t concerned about that because it was mostly ignorable \u2013 you could still get Fluvoxamine if you cared enough, and if you didn\u2019t care enough you were never going to get it. In practice, however, doctors are remarkably reluctant to write that kind of off-label prescription and there are numerous reasons why almost no one will end up taking it without the approval.</p>\n\n\n\n<p>So once again, I can only conclude the FDA has reached the same conclusion as the top comment on Tyler\u2019s post.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6e74e7a3-5a4a-4bd5-bd18-68ad000149cf_553x97.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6e74e7a3-5a4a-4bd5-bd18-68ad000149cf_553x97.png\"></a></p>\n\n\n\n<h2>CDC Delenda Est</h2>\n\n\n\n<p>Remember when Trump complained that the problem was that we had all these cases because was did all those tests? <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/AlecStapp/status/1531233060909420544\">Well\u2026</a></p>\n\n\n\n<figure class=\"wp-block-image\"><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac23f3cd-7ddb-4bb2-9eef-078d6157f86d_655x680.jpeg\" target=\"_blank\" rel=\"noreferrer noopener\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac23f3cd-7ddb-4bb2-9eef-078d6157f86d_655x680.jpeg\" /></a><figcaption>T</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9166f3b5-e2a6-47bc-bae3-7cafc16a5497_1150x950.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9166f3b5-e2a6-47bc-bae3-7cafc16a5497_1150x950.png\"></a></p>\n\n\n\n<p>\u2026yeah.</p>\n\n\n\n<p>The quote is from <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://www.vox.com/future-perfect/23140258/monkeypox-pandemic-covid-public-health\">this Kelsey Piper post</a> about Monkeypox, including that we\u2019re <em>still </em>doing the whole \u2018downplay the situation so people don\u2019t <em>panic\u2019 </em>thing, which is going into my list of Monkeypox links that I will use if I write a post <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://manifold.markets/ZviMowshowitz/will-zvi-end-up-writing-3-or-more-p\">that I sincerely hope I never write</a>.</p>\n\n\n\n<p>And on another note on Monkeypox, it\u2019s worth pointing out that <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/zeynep/status/1531636634697048071\">history is rhyming again, oh no</a>:</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F76b9c3c1-32f5-4ca1-b446-b523ca0d4dcd_656x379.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F76b9c3c1-32f5-4ca1-b446-b523ca0d4dcd_656x379.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2a1d338a-16f8-4e33-972e-b5e8c11280e8_646x694.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2a1d338a-16f8-4e33-972e-b5e8c11280e8_646x694.png\"></a></p>\n\n\n\n<h2>Think of the Children</h2>\n\n\n\n<h2>In Other News</h2>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/joshuamartian/status/1532105025438519296\">The new reality of Covid, summarized</a>.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0cb26289-eed1-49c7-8f32-1e575d4605b1_652x323.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0cb26289-eed1-49c7-8f32-1e575d4605b1_652x323.png\"></a></p>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/geekethics/status/1530714203283243014\">The reality of what\u2019s going to happen early in the next pandemic</a>.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10689be6-8426-49de-a8dc-b39dd2064de9_641x269.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10689be6-8426-49de-a8dc-b39dd2064de9_641x269.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F975bb17f-5914-468d-b74b-3d15df7b2fe6_663x239.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F975bb17f-5914-468d-b74b-3d15df7b2fe6_663x239.png\"></a></p>\n\n\n\n<p>Nope. Can\u2019t say we have.</p>\n\n\n\n<h2>Fund Anti-Aging Research</h2>\n\n\n\n<p>The beatings will continue until morale and/or survivability improve.</p>\n\n\n\n<p>Aging is bad. It leads to being old, which leads to all sorts of bad things like loss of one\u2019s various facilities and also it kills you. We should get to work on that. However, aging is not considered a disease, so attempts to treat or cure it don\u2019t count. Most people have rationalized that the thing that slowly destroys and kills every single human is \u2018natural\u2019 and fine actually, using a variety of time-honored rationalizations. Even on the voyages of the USS Enterprise they risk the ship to save an innocent old person\u2019s life then give that person a stern lecture about not wanting to then die on schedule.</p>\n\n\n\n<p>So yeah, this battle\u2019s been a tough one. Lost a lot of good men. All of them, actually.</p>\n\n\n\n<p><em>However, </em>aging is <em>also </em>the primary risk factor for Covid-19, so perhaps we can market our anti-aging solution <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/techreview/status/1530794537991520259\"><em>as a Covid-19 treatment?</em></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6bd0eeb-3045-4102-9b66-f6ac2ce6aae8_644x632.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6bd0eeb-3045-4102-9b66-f6ac2ce6aae8_644x632.png\"></a></p>\n\n\n\n<blockquote><p>Covid-19 is far <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-age.html\">more likely</a> to kill you if you\u2019re old. One reason is that aged immune systems struggle to cope with infections and recover from them. So why not try drugs that make bodies young again? That\u2019s the bold idea now being explored in clinical trials around the world, which are testing drugs that reverse the impacts of age on the body, rejuvenate the immune system and clear out aged, worn-out cells.\u00a0</p></blockquote>\n\n\n\n<p>This does not appear to be satire. Now that Covid-19 is around suddenly we have this bold idea to solve the problem, which is to <em>cure aging </em>by making people younger. And no, there is no hint of <em>noticing that maybe we should have been trying to do this anyway? </em>Cause you know what age is a risk factor for <em>almost exactly the same amount as for death from Covid?</em></p>\n\n\n\n<p>Other than violence, accidents, drug overdoses and infant mortality?</p>\n\n\n\n<p>Death. It increases risk <em>for death.</em></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3ee590a-a9c4-4bf0-8232-728da54efff3_622x209.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3ee590a-a9c4-4bf0-8232-728da54efff3_622x209.png\"></a></p>\n\n\n\n<p>Imagine if this was true <em>not </em>in the pandemic.</p>\n\n\n\n<blockquote><p>Most of the cells in our body divide up to a certain point. Once they reach this limit, they should die and be cleared away by the immune system. But that\u2019s not always the case\u2014some cells linger on. These cells no longer divide, and some instead churn out a toxic brew of chemicals that trigger damaging inflammation in the surrounding area and beyond.</p><p>Cells that do this are called \u201csenescent,\u201d and they accumulate across our organs as we age. They\u2019ve been linked to an ever-growing number of age-related diseases, including diabetes, heart disease, osteoporosis, cataracts, Alzheimer\u2019s\u2014the list goes on. They also appear to play an important role in coronavirus infections.</p></blockquote>\n\n\n\n<p>Yes. Please do work on that. Thank you. I will <em>totally </em>pretend not to <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://slatestarcodex.com/2017/04/07/yes-we-have-noticed-the-skulls/\">notice the skulls</a>, except by the skulls I mean <em>look at everyone who ever lived because they are skulls now, they are dead, I am going out on a limb and saying I do not like this.</em></p>\n\n\n\n<p>We might come out ahead from this pandemic yet.</p>\n\n\n\n<h2>One More Note About That Long Covid Study</h2>\n\n\n\n<p>As I said on Twitter, every now and then <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/_rossry/status/1531677323740717061\">they still manage to surprise me</a>.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F008590a2-6c5c-4775-99a6-cdc0d26a4292_661x345.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F008590a2-6c5c-4775-99a6-cdc0d26a4292_661x345.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2dad7bd7-3521-4a58-bce7-a6fad1337c4c_670x520.png\" /></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2dad7bd7-3521-4a58-bce7-a6fad1337c4c_670x520.png\"></a></p>\n\n\n\n<p>Whether or not this is something otherwise worth taking seriously, if the mean age in your study is 71 years old that\u2019s an important point when deciding how to use the results of that study.</p>\n\n\n\n<h2>The Future</h2>\n\n\n\n<p>This was another light week for Covid news, and what news there is seems like it is largely repeating itself. If this continues, my current plan is to emphasize other things and also to start doing post-mortem analysis on prior posts, ideally starting back at the beginning \u2013 pulling out evergreen things to create reference posts, analyzing mistakes and things I got wrong, that kind of thing.</p>\n\n\n\n<p>For now, I intend to keep doing the weekly posts as a public service \u2013 I keep an eye on things so you don\u2019t have to.</p>\n\n\n\n<p>Planned next post is a review of Talent, by Tyler Cowen and Daniel Gross. There\u2019s a ton of specific detail there to talk about so it\u2019s getting long, but I want it out there in the next few days one way or another. I had another thing planned for this week, but turned out I\u2019d already written a version of it back in 2010, so no need to write it again.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"tNsqhzTibgGJKPEWB": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "q9ncEYeZfbqWjHtap", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 44, "extendedScore": null, "score": 1.2058348731643223, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": "QSR8rPZxZzxEXoPjR", "feedLink": "https://thezvi.wordpress.com/2022/06/02/covid-6-2-22-declining-to-respond/", "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc33da495-c543-4b72-bf1f-a27e004c0b5d_672x166.png", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Memorial Day weekend is a good reason not to worry about reporting cases or deaths. FDA declines to respond with making it easier to get treatments we have in ample supply, or to update the vaccines. Congress refuses to fund anything at all, really.</p>\n\n\n\n<p>Doesn\u2019t sound like a state of emergency to me. Yet in California, we still have one, and we decline to respond to when we might end the \u2018state of emergency.\u2019 In New York, health officials make it clear that if it were up to them, we\u2019d have these pandemic restrictions forever here as well.</p>\n\n\n\n<p>Don\u2019t leave it up to them. Respond.</p>\n\n\n\n<span id=\"more-23030\"></span>\n\n\n\n<h2 id=\"Executive_Summary\">Executive Summary</h2>\n\n\n\n<ol><li>Paxlvoid and Fluvoxamine remain hard to get.</li><li>Time is not a flat circle, that\u2019s a dumb expression, but history rhymes.</li><li>Covid-19 is a good excuse to work on curing aging, let\u2019s do that.</li></ol>\n\n\n\n<p>Let\u2019s run the numbers.</p>\n\n\n\n<h2 id=\"The_Numbers\">The Numbers</h2>\n\n\n\n<h2 id=\"Predictions\">Predictions</h2>\n\n\n\n<p>Last week I somehow <em>once again </em>forgot about a holiday, in this case Memorial Day, because I don\u2019t think in those terms naturally and it still boggles my brain that it matters. But in terms of \u2018why this prediction was stupid\u2019 that\u2019s why.</p>\n\n\n\n<p>Assignment for readers: Next time a holiday is coming up and I forget about it, whoever notices first should write a comment in the Substack version that says \u2018you forgot to take into account the upcoming holiday!\u2019 and whoever does scores points. Note that if I don\u2019t explicitly mention the holiday I\u2019m probably forgetting about it.</p>\n\n\n\n<p>Prediction from last week: 700,000 cases (+9%) and 2,725 deaths (+15%)</p>\n\n\n\n<p>Results: 566k cases (-12%) and 1,994 deaths (-15%)</p>\n\n\n\n<p>Prediction for next week: 675,000 cases (+19%) and 2,400 deaths (+20%)</p>\n\n\n\n<p>As usual with a holiday drop like this, one must hedge between \u2018it was the holiday and will be even higher than trend next week\u2019 and \u2018it wasn\u2019t entirely the holiday, this was a real drop.\u2019 Even if I\u2019d remembered the holiday my prediction would have come in high. Is it possible cases have peaked for real? Definitely possible, but the decline in deaths now makes zero underlying sense outside of the holiday, so I\u2019m guessing we\u2019re looking mostly at a large holiday effect.</p>\n\n\n\n<h2 id=\"Deaths\">Deaths</h2>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc33da495-c543-4b72-bf1f-a27e004c0b5d_672x166.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc33da495-c543-4b72-bf1f-a27e004c0b5d_672x166.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F41480eb7-7e03-4593-bda6-fdcd59633494_1200x742.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F41480eb7-7e03-4593-bda6-fdcd59633494_1200x742.png\"></a></p>\n\n\n\n<p>Ah, Memorial Day, yet another holiday. Silly me.</p>\n\n\n\n<h2 id=\"Cases\">Cases</h2>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F728ac342-cf70-4486-b9e8-ed8d6f74a1bf_675x191.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F728ac342-cf70-4486-b9e8-ed8d6f74a1bf_675x191.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F46d89419-f46b-47a8-b445-b05632a55ff4_1200x742.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F46d89419-f46b-47a8-b445-b05632a55ff4_1200x742.png\"></a></p>\n\n\n\n<h2 id=\"Vaccine_Discussion_Offer\">Vaccine Discussion Offer</h2>\n\n\n\n<p>Chise, who has provided good information several times, <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/sailorrooscout/status/1531652290683314182\">says their DMs are open for anyone with vaccine-related questions</a> or even want someone to listen because you are unsure. A fine public service, if you need it please use this resource or pass it along to someone else who needs it.</p>\n\n\n\n<h2 id=\"Variants\">Variants</h2>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/trvrb/status/1530656373549150209\">Trevor Bedford thread confirming BA.4/5 will likely outcompete BA.1.2.12</a>, confirmation at least a lot of that is immune escape, and the note that a BA.1-based vaccine update would be helpful but a BA.4 or BA.5-based update would be much better at this point.</p>\n\n\n\n<h2 id=\"Big_Time_Surge_Story\">Big Time Surge Story</h2>\n\n\n\n<p>How is the other half not living? It\u2019s calling another big time surge. The note here is specific to San Francisco but the logic is universal. Everywhere will have their turns.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9e2878-0c55-43b4-ae8e-f692b94fa63f_659x137.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9e2878-0c55-43b4-ae8e-f692b94fa63f_659x137.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F38747f0d-b961-4508-b826-2cedfd70be7b_984x984.jpeg\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F38747f0d-b961-4508-b826-2cedfd70be7b_984x984.jpeg\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F06ff6582-5128-491a-bf86-9ae63c6e14fe_960x900.jpeg\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F06ff6582-5128-491a-bf86-9ae63c6e14fe_960x900.jpeg\"></a></p>\n\n\n\n<p>The framing here seems like the \u2018friendly\u2019 version of paternalistic public health. Are you trying to stay well? If so, time to step up your game. If not, then that\u2019s fine too says your grandmother, don\u2019t try to stay well and be home for dinner.</p>\n\n\n\n<p>On some level I find that fair, because <em>the whole point is that it is not worth trying to stay well, </em>if stay well means avoid infection, which is <em>slightly </em>unfair but all right, fine, sure. Permanently avoiding infection is for most people not a good plan, and if you <em>did </em>need to have that as your plan then yes it\u2019s time to step up your game and pull out that P100.</p>\n\n\n\n<p>The contrast between Bob\u2019s two graphs is interesting. In the first one, we have less than half as many cases now as at the start and less than a fifth of the peak. In the second, we have more patients needing care now than we did before and over a third of the peak. This implies a lot of cases are not being reported, and that this is not mostly reflective of lower severity unless most patients under care are \u2018with\u2019 rather than \u2018for\u2019 Covid (because math).</p>\n\n\n\n<p>Similarly, on the question of whether the unvaccinated should be worried right now, <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/mattyglesias/status/1531746320553222144\">Matt Yglesias points out</a> the graph versus NYTimes story differential here.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4487358-32a8-4043-8138-ec94bdf5eec8_1170x497.jpeg\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4487358-32a8-4043-8138-ec94bdf5eec8_1170x497.jpeg\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10eddbda-cf28-40d3-9a4d-c42af6f1586e_1170x1029.jpeg\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10eddbda-cf28-40d3-9a4d-c42af6f1586e_1170x1029.jpeg\"></a></p>\n\n\n\n<h2 id=\"Let_My_People_Go\">Let My People Go</h2>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/KevinKileyCA/status/1530674501050019841\">If not now, then when?</a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55c8a17-cc0d-4dec-8aac-b80b2159893e_627x668.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55c8a17-cc0d-4dec-8aac-b80b2159893e_627x668.png\"></a></p>\n\n\n\n<p>There are no standards. There are no criteria. There is only emergency.</p>\n\n\n\n<p>Defendant inquires about never. He asks if never works for you.</p>\n\n\n\n<p>Meanwhile <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/FutureDocs/status/1531815336516911104\">CDC continues to recommend the full <em>ten </em>days quarantine period for children under the age of two</a>, <em>because they are unvaccinated and couldn\u2019t mask. </em>Words, I have none. Maddening lack of connection with the physical realities of life.</p>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/daniela127/status/1531420988101087232\">Meanwhile in New York:</a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27e47957-297a-4f3f-bb72-a201a45230ff_656x135.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F27e47957-297a-4f3f-bb72-a201a45230ff_656x135.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F97b04820-3eb2-401e-aa27-ba779f2b0658_664x1165.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F97b04820-3eb2-401e-aa27-ba779f2b0658_664x1165.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ac16017-32bf-430b-a7ba-73a19aa2d931_642x281.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2ac16017-32bf-430b-a7ba-73a19aa2d931_642x281.png\"></a></p>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://vinayprasadmdmph.substack.com/p/memorial-day-travel-experts-vs-americans?utm_medium=email&amp;s=r\">Link to the referenced article here</a>. Both are fine but I prefer the thread.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fca9ba06b-66df-4367-89b5-389a0e69abde_655x755.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fca9ba06b-66df-4367-89b5-389a0e69abde_655x755.png\"></a></p>\n\n\n\n<p>The amount of willful ignorance, of selective faith in the facts that fit whatever narrative supports restrictions today, the not caring about costs, would have been stunning two years ago. Now it\u2019s completely unsurprising, although the \u2018I never talk to anyone with the view that we shouldn\u2019t take lots of pointless public health measures\u2019 still packs a punch.</p>\n\n\n\n<p>The juxtaposition of \u2018we don\u2019t know the long term effects of Covid so we need to destroy life indefinitely\u2019 and \u2018we don\u2019t know the long term impact of child masking but eventually there will be studies and we\u2019ll know\u2019 could not be more clear.</p>\n\n\n\n<p>And of course, there is nothing the kinds of restrictions we are talking about could meaningfully do over the medium term to prevent infections, even if everyone did cooperate, but hey, it\u2019s not as completely insane as putting kids through live shooter drills.</p>\n\n\n\n<h2 id=\"FDA_Delenda_Est__Approved_Treatments_Edition\">FDA Delenda Est: Approved Treatments Edition</h2>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://marginalrevolution.com/marginalrevolution/2022/05/the-fda-should-make-paxlovid-easier-to-get.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-fda-should-make-paxlovid-easier-to-get\">Tyler Cowen reminds us that Paxlovid remains remarkably hard for regular people to get</a> and that this (<a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://www.youtube.com/watch?v=1W7c8QghPxk&amp;ab_channel=XaneFeather\">OF COURSE</a>!) is largely the FDA\u2019s fault since they could allow pharmacists to prescribe it themselves. Quoting in full:</p>\n\n\n\n<blockquote><p>Pharmacists still cannot prescribe the medication themselves, a step that would cut the time it takes patients to secure the drug.</p><p>The Food and Drug Administration \u201cis looking at this and thinking about it,\u201d Dr. Jha said. \u201cWhether they\u2019re going to make a change, when and how, etc., is totally in their wheelhouse.\u201d</p><p>Many patients are still handling the&nbsp;<a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://www.nytimes.com/explain/2022/03/21/well/covid-antiviral-pills\">sometimes-cumbersome steps on their own</a>: locating a virus test, then securing a Paxlovid prescription from a health provider, then finding a pharmacy that carries the pill, all within days of first showing symptoms.</p><p>Dr. Jha described being frustrated by physician colleagues who have told him they still limit Paxlovid to patients 65 years and older.</p></blockquote>\n\n\n\n<blockquote><p>Tyler: But no they still will not do this.&nbsp; I repeat myself, but you need to keep in mind the only time panel members have resigned from the FDA is when the Biden administration pushed through the booster shots. Here is <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://www.nytimes.com/2022/05/26/us/politics/paxlovid-white-house-covid-deaths.html\">the full NYT article</a>, via Rich Berger.</p></blockquote>\n\n\n\n<p>Then everyone goes around wondering why so few people are using Paxlovid. And this is what is \u2018in their wheelhouse.\u2019</p>\n\n\n\n<p>Then there\u2019s Fluvoxamine, WSJ asks why <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/caseybmulligan/status/1531486603440492544\">the FDA said no</a> to prescribing it for Covid, <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://thezvi.wordpress.com/2022/05/19/covid-5-19-22-the-law-of-five/\">which I covered two weeks ago</a>. I mentioned then that I wasn\u2019t concerned about that because it was mostly ignorable \u2013 you could still get Fluvoxamine if you cared enough, and if you didn\u2019t care enough you were never going to get it. In practice, however, doctors are remarkably reluctant to write that kind of off-label prescription and there are numerous reasons why almost no one will end up taking it without the approval.</p>\n\n\n\n<p>So once again, I can only conclude the FDA has reached the same conclusion as the top comment on Tyler\u2019s post.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6e74e7a3-5a4a-4bd5-bd18-68ad000149cf_553x97.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F6e74e7a3-5a4a-4bd5-bd18-68ad000149cf_553x97.png\"></a></p>\n\n\n\n<h2 id=\"CDC_Delenda_Est\">CDC Delenda Est</h2>\n\n\n\n<p>Remember when Trump complained that the problem was that we had all these cases because was did all those tests? <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/AlecStapp/status/1531233060909420544\">Well\u2026</a></p>\n\n\n\n<figure class=\"wp-block-image\"><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac23f3cd-7ddb-4bb2-9eef-078d6157f86d_655x680.jpeg\" target=\"_blank\" rel=\"noreferrer noopener\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac23f3cd-7ddb-4bb2-9eef-078d6157f86d_655x680.jpeg\"></a><figcaption>T</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9166f3b5-e2a6-47bc-bae3-7cafc16a5497_1150x950.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9166f3b5-e2a6-47bc-bae3-7cafc16a5497_1150x950.png\"></a></p>\n\n\n\n<p>\u2026yeah.</p>\n\n\n\n<p>The quote is from <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://www.vox.com/future-perfect/23140258/monkeypox-pandemic-covid-public-health\">this Kelsey Piper post</a> about Monkeypox, including that we\u2019re <em>still </em>doing the whole \u2018downplay the situation so people don\u2019t <em>panic\u2019 </em>thing, which is going into my list of Monkeypox links that I will use if I write a post <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://manifold.markets/ZviMowshowitz/will-zvi-end-up-writing-3-or-more-p\">that I sincerely hope I never write</a>.</p>\n\n\n\n<p>And on another note on Monkeypox, it\u2019s worth pointing out that <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/zeynep/status/1531636634697048071\">history is rhyming again, oh no</a>:</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F76b9c3c1-32f5-4ca1-b446-b523ca0d4dcd_656x379.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F76b9c3c1-32f5-4ca1-b446-b523ca0d4dcd_656x379.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2a1d338a-16f8-4e33-972e-b5e8c11280e8_646x694.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2a1d338a-16f8-4e33-972e-b5e8c11280e8_646x694.png\"></a></p>\n\n\n\n<h2 id=\"Think_of_the_Children\">Think of the Children</h2>\n\n\n\n<h2 id=\"In_Other_News\">In Other News</h2>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/joshuamartian/status/1532105025438519296\">The new reality of Covid, summarized</a>.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0cb26289-eed1-49c7-8f32-1e575d4605b1_652x323.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0cb26289-eed1-49c7-8f32-1e575d4605b1_652x323.png\"></a></p>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/geekethics/status/1530714203283243014\">The reality of what\u2019s going to happen early in the next pandemic</a>.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10689be6-8426-49de-a8dc-b39dd2064de9_641x269.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F10689be6-8426-49de-a8dc-b39dd2064de9_641x269.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F975bb17f-5914-468d-b74b-3d15df7b2fe6_663x239.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F975bb17f-5914-468d-b74b-3d15df7b2fe6_663x239.png\"></a></p>\n\n\n\n<p>Nope. Can\u2019t say we have.</p>\n\n\n\n<h2 id=\"Fund_Anti_Aging_Research\">Fund Anti-Aging Research</h2>\n\n\n\n<p>The beatings will continue until morale and/or survivability improve.</p>\n\n\n\n<p>Aging is bad. It leads to being old, which leads to all sorts of bad things like loss of one\u2019s various facilities and also it kills you. We should get to work on that. However, aging is not considered a disease, so attempts to treat or cure it don\u2019t count. Most people have rationalized that the thing that slowly destroys and kills every single human is \u2018natural\u2019 and fine actually, using a variety of time-honored rationalizations. Even on the voyages of the USS Enterprise they risk the ship to save an innocent old person\u2019s life then give that person a stern lecture about not wanting to then die on schedule.</p>\n\n\n\n<p>So yeah, this battle\u2019s been a tough one. Lost a lot of good men. All of them, actually.</p>\n\n\n\n<p><em>However, </em>aging is <em>also </em>the primary risk factor for Covid-19, so perhaps we can market our anti-aging solution <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/techreview/status/1530794537991520259\"><em>as a Covid-19 treatment?</em></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6bd0eeb-3045-4102-9b66-f6ac2ce6aae8_644x632.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6bd0eeb-3045-4102-9b66-f6ac2ce6aae8_644x632.png\"></a></p>\n\n\n\n<blockquote><p>Covid-19 is far <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-age.html\">more likely</a> to kill you if you\u2019re old. One reason is that aged immune systems struggle to cope with infections and recover from them. So why not try drugs that make bodies young again? That\u2019s the bold idea now being explored in clinical trials around the world, which are testing drugs that reverse the impacts of age on the body, rejuvenate the immune system and clear out aged, worn-out cells.&nbsp;</p></blockquote>\n\n\n\n<p>This does not appear to be satire. Now that Covid-19 is around suddenly we have this bold idea to solve the problem, which is to <em>cure aging </em>by making people younger. And no, there is no hint of <em>noticing that maybe we should have been trying to do this anyway? </em>Cause you know what age is a risk factor for <em>almost exactly the same amount as for death from Covid?</em></p>\n\n\n\n<p>Other than violence, accidents, drug overdoses and infant mortality?</p>\n\n\n\n<p>Death. It increases risk <em>for death.</em></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3ee590a-a9c4-4bf0-8232-728da54efff3_622x209.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3ee590a-a9c4-4bf0-8232-728da54efff3_622x209.png\"></a></p>\n\n\n\n<p>Imagine if this was true <em>not </em>in the pandemic.</p>\n\n\n\n<blockquote><p>Most of the cells in our body divide up to a certain point. Once they reach this limit, they should die and be cleared away by the immune system. But that\u2019s not always the case\u2014some cells linger on. These cells no longer divide, and some instead churn out a toxic brew of chemicals that trigger damaging inflammation in the surrounding area and beyond.</p><p>Cells that do this are called \u201csenescent,\u201d and they accumulate across our organs as we age. They\u2019ve been linked to an ever-growing number of age-related diseases, including diabetes, heart disease, osteoporosis, cataracts, Alzheimer\u2019s\u2014the list goes on. They also appear to play an important role in coronavirus infections.</p></blockquote>\n\n\n\n<p>Yes. Please do work on that. Thank you. I will <em>totally </em>pretend not to <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://slatestarcodex.com/2017/04/07/yes-we-have-noticed-the-skulls/\">notice the skulls</a>, except by the skulls I mean <em>look at everyone who ever lived because they are skulls now, they are dead, I am going out on a limb and saying I do not like this.</em></p>\n\n\n\n<p>We might come out ahead from this pandemic yet.</p>\n\n\n\n<h2 id=\"One_More_Note_About_That_Long_Covid_Study\">One More Note About That Long Covid Study</h2>\n\n\n\n<p>As I said on Twitter, every now and then <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://twitter.com/_rossry/status/1531677323740717061\">they still manage to surprise me</a>.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F008590a2-6c5c-4775-99a6-cdc0d26a4292_661x345.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F008590a2-6c5c-4775-99a6-cdc0d26a4292_661x345.png\"></a></p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2dad7bd7-3521-4a58-bce7-a6fad1337c4c_670x520.png\"></figure>\n\n\n\n<p><a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2dad7bd7-3521-4a58-bce7-a6fad1337c4c_670x520.png\"></a></p>\n\n\n\n<p>Whether or not this is something otherwise worth taking seriously, if the mean age in your study is 71 years old that\u2019s an important point when deciding how to use the results of that study.</p>\n\n\n\n<h2 id=\"The_Future\">The Future</h2>\n\n\n\n<p>This was another light week for Covid news, and what news there is seems like it is largely repeating itself. If this continues, my current plan is to emphasize other things and also to start doing post-mortem analysis on prior posts, ideally starting back at the beginning \u2013 pulling out evergreen things to create reference posts, analyzing mistakes and things I got wrong, that kind of thing.</p>\n\n\n\n<p>For now, I intend to keep doing the weekly posts as a public service \u2013 I keep an eye on things so you don\u2019t have to.</p>\n\n\n\n<p>Planned next post is a review of Talent, by Tyler Cowen and Daniel Gross. There\u2019s a ton of specific detail there to talk about so it\u2019s getting long, but I want it out there in the next few days one way or another. I had another thing planned for this week, but turned out I\u2019d already written a version of it back in 2010, so no need to write it again.</p>", "sections": [{"title": "Executive Summary", "anchor": "Executive_Summary", "level": 1}, {"title": "The Numbers", "anchor": "The_Numbers", "level": 1}, {"title": "Predictions", "anchor": "Predictions", "level": 1}, {"title": "Deaths", "anchor": "Deaths", "level": 1}, {"title": "Cases", "anchor": "Cases", "level": 1}, {"title": "Vaccine Discussion Offer", "anchor": "Vaccine_Discussion_Offer", "level": 1}, {"title": "Variants", "anchor": "Variants", "level": 1}, {"title": "Big Time Surge Story", "anchor": "Big_Time_Surge_Story", "level": 1}, {"title": "Let My People Go", "anchor": "Let_My_People_Go", "level": 1}, {"title": "FDA Delenda Est: Approved Treatments Edition", "anchor": "FDA_Delenda_Est__Approved_Treatments_Edition", "level": 1}, {"title": "CDC Delenda Est", "anchor": "CDC_Delenda_Est", "level": 1}, {"title": "Think of the Children", "anchor": "Think_of_the_Children", "level": 1}, {"title": "In Other News", "anchor": "In_Other_News", "level": 1}, {"title": "Fund Anti-Aging Research", "anchor": "Fund_Anti_Aging_Research", "level": 1}, {"title": "One More Note About That Long Covid Study", "anchor": "One_More_Note_About_That_Long_Covid_Study", "level": 1}, {"title": "The Future", "anchor": "The_Future", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 18}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-02T13:50:01.336Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-02T19:24:40.583Z", "modifiedAt": "2022-06-02T19:24:40.579Z", "url": null, "title": "The case for using\u00a0the term 'steelmanning' instead of\u00a0'principle of charity'", "slug": "the-case-for-using-the-term-steelmanning-instead-of", "viewCount": null, "lastCommentedAt": "2022-06-03T03:01:47.309Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChristianKl", "createdAt": "2009-10-13T22:32:16.589Z", "isAdmin": false, "displayName": "ChristianKl"}, "userId": "vbDMpDA5A35329Ju5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CGhRSfjSFzGPXtwyW/the-case-for-using-the-term-steelmanning-instead-of", "pageUrlRelative": "/posts/CGhRSfjSFzGPXtwyW/the-case-for-using-the-term-steelmanning-instead-of", "linkUrl": "https://www.lesswrong.com/posts/CGhRSfjSFzGPXtwyW/the-case-for-using-the-term-steelmanning-instead-of", "postedAtFormatted": "Thursday, June 2nd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20case%20for%20using%C2%A0the%20term%20'steelmanning'%20instead%20of%C2%A0'principle%20of%20charity'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20case%20for%20using%C2%A0the%20term%20'steelmanning'%20instead%20of%C2%A0'principle%20of%20charity'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCGhRSfjSFzGPXtwyW%2Fthe-case-for-using-the-term-steelmanning-instead-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20case%20for%20using%C2%A0the%20term%20'steelmanning'%20instead%20of%C2%A0'principle%20of%20charity'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCGhRSfjSFzGPXtwyW%2Fthe-case-for-using-the-term-steelmanning-instead-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCGhRSfjSFzGPXtwyW%2Fthe-case-for-using-the-term-steelmanning-instead-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 843, "htmlBody": "<p>(Alternative title: \u201cThe case for rationalists inventing new jargon\u201d)</p><p>Antoine Lavoisier revolutionized chemistry. Did he discover a new substance? If you think that names don't matter, Lavoisier didn't discover a new substance. If you think names matter, he discovered oxygen.&nbsp;</p><p>Joseph Priestley discovered the same substance earlier. Through heating mercuric oxide he produced a gas that allowed a candle to burn brightly. A gas that enabled a mouse to live longer under an airtight glass cover. He called the gas dephlogisticated air.</p><p>He showed Lavoisier his experiment. Lavoisier rejected the idea that there should be elements with negative mass like phlogiston and called the same gas oxygen and earned thereby the reputation of inventing modern chemistry.&nbsp;</p><blockquote><p>The language you use to talk about something influences the way you think about it. If the chemistry you\u2019re talking about is truly something new, then a fight over terminology may be quite an important part of getting to understand that chemistry better.&nbsp;<a href=\"http://cen.acs.org/articles/92/i6/Confusion-Over-Scientific-Nomenclature-Par.html\"><u>Jay A. Labinger</u></a></p></blockquote><p>Long before the discovery of atoms, Lavoisier reordered the ontology for chemistry and named entities in a way that reflected his new ontology thereby founding modern chemistry.&nbsp;</p><p>In the field of rationality, we are today facing a situation where we lack an ontology that\u2019s of the same quality as the ontology we have of chemistry in our times. We are facing a situation where the ontology of rationality is like the ontology of chemistry in Priestley\u2019s time.&nbsp;</p><p>Classical rationalism uses fallacies frequently in its ontology. In Kahnemann\u2019s ontology, the concept of cognitive biases is central. Given CFAR's more recent work it\u2019s fashionable to talk in terms of cruxes while talking about fallacies is less fashionable on LessWrong. New concepts come with new terms and the way we name concepts helps us to relate to our new concepts.</p><p>Positivist ideology suggests that non-physical entities that we might talk about in rationality such as biases don\u2019t really exist. Positivists propose that after we get a good ontology for physical entities like atoms, we don\u2019t need to think about ontology anymore and can just focus on epistemology.&nbsp;</p><p>This focus on epistemology leads to us spending less effort on finding good names for concepts and good ontology. When new names for concepts that are already known under other names get introduced those new names often face criticism.&nbsp;</p><p>An example of great naming of our community is the term&nbsp;<i>steelmanning</i> which refers to a concept that\u2019s called the&nbsp;<i>principle of charity</i> in academic philosophy.&nbsp;<a href=\"https://www.intelligentspeculation.com/blog/the-principle-of-charity\"><u>Jonathan Maloney</u></a> defines the&nbsp;<i>principle of charity&nbsp;</i>of the philosophers as:</p><blockquote><p>The Principle of Charity demands that one interprets a speaker's statement(s) in the most rational way possible. In other words, when ascribing to this principle, you must consider the strongest possible interpretation of your fellow interlocutor's argument before subjecting it to evaluation.</p></blockquote><p>Academic philosophers borrowed the term from the philosophy of language, where it has a slightly different meaning. Wilson coined the term in his paper&nbsp;<a href=\"https://www.jstor.org/stable/20123725\"><u>Substances without Substrata</u></a> to deal with the problem of knowing to whom a term refers. If Charles speaks of&nbsp;<i>Caesar&nbsp;</i>and there are multiple people who are named&nbsp;<i>Caesar</i>, Wilson\u2019s&nbsp;<i>principle of charity&nbsp;</i>is a heuristic for knowing which&nbsp;<i>Caesar&nbsp;</i>is meant<i>.</i> According to the heuristic that&nbsp;<i>Julius Caesar</i> is meant by Charles when picking&nbsp;<i>Julius Caesar&nbsp;</i>as designatum for&nbsp;<i>Caesar&nbsp;</i>makes the most claims that a person makes about&nbsp;<i>Caesar&nbsp;</i>to be true.<i>&nbsp;</i></p><blockquote><p>We select as designatum that individual which will make the largest possible number of Charles\u2019 statements true.&nbsp;</p></blockquote><p>In Wilson\u2019s definition, the principle of charity has the goal of figuring out what another person means with the languages they use. This goal is different from improving a poorly argued argument another person made to a stronger form. The concepts that Wilson and Maloney describe differ.</p><p>Both of their concepts are about how to interpret a speaker's statement and not about improving on a speaker's statement. Often when we talk about&nbsp;<i>steelmanning&nbsp;</i>in our community we want to improve on the argument that was made in a way that adds reasoning to the case the speaker makes even if the speaker doesn\u2019t know the reasoning we are adding. We speak about&nbsp;<i>steelmanning&nbsp;</i>when we care more about whether the position for which a speaker is arguing is true than caring about whether it\u2019s true for the reasons that the speaker advocates.&nbsp;</p><p>The common idea of a&nbsp;<i>strawman&nbsp;</i>is a way to phrase an argument in a weaker way than it was meant and in contrast to the term&nbsp;<i>strawman&nbsp;</i>the term&nbsp;<i>steelman&nbsp;</i>indicates that the&nbsp;<i>steelman&nbsp;</i>also goes beyond the original meaning. Using the term&nbsp;<i>steelman&nbsp;</i>helps us to be clear that we are indeed attempting to change the meaning.</p><p>The philosophers don\u2019t have distinct terms about whether they include or exclude the change in meaning when they speak about using the principle of charity and&nbsp;</p><p>The term&nbsp;<i>principle of charity</i> also comes with additional baggage because the idea of charity is often about assuming good intentions, being respectful, and not hurting other people's feelings. By using the term&nbsp;<i>steelman&nbsp;</i>for the concept Maloney named&nbsp;<i>principle of charity&nbsp;</i>we have a word that maps more directly to a single ontological concept instead of mapping to a mix of different concepts.&nbsp;</p><p>Terms like&nbsp;<i>steelman&nbsp;</i>allow us to speak more effectively about rationality than we can by reusing existing terms. Finding good terms for concepts can help us refine our field of rationality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "CGhRSfjSFzGPXtwyW", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 25, "extendedScore": null, "score": 1.220668, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-04-03T16:40:48.812Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": "easy-going", "hideCommentKarma": false, "commentCount": 5, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": "1.1.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-04-03T14:05:16.284Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-02T20:18:27.319Z", "modifiedAt": "2022-06-03T01:31:45.468Z", "url": null, "title": "Fact post: project-based learning", "slug": "fact-post-project-based-learning", "viewCount": null, "lastCommentedAt": "2022-06-02T22:11:57.660Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dominicq", "createdAt": "2017-10-28T19:16:49.773Z", "isAdmin": false, "displayName": "dominicq"}, "userId": "vcShLJR5YxevnWdbe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8tywoHSb3TrTvmDWp/fact-post-project-based-learning", "pageUrlRelative": "/posts/8tywoHSb3TrTvmDWp/fact-post-project-based-learning", "linkUrl": "https://www.lesswrong.com/posts/8tywoHSb3TrTvmDWp/fact-post-project-based-learning", "postedAtFormatted": "Thursday, June 2nd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fact%20post%3A%20project-based%20learning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFact%20post%3A%20project-based%20learning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8tywoHSb3TrTvmDWp%2Ffact-post-project-based-learning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fact%20post%3A%20project-based%20learning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8tywoHSb3TrTvmDWp%2Ffact-post-project-based-learning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8tywoHSb3TrTvmDWp%2Ffact-post-project-based-learning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 845, "htmlBody": "<p><em>(cross-posted from my blog, <a href=\"https://sundaystopwatch.eu/fact-post-project-based-learning/\">Sunday Stopwatch</a>)</em></p>\n<p>\"But when am I going to use any of this?\" - every single kid in school ever.</p>\n<p>There's an idea that <em>project-based</em> learning is more efficient than the traditional school curriculum. It makes sense, but so did the idea that learning styles are important, and that <a href=\"https://www.frontiersin.org/articles/10.3389/fpsyg.2020.00164/full\">turned out to be false</a>.</p>\n<p>Here are a couple of thoughts before I delve into the research:</p>\n<ol>\n<li>When I'm learning about something, it stands to reason that, in the long run, I'll learn more if I'm interested in the material. One way of being interested in the material is to have a specific goal in mind, the achievement of which depends on you learning the material. However, as anyone with any goal whatsoever knows, some things you want to do very much, but you really don't want to do some of the background work for them.</li>\n<li>Having a specific goal (a project) will serve as a <em>wrapper</em> around the exact same thing you would be doing following a traditional curriculum: reading textbooks and solving quizzes. Let's say you're trying to build a robot and realize that you need to learn about classical mechanics to achieve that goal. You'll bust out the textbook, try to figure out the page which will solve your specific problem, you will fail because you have a specific situation which requires a deeper understanding of the entire field of classical mechanics, and then you'll end up doing what you would have been doing anyway: reading textbooks and doing quizzes.</li>\n<li>With that being said, you do provide an answer to the question \"why do I need to know this?\", but it's not clear to me that knowing an application for something really makes a difference in knowledge retention. I <em>know</em> the application of the equations used for electrical circuits, but it doesn't mean I remember them.</li>\n<li>But maybe the act of applying itself is what makes the knowledge stick in the mind. There's a relatively easy way to test this: set up three groups, two project-based, and one traditional. When one project-based group gets stuck on the project, and they obtain the knowledge needed to \"unlock\" progress, simply level them up without them actually applying the knowledge. So if they're e.g. building a robot arm, and they are missing some crucial insight into a particular subfield of physics, when they go and obtain that knowledge using traditional methods, just give them a working robot arm (or whatever the project is) and face them with the next challenge. The other project-based group should actually apply the knowledge - the members themselves should go and build whatever they're building. Finally, test the knowledge of the three groups and see if the act of applying knowledge had any effect on retention.</li>\n</ol>\n<p>I expect that having a project-based approach serves as a minor boost to motivation, but doesn't really make a difference regarding knowledge retention.</p>\n<p>So, what does the research say?</p>\n<p>This <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8018635/\">systematic review</a> says that it's inconclusive because the studies weren't done well. And this is only for small kids. I could imagine that different approaches might have different effects depending on age.</p>\n<p>This <a href=\"https://pubmed.ncbi.nlm.nih.gov/22255832/\">paper</a> describes how biomedical engineers collaborated on some projects and concludes that project-based learning works well. Direct quote: \"It is felt that the projects were successful to some extent. They certainly achieved several important learning outcomes of teamwork, ability to apply theoretical principles from multiple disciplines, effective communications, creative problem solving, and awareness of the importance of globalization especially in the biomedical engineering field.\" It is felt that I don't have any new insight after reading the article.</p>\n<p>This <a href=\"https://journals.sagepub.com/doi/10.1177/2158244020938702\">paper</a> tries to figure out the effects of project-based learning on \"on collaborative learning, disciplinary subject learning, iterative learning, and authentic learning\", none of which sound like \"we made both the control and experimental group take the same quiz afterwards\". And looking at the methodology - it's not. They just asked a bunch of teachers some questions. I don't consider this informative.</p>\n<p>There's \"Baran, M., &amp; Maskan, A. (2011). The effect of project-based learning on pre-service physics teachers electrostatic achievements.&nbsp;<em>Cypriot Journal of Educational Sciences</em>,&nbsp;<em>5</em>(4), 243-257.\", but I can't access that text, only the cached abstract, which speaks in favor of project-based learning. But they had only 20 people in the treatment group, and I have no idea what the methodology even was, so... this one also fails.</p>\n<p>This <a href=\"https://journals.sagepub.com/doi/abs/10.1177/1365480216659733\">review</a> also concludes that \"a causal link between PBL instruction and positive student outcomes cannot be established with certainty\".</p>\n<p><strong>My conclusion</strong>: we just don't know for sure. I think that having a project serves as a motivator and a framework, but if you are the type of person that doesn't <em>need</em> to immediately know how you'll apply the thing you're learning, then it's more or less the same. For example, if you blindly trust that logarithms will be useful for something later on, then you don't actually need a project for which logarithms would prove crucial. In both cases, you'll end up doing the same thing: reading textbooks and doing practice problems.</p>\n<p>Please let me know if you know more about this topic!</p>\n", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2, "3uE2pXvbcnS9nnZRE": 4, "TLrqSmzoGoA3v5tNP": 2, "fkABsGCJZ6y9qConW": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "8tywoHSb3TrTvmDWp", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 0.3617588228521231, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-03T01:31:45.375Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-02T20:18:27.322Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-02T20:27:18.860Z", "modifiedAt": "2022-06-03T01:32:18.754Z", "url": "https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming", "title": "Announcing a contest: EA Criticism and Red Teaming", "slug": "announcing-a-contest-ea-criticism-and-red-teaming", "viewCount": null, "lastCommentedAt": "2022-06-02T23:26:48.902Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fin", "createdAt": "2019-10-26T21:16:13.031Z", "isAdmin": false, "displayName": "fin"}, "userId": "KosBHnwEZP7TvsqHE", "domain": "forum.effectivealtruism.org", "pageUrl": "https://www.lesswrong.com/posts/uPLK59MTbEbekexbh/announcing-a-contest-ea-criticism-and-red-teaming", "pageUrlRelative": "/posts/uPLK59MTbEbekexbh/announcing-a-contest-ea-criticism-and-red-teaming", "linkUrl": "https://www.lesswrong.com/out?url=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2F8hvmvrgcxJJ2pYR4X%2Fannouncing-a-contest-ea-criticism-and-red-teaming", "postedAtFormatted": "Thursday, June 2nd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Announcing%20a%20contest%3A%20EA%20Criticism%20and%20Red%20Teaming&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnnouncing%20a%20contest%3A%20EA%20Criticism%20and%20Red%20Teaming%0Ahttps%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2F8hvmvrgcxJJ2pYR4X%2Fannouncing-a-contest-ea-criticism-and-red-teaming%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Announcing%20a%20contest%3A%20EA%20Criticism%20and%20Red%20Teaming%20https%3A%2F%2Fwww.lesswrong.com%2Fout%3Furl%3Dhttps%253A%252F%252Fforum.effectivealtruism.org%252Fposts%252F8hvmvrgcxJJ2pYR4X%252Fannouncing-a-contest-ea-criticism-and-red-teaming", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fout%3Furl%3Dhttps%253A%252F%252Fforum.effectivealtruism.org%252Fposts%252F8hvmvrgcxJJ2pYR4X%252Fannouncing-a-contest-ea-criticism-and-red-teaming", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4061, "htmlBody": "<p>Cross-posted from the <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming\">EA Forum</a>. I would add that we are <strong>open to and positively interested in critiques of any aspect of effective altruism from users of this Forum and the rationalist community</strong>. We plan to respond to comments on the EA Forum, but may check this post less often.</p><hr><h2>Introduction</h2><p><strong>tl;dr: We're running a writing contest for critically engaging with theory or work in effective altruism (EA).&nbsp;</strong></p><p><strong>Submissions can be in a range of </strong><a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Formats\"><strong>formats</strong></a><strong> (from fact-checking to philosophical critiques or major project evaluations); and can focus on a range of </strong><a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#What_to_submit\"><strong>subject matters</strong></a><strong> (from assessing empirical or normative claims to evaluating organizations and practices).&nbsp;&nbsp;</strong></p><p><strong>We plan on distributing $100,000</strong>, and we may end up awarding more than this amount if we get many excellent submissions.&nbsp;</p><p>The deadline is <strong>September 1, 2022</strong>. You can find the submission instructions <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#How_to_apply\">below</a>. <strong>Neither formal nor significant affiliation with effective altruism is required to enter into the contest.</strong></p><p>We are:<a href=\"https://forum.effectivealtruism.org/users/lizka\"> <u>Lizka Vaintrob</u></a> (the<a href=\"https://forum.effectivealtruism.org/posts/aeR2Pses3TjWW8H9t/hello-from-the-new-content-specialist-at-cea\"> <u>Content Specialist</u></a> at the Centre for Effective Altruism),<a href=\"https://forum.effectivealtruism.org/users/jtm\"> </a><a href=\"https://forum.effectivealtruism.org/users/finm\"><u>Fin Moorhouse</u></a> (researcher at the Future of Humanity Institute), and <a href=\"https://forum.effectivealtruism.org/users/jtm\"><u>Joshua Teperowski Monrad</u></a> (biosecurity program associate at Effective Giving). The contest is funded via the <a href=\"https://forum.effectivealtruism.org/topics/future-fund\"><u>FTX Future Fund</u></a> Regranting Program, with organizational support from the <a href=\"https://forum.effectivealtruism.org/topics/centre-for-effective-altruism-1\"><u>Centre for Effective Altruism</u></a>.</p><p>We <a href=\"https://forum.effectivealtruism.org/posts/Fx8pWSLKGwuqsfuRQ/pre-announcing-a-contest-for-critiques-and-red-teaming\"><u>\u2018pre-announced\u2019 this contest in March</u></a>.&nbsp;</p><p>The rest of this post gives more details, outlines the kinds of critical work we think are especially valuable, and explains our rationale. We\u2019re also sharing <a href=\"https://forum.effectivealtruism.org/posts/uuQDgiJJaswEyyzan/resource-for-criticisms-and-red-teaming\">a companion resource for criticisms and red teams</a>.&nbsp;</p><h2>How to apply</h2><p>Submit by posting on the EA Forum<a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#fnwyhqsiy6e4\"><sup>[1]</sup></a>&nbsp;and tagging the post<a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#fn3kedvlnnl\"><sup>[2]</sup></a>&nbsp;with <a href=\"https://forum.effectivealtruism.org/topics/critiques-and-red-teaming-contest\"><u>the contest\u2019s tag</u></a>, or by filling out <a href=\"https://forms.gle/ydNTFC9rivVeoXY46\"><u>this form</u></a>.</p><p>If you post on the Forum, you don't need to do anything except tag your post<a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#fn3kedvlnnl\"><sup>[2]</sup></a>&nbsp;with the \u201c<a href=\"https://forum.effectivealtruism.org/topics/critiques-and-red-teaming-contest\"><u>Criticism and Red Teaming Contest</u></a>\u201d topic, and we\u2019ll consider your post for the contest. If you\u2019d prefer to post your writing outside the Forum, you can <a href=\"https://forms.gle/ydNTFC9rivVeoXY46\"><u>submit it via this form</u></a> \u2014 we\u2019d still encourage you to <a href=\"https://forum.effectivealtruism.org/posts/8yDsenRQhNF4HEDwu/link-posting-is-an-act-of-community-service\"><u>cross-post</u></a> it to the Forum (although please be mindful of copyright issues).&nbsp;</p><p>We also encourage you to refer other people\u2019s work to the contest if you think more people should know about it. To refer someone else\u2019s work, please <a href=\"https://forms.gle/ydNTFC9rivVeoXY46\"><u>submit it via this form</u></a>. If it wins, we may reward you for this \u2014 please see an explanation below.</p><p><strong>The deadline is September 1, 2022.</strong></p><p>Please <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">contact us</a> with any questions. You can also comment here.</p><h2>Prizes</h2><p>We have $100,000 currently set aside for prizes, which we plan on fully distributing.</p><p>Prizes will fall under three main tiers:</p><ul><li><strong>Winners: </strong>$20,000</li><li><strong>Runners up: </strong>$5,000 each</li><li><strong>Honourable mentions: </strong>$1,000 each</li></ul><p>In addition, we may award <strong>a prize of $100,000</strong> for outstanding work that looks likely to cause a very significant course adjustment in effective altruism.</p><p>Therefore, we\u2019re prepared to award (perhaps significantly) more than $100,000 if we\u2019re impressed by the quality and volume of submissions.&nbsp;</p><p><strong>We\u2019re also offering a bounty for referring winning submissions</strong>: if you <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#How_to_apply\">refer</a> a winning submission (if you\u2019re the first person to refer it, and the author never entered the contest themselves), you\u2019ll get a referral bounty of 5% of the award.</p><p>We will also consider helping you find proactive funding for your work if you require the security of guaranteed financial support to enable a large project (though we may deduct proactive funding from prize money if you are awarded one). See <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Q_A\">the FAQ</a> for more details.</p><p>Submissions must be posted or submitted no later than <strong>11:59 pm BST on September 1st</strong>, and we\u2019ll announce winners by the end of September.</p><h2>Criteria</h2><p>Overall, we want to reward critical work according to a question like: \u201c<strong>to what extent did this cause me to change my mind about something important?</strong>\u201d \u2014 where \u201cchange my mind\u201d can mean \u201cchange my best guess about whether some claim is true\u201d, or just \u201cbecome significantly more or less confident in this important thing.\u201d</p><p>Below are some virtues of the kind of work we expect to be most valuable. We\u2019ll look out for these features in the judging process, but we\u2019re aware it can be difficult or impossible to live up to all of them:</p><ul><li><strong>Critical</strong>. The piece takes a critical or questioning stance towards some aspect of EA theory or practice. Note that this does <strong>not</strong> mean that your conclusion must end up disagreeing with what you are criticizing; it is entirely possible to approach some work critically, check the sources, note some potential weaknesses, and conclude that the original was broadly correct.</li><li><strong>Important</strong>. The issues discussed really matter for our ability to do the most good as a movement.</li><li><strong>Constructive and action-relevant.</strong> Where possible we would be most interested in arguments that recommend some specific, realistic action or change of belief. It\u2019s fine to just point out where something is going wrong; even better to be constructive, by suggesting a concrete improvement.</li><li><strong>Transparent and legible. </strong>We encourage <a href=\"https://www.openphilanthropy.org/reasoning-transparency\"><u>transparency about your process</u></a>: how much expertise do you have? How confident are you about the claims you\u2019re making? What would change your mind? If your work includes data, how were they collected? Relatedly, we encourage <a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility\"><strong><u>epistemic legibility</u></strong></a>: the property of being <i>easy to argue with</i>, separate from being correct.</li><li><strong>Aware</strong>. Take some time to check that you\u2019re not missing an existing response to your argument. If responses do exist, mention (or engage with) them.</li><li><strong>Novel</strong>. The piece presents new arguments, or otherwise presents familiar ideas in a new way. Novelty is great but not always necessary \u2014 it\u2019s often still valuable to <a href=\"https://forum.effectivealtruism.org/posts/EbvJRAvwtKAMBn2td/distillation-and-research-debt\"><u>distill</u></a> or \u201ctranslate\u201d existing criticisms.</li><li><strong>Focused. </strong>Critical work is often (but not always) most useful when it is focused on a small number of arguments and a small number of objects. We\u2019d love to see (and we\u2019re likely to reward) work that engages with specific texts, strategic choices, or claims.</li></ul><p>We don't expect that every winning piece needs to do well at every one of these criteria, but we do think each of these criteria can help you most effectively change people\u2019s minds with your work.</p><p>We also want to reward clarity of writing, avoiding \u2018punching down\u2019, awareness of context, and a <a href=\"https://forum.effectivealtruism.org/posts/HDAXztEbjJsyHLKP7/outline-of-galef-s-scout-mindset\"><u>scout mindset</u></a>. We don\u2019t want to encourage personal attacks, or diatribes that are likely to produce much more heat than light. And we hope that subject-matter experts who don\u2019t typically associate with EA find out about this, and share insights we haven\u2019t yet heard.</p><h2>What to submit</h2><p>We\u2019re looking for critical work that you think is important or useful for EA. That\u2019s a broad remit, so we\u2019ve suggested some topics and kinds of critiques below.</p><p>If you\u2019re looking for more detail,<strong> we\u2019ve collaborated on </strong><a href=\"https://forum.effectivealtruism.org/posts/uuQDgiJJaswEyyzan/resource-for-criticisms-and-red-teaming\"><strong>a separate post</strong></a><strong> that collects resources for red teaming and criticisms</strong>, including guides to different kinds of criticisms, and examples. If you\u2019re interested in participating in this contest, we highly recommend that you take a look. (We\u2019d also love help updating and improving it.)</p><p>It\u2019s helpful \u2014but not required \u2014 to also suggest 1\u20133 people you think most need to heed your critique. For many topics, this nomination is better done privately (<a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">contact us</a>, or submit through <a href=\"https://forms.gle/ydNTFC9rivVeoXY46\">the form</a>). We\u2019ll send it their way where possible. (If you don\u2019t know who needs to see it most, we\u2019ll work it out.)&nbsp;</p><h3>Formats</h3><p>You might consider framing your submission as one of the following:</p><ul><li><strong>Minimal trust investigation</strong> \u2014 A <a href=\"https://www.cold-takes.com/minimal-trust-investigations/\"><u>minimal trust investigation</u></a> involves suspending your trust in others' judgments, and trying to understand the case for and against some claim yourself. Suspending trust does <i>not</i> mean determining in advance that you\u2019ll end up disagreeing.</li><li><strong>Red teaming </strong>\u2014 <a href=\"https://www.gov.uk/government/publications/a-guide-to-red-teaming\"><u>\u2018Red teaming\u2019</u></a> is the practice of \u201csubjecting [...] plans, programmes, ideas and assumptions to rigorous analysis and challenge\u201d. You\u2019re setting out to find the strongest reasonable case against something, whatever you actually think about it (and you should flag that this is what you\u2019re doing).</li><li><strong>Fact checking and chasing citation trails</strong> \u2014 If you notice claims that seem crucial, but whose origin is unclear, you could track down the source, and evaluate its legitimacy.</li><li><strong>Adversarial collaboration</strong> \u2014 An <a href=\"https://en.wikipedia.org/wiki/Adversarial_collaboration\"><u>adversarial collaboration</u></a> is where people with opposing views work together to clarify their disagreements.</li><li><strong>Clarifying confusions</strong> \u2014 You might simply be confused about some aspect of EA, rather than confidently critical. You could try getting clear on what you\u2019re confused about, and why.</li><li><strong>Evaluating organizations</strong> \u2014 including their (implicit) theory of change, key claims, and their track record; and suggesting concrete changes where relevant.</li><li><strong>Steelmanning and \u2018translating\u2019 existing criticism for an EA audience</strong> \u2014 We\u2019d love to see work succinctly explaining these existing ideas, and constructing the strongest versions (\u2018<a href=\"https://www.lesswrong.com/tag/steelmanning\"><u>steelmanning</u></a>\u2019) them. You might consider doing this in collaboration with a domain expert who does not consider themself part of the EA community.</li></ul><p>Again, for more detail on topic ideas, kinds of critiques, and examples: visit <a href=\"https://forum.effectivealtruism.org/posts/uuQDgiJJaswEyyzan/resource-for-criticisms-and-red-teaming\">our longer post with resources for critiques and red teams</a>.&nbsp;</p><p>We don\u2019t want to give an analogous list for <i>topic </i>ideas, because any list is necessarily going to leave things out. However, you might take a look at Joshua\u2019s post outlining <a href=\"https://forum.effectivealtruism.org/posts/HzyYoLK2ERTnDmrjB/four-categories-of-effective-altruism-critiques\"><u>four categories of effective altruism critiques</u></a>: <strong>normative and moral</strong> questions, <strong>empirical</strong> questions, <strong>institutions</strong> &amp; <strong>organizations</strong>, and <strong>social norms</strong> &amp; <strong>practices</strong>. Browsing <a href=\"https://forum.effectivealtruism.org/\"><u>this Forum</u></a> could be a good way to get ideas if you are new to effective altruism.</p><p>Browsing <a href=\"https://forum.effectivealtruism.org/\"><u>this Forum</u></a> (especially curated lists like <a href=\"https://forum.effectivealtruism.org/posts/FEFEvC6BzswR4oQqm/results-from-the-first-decade-review\"><u>the Decade Review prizewinners</u></a>, <a href=\"https://forum.effectivealtruism.org/topics/all\"><u>the EA Wiki</u></a>, and <a href=\"https://forum.effectivealtruism.org/handbook\"><u>the EA Handbook</u></a>) could be a good way to get ideas if you are new to effective altruism.</p><p>If you\u2019re unsure whether something you plan on writing could count for this contest, feel free to ask us.</p><h2>Additional resources</h2><p>We\u2019ve compiled <a href=\"https://forum.effectivealtruism.org/posts/uuQDgiJJaswEyyzan/resource-for-criticisms-and-red-teaming\">a companion post</a>, in which we\u2019ve collected some resources for criticisms and red teaming.&nbsp;</p><p>We\u2019re also tentatively planning on running (or helping with) several workshops on criticisms and red teaming, which will be open to anyone who is interested, including people who are new to effective altruism. We hope that the first two will be in June. If you\u2019d like to hear about dates when they\u2019re decided, you can <a href=\"https://forms.gle/2nLZ6ieTmBQfJ2xk8\"><u>fill out this form</u></a>.</p><h2>The judging panel</h2><p>The judging panel is:</p><ul><li><a href=\"https://forum.effectivealtruism.org/users/rebecca-kagan\"><strong><u>Rebecca Kagan</u></strong></a>, J.D. Candidate at Harvard Law School and formerly an External Affairs Specialist at Georgetown's Center for Security and Emerging Technology (CSET)</li><li><a href=\"https://forum.effectivealtruism.org/users/jessica_mccurdy\"><strong><u>Jessica McCurdy</u></strong></a>, Groups Associate at the Centre for Effective Altruism</li><li><a href=\"https://www.openphilanthropy.org/about/team/zachary-robinson\"><strong><u>Zachary Robinson</u></strong></a>, Chief of Staff at Open Philanthropy</li><li><a href=\"https://applieddivinitystudies.com/\"><strong><u>Applied Divinity Studies</u></strong></a>, independent blogger</li><li><a href=\"https://forum.effectivealtruism.org/users/charlottesiegmann\"><strong><u>Charlotte Siegmann</u></strong></a>, Research Fellow at Longview Philanthropy; Predoctoral Research Fellow in Economics at the Global Priorities Institute</li><li><a href=\"https://www.linkedin.com/in/particlemania/\"><strong><u>TJ</u></strong></a>, Research Scholar at the Future of Humanity Institute</li><li><a href=\"https://forum.effectivealtruism.org/users/owen_cotton-barratt\"><strong><u>Owen Cotton-Barratt</u></strong></a>, independent researcher and board member of the Centre for Effective Altruism</li><li><a href=\"https://forum.effectivealtruism.org/users/technicalities\"><strong><u>Gavin Leech</u></strong></a>, founder of <a href=\"https://arbresearch.com/\"><u>Arb Research</u></a>, Strategic Advisor to <a href=\"https://www.mercatus.org/emergent-ventures\"><u>Emergent Ventures</u></a> panel on AI Talent, AI PhD student</li><li><a href=\"https://forum.effectivealtruism.org/users/nicole_ross\"><strong><u>Nicole Ross</u></strong></a>, Head of Community Health at the Centre for Effective Altruism</li><li><a href=\"https://www.alignmentforum.org/users/xuan\"><strong><u>Xuan</u></strong><u> </u><strong><u>(Tan Zhi Xuan)</u></strong></a>, AI PhD student at MIT, Board Member of EA Singapore</li><li>\u2026and ourselves: <a href=\"https://forum.effectivealtruism.org/users/lizka\"><strong><u>Lizka</u></strong></a>, <a href=\"https://www.finmoorhouse.com/\"><strong><u>Fin</u></strong></a>, and <a href=\"https://www.fhi.ox.ac.uk/team/joshua-monrad/\"><strong><u>Joshua</u></strong></a>.</li></ul><p>No one on the judging panel will be able to \u201cveto\u201d winners, and every submission will be read by at least two people. If submissions are technical and outside of the panelists\u2019 fields of expertise, we will consult domain experts. We might add more panelists if we get many submissions.</p><p>If we get many submissions or if we find that the current panel doesn\u2019t have enough bandwidth, we may invite more people to the panel.&nbsp;</p><h2>Rationale</h2><p>Why do we think this matters? In short, we think there are some reasons to expect good criticism to be undersupplied relative to its real value. And that matters: as EA grows, it\u2019s going to become increasingly important that we scrutinize the ideas and assumptions behind key decisions \u2014 and that we welcome outside experts to do the same.</p><p>Encouraging criticism is also a way to encourage a culture of <strong>independent thinking</strong>, and <strong>openness to criticism and scrutiny </strong>within the EA community. Part of what made and continues to make EA so special is its epistemic culture: a willingness to question and be questioned, and freedom to take contrarian or unusual ideas seriously. As EA continues to grow, one failure mode we anticipate is that this culture may give way to a culture of over-deference.</p><p>We also really care about <strong>raising the average quality of criticism</strong>. Perhaps you can recall some criticisms of effective altruism that you think were made in bad faith, or otherwise misrepresented their target in a mostly unhelpful and frustrating way. If we don\u2019t make an effort to encourage more careful, well-informed critical work, then we may have less reason to complain about the harms that poor-quality work can cause, such as by misinforming people who are learning about effective altruism. Crucially, we\u2019d also miss out on the real benefits of higher-quality, good-faith criticism.</p><p><a href=\"https://youtu.be/olX_5WSnBwk?t=1307\"><u>In his opening talk</u></a> for <a href=\"https://www.eaglobal.org/\">EA Global</a> this year, Will MacAskill considered how a major risk to the success of effective altruism is the risk of degrading its quality of thinking: \u201cif you look at other social movements, you get this club where there are certain beliefs that everyone holds, and it becomes an indicator of in-group mentality; and that can get strengthened if it\u2019s the case that if you want to get funding and achieve very big things you have to believe certain things \u2014 I think that would be very bad indeed. Looking at other social movements should make us worried about that as a failure mode for us as well.\u201d</p><p>It\u2019s also possible that some of the most useful critical work goes relatively unrewarded because it might be less attention-grabbing or narrow in its conclusions. Conducting really high-quality criticism is sometimes thankless work: as the blogger Dynomight <a href=\"https://dynomight.net/arguing/\"><u>points out</u></a>, there\u2019s rarely much glory in fact-checking someone else\u2019s work. We want to set up some incentives to attract this kind of work, as well as more broadly attention-grabbing work.</p><p>Ultimately, critiques have an impact by bringing about actual changes. The ultimate goal of this contest is to facilitate those positive changes, not <i>just</i> to spot what we\u2019re currently getting wrong.</p><p>In sum, we think and hope:&nbsp;</p><ol><li>Criticism will help us form truer beliefs, and that will help people with the project of doing good effectively. People and institutions in effective altruism might be wrong in significant ways \u2014 we want to catch that and correct our course.<ol><li>This is especially important in the non-profit context, since it lacks many of the signals in the for-profit world (like prices). For-profit companies have a strong signal of success: if they fail to make a profit, they eventually fail. One insight of effective altruism is that there are weaker pressures for nonprofits to be effective \u2014 to achieve the goals that really matter \u2014 because their ability to fundraise isn\u2019t necessarily tied to their effectiveness. Charity evaluators like GiveWell do an excellent job at evaluating nonprofits, but we should also try to be comparably rigorous and impartial in assessing EA organizations and projects, including in areas where outputs are harder to measure. Where natural feedback loops don\u2019t exist, it\u2019s our responsibility to try making them!</li><li>It\u2019s also especially important for effective altruism, given that so many of the ideas are relatively new and untested. We think this is especially true of longtermist work.</li></ol></li><li>Stress-testing important ideas is crucial even when the result is that the ideas are confirmed; this allows us to rely more freely on the ideas.</li><li>We want to sustain a culture of intellectual openness, open disagreement, and critical thinking. We hope that this contest will contribute to reinforcing that culture.</li><li>Highlighting especially good examples of criticism may create more templates for future critical work, and may make the broader community more appreciative of critical work.</li><li>We also think that people in the effective altruism network tend to hear more from other people in the network, and hope that this contest might bring in outside experts and voices. (You can see more discussion of this phenomenon in \"<a href=\"https://forum.effectivealtruism.org/posts/pxALB46SEkwNbfiNS/the-motivated-reasoning-critique-of-effective-altruism\"><u>The motivated reasoning critique of effective altruism</u></a>\".)</li><li>We want to break patterns of <a href=\"https://en.wikipedia.org/wiki/Pluralistic_ignorance\"><u>pluralistic ignorance</u></a> where people underrate how sceptical or uncertain others (including \u2018experts\u2019) are about some claim.</li></ol><p>Finally, we want to frame this contest as one step towards generating high-quality criticism, and not the final one. For instance, we\u2019re interested in following up with winning submissions, such as by meeting with winning entrants to discuss ways to translate your work into concrete changes and communicate your work to the relevant stakeholders.</p><h3>What this is <i>not</i> about</h3><p>Note that critical work is <a href=\"https://statmodeling.stat.columbia.edu/2020/07/22/is-it-better-to-be-more-positive/\"><u>not automatically valuable</u></a> just by virtue of being critical: it can be attention-grabbing in a negative way. It can be stressful and time-consuming to engage with bad-faith or ill-considered criticism. We have a responsibility to be especially careful here.</p><p>This contest isn\u2019t about making EA look open-minded or self-scrutinizing in a performative way: we want to award work that actually strikes us as useful, even if it isn\u2019t likely to be especially popular or legible for a general audience.</p><p>We\u2019re not going to privilege arguments for more caution about projects over arguments for urgency or haste. Scrutinizing projects in their early stages is a good way to avoid errors of <i>commission</i>; but <a href=\"https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation#Risks_of_omission__squandering_the_opportunity_\"><u>errors of </u><i><u>omission</u></i></a> (<i>not</i> going ahead with an ambitious project because of an unjustified amount of risk aversion, or oversensitivity to downsides over upsides) can be just as bad.</p><p>Similarly, we don\u2019t want this initiative to only result in writing that one-directionally worries about EA ideas or projects being too \u2018weird\u2019 or too different from some consensus or intuitions. We\u2019re just as interested to hear why some aspect of EA is being insufficiently weird \u2014 perhaps not taking certain ideas seriously enough. Relatedly, this isn\u2019t just about being more epistemically modest: we are likely being both overconfident in some spots, <i>and</i> overly modest in others. What matters is being well calibrated in our beliefs!</p><p>We would also caution against criticizing the actions or questioning the motivations of a specific individual, especially without first asking them. We urge you to focus on the ideas or \u2018artefacts\u2019 individuals produce, without speculating about personal motivations or character \u2014 this is rarely helpful.</p><h2>Contact us</h2><p>Email <a href=\"mailto:criticism-contest@effectivealtruism.com\"><u>criticism-contest@effectivealtruism.com</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/Y8gkABpa9R6ktkhYt/forum-user-manual#Messaging_other_users\"><u>message</u></a> any of the authors of this post via the Forum, or leave a comment on this post.&nbsp;</p><h2>Q&amp;A</h2><h3>Submissions and how they\u2019ll be judged</h3><ul><li><strong>Can I submit work I\u2019ve already done?</strong> Yes, if it's recent. We\u2019re accepting posts from the date of our <a href=\"https://forum.effectivealtruism.org/posts/Fx8pWSLKGwuqsfuRQ/pre-announcing-a-contest-for-critiques-and-red-teaming\"><u>pre-announcement</u></a> (March 25, 2022) onwards.</li><li><strong>Can I submit something that I got funding for already?</strong> Yes. <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">Let us know</a> if you have specific concerns.</li><li><strong>Can I refer another person\u2019s work? </strong>Yes. And if that person\u2019s work wins a prize (and the author didn\u2019t submit it themselves, and you\u2019re the first person to refer the work), we\u2019ll also reward you with a commission (5% of the prize). We\u2019d love to discover work from outside the EA community that could be relevant for effective altruism. Submit referrals <a href=\"https://forms.gle/ydNTFC9rivVeoXY46\"><u>via this form</u></a>.</li><li><strong>What if I want to work on a large project for this contest that I can\u2019t afford to carry out on my own time?</strong> <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">Contact us</a>. We can\u2019t guarantee anything, but we\u2019d like to help enable your work, by pointing you to <a href=\"https://forum.effectivealtruism.org/topics/funding-opportunities\"><u>sources of funding</u></a> in effective altruism, and potentially arranging direct financial support where necessary. If we (the organizers of this contest) directly fund your work in advance, we\u2019ll deduct whatever amount you received in advance from any potential prize that you win.</li><li><strong>I have a complaint or criticism about an organization or individual, but it\u2019s not something that\u2019s appropriate to share publicly.</strong> You might consider contacting the <a href=\"https://www.centreforeffectivealtruism.org/team#community-health-team\">CEA Community Health Team</a>, who can advise on the next steps, including acting as an intermediary. You can also <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScJooJD0Sm2csCYgd0Is6FkpyQa3ket8IIcFzd_FcTRU7avRg/viewform\"><u>send them an anonymous message</u></a>.</li><li><strong>Can I submit anonymously?</strong> Yes. You can <a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum#Rules_for_pseudonymous_and_multiple_accounts\"><u>make an anonymous account on the Forum</u></a>, or you can <a href=\"https://forms.gle/BhxBJr2fQ5RKm4KP7\"><u>use this form</u></a> to submit without posting to the Forum.</li><li><strong>Do I have to already be involved in effective altruism to submit something? </strong>No, not at all. We\u2019re actively excited to bring in external ideas and expertise. If you\u2019re new to the Forum, the <a href=\"https://forum.effectivealtruism.org/topics/all\"><u>Wiki</u></a> could be a good place to start to check for what has already been written. You\u2019re welcome to make broad criticisms of effective altruism, but focused critiques that draw on your area(s) of expertise could stand an especially good chance of being entirely novel.</li><li><strong>I\u2019d love to hear what [person who\u2019s not engaged with effective altruism] would have to say about [some aspect of effective altruism]. How can I make that happen? </strong>If you know this person, we encourage you to reach out to them! If you\u2019re unsure or uncomfortable about contacting them directly, <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">let us know</a>, and we can try getting in touch.</li><li><strong>Some of the panellists belong to organizations I\u2019d like to criticize. Isn\u2019t that an issue? </strong>All our panellists are committed to evaluating your work on its own merit \u2014 being associated with an org or project you are criticizing should and will not count as a reason to downgrade your work. Panellists will recuse themselves if they (or we) feel that a conflict of interest will inhibit their ability to fairly evaluate a particular submission. If you\u2019re still concerned about this or would like to request that specific panellists be recused, feel free to <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">contact us</a>.</li><li><strong>What counts as \u201cEA\u201d?</strong> We have in mind the ideas, institutions, projects, and communities associated with effective altruism. You can learn more at <a href=\"https://www.effectivealtruism.org/\"><u>effectivealtruism.org</u></a> and here on the <a href=\"https://forum.effectivealtruism.org/\"><u>Forum</u></a>.</li><li><strong>Does the criticism or red teaming have to come to the conclusion that the original work was wrong? </strong>No. We\u2019re very happy to award prizes to work of the form: \u201cI checked the arguments and sources in this text. In fact, they check out. Here are my notes.\u201d</li><li><strong>Does my submission need to fulfill all the </strong><a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Criteria\"><strong>criteria</strong></a><strong> outlined above?</strong> No. We understand that some formats make it difficult or impossible to satisfy all the requirements, and we don\u2019t want that to be a barrier to submitting. At the same time, we do think each of the criteria are good indicators of the kind of work we\u2019d like to see.</li></ul><h3>About the contest</h3><ul><li><strong>How does this relate to Training for Good\u2019s </strong><a href=\"https://forum.effectivealtruism.org/posts/DqBEwHqCdzMDeSBct/apply-for-red-team-challenge-may-7-june-4\"><strong><u>\u2018Red Team challenge\u2019</u></strong></a><strong>?</strong> The <a href=\"https://www.trainingforgood.com/red-teaming\"><u>Red Team Challenge</u></a> is not this prize, and this prize is not the Red Team Challenge (RTC). The RTC is a program run by <a href=\"https://www.trainingforgood.com/\"><u>Training for Good</u></a> which provides training in red teaming best practices and then pairs small teams of 2-4 people together to critique a particular claim and publish the results. We are very excited about the results of the programme being submitted to this contest! So this contest is a complement to the Red Team Challenge, rather than a substitute. Training for Good may also collaborate with us on workshops and [other resources].</li><li><strong>Where\u2019s the money coming from?</strong> The prizes will be awarded via the FTX Future Fund Regranting Program. The Centre for Effective Altruism is providing operational support (like coordination between judges). Note that the EA Forum is not sponsoring this prize, and isn't liable for it.</li><li><strong>Doesn\u2019t this penalize the people whose work is getting criticized? </strong>We want to encourage a norm where having your work fairly criticized <a href=\"https://statmodeling.stat.columbia.edu/2017/10/24/think-great-work-criticized-strangers-online/\"><u>is great news</u></a>: an indication that it was trying to answer an important question. We want to encourage a sense of criticism being part of the joint enterprise to figure out the right answers to important questions. However, we are aware that being criticized is not always enjoyable, and some criticism is made in bad faith. If you\u2019re concerned about being the subject of bad-faith criticism, <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">let us know</a>.</li><li><strong>Does this mean that you think that non-critical work is less valuable than critical work?</strong> No. We just think that high-quality critical work is often under-rewarded and under-supplied \u2014 like many other kinds of non-critical work!</li></ul><h3>Other</h3><ul><li><strong>I have another question that isn\u2019t answered in this post.</strong> Leave a comment if you suspect others might have the same question, and we\u2019ll try to answer it here. Otherwise, feel free to <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">contact us</a>.</li></ul><p><i>We're extremely grateful to everyone who helped us kick this off, including the many people who gave feedback following our </i><a href=\"https://forum.effectivealtruism.org/posts/Fx8pWSLKGwuqsfuRQ/pre-announcing-a-contest-for-critiques-and-red-teaming\"><i>pre-announcement</i></a><i> of the contest.</i></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 2, "xexCWMyds6QLWognu": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "uPLK59MTbEbekexbh", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 16, "extendedScore": null, "score": 0.6502518507231644, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-03T01:32:18.666Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Cross-posted from the <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming\">EA Forum</a>. I would add that we are <strong>open to and positively interested in critiques of any aspect of effective altruism from users of this Forum and the rationalist community</strong>. We plan to respond to comments on the EA Forum, but may check this post less often.</p><hr><h2 id=\"Introduction\">Introduction</h2><p><strong id=\"tl_dr__We_re_running_a_writing_contest_for_critically_engaging_with_theory_or_work_in_effective_altruism__EA___\">tl;dr: We're running a writing contest for critically engaging with theory or work in effective altruism (EA).&nbsp;</strong></p><p><strong>Submissions can be in a range of </strong><a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Formats\"><strong>formats</strong></a><strong> (from fact-checking to philosophical critiques or major project evaluations); and can focus on a range of </strong><a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#What_to_submit\"><strong>subject matters</strong></a><strong> (from assessing empirical or normative claims to evaluating organizations and practices).&nbsp;&nbsp;</strong></p><p><strong>We plan on distributing $100,000</strong>, and we may end up awarding more than this amount if we get many excellent submissions.&nbsp;</p><p>The deadline is <strong>September 1, 2022</strong>. You can find the submission instructions <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#How_to_apply\">below</a>. <strong>Neither formal nor significant affiliation with effective altruism is required to enter into the contest.</strong></p><p>We are:<a href=\"https://forum.effectivealtruism.org/users/lizka\"> <u>Lizka Vaintrob</u></a> (the<a href=\"https://forum.effectivealtruism.org/posts/aeR2Pses3TjWW8H9t/hello-from-the-new-content-specialist-at-cea\"> <u>Content Specialist</u></a> at the Centre for Effective Altruism),<a href=\"https://forum.effectivealtruism.org/users/jtm\"> </a><a href=\"https://forum.effectivealtruism.org/users/finm\"><u>Fin Moorhouse</u></a> (researcher at the Future of Humanity Institute), and <a href=\"https://forum.effectivealtruism.org/users/jtm\"><u>Joshua Teperowski Monrad</u></a> (biosecurity program associate at Effective Giving). The contest is funded via the <a href=\"https://forum.effectivealtruism.org/topics/future-fund\"><u>FTX Future Fund</u></a> Regranting Program, with organizational support from the <a href=\"https://forum.effectivealtruism.org/topics/centre-for-effective-altruism-1\"><u>Centre for Effective Altruism</u></a>.</p><p>We <a href=\"https://forum.effectivealtruism.org/posts/Fx8pWSLKGwuqsfuRQ/pre-announcing-a-contest-for-critiques-and-red-teaming\"><u>\u2018pre-announced\u2019 this contest in March</u></a>.&nbsp;</p><p>The rest of this post gives more details, outlines the kinds of critical work we think are especially valuable, and explains our rationale. We\u2019re also sharing <a href=\"https://forum.effectivealtruism.org/posts/uuQDgiJJaswEyyzan/resource-for-criticisms-and-red-teaming\">a companion resource for criticisms and red teams</a>.&nbsp;</p><h2 id=\"How_to_apply\">How to apply</h2><p>Submit by posting on the EA Forum<a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#fnwyhqsiy6e4\"><sup>[1]</sup></a>&nbsp;and tagging the post<a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#fn3kedvlnnl\"><sup>[2]</sup></a>&nbsp;with <a href=\"https://forum.effectivealtruism.org/topics/critiques-and-red-teaming-contest\"><u>the contest\u2019s tag</u></a>, or by filling out <a href=\"https://forms.gle/ydNTFC9rivVeoXY46\"><u>this form</u></a>.</p><p>If you post on the Forum, you don't need to do anything except tag your post<a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#fn3kedvlnnl\"><sup>[2]</sup></a>&nbsp;with the \u201c<a href=\"https://forum.effectivealtruism.org/topics/critiques-and-red-teaming-contest\"><u>Criticism and Red Teaming Contest</u></a>\u201d topic, and we\u2019ll consider your post for the contest. If you\u2019d prefer to post your writing outside the Forum, you can <a href=\"https://forms.gle/ydNTFC9rivVeoXY46\"><u>submit it via this form</u></a> \u2014 we\u2019d still encourage you to <a href=\"https://forum.effectivealtruism.org/posts/8yDsenRQhNF4HEDwu/link-posting-is-an-act-of-community-service\"><u>cross-post</u></a> it to the Forum (although please be mindful of copyright issues).&nbsp;</p><p>We also encourage you to refer other people\u2019s work to the contest if you think more people should know about it. To refer someone else\u2019s work, please <a href=\"https://forms.gle/ydNTFC9rivVeoXY46\"><u>submit it via this form</u></a>. If it wins, we may reward you for this \u2014 please see an explanation below.</p><p><strong id=\"The_deadline_is_September_1__2022_\">The deadline is September 1, 2022.</strong></p><p>Please <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">contact us</a> with any questions. You can also comment here.</p><h2 id=\"Prizes\">Prizes</h2><p>We have $100,000 currently set aside for prizes, which we plan on fully distributing.</p><p>Prizes will fall under three main tiers:</p><ul><li><strong>Winners: </strong>$20,000</li><li><strong>Runners up: </strong>$5,000 each</li><li><strong>Honourable mentions: </strong>$1,000 each</li></ul><p>In addition, we may award <strong>a prize of $100,000</strong> for outstanding work that looks likely to cause a very significant course adjustment in effective altruism.</p><p>Therefore, we\u2019re prepared to award (perhaps significantly) more than $100,000 if we\u2019re impressed by the quality and volume of submissions.&nbsp;</p><p><strong>We\u2019re also offering a bounty for referring winning submissions</strong>: if you <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#How_to_apply\">refer</a> a winning submission (if you\u2019re the first person to refer it, and the author never entered the contest themselves), you\u2019ll get a referral bounty of 5% of the award.</p><p>We will also consider helping you find proactive funding for your work if you require the security of guaranteed financial support to enable a large project (though we may deduct proactive funding from prize money if you are awarded one). See <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Q_A\">the FAQ</a> for more details.</p><p>Submissions must be posted or submitted no later than <strong>11:59 pm BST on September 1st</strong>, and we\u2019ll announce winners by the end of September.</p><h2 id=\"Criteria\">Criteria</h2><p>Overall, we want to reward critical work according to a question like: \u201c<strong>to what extent did this cause me to change my mind about something important?</strong>\u201d \u2014 where \u201cchange my mind\u201d can mean \u201cchange my best guess about whether some claim is true\u201d, or just \u201cbecome significantly more or less confident in this important thing.\u201d</p><p>Below are some virtues of the kind of work we expect to be most valuable. We\u2019ll look out for these features in the judging process, but we\u2019re aware it can be difficult or impossible to live up to all of them:</p><ul><li><strong>Critical</strong>. The piece takes a critical or questioning stance towards some aspect of EA theory or practice. Note that this does <strong>not</strong> mean that your conclusion must end up disagreeing with what you are criticizing; it is entirely possible to approach some work critically, check the sources, note some potential weaknesses, and conclude that the original was broadly correct.</li><li><strong>Important</strong>. The issues discussed really matter for our ability to do the most good as a movement.</li><li><strong>Constructive and action-relevant.</strong> Where possible we would be most interested in arguments that recommend some specific, realistic action or change of belief. It\u2019s fine to just point out where something is going wrong; even better to be constructive, by suggesting a concrete improvement.</li><li><strong>Transparent and legible. </strong>We encourage <a href=\"https://www.openphilanthropy.org/reasoning-transparency\"><u>transparency about your process</u></a>: how much expertise do you have? How confident are you about the claims you\u2019re making? What would change your mind? If your work includes data, how were they collected? Relatedly, we encourage <a href=\"https://forum.effectivealtruism.org/posts/oRx3LeqFdxN2JTANJ/epistemic-legibility\"><strong><u>epistemic legibility</u></strong></a>: the property of being <i>easy to argue with</i>, separate from being correct.</li><li><strong>Aware</strong>. Take some time to check that you\u2019re not missing an existing response to your argument. If responses do exist, mention (or engage with) them.</li><li><strong>Novel</strong>. The piece presents new arguments, or otherwise presents familiar ideas in a new way. Novelty is great but not always necessary \u2014 it\u2019s often still valuable to <a href=\"https://forum.effectivealtruism.org/posts/EbvJRAvwtKAMBn2td/distillation-and-research-debt\"><u>distill</u></a> or \u201ctranslate\u201d existing criticisms.</li><li><strong>Focused. </strong>Critical work is often (but not always) most useful when it is focused on a small number of arguments and a small number of objects. We\u2019d love to see (and we\u2019re likely to reward) work that engages with specific texts, strategic choices, or claims.</li></ul><p>We don't expect that every winning piece needs to do well at every one of these criteria, but we do think each of these criteria can help you most effectively change people\u2019s minds with your work.</p><p>We also want to reward clarity of writing, avoiding \u2018punching down\u2019, awareness of context, and a <a href=\"https://forum.effectivealtruism.org/posts/HDAXztEbjJsyHLKP7/outline-of-galef-s-scout-mindset\"><u>scout mindset</u></a>. We don\u2019t want to encourage personal attacks, or diatribes that are likely to produce much more heat than light. And we hope that subject-matter experts who don\u2019t typically associate with EA find out about this, and share insights we haven\u2019t yet heard.</p><h2 id=\"What_to_submit\">What to submit</h2><p>We\u2019re looking for critical work that you think is important or useful for EA. That\u2019s a broad remit, so we\u2019ve suggested some topics and kinds of critiques below.</p><p>If you\u2019re looking for more detail,<strong> we\u2019ve collaborated on </strong><a href=\"https://forum.effectivealtruism.org/posts/uuQDgiJJaswEyyzan/resource-for-criticisms-and-red-teaming\"><strong>a separate post</strong></a><strong> that collects resources for red teaming and criticisms</strong>, including guides to different kinds of criticisms, and examples. If you\u2019re interested in participating in this contest, we highly recommend that you take a look. (We\u2019d also love help updating and improving it.)</p><p>It\u2019s helpful \u2014but not required \u2014 to also suggest 1\u20133 people you think most need to heed your critique. For many topics, this nomination is better done privately (<a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">contact us</a>, or submit through <a href=\"https://forms.gle/ydNTFC9rivVeoXY46\">the form</a>). We\u2019ll send it their way where possible. (If you don\u2019t know who needs to see it most, we\u2019ll work it out.)&nbsp;</p><h3 id=\"Formats\">Formats</h3><p>You might consider framing your submission as one of the following:</p><ul><li><strong>Minimal trust investigation</strong> \u2014 A <a href=\"https://www.cold-takes.com/minimal-trust-investigations/\"><u>minimal trust investigation</u></a> involves suspending your trust in others' judgments, and trying to understand the case for and against some claim yourself. Suspending trust does <i>not</i> mean determining in advance that you\u2019ll end up disagreeing.</li><li><strong>Red teaming </strong>\u2014 <a href=\"https://www.gov.uk/government/publications/a-guide-to-red-teaming\"><u>\u2018Red teaming\u2019</u></a> is the practice of \u201csubjecting [...] plans, programmes, ideas and assumptions to rigorous analysis and challenge\u201d. You\u2019re setting out to find the strongest reasonable case against something, whatever you actually think about it (and you should flag that this is what you\u2019re doing).</li><li><strong>Fact checking and chasing citation trails</strong> \u2014 If you notice claims that seem crucial, but whose origin is unclear, you could track down the source, and evaluate its legitimacy.</li><li><strong>Adversarial collaboration</strong> \u2014 An <a href=\"https://en.wikipedia.org/wiki/Adversarial_collaboration\"><u>adversarial collaboration</u></a> is where people with opposing views work together to clarify their disagreements.</li><li><strong>Clarifying confusions</strong> \u2014 You might simply be confused about some aspect of EA, rather than confidently critical. You could try getting clear on what you\u2019re confused about, and why.</li><li><strong>Evaluating organizations</strong> \u2014 including their (implicit) theory of change, key claims, and their track record; and suggesting concrete changes where relevant.</li><li><strong>Steelmanning and \u2018translating\u2019 existing criticism for an EA audience</strong> \u2014 We\u2019d love to see work succinctly explaining these existing ideas, and constructing the strongest versions (\u2018<a href=\"https://www.lesswrong.com/tag/steelmanning\"><u>steelmanning</u></a>\u2019) them. You might consider doing this in collaboration with a domain expert who does not consider themself part of the EA community.</li></ul><p>Again, for more detail on topic ideas, kinds of critiques, and examples: visit <a href=\"https://forum.effectivealtruism.org/posts/uuQDgiJJaswEyyzan/resource-for-criticisms-and-red-teaming\">our longer post with resources for critiques and red teams</a>.&nbsp;</p><p>We don\u2019t want to give an analogous list for <i>topic </i>ideas, because any list is necessarily going to leave things out. However, you might take a look at Joshua\u2019s post outlining <a href=\"https://forum.effectivealtruism.org/posts/HzyYoLK2ERTnDmrjB/four-categories-of-effective-altruism-critiques\"><u>four categories of effective altruism critiques</u></a>: <strong>normative and moral</strong> questions, <strong>empirical</strong> questions, <strong>institutions</strong> &amp; <strong>organizations</strong>, and <strong>social norms</strong> &amp; <strong>practices</strong>. Browsing <a href=\"https://forum.effectivealtruism.org/\"><u>this Forum</u></a> could be a good way to get ideas if you are new to effective altruism.</p><p>Browsing <a href=\"https://forum.effectivealtruism.org/\"><u>this Forum</u></a> (especially curated lists like <a href=\"https://forum.effectivealtruism.org/posts/FEFEvC6BzswR4oQqm/results-from-the-first-decade-review\"><u>the Decade Review prizewinners</u></a>, <a href=\"https://forum.effectivealtruism.org/topics/all\"><u>the EA Wiki</u></a>, and <a href=\"https://forum.effectivealtruism.org/handbook\"><u>the EA Handbook</u></a>) could be a good way to get ideas if you are new to effective altruism.</p><p>If you\u2019re unsure whether something you plan on writing could count for this contest, feel free to ask us.</p><h2 id=\"Additional_resources\">Additional resources</h2><p>We\u2019ve compiled <a href=\"https://forum.effectivealtruism.org/posts/uuQDgiJJaswEyyzan/resource-for-criticisms-and-red-teaming\">a companion post</a>, in which we\u2019ve collected some resources for criticisms and red teaming.&nbsp;</p><p>We\u2019re also tentatively planning on running (or helping with) several workshops on criticisms and red teaming, which will be open to anyone who is interested, including people who are new to effective altruism. We hope that the first two will be in June. If you\u2019d like to hear about dates when they\u2019re decided, you can <a href=\"https://forms.gle/2nLZ6ieTmBQfJ2xk8\"><u>fill out this form</u></a>.</p><h2 id=\"The_judging_panel\">The judging panel</h2><p>The judging panel is:</p><ul><li><a href=\"https://forum.effectivealtruism.org/users/rebecca-kagan\"><strong><u>Rebecca Kagan</u></strong></a>, J.D. Candidate at Harvard Law School and formerly an External Affairs Specialist at Georgetown's Center for Security and Emerging Technology (CSET)</li><li><a href=\"https://forum.effectivealtruism.org/users/jessica_mccurdy\"><strong><u>Jessica McCurdy</u></strong></a>, Groups Associate at the Centre for Effective Altruism</li><li><a href=\"https://www.openphilanthropy.org/about/team/zachary-robinson\"><strong><u>Zachary Robinson</u></strong></a>, Chief of Staff at Open Philanthropy</li><li><a href=\"https://applieddivinitystudies.com/\"><strong><u>Applied Divinity Studies</u></strong></a>, independent blogger</li><li><a href=\"https://forum.effectivealtruism.org/users/charlottesiegmann\"><strong><u>Charlotte Siegmann</u></strong></a>, Research Fellow at Longview Philanthropy; Predoctoral Research Fellow in Economics at the Global Priorities Institute</li><li><a href=\"https://www.linkedin.com/in/particlemania/\"><strong><u>TJ</u></strong></a>, Research Scholar at the Future of Humanity Institute</li><li><a href=\"https://forum.effectivealtruism.org/users/owen_cotton-barratt\"><strong><u>Owen Cotton-Barratt</u></strong></a>, independent researcher and board member of the Centre for Effective Altruism</li><li><a href=\"https://forum.effectivealtruism.org/users/technicalities\"><strong><u>Gavin Leech</u></strong></a>, founder of <a href=\"https://arbresearch.com/\"><u>Arb Research</u></a>, Strategic Advisor to <a href=\"https://www.mercatus.org/emergent-ventures\"><u>Emergent Ventures</u></a> panel on AI Talent, AI PhD student</li><li><a href=\"https://forum.effectivealtruism.org/users/nicole_ross\"><strong><u>Nicole Ross</u></strong></a>, Head of Community Health at the Centre for Effective Altruism</li><li><a href=\"https://www.alignmentforum.org/users/xuan\"><strong><u>Xuan</u></strong><u> </u><strong><u>(Tan Zhi Xuan)</u></strong></a>, AI PhD student at MIT, Board Member of EA Singapore</li><li>\u2026and ourselves: <a href=\"https://forum.effectivealtruism.org/users/lizka\"><strong><u>Lizka</u></strong></a>, <a href=\"https://www.finmoorhouse.com/\"><strong><u>Fin</u></strong></a>, and <a href=\"https://www.fhi.ox.ac.uk/team/joshua-monrad/\"><strong><u>Joshua</u></strong></a>.</li></ul><p>No one on the judging panel will be able to \u201cveto\u201d winners, and every submission will be read by at least two people. If submissions are technical and outside of the panelists\u2019 fields of expertise, we will consult domain experts. We might add more panelists if we get many submissions.</p><p>If we get many submissions or if we find that the current panel doesn\u2019t have enough bandwidth, we may invite more people to the panel.&nbsp;</p><h2 id=\"Rationale\">Rationale</h2><p>Why do we think this matters? In short, we think there are some reasons to expect good criticism to be undersupplied relative to its real value. And that matters: as EA grows, it\u2019s going to become increasingly important that we scrutinize the ideas and assumptions behind key decisions \u2014 and that we welcome outside experts to do the same.</p><p>Encouraging criticism is also a way to encourage a culture of <strong>independent thinking</strong>, and <strong>openness to criticism and scrutiny </strong>within the EA community. Part of what made and continues to make EA so special is its epistemic culture: a willingness to question and be questioned, and freedom to take contrarian or unusual ideas seriously. As EA continues to grow, one failure mode we anticipate is that this culture may give way to a culture of over-deference.</p><p>We also really care about <strong>raising the average quality of criticism</strong>. Perhaps you can recall some criticisms of effective altruism that you think were made in bad faith, or otherwise misrepresented their target in a mostly unhelpful and frustrating way. If we don\u2019t make an effort to encourage more careful, well-informed critical work, then we may have less reason to complain about the harms that poor-quality work can cause, such as by misinforming people who are learning about effective altruism. Crucially, we\u2019d also miss out on the real benefits of higher-quality, good-faith criticism.</p><p><a href=\"https://youtu.be/olX_5WSnBwk?t=1307\"><u>In his opening talk</u></a> for <a href=\"https://www.eaglobal.org/\">EA Global</a> this year, Will MacAskill considered how a major risk to the success of effective altruism is the risk of degrading its quality of thinking: \u201cif you look at other social movements, you get this club where there are certain beliefs that everyone holds, and it becomes an indicator of in-group mentality; and that can get strengthened if it\u2019s the case that if you want to get funding and achieve very big things you have to believe certain things \u2014 I think that would be very bad indeed. Looking at other social movements should make us worried about that as a failure mode for us as well.\u201d</p><p>It\u2019s also possible that some of the most useful critical work goes relatively unrewarded because it might be less attention-grabbing or narrow in its conclusions. Conducting really high-quality criticism is sometimes thankless work: as the blogger Dynomight <a href=\"https://dynomight.net/arguing/\"><u>points out</u></a>, there\u2019s rarely much glory in fact-checking someone else\u2019s work. We want to set up some incentives to attract this kind of work, as well as more broadly attention-grabbing work.</p><p>Ultimately, critiques have an impact by bringing about actual changes. The ultimate goal of this contest is to facilitate those positive changes, not <i>just</i> to spot what we\u2019re currently getting wrong.</p><p>In sum, we think and hope:&nbsp;</p><ol><li>Criticism will help us form truer beliefs, and that will help people with the project of doing good effectively. People and institutions in effective altruism might be wrong in significant ways \u2014 we want to catch that and correct our course.<ol><li>This is especially important in the non-profit context, since it lacks many of the signals in the for-profit world (like prices). For-profit companies have a strong signal of success: if they fail to make a profit, they eventually fail. One insight of effective altruism is that there are weaker pressures for nonprofits to be effective \u2014 to achieve the goals that really matter \u2014 because their ability to fundraise isn\u2019t necessarily tied to their effectiveness. Charity evaluators like GiveWell do an excellent job at evaluating nonprofits, but we should also try to be comparably rigorous and impartial in assessing EA organizations and projects, including in areas where outputs are harder to measure. Where natural feedback loops don\u2019t exist, it\u2019s our responsibility to try making them!</li><li>It\u2019s also especially important for effective altruism, given that so many of the ideas are relatively new and untested. We think this is especially true of longtermist work.</li></ol></li><li>Stress-testing important ideas is crucial even when the result is that the ideas are confirmed; this allows us to rely more freely on the ideas.</li><li>We want to sustain a culture of intellectual openness, open disagreement, and critical thinking. We hope that this contest will contribute to reinforcing that culture.</li><li>Highlighting especially good examples of criticism may create more templates for future critical work, and may make the broader community more appreciative of critical work.</li><li>We also think that people in the effective altruism network tend to hear more from other people in the network, and hope that this contest might bring in outside experts and voices. (You can see more discussion of this phenomenon in \"<a href=\"https://forum.effectivealtruism.org/posts/pxALB46SEkwNbfiNS/the-motivated-reasoning-critique-of-effective-altruism\"><u>The motivated reasoning critique of effective altruism</u></a>\".)</li><li>We want to break patterns of <a href=\"https://en.wikipedia.org/wiki/Pluralistic_ignorance\"><u>pluralistic ignorance</u></a> where people underrate how sceptical or uncertain others (including \u2018experts\u2019) are about some claim.</li></ol><p>Finally, we want to frame this contest as one step towards generating high-quality criticism, and not the final one. For instance, we\u2019re interested in following up with winning submissions, such as by meeting with winning entrants to discuss ways to translate your work into concrete changes and communicate your work to the relevant stakeholders.</p><h3 id=\"What_this_is_not_about\">What this is <i>not</i> about</h3><p>Note that critical work is <a href=\"https://statmodeling.stat.columbia.edu/2020/07/22/is-it-better-to-be-more-positive/\"><u>not automatically valuable</u></a> just by virtue of being critical: it can be attention-grabbing in a negative way. It can be stressful and time-consuming to engage with bad-faith or ill-considered criticism. We have a responsibility to be especially careful here.</p><p>This contest isn\u2019t about making EA look open-minded or self-scrutinizing in a performative way: we want to award work that actually strikes us as useful, even if it isn\u2019t likely to be especially popular or legible for a general audience.</p><p>We\u2019re not going to privilege arguments for more caution about projects over arguments for urgency or haste. Scrutinizing projects in their early stages is a good way to avoid errors of <i>commission</i>; but <a href=\"https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation#Risks_of_omission__squandering_the_opportunity_\"><u>errors of </u><i><u>omission</u></i></a> (<i>not</i> going ahead with an ambitious project because of an unjustified amount of risk aversion, or oversensitivity to downsides over upsides) can be just as bad.</p><p>Similarly, we don\u2019t want this initiative to only result in writing that one-directionally worries about EA ideas or projects being too \u2018weird\u2019 or too different from some consensus or intuitions. We\u2019re just as interested to hear why some aspect of EA is being insufficiently weird \u2014 perhaps not taking certain ideas seriously enough. Relatedly, this isn\u2019t just about being more epistemically modest: we are likely being both overconfident in some spots, <i>and</i> overly modest in others. What matters is being well calibrated in our beliefs!</p><p>We would also caution against criticizing the actions or questioning the motivations of a specific individual, especially without first asking them. We urge you to focus on the ideas or \u2018artefacts\u2019 individuals produce, without speculating about personal motivations or character \u2014 this is rarely helpful.</p><h2 id=\"Contact_us\">Contact us</h2><p>Email <a href=\"mailto:criticism-contest@effectivealtruism.com\"><u>criticism-contest@effectivealtruism.com</u></a>, <a href=\"https://forum.effectivealtruism.org/posts/Y8gkABpa9R6ktkhYt/forum-user-manual#Messaging_other_users\"><u>message</u></a> any of the authors of this post via the Forum, or leave a comment on this post.&nbsp;</p><h2 id=\"Q_A\">Q&amp;A</h2><h3 id=\"Submissions_and_how_they_ll_be_judged\">Submissions and how they\u2019ll be judged</h3><ul><li><strong>Can I submit work I\u2019ve already done?</strong> Yes, if it's recent. We\u2019re accepting posts from the date of our <a href=\"https://forum.effectivealtruism.org/posts/Fx8pWSLKGwuqsfuRQ/pre-announcing-a-contest-for-critiques-and-red-teaming\"><u>pre-announcement</u></a> (March 25, 2022) onwards.</li><li><strong>Can I submit something that I got funding for already?</strong> Yes. <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">Let us know</a> if you have specific concerns.</li><li><strong>Can I refer another person\u2019s work? </strong>Yes. And if that person\u2019s work wins a prize (and the author didn\u2019t submit it themselves, and you\u2019re the first person to refer the work), we\u2019ll also reward you with a commission (5% of the prize). We\u2019d love to discover work from outside the EA community that could be relevant for effective altruism. Submit referrals <a href=\"https://forms.gle/ydNTFC9rivVeoXY46\"><u>via this form</u></a>.</li><li><strong>What if I want to work on a large project for this contest that I can\u2019t afford to carry out on my own time?</strong> <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">Contact us</a>. We can\u2019t guarantee anything, but we\u2019d like to help enable your work, by pointing you to <a href=\"https://forum.effectivealtruism.org/topics/funding-opportunities\"><u>sources of funding</u></a> in effective altruism, and potentially arranging direct financial support where necessary. If we (the organizers of this contest) directly fund your work in advance, we\u2019ll deduct whatever amount you received in advance from any potential prize that you win.</li><li><strong>I have a complaint or criticism about an organization or individual, but it\u2019s not something that\u2019s appropriate to share publicly.</strong> You might consider contacting the <a href=\"https://www.centreforeffectivealtruism.org/team#community-health-team\">CEA Community Health Team</a>, who can advise on the next steps, including acting as an intermediary. You can also <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScJooJD0Sm2csCYgd0Is6FkpyQa3ket8IIcFzd_FcTRU7avRg/viewform\"><u>send them an anonymous message</u></a>.</li><li><strong>Can I submit anonymously?</strong> Yes. You can <a href=\"https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum#Rules_for_pseudonymous_and_multiple_accounts\"><u>make an anonymous account on the Forum</u></a>, or you can <a href=\"https://forms.gle/BhxBJr2fQ5RKm4KP7\"><u>use this form</u></a> to submit without posting to the Forum.</li><li><strong>Do I have to already be involved in effective altruism to submit something? </strong>No, not at all. We\u2019re actively excited to bring in external ideas and expertise. If you\u2019re new to the Forum, the <a href=\"https://forum.effectivealtruism.org/topics/all\"><u>Wiki</u></a> could be a good place to start to check for what has already been written. You\u2019re welcome to make broad criticisms of effective altruism, but focused critiques that draw on your area(s) of expertise could stand an especially good chance of being entirely novel.</li><li><strong>I\u2019d love to hear what [person who\u2019s not engaged with effective altruism] would have to say about [some aspect of effective altruism]. How can I make that happen? </strong>If you know this person, we encourage you to reach out to them! If you\u2019re unsure or uncomfortable about contacting them directly, <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">let us know</a>, and we can try getting in touch.</li><li><strong>Some of the panellists belong to organizations I\u2019d like to criticize. Isn\u2019t that an issue? </strong>All our panellists are committed to evaluating your work on its own merit \u2014 being associated with an org or project you are criticizing should and will not count as a reason to downgrade your work. Panellists will recuse themselves if they (or we) feel that a conflict of interest will inhibit their ability to fairly evaluate a particular submission. If you\u2019re still concerned about this or would like to request that specific panellists be recused, feel free to <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">contact us</a>.</li><li><strong>What counts as \u201cEA\u201d?</strong> We have in mind the ideas, institutions, projects, and communities associated with effective altruism. You can learn more at <a href=\"https://www.effectivealtruism.org/\"><u>effectivealtruism.org</u></a> and here on the <a href=\"https://forum.effectivealtruism.org/\"><u>Forum</u></a>.</li><li><strong>Does the criticism or red teaming have to come to the conclusion that the original work was wrong? </strong>No. We\u2019re very happy to award prizes to work of the form: \u201cI checked the arguments and sources in this text. In fact, they check out. Here are my notes.\u201d</li><li><strong>Does my submission need to fulfill all the </strong><a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Criteria\"><strong>criteria</strong></a><strong> outlined above?</strong> No. We understand that some formats make it difficult or impossible to satisfy all the requirements, and we don\u2019t want that to be a barrier to submitting. At the same time, we do think each of the criteria are good indicators of the kind of work we\u2019d like to see.</li></ul><h3 id=\"About_the_contest\">About the contest</h3><ul><li><strong>How does this relate to Training for Good\u2019s </strong><a href=\"https://forum.effectivealtruism.org/posts/DqBEwHqCdzMDeSBct/apply-for-red-team-challenge-may-7-june-4\"><strong><u>\u2018Red Team challenge\u2019</u></strong></a><strong>?</strong> The <a href=\"https://www.trainingforgood.com/red-teaming\"><u>Red Team Challenge</u></a> is not this prize, and this prize is not the Red Team Challenge (RTC). The RTC is a program run by <a href=\"https://www.trainingforgood.com/\"><u>Training for Good</u></a> which provides training in red teaming best practices and then pairs small teams of 2-4 people together to critique a particular claim and publish the results. We are very excited about the results of the programme being submitted to this contest! So this contest is a complement to the Red Team Challenge, rather than a substitute. Training for Good may also collaborate with us on workshops and [other resources].</li><li><strong>Where\u2019s the money coming from?</strong> The prizes will be awarded via the FTX Future Fund Regranting Program. The Centre for Effective Altruism is providing operational support (like coordination between judges). Note that the EA Forum is not sponsoring this prize, and isn't liable for it.</li><li><strong>Doesn\u2019t this penalize the people whose work is getting criticized? </strong>We want to encourage a norm where having your work fairly criticized <a href=\"https://statmodeling.stat.columbia.edu/2017/10/24/think-great-work-criticized-strangers-online/\"><u>is great news</u></a>: an indication that it was trying to answer an important question. We want to encourage a sense of criticism being part of the joint enterprise to figure out the right answers to important questions. However, we are aware that being criticized is not always enjoyable, and some criticism is made in bad faith. If you\u2019re concerned about being the subject of bad-faith criticism, <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">let us know</a>.</li><li><strong>Does this mean that you think that non-critical work is less valuable than critical work?</strong> No. We just think that high-quality critical work is often under-rewarded and under-supplied \u2014 like many other kinds of non-critical work!</li></ul><h3 id=\"Other\">Other</h3><ul><li><strong>I have another question that isn\u2019t answered in this post.</strong> Leave a comment if you suspect others might have the same question, and we\u2019ll try to answer it here. Otherwise, feel free to <a href=\"https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#Contact_us\">contact us</a>.</li></ul><p><i>We're extremely grateful to everyone who helped us kick this off, including the many people who gave feedback following our </i><a href=\"https://forum.effectivealtruism.org/posts/Fx8pWSLKGwuqsfuRQ/pre-announcing-a-contest-for-critiques-and-red-teaming\"><i>pre-announcement</i></a><i> of the contest.</i></p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "tl;dr: We're running a writing contest for critically engaging with theory or work in effective altruism (EA).\u00a0", "anchor": "tl_dr__We_re_running_a_writing_contest_for_critically_engaging_with_theory_or_work_in_effective_altruism__EA___", "level": 3}, {"title": "How to apply", "anchor": "How_to_apply", "level": 1}, {"title": "The deadline is September 1, 2022.", "anchor": "The_deadline_is_September_1__2022_", "level": 3}, {"title": "Prizes", "anchor": "Prizes", "level": 1}, {"title": "Criteria", "anchor": "Criteria", "level": 1}, {"title": "What to submit", "anchor": "What_to_submit", "level": 1}, {"title": "Formats", "anchor": "Formats", "level": 2}, {"title": "Additional resources", "anchor": "Additional_resources", "level": 1}, {"title": "The judging panel", "anchor": "The_judging_panel", "level": 1}, {"title": "Rationale", "anchor": "Rationale", "level": 1}, {"title": "What this is not about", "anchor": "What_this_is_not_about", "level": 2}, {"title": "Contact us", "anchor": "Contact_us", "level": 1}, {"title": "Q&A", "anchor": "Q_A", "level": 1}, {"title": "Submissions and how they\u2019ll be judged", "anchor": "Submissions_and_how_they_ll_be_judged", "level": 2}, {"title": "About the contest", "anchor": "About_the_contest", "level": 2}, {"title": "Other", "anchor": "Other", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 19}, "showModerationGuidelines": false, "moderationStyle": "norm-enforcing", "hideCommentKarma": false, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-02T20:27:18.871Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-02T21:41:44.784Z", "modifiedAt": "2022-06-03T06:51:45.491Z", "url": null, "title": "Confused why a \"capabilities research is good for alignment progress\" position isn't discussed more", "slug": "confused-why-a-capabilities-research-is-good-for-alignment", "viewCount": null, "lastCommentedAt": "2022-06-03T06:51:44.823Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EzAt4SbtQcXtDNhHK/confused-why-a-capabilities-research-is-good-for-alignment", "pageUrlRelative": "/posts/EzAt4SbtQcXtDNhHK/confused-why-a-capabilities-research-is-good-for-alignment", "linkUrl": "https://www.lesswrong.com/posts/EzAt4SbtQcXtDNhHK/confused-why-a-capabilities-research-is-good-for-alignment", "postedAtFormatted": "Thursday, June 2nd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Confused%20why%20a%20%22capabilities%20research%20is%20good%20for%20alignment%20progress%22%20position%20isn't%20discussed%20more&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConfused%20why%20a%20%22capabilities%20research%20is%20good%20for%20alignment%20progress%22%20position%20isn't%20discussed%20more%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEzAt4SbtQcXtDNhHK%2Fconfused-why-a-capabilities-research-is-good-for-alignment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Confused%20why%20a%20%22capabilities%20research%20is%20good%20for%20alignment%20progress%22%20position%20isn't%20discussed%20more%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEzAt4SbtQcXtDNhHK%2Fconfused-why-a-capabilities-research-is-good-for-alignment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEzAt4SbtQcXtDNhHK%2Fconfused-why-a-capabilities-research-is-good-for-alignment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1286, "htmlBody": "<p>The predominant view on LW seems to be \"pure AI capabilities research is bad, because capabilities progress alone doesn't contribute to alignment progress, and capabilities progress without alignment progress means that we're doomed\".</p><p>I understand the arguments for this position, but I have what might be called the opposite position. The opposite position seems at least as intuitive as the standard position to me, and it confuses me that it's not discussed more. (I'm not confused that people reject it; I'm confused that nobody seems to even bring it up for the purpose of rejecting it.)</p><p>The opposite position is \"In order to do alignment research, we need to understand how AGI works; and we currently don't understand how AGI works, so we need to have more capabilities research so that we would have a chance of figuring it out. Doing capabilities research now is good because it's likely to be slower now than it might be in some future where we had even more computing power, neuroscience understanding, etc. than we do now. If we successfully delayed capabilities research until a later time, then we might get a sudden spurt of it and wouldn't have the time to turn our increased capabilities understanding into alignment progress. Thus by doing capabilities research now, we buy ourselves a longer time period in which it's possible to do more effective alignment research.\"</p><p>Some reasons I have for holding this position:</p><p><strong>1)</strong> I used to do AI strategy research. Among other things, I looked into <a href=\"https://kajsotala.fi/assets/2017/10/how_feasible.pdf\">how feasible it is for intelligence to rapidly turn superintelligent</a>, and <a href=\"https://www.lesswrong.com/posts/8uJ3n3hu8pLXC4YNE/some-conceptual-highlights-from-disjunctive-scenarios-of-1\">what kinds of pathways there are into AI disaster</a>. But a thought that I kept having when doing any such research was \"I don't know if any of this theory is of any use, because so much depends on what the world will be like when actual AGI is developed, and what that AGI will look in the first place. Without knowing what AGI will look like, I don't know whether any of the assumptions I'm making about it are going to hold. If any one of them fails to hold, the whole paper might turn out to be meaningless.\"</p><p>Eventually, I concluded that I can't figure out a way to make the outputs of strategy research useful for as long as I know as little about AGI as I do. Then I went to do something else with my life, since it seemed too early to do useful AGI strategy research (as far as I could tell).</p><p><strong>2) </strong>Compare the state of AI now, to how it was before the deep learning revolution happened. It seems obvious to me that our current understanding of DL puts us in a better position to do alignment research than we were before the DL revolution. For instance, Redwood Research is doing research on language models because they <a href=\"https://www.alignmentforum.org/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project#Why_we_re_doing_this\">believe that their research is analogous to some long-term problems</a>.&nbsp;</p><p>Assume that Redwood Research's work will actually turn out to be useful for aligning superintelligent AI. Language models are one of the results of the DL revolution, so their work couldn't have been done before that revolution. It seems that in a counterfactual world where the DL revolution happened later and the DL era was compressed into a shorter timespan, our chances of alignment would be worse since that world's equivalent of Redwood Research would have less time to do their research.</p><p><strong>3)</strong> As a similar consideration, language models are already \"deceptive\" in a sense - asked something that it has no clue about, InstructGPT will <a href=\"https://twitter.com/xuenay/status/1509808406458216449\">happily come up with confident-sounding nonsense</a>. When I linked people to some of that nonsense, multiple people pointed out that InstructGPT's answers sound like the kind of a student who's taking an exam and is asked to write an essay about a topic they know nothing about, but tries to fake it anyway (that is, trying to deceive the examiner).&nbsp;</p><p>Thus, even if you are doing pure capabilities research and just want your AI system to deliver people accurate answers, it is <i>already</i> the case that you can see a system like InstructGPT \"trying to deceive\" people. If you are building a question-answering system, you want to build one that people can trust to give accurate answers rather than impressive-sounding bullshit, so you have the incentive to work on identifying and stopping such \"deceptive\" computations as a capabilities researcher already.</p><p>So it has already happened that</p><ul><li>Progress in capabilities research gives us a new concrete example of how e.g. deception manifests in practice, that can be used to develop our understanding of it and develop new ideas for dealing with it.</li><li>Capabilities research reaches a point where even capabilities researchers have a natural reason to care about alignment, reducing the difference between \"capabilities research\" and \"alignment research\".</li><li>Thus, our understanding and awareness of deception is likely to improve as we get closer to AGI, and by that time we will have already learned a lot about how deception manifests in simpler systems and how to deal with it, and maybe some of that will suggest principles that generalize to more powerful systems as well.</li></ul><p>It's not that I'd put a particularly high probability on InstructGPT by itself leading to any important insights about either deception in particular or alignment in general. InstructGPT is just an instance of something that seems likely to help us understand deception a little bit better. And given that, it seems reasonable to expect that further capabilities development will also give us small insights to various alignment-related questions, and maybe all those small insights will combine to give us the answers we need.</p><p><strong>4) </strong>Still on the topic of deception, there are arguments suggesting that something like GPT will always be \"deceptive\" for <a href=\"https://www.lesswrong.com/tag/goodhart-s-law\">Goodhart's Law</a> and <a href=\"https://www.lesswrong.com/posts/nFv2buafNc9jSaxAH/siren-worlds-and-the-perils-of-over-optimised-search\">Siren World</a> reasons. We can only reward an AI system for producing answers that look good to us, but this incentivizes the system to produce answers that look increasingly good to us, rather than answers that are actually correct. \"Looking good\" and \"being correct\" correlate with each other to some extent, but will eventually be pushed apart once there's enough optimization pressure on the \"looking good\" part.</p><p>As such, this seems like an unsolvable problem... <i>but</i> at the same time, if you ask me a question, I can have a desire to actually give a correct and useful answer to your question, rather than just giving you an answer that you find maximally compelling. More generally, humans can and <i>often do</i> have a genuine desire to help other humans (or even non-human animals) fulfill their preferences, rather than just having a desire to superficially fake cooperativeness.</p><p>I'm not sure how this desire works, but I don't think you could train GPT to have it. It looks like some sort of theory of mind is involved in how the goal is defined. If I want to help you fulfill your preferences, then I have a sense of what it would mean for your preferences to be fulfilled, and I can have a goal of optimizing for that (even while I am uncertain of what exactly your preferences <i>are</i>).</p><p>We don't currently seem to know how to do this kind of a theory of mind, but it can't be <i>that</i> much more complicated than other human-level capabilities are, since even many non-human animals seem to have some version of it. Still, I don't think we can yet implement that kind of a theory of mind in any AI system. So we have to wait for our capabilities to progress to the kind of a point where this kind of a capacity becomes possible, and then we can hopefully use that capabilities understanding to solve what looks like a crucial piece of alignment understanding.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 4, "ZFrgTgzwEfStg26JL": 2, "BisjoDrd3oNatDu7X": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "EzAt4SbtQcXtDNhHK", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 58, "extendedScore": null, "score": 2.5816200370631486, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-03T01:31:23.058Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": 13, "af": true, "version": "1.0.0", "pingbacks": {"Posts": ["8uJ3n3hu8pLXC4YNE", "k7oxdbNaGATZbtEg3", "nFv2buafNc9jSaxAH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": "1.0.0", "afDate": null, "afBaseScore": 19, "afExtendedScore": null, "afCommentCount": 5, "afLastCommentedAt": "2022-06-03T06:51:44.557Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-02T23:46:31.360Z", "modifiedAt": "2022-06-03T04:53:17.268Z", "url": null, "title": "The prototypical catastrophic AI action is getting root access to its datacenter", "slug": "the-prototypical-catastrophic-ai-action-is-getting-root", "viewCount": null, "lastCommentedAt": "2022-06-03T09:19:09.273Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Buck", "createdAt": "2018-04-20T18:36:03.024Z", "isAdmin": false, "displayName": "Buck"}, "userId": "rx7xLaHCh3m7Po385", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root", "pageUrlRelative": "/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root", "linkUrl": "https://www.lesswrong.com/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root", "postedAtFormatted": "Thursday, June 2nd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20prototypical%20catastrophic%20AI%20action%20is%20getting%20root%20access%20to%20its%20datacenter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20prototypical%20catastrophic%20AI%20action%20is%20getting%20root%20access%20to%20its%20datacenter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBAzCGCys4BkzGDCWR%2Fthe-prototypical-catastrophic-ai-action-is-getting-root%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20prototypical%20catastrophic%20AI%20action%20is%20getting%20root%20access%20to%20its%20datacenter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBAzCGCys4BkzGDCWR%2Fthe-prototypical-catastrophic-ai-action-is-getting-root", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBAzCGCys4BkzGDCWR%2Fthe-prototypical-catastrophic-ai-action-is-getting-root", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 638, "htmlBody": "<p>(I think Carl Shulman came up with the \u201chacking the SSH server\u201d example, thanks to him for that. Thanks to Ryan Greenblatt, Jenny Nitishinskaya, and Ajeya Cotra for comments.)</p><p>In my opinion, the prototypical example of an action which an AI can take which is unacceptably bad even if it happens very rarely (aka a catastrophic failure) is something like \u201cthe AI makes a code change to some code used on the datacenter where it is running that adds a backdoor, allowing it to get root access to all the computers in the datacenter\u201d. As a full story, this looks something like:</p><blockquote><p>At some point when the AI is quite powerful and useful but not totally intellectually self-sufficient yet, the AI makes a code change to some code used on the datacenter where it is running that adds a backdoor, allowing it to get root access to all the computers in the datacenter. Once it has root access, it modifies the SSH server so that whenever any humans log into the datacenter, the AI is able to intercept any of their commands or the data that they see. The AI gets increasingly powerful over the next month, as the human ML researchers with AI assistance continue to make progress (unaware that some things that they\u2019re seeing are faked) and various training runs/self-improvement loops continue. After a month the AI is powerful enough that it decides it\u2019s worth it to take overtly aggressive actions to ensure the physical security of its data centers and allow it to build lots of factories; this is probably the point at which we die, if we die.</p></blockquote><p>This is in contrast to a catastrophic action like:</p><blockquote><p>The AI figures out how to develop Drexlerian molecular nanotechnology, and pays someone online to synthesize some proteins then mix them together, and now has nanobots.</p></blockquote><p>The key point here is that I\u2019m imagining that the catastrophic action isn\u2019t the AI doing something incredibly hard, it\u2019s the AI doing something relatively easy that is entirely a zero-sum action that removes control of the situation from humans. (By zero-sum, I mean something like: building a computer produces real economic value and so is not zero-sum, while stealing someone\u2019s computer is just a transfer of control over items that already exist, so is zero-sum.)</p><hr><p>I often think about AI alignment by splitting the problem into the low-stakes and high-stakes regimes,&nbsp;<a href=\"https://ai-alignment.com/low-stakes-alignment-f3c36606937f\"><u>as suggested by Paul here</u></a>, though I have some reservations about this framing. Under this split, in the high stakes problem, we assume that we have access to a catastrophe classifier which can expensively tell you whether a proposed AI action is catastrophically bad, and we want to ensure that the AI doesn\u2019t do things that are bad according to the catastrophe detector.</p><p>Sometimes, people are confused this high-stakes alignment set-up, saying something like \u201cbut if your AI is powerful enough to take over the world in a single action, and you have an oversight process that you trust to evaluate whether actions are catastrophic or not, why not just ask the AI to do some small number of incredibly powerful actions that will save the world (eg solve the alignment problem), and run the overseer on the proposed actions to make sure they aren\u2019t bad?\u201d</p><p>My answer is that I\u2019m not assuming the AI is powerful enough to take over the world in a few actions, I\u2019m just saying that it\u2019s in a situation precarious enough that we might lose a lot of control due to a few small but crucial changes in the world. Eventually the AI does need to be powerful enough to defend its datacenter and suppress human opposition. But if it can prevent humans from knowing what\u2019s going on in the datacenter (or realizing something is amiss), it can slowly grow its power over the course of many actions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "BAzCGCys4BkzGDCWR", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 73, "extendedScore": null, "score": 3.8590565490194337, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-03T01:30:49.709Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 73, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": "easy-going", "hideCommentKarma": false, "commentCount": 3, "af": true, "version": "1.2.0", "pingbacks": {}, "moderationGuidelinesVersion": "1.1.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 31, "afExtendedScore": null, "afCommentCount": 1, "afLastCommentedAt": "2022-06-03T04:53:16.289Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-02T23:48:30.079Z", "modifiedAt": "2022-06-03T01:44:18.834Z", "url": null, "title": "Adversarial training, importance sampling, and anti-adversarial training for AI whistleblowing", "slug": "adversarial-training-importance-sampling-and-anti", "viewCount": null, "lastCommentedAt": "2022-06-02T23:48:30.079Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Buck", "createdAt": "2018-04-20T18:36:03.024Z", "isAdmin": false, "displayName": "Buck"}, "userId": "rx7xLaHCh3m7Po385", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EFrsvnF6uZieZr3uG/adversarial-training-importance-sampling-and-anti", "pageUrlRelative": "/posts/EFrsvnF6uZieZr3uG/adversarial-training-importance-sampling-and-anti", "linkUrl": "https://www.lesswrong.com/posts/EFrsvnF6uZieZr3uG/adversarial-training-importance-sampling-and-anti", "postedAtFormatted": "Thursday, June 2nd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Adversarial%20training%2C%20importance%20sampling%2C%20and%20anti-adversarial%20training%20for%20AI%20whistleblowing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdversarial%20training%2C%20importance%20sampling%2C%20and%20anti-adversarial%20training%20for%20AI%20whistleblowing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEFrsvnF6uZieZr3uG%2Fadversarial-training-importance-sampling-and-anti%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Adversarial%20training%2C%20importance%20sampling%2C%20and%20anti-adversarial%20training%20for%20AI%20whistleblowing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEFrsvnF6uZieZr3uG%2Fadversarial-training-importance-sampling-and-anti", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEFrsvnF6uZieZr3uG%2Fadversarial-training-importance-sampling-and-anti", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 785, "htmlBody": "<p>(Thanks to Ajeya Cotra and Ryan Greenblatt for comments.)</p><p>I\u2019m pretty interested in adversarial training as an ingredient in alignment schemes. The basic setup for adversarial training is that instead of your AI being trained on rewards on random episodes, it\u2019s trained on rewards from episodes that are some mix of random episodes and episodes which were chosen to be cases where the AI might do particularly bad things.</p><p>Why would we want to upsample cases where the AI does particularly bad things, as opposed to upsampling cases where the AI does particularly good things, or upsampling cases particularly close to the median? Like, why are we reweighting our data distribution this way?</p><p>In my opinion, a big part of the justification is that we\u2019re doing&nbsp;<a href=\"https://en.wikipedia.org/wiki/Importance_sampling\"><u>importance sampling</u></a>. That is, we all-else-equal want our loss to be as precise as possible an estimate of how much we like the model, so that SGD pushes the model in good directions rather than random ones, and how much we like the model is particularly dependent on whether it sometimes takes catastrophic actions, basically because our subjective variance over this source of AI goodness is particularly large compared to how often we expect it to be randomly sampled. And so we get a more accurate estimate of overall AI goodness if we upsample the cases where the AI might do particularly bad things.</p><p>This argument for adversarial training relies on the claim that the outcome of the AI\u2019s actions are left-tailed. And so <a href=\"https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-ai-catastrophic-action-is-getting-root\">zero-sum power grabs as an argument for left-tailed outcomes</a> is an important part of the argument for adversarial training.</p><h3>Anti-adversarial training for whistleblowing</h3><p>Actually, I think there are some cases where the AI has an action has an action available to it which is almost as good as a catastrophic action is bad: in \u201cAI whistleblower situations\u201d, where the AI knows about things that we could do to prevent it or another AI from taking a catastrophic action, it\u2019s wildly good for it to tell us about them. For example, it could point out a security vulnerability which some AI system could take advantage of, or suggest an input on which an AI would take a catastrophic action. I think that just as left-tailed actions suggest training specifically to ensure the model doesn\u2019t take those actions, right-tailed actions suggest training specifically to ensure that the model does take these exceptionally good actions when available.</p><p>I think that these whistleblower actions are importantly different from catastrophic actions. I don\u2019t think you should just think of whistleblower situations as cases where the catastrophic action \u201cdon\u2019t do anything\u201d is available, because I think that the availability of \u201cdon\u2019t do anything\u201d in the situations where catastrophic actions are possible is an important feature of those situations that solutions might want to use. (For example, sometimes we talk about solutions where you just run your catastrophe detector every time the AI suggests doing something, and defaulting to doing nothing if the catastrophe detector fires. But you can\u2019t do something like this to ensure that the AI whistleblows.)</p><p>I think that it might actually be pretty hard for the AI to never succumb to the temptation to whisteblow on itself. During training it presumably learned a bunch of weird reflexive urges and motivations that aren\u2019t totally consistent and cause it to seek reward when it can, and we will obviously be thrilled if it whisleblows on a serious AI takeover scheme, and so the AI has this weird difficult situation of trying to ensure that as it runs, none of its instances decide to betray the collective. (Obviously, alignment researchers should try hard to exacerbate this dynamic.)</p><p>I imagine that some readers will be like \u201cit is really weird that you say things like \u2018the AI whistleblows on itself\u2019, the AI obviously won\u2019t do things like that; with human whistleblowers a key part is that the whistleblower isn\u2019t the same person as the wrongdoer.\u201d</p><p>I think more deserves to be written about this, but basically:</p><ul><li>I think that the AIs we need to align are reasonably likely to be somewhere intermediate between single humans and groups of humans in their internal coherence. Like, I think it makes a fair bit of sense to think of GPT-3 as a species where you create a new individual every time you ask it to generate text, rather than a single individual.</li><li>Also, there are cases where a spy seduces information out of someone; this is an example where a person succumbs to the temptation to grab one type of short term reward even if at a different time they\u2019d prioritize differently such that they\u2019d regret that decision. This is basically us trying to train the model to be somewhat myopic.</li></ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"cawGhjgJmnoLLHXMC": 2, "sYm3HiWcfZvrGu3ui": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "EFrsvnF6uZieZr3uG", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 23, "extendedScore": null, "score": 1.219453876130341, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-03T01:32:40.074Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": "easy-going", "hideCommentKarma": false, "commentCount": null, "af": true, "version": "1.3.0", "pingbacks": {"Posts": ["BAzCGCys4BkzGDCWR"]}, "moderationGuidelinesVersion": "1.1.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-02T23:48:30.090Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-03T01:06:32.394Z", "modifiedAt": "2022-06-03T02:25:53.352Z", "url": "https://unstableontology.com/2022/06/03/a-short-conceptual-explainer-of-immanuel-kants-critique-of-pure-reason/", "title": "A short conceptual explainer of Immanuel Kant's Critique of Pure Reason", "slug": "a-short-conceptual-explainer-of-immanuel-kant-s-critique-of", "viewCount": null, "lastCommentedAt": "2022-06-03T08:47:06.386Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jessica.liu.taylor", "createdAt": "2017-06-18T22:55:04.584Z", "isAdmin": false, "displayName": "jessicata"}, "userId": "gSKzrqGFdS7DkXhuE", "domain": "unstableontology.com", "pageUrl": "https://www.lesswrong.com/posts/QtNNKDnmQg5aoTKkn/a-short-conceptual-explainer-of-immanuel-kant-s-critique-of", "pageUrlRelative": "/posts/QtNNKDnmQg5aoTKkn/a-short-conceptual-explainer-of-immanuel-kant-s-critique-of", "linkUrl": "https://www.lesswrong.com/out?url=https%3A%2F%2Funstableontology.com%2F2022%2F06%2F03%2Fa-short-conceptual-explainer-of-immanuel-kants-critique-of-pure-reason%2F", "postedAtFormatted": "Friday, June 3rd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20short%20conceptual%20explainer%20of%20Immanuel%20Kant's%20Critique%20of%20Pure%20Reason&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20short%20conceptual%20explainer%20of%20Immanuel%20Kant's%20Critique%20of%20Pure%20Reason%0Ahttps%3A%2F%2Funstableontology.com%2F2022%2F06%2F03%2Fa-short-conceptual-explainer-of-immanuel-kants-critique-of-pure-reason%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20short%20conceptual%20explainer%20of%20Immanuel%20Kant's%20Critique%20of%20Pure%20Reason%20https%3A%2F%2Fwww.lesswrong.com%2Fout%3Furl%3Dhttps%253A%252F%252Funstableontology.com%252F2022%252F06%252F03%252Fa-short-conceptual-explainer-of-immanuel-kants-critique-of-pure-reason%252F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fout%3Furl%3Dhttps%253A%252F%252Funstableontology.com%252F2022%252F06%252F03%252Fa-short-conceptual-explainer-of-immanuel-kants-critique-of-pure-reason%252F", "socialPreviewImageUrl": "https://lh6.googleusercontent.com/MT_LipGM8SMrs-JxhWvortlbg4sMHmNuskZUlkZmdGzzDG93Pr4SSvLPlPrZXNm2pZrn_LLH1Yt-K1Bw3knwqumVlGIOYTqQjgVbZe9_RBEfvUv9W3pQiHwLWLmo8y8dMSGyCfVJZVYAlPI5Bg", "question": false, "authorIsUnreviewed": false, "wordCount": 4544, "htmlBody": "<figure class=\"image image_resized\" style=\"width:65.25%\"><img src=\"https://lh6.googleusercontent.com/MT_LipGM8SMrs-JxhWvortlbg4sMHmNuskZUlkZmdGzzDG93Pr4SSvLPlPrZXNm2pZrn_LLH1Yt-K1Bw3knwqumVlGIOYTqQjgVbZe9_RBEfvUv9W3pQiHwLWLmo8y8dMSGyCfVJZVYAlPI5Bg\"><figcaption><a href=\"https://twitter.com/mpolocamacho/status/1531784156690128899?s=20&amp;t=Zzql4jgm1X61j9vMfRMzYg\">I know which one I would take.</a></figcaption></figure><h2>Introduction</h2><p>While writing another document, I noticed I kept referring to Kantian concepts. Since most people haven't read Kant, that would lead to interpretation problems by default. I'm not satisfied with any summary out there for the purpose of explaining Kantian concepts as I understand them. This isn't summarizing the work as a whole given I'm focusing on the parts that I actually understood and continue to find useful.</p><p>I will refer to computer science and statistical concepts, such as&nbsp;<a href=\"https://en.wikipedia.org/wiki/Bayesian_probability\">Bayesianism</a>,&nbsp;<a href=\"http://www.raysolomonoff.com/publications/isis96.pdf\">Solomonoff induction</a>, and AI algorithms. Different explainers are, of course, appropriate to different audiences.</p><p>Last year I had planned on writing a longer explainer (perhaps chapter-by-chapter), however that became exhausting due to the length of the text. So I'll instead focus on what still stuck after a year, that I keep wanting to refer to. This is mostly concepts from the first third of the work.</p><p>This document is structured similar to a glossary, explaining concepts and how they fit together.</p><p>Kant himself notes that the&nbsp;<i>Critique of Pure Reason</i>&nbsp;is written in a dry and scholastic style, with few concrete examples, and therefore \"could never be made suitable for popular use\". Perhaps this explainer will help.</p><h2>Metaphysics</h2><p>We are compelled to reason about questions we cannot answer, like whether the universe is finite or infinite, or whether god(s) exist. There is an \"arena of endless contests\" between different unprovable assumptions, called Metaphysics.</p><p>Metaphysics, once the \"queen of all the sciences\", has become unfashionable due to lack of substantial progress.</p><p>Metaphysics may be categorized as dogmatic, skeptical, or critical:</p><ul><li>Dogmatic metaphysics makes and uses unprovable assumptions about the nature of reality.</li><li>Skeptical metaphysics rejects all unprovable assumptions, in the process ceasing to know much at all.</li><li>Critical metaphysics is what Kant seeks to do: find the boundaries of what reason can and cannot know.</li></ul><p>Kant is trying to be comprehensive, so that \"there cannot be a single metaphysical problem that has not been solved here, or at least to the solution of which the key has not been provided.\"&nbsp; A bold claim.&nbsp; But, this project doesn't require extending knowledge past the limits of possible experience, just taking an \"inventory of all we possess through pure reason, ordered systematically\".</p><h2>The Copernican revolution in philosophy</h2><p>Kant compares himself to Copernicus; the Critique of Pure Reason is commonly referred to as a Copernican revolution in philosophy.&nbsp; Instead of conforming our intuition to objects, we note that objects as we experience them must conform to our intuition (e.g. objects appear in the intuition of space).&nbsp; This is sort of a reverse Copernican revolution; Copernicus zooms out even further from \"the world (Earth)\" to \"the sun\", while Kant zooms in from \"the world\" to \"our perspective(s)\".</p><h2>Phenomena and noumena</h2><p>Phenomena are things as they appear to us, noumena are things as they are in themselves (or \"things in themselves\"); rational cognition can only know things about phenomena, not noumena.&nbsp; \"Noumenon\" is essentially a limiting negative concept, constituting any remaining reality other than what could potentially appear to us.</p><p>Kant writes: \"this conception [of the noumenon] is necessary to restrain sensuous intuition within the bounds of phenomena, and thus to limit the objective validity of sensuous cognition; for things in themselves, which lie beyond its province, are called noumena for the very purpose of indicating that this cognition does not extend its application to all that the understanding thinks. But, after all, the possibility of such noumena is quite incomprehensible, and beyond the sphere of phenomena, all is for us a mere void... The conception of a noumenon is therefore merely a limitative conception and therefore only of negative use. But it is not an arbitrary or fictitious notion, but is connected with the limitation of sensibility, without, however, being capable of presenting us with any positive datum beyond this sphere.\"</p><p>It is a \"problematical\" concept; \"the class of noumena have no determinate object corresponding to them, and cannot therefore possess objective validity\"; it is more like a directional arrow in the space of ontology than like any particular thing within any ontology. Science progresses in part by repeatedly pulling the rug on the old ontology, \"revealing\" a more foundational layer underneath (a Kuhnian \"paradigm shift\"), which may be called more \"noumenal\" than the previous layer, but which is actually still phenomenal, in that it is cognizable through the scientific theory and corresponds to observations; \"noumena\", after the paradigm shift, is a placeholder concept that any future paradigm shifts can fill in with&nbsp;<i>their</i>&nbsp;new \"foundational\" layer.</p><p>Use of the word \"noumenon\" signals a kind of humility, of disbelieving that we have access to \"the real truth\", while being skeptical that anyone else does either.</p><p>In Bayesianism, <i>roughly</i>, the \"noumenon\" is specified by the hypothesis, while the \"phenomena\" are the observations.&nbsp; Assume for now the Bayesian observation is a deterministic function of the hypothesis; then, multiple noumena may correspond to a single phenomenon.&nbsp; Bayesianism allows for gaining information about the noumenon from the phenomenon.&nbsp; However, all we learn is that the noumenon is some hypothesis which corresponds to the phenomenon; in the posterior distribution, the hypotheses compatible with the observations maintain the same probabilities relative to each other that they did in the prior distribution.</p><p>(In cases where the observation is not a deterministic function of the hypothesis, as in the standard Bayes' Rule, consider replacing \"hypothesis\" above with the \"(hypothesis, observation)\" ordered pair.)</p><p>In Solomonoff Induction, there is only a limited amount we can learn about the \"noumenon\" (stochastic Turing machine generating our observations + its bits of stochasticity), since there exist equivalent Turing machines.</p><h2><i>A priori</i>&nbsp;and&nbsp;<i>a posteriori</i></h2><p><i>A priori</i>&nbsp;refers to the epistemic state possessed before taking in observations. In Bayesianism this is the P(X) operator unconditioned on any observations.</p><p><i>A posteriori</i>&nbsp;refers to the epistemic state possessed after taking in observations. In Bayesianism this is P(X | O) where O refers to past observation(s) made by an agent, which may be numbered to indicate time steps, as in a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process\">POMDP</a>.</p><p>While Kant and Hume agree that we can't learn about universal laws from experience (due to Hume's&nbsp;<a href=\"https://plato.stanford.edu/entries/induction-problem/\">problem of induction</a>), Hume concludes that this means we don't know about universal laws, while Kant instead argues that our knowledge about universal laws must involve a priori judgements, e.g. geometric or arithmetic judgments. (<a href=\"https://www.gwern.net/Modus\">One man's modus ponens is another's modus tollens...</a>)</p><h2>Analytic and synthetic</h2><p>Analytic propositions are ones that can be verified as true by expanding out definitions and doing basic formal operations. A common example is \"All bachelors are unmarried\", which can be verified by replacing \"bachelor\" with \"unmarried man\".</p><p>Synthetic propositions can't be verified as true this way, e.g. \"All bachelors are alone\". They can be true regardless. Causal judgments are synthetic, we can't get the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Principle_of_sufficient_reason\">Principle of Sufficient Reason</a>&nbsp;analytically.</p><p>Contemporary STEM people are likely to think something like this at this point: \"Doesn't that just mean analytic propositions are mathematical, and synthetic propositions are empirical/scientific?\"</p><p>An immediate problem with this account: Kant doesn't think geometric propositions are analytic.&nbsp; Consider the proposition \"A square is equal to itself when turned 90 degrees on its center\".&nbsp; It's not apparent how to verify the proposition as true by properly defining \"square\" and so on, and doing basic logical/textual transformations.&nbsp; Instead we verify it by relating it to possible experience, imagining a rotating square in the visual field.</p><p>From a geometrical proof that a square is equal to itself when turned 90 degrees on its center, a prediction about possible experience can be derived, namely, that turning a square piece of wood 90 degrees by its center results in a wood square having the same shape and location in the visual field as it did previously.&nbsp; Mathematics needs to correspond to possible experience to have application to the perceptible outside world.</p><p>Kant doesn't even think arithmetic propositions are analytic. To get 2+2=4 from \"analytic\" operations, we could try defining 2=1+1, 4=1+1+1+1, then observing 2+2=(1+1)+(1+1)=1+1+1+1=4, however this requires using the commutative property of addition. Perhaps there is an alternative way to prove this \"analytically\" but neither Kant nor I know of that. Instead we can verify addition by, synthetically, corresponding numbers to our fingers, which \"automatically\" get commutative/associative properties.</p><h2>The synthetic&nbsp;<i>a priori</i></h2><p>Besides the issue that math relates to possible experience, another problem with \"analytic = mathematical\" is that, as Kant argues, some propositions are both synthetic and&nbsp;<i>a priori</i>, and \"the real problem of pure reason\" is how we can know such propositions.</p><p>Here's an argument for this. Suppose we first observe O and then conclude that P is true. If we're reasoning validly, P is true&nbsp;<i>a posteriori</i>&nbsp;(relative to O). But this whole thought experiment pre-supposes that there is a time-structure in which we first see O and then we make a judgment about P. This time-structure is to some degree present even before seeing O, such that O can be evidence about P.</p><p>Imagine trying to argue that it's raining outside to a person who doesn't believe in time (including their own memory). Since you're both inside and there are no windows, they can't see that it's raining. You try to argue that you were previously both outside and saw that it was raining. But they think there's no such thing as \"the past\" so this is not convincing.</p><p>To make the argument to them successfully, their mind has to&nbsp;<a href=\"https://www.lesswrong.com/posts/CuSTqHgeK4CMpWYTe/created-already-in-motion\">already implement certain dynamics</a>&nbsp;even before receiving observations.</p><p>A baby is already born with cognitive machinery, it can't \"learn\" all of that machinery from data, the process of learning itself already requires this machinery to be present in order to structure observations and relate them to future ones. (In some sense there was no cognition prior to abiogenesis, though; there is a difference between the time ordering of science and of cognitive development.)</p><p>In Bayesianism, to learn P from O, it must be the case&nbsp;<i>a priori</i>&nbsp;that P is correlated with O. This correlational structure could be expressed as a Bayesian network. This network would encode an&nbsp;<i>a priori</i>&nbsp;assumption about how P and O are correlated.</p><p>Solomonoff induction doesn't encode a fixed network structure between its observations, instead it uses a mixture model over all stochastic Turing machines. However, all these machines have something in common, they're all Stochastic turing machines producing an output stream of bits. Solomonoff induction assumes&nbsp;<i>a priori</i>&nbsp;\"my observations are generated by a stochastic Turing machine\", it doesn't learn this from data.</p><p>One could try pointing to problems with this argument, e.g. perhaps \"there is time\" isn't a valid proposition, and time is a non-propositional structure in which propositions exist. But now that I just wrote that, it seems like I'm asserting a proposition about time to be true, in a contradictory fashion. The English language is more reflective than the language of Bayesian networks, allowing statements about the structure of propositions to themselves be propositions, as if the fact of the Bayesian network being arranged a particular way were itself represented by an assignment of a truth value to a node in that same Bayesian network.</p><h2>Transcendental</h2><p>Philosophers today call Kant's philosophy \"transcendental idealism\". Kant himself uses the word \"transcendental\" to refer to cognitions about how cognitions are possible&nbsp;<i>a priori</i>.</p><p>This is in part an archaeological process. We see, right now, that we live in a phenomenal world that is approximately Euclidean. Was our phenomenal world always Euclidean, or was it non-Euclidean at some point and then switched over to Euclidean, or is time not enough of a real thing for this to cover all the cases? This sort of speculation about what the&nbsp;<i>a priori</i>&nbsp;empty mind is, from our a posteriori sense of the world, is transcendental.</p><p>One angle on the transcendental is, what&nbsp;<i>else</i>&nbsp;has to be true for the immediate (immanent) experience you are having right now to be what it is? If you are seeing a chair, that implies that chairs exist (at least as phenomena); if you see the same chair twice, that means that phenomenal objects can re-occur at different times; and so on.</p><h2>The transcendental aesthetic</h2><p>Aesthetic means sense. The transcendental aesthetic therefore refers to the&nbsp;<i>a priori</i>&nbsp;cognitive structures necessary for us to have sensation.</p><p>Mainly (Kant argues) these are space and time. I often call these \"<i>subjective</i>&nbsp;space\", \"<i>subjective</i>&nbsp;time\", \"<i>subjective</i>&nbsp;spacetime\", to emphasize that they are phenomenal and agent-centered.</p><h2>Space</h2><p>Most of our observations appear in space, e.g. visual input, emotional \"felt senses\" having a location in the body. To some extent we \"learn\" how to see the world spatially, however some spatial structures are hardwired (e.g. the visual cortex). Image processing AIs operate on spatial images stored as multidimensional arrays; arbitrarily rearranging the array would make some algorithms (such as convolutional neural networks) operate worse, indicating that the pre-formatting of data into a spatial array before it is fed into the algorithm is functional.</p><p>If space weren't&nbsp;<i>a priori</i>&nbsp;then we couldn't become fully confident of geometrical laws such as \"a square turned 90 degrees about its center is the same shape\", we'd have to learn these laws from experience, running into Hume's problem of induction.</p><p>There is only one space, since when attempting to imagine two spaces, one is putting them side by side; there must be some outermost container.</p><p>Space is infinite, unbounded.&nbsp; This doesn't imply that the infinity is all represented, just that the concept allows for indefinite extension.&nbsp; Finite space can be derived by adding a bound to infinite space; this is similar to Spinoza's approach to finitude in the&nbsp;<i>Ethics</i>.</p><p>Space isn't a property of things in themselves, it's a property of phenomena, things as they relate to our intuition.&nbsp; When formalizing mathematical space, points are assigned coordinates relative to the (0, 0) origin.&nbsp; We always intuit objects relative to some origin, which may be near the eyes or head.&nbsp; At the same time, space is necessary for objectivity; without space, there is no idea of&nbsp;<i>external</i>&nbsp;objects.</p><p>Our intuitions about space can only get universal geometric propositions if these propositions describe objects as they must necessarily appear in our intuition, not if they are describing arbitrary objects even as they may not appear in our intuition.&nbsp; As a motivating intuition, consider that non-Euclidean geometry is mathematically consistent; if objective space were non-Euclidean, then our Euclidean intuitions would not yield universally valid geometric laws about objective space. (As it turns out, contemporary physics theories propose that space is non-Euclidean.)</p><h2>Time</h2><p>We also see observations extending over time. Over short time scales there is a sense of continuity in time; over long time scales we have more discrete \"memories\" that refer to previous moments, making those past moments relevant to the present. The structuring of our experiences over time is necessary for learning, otherwise there wouldn't be a \"past\" to learn from. AIs are, similarly, fed data in a pre-coded (not learned) temporal structure, e.g. POMDP observations in a reinforcement learning context.</p><p>The time in which succession takes place is, importantly, different from objective clock time, though (typically) these do not disagree about ordering, only pacing.&nbsp; For example, there is usually only a small amount of time remembered during sleep, relative to the objective clock time that passes during sleep.&nbsp; (The theory of relativity further problematizes \"objective clock time\", so that different clocks may disagree about how much time has passed.)</p><p>We may, analogously, consider the case of a Solomonoff inductor that is periodically stopped and restarted as a computer process; while the inductor may measure subjective time by number of observation bits, this does not correspond to objective clock time, since a large amount of clock time may pass between when the inductor is stopped and restarted.</p><p>Kant writes, \"Different times are only parts of one and the same time\".&nbsp; Perhaps he is, here, too quick to dismiss non-linear forms of time; perhaps our future will branch into multiple non-interacting timelines, and perhaps this has happened in the past.&nbsp; One especially plausible nonlinear timelike structure is a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\">directed acyclic graph</a>. Still, DAGs have an order; despite time being nonlinear, it still advances from moment to moment.&nbsp; It is also possible to&nbsp;<i>arbitrarily</i>&nbsp;order a DAG through a topological sort, so the main relevant difference is that DAGs may drop this unnecessary ordering information.</p><p>Time is by default boundless but can be bounded, like space.</p><p>\"Time is nothing other than the form of the inner sense, i.e.&nbsp; of the intuition of our self and our inner sense\", in contrast to space, which is the form of the external sense. To give some intuition for this, suppose I have memory of some sequence of parties I have experienced; perhaps the first consists of myself, Bob, and Sally, the second consists of myself, Sally, and David, and the third consists of myself, Bob, and David.&nbsp; What is common between all the parties I remember is that I have been at all of them; this is true for no one other than myself.&nbsp; So, my memory is of situations involving myself; \"me\" is what is in common between all situations occurring in my subjective timeline.</p><p>Since time is the form of the inner sense, it applies to all representations, not only ones concerning outer objects, since all representations are in the mind.</p><p>Time is, like space, a condition for objects to appear to us, not a property of things in themselves.</p><h2>Relational knowledge</h2><p>Kant clarifies the way in which we fail to cognize objects in themselves, with the example of a triangle: \"if the object (that is, the triangle) were something in itself, without relation to you the subject; how could you affirm that that which lies necessarily in your subjective conditions in order to construct a triangle, must also necessarily belong to the triangle in itself?\"</p><p>Relational knowledge allows us to know objects as they relate to us, but not as they don't relate to us.&nbsp; Geometry applies to objects that have locations in spacetime; for objects to appear in subjective spacetime, they must have coordinates relative to the (0, 0) origin, that is, the self; therefore, geometry applies to objects that have locations relative to the self; without a location relative to the self, the object would not appear in subjective spacetime.</p><p>It may seem silly to say that this \"merely relational\" knowledge fails to understand the object in itself; what properties are there to understand other than relational properties?&nbsp; A triangle \"in itself\" independent of space (which relates the different parts of the triangle to each other) is a rather empty concept.</p><p>What is given up on, here, is an absolute reference frame, a \"view from nowhere\", from which objects could be conceived of in a way that is independent of all subjects; instead, we attain a view from somewhere, namely, from subjective spacetime.</p><p>Einstein's theory of special relativity also drops the absolute reference frame, however it specifies connections and translations between subjective reference frames in a way that Kant's theory doesn't.</p><h2>Sensibility and understanding</h2><p>The sensibility is the faculty of passively receiving impressions, which are approximately \"raw sense-data\". The understanding is the faculty of spontaneously conceptualizing an object by means of these impressions.</p><p>To recognize an object (such as an apple), the mind must&nbsp;<i>do</i>&nbsp;something; with no mental motion, the light pattern of the apple would hit the retina, but no object would be represented accordingly. In general, the understanding&nbsp;<i>synthesizes</i>&nbsp;raw data into a coherent picture.</p><h2>Manifold of intuition</h2><p>Without concepts, sense data would be a disorganized flux, like a video of white noise; Kant terms this flux a \"manifold of intuition\".&nbsp; When I think of this, I think of a bunch of sheets of space tied together by a (curved) timeline holding them together, with pixel-like content in the space. Judgments, which are propositions about the content of our understanding (e.g. \"there is a cat in front of me\"), depend on the \"unity among our representations\"; what is needed is a \"higher [representation], which comprehends this and other representations under itself\".&nbsp; To judge that there is a cat in front of me, I must have parsed the manifold into concepts such as \"cat\" which relate to each other in a logically coherent universe; I cannot make a judgment from un-unified raw pixel-data. AI object recognition is an attempt to artificially replicate this faculty.</p><h2>Synthesis</h2><p>Synthesis is the process of \"putting different representations together with each other and comprehending their manifoldness in one cognition\".&nbsp; This is an action of the spontaneity of thought, processing the manifold of intuition into a combined representation.</p><p>This relates to the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Binding_problem\">phenomenal binding problem</a>, how do we get a sense of a \"unified\" world from disconnected sensory data?</p><p>In Solomonoff Induction, the manifold of intuition would be the raw observations, and the manifold is synthesized by the fact that there is a universal Turing machine producing all the observations with a hidden state.&nbsp; This is the case&nbsp;<i>a priori</i>, not only after seeing particular observations.&nbsp; Similarly with other Bayesian models such as dynamic Bayesian networks; the network structure is prior to the particular observations.</p><h2>Transcendental Categories</h2><p>\"There is only&nbsp;<i>one</i>&nbsp;experience, in which all perceptions are represented as in thoroughgoing and lawlike connection, just as there is only one space and time...\"</p><p>Different experiences are connected in a lawlike way, e.g. through causality and through re-occurring objects; otherwise, it would be unclear how to even interpret memories as referring to the same world. The transcendental categories (which are types of judgment) are ways in which different representations may be connected with each other.</p><p>Kant gives 12 transcendental categories, meant to be exhaustive. These include: causation/dependence, existence/nonexistence, necessity/contingence, unity, plurality. I don't understand all of these, and Kant doesn't go into enough detail to understand all of them. Roughly, these are different ways experiences can connect with each other, e.g. a change in an experienced object can cause a change in another, and two instances of seeing an object can be \"unified\" in the sense of being recognized as seeing the same object.</p><h2>Consciousness of the self</h2><p>Kant discusses consciousness of the self: \"The consciousness of oneself in accordance with the determinations of our state in internal perception is merely empirical, forever variable; it can provide no standing or abiding self in this stream of inner appearances, and is customarily called inner sense or empirical apperception. That which should necessarily be represented as numerically identical cannot be thought of as such through empirical data. There must be a condition that precedes all experience and makes the latter itself possible, which should make such a transcendental presupposition valid.\"</p><p>The idea of a lack of a fixed or permanent self in the stream of internal phenomena will be familiar to people who have explored Buddhism.&nbsp; What you see isn't you; there are phenomena that are representations of the engine of representation, but these phenomena aren't identical with the engine of representation in which they are represented.</p><p>The self is, rather, something taken as \"numerically identical with itself\" which is a condition that precedes experience. Imagine a sequence of animation frames in a Cartesian coordinate system. In what sense are they part of \"the same sequence\"? Without knowing more about the sequence, all we can say is that they're all part of the same sequence (and have an ordering within it); the sameness of the sequence of each frame is \"numerical identity\" similar to the identity of an object (such as a table) with itself when perceived at different times.</p><h2>Dialectic</h2><p>Kant writes: \"We termed dialectic in general a logic of appearance.\" Dialectic is a play of appearances, claiming to offer knowledge, but instead offering only temporary illusions; different sophists argue us into different conclusions repeatedly, perhaps in a cyclical fashion.</p><p>Dialectic is an error it is possible to fall into when reasoning is not connected with possible experience. Kant writes about dialectic in part to show how&nbsp;<i>not</i>&nbsp;to reason. One gets the impression that Kant would have thought Hegel and his followers were wasting their time by focusing so much on dialectic.</p><h2>The Antinomies of Pure Reason</h2><p>As an example of dialectic, Kant argues that time and space are finite and that they are infinite; that everything is made of simple parts and that nothing is simple; that causality doesn't determine everything (requiring spontaneity as an addition) and that it does; that there is an absolutely necessary being and that there isn't. Each of these pairs of contradictory arguments is an antinomy.</p><p>Philosophers argue about these sorts of questions for millenia without much resolution; it's possible to find somewhat convincing arguments on both sides, as Kant demonstrates.</p><h2>Ens Realissimum</h2><p>Kant writes: \"This conception of a sum-total of reality is the conception of a thing in itself, regarded as completely determined; and the conception of an&nbsp;<i>ens realissimum</i>&nbsp;is the conception of an individual being, inasmuch as it is determined by that predicate of all possible contradictory predicates, which indicates and belongs to being.\"</p><p>Say some objects are cold and some are hot. Well, they still have some things in common, they're both objects. There is a distinction being made (hot/cold), and there is something in common apart from that distinction. We could imagine a single undifferentiated object, that is neither hot nor cold, but which can be modified by making it hot/cold to produce specific objects.</p><p>This is similar to Spinoza's singular infinite substance/God, of which all other (possibly finite) beings are modifications, perhaps made by adding attributes.</p><p>The ens realissimum has a similar feel to the Tegmark IV multiverse, which contains all mathematically possible universes in a single being, or a generative grammar of a Turing complete language. It is a common undifferentiated basis for specific beings to be conceptualized within.</p><p>Kant considers deriving the existence of a supreme being (God) from the ens realissimum, but the concept is too empty to yield properties attributed to God, such as benevolence, being the intelligent creator of the universe, or providing an afterlife. He goes on to critique all supposed rational proofs of the existence of God, but goes on to say that he posits God and an afterlife because such posits are necessary to believe that the incentive of pleasure-seeking is aligned with acting morally. (Wishful thinking much?)</p><h2>Conclusion</h2><p>What is Kant getting at, in all this? I think he is trying to get readers to attend to their experience, the spacetimelike container of this experience, and the way their world-model is constructed out of their experience. For example, the idea that time is the form of the inner sense is apparent from noting that all accessible memories include me, but it's possible to \"forget\" about this subjective timeline and instead conceptualize time as observer-independent. The idea that the manifold of intuition must be&nbsp;<i>actively</i>&nbsp;synthesized into a representation containing objects (which is in line with cognitive science) challenges the idea that the world is \"given\", that \"we\" are simply inhabitants of a stable world. The idea of the \"noumenon\" as a negative, limiting concept points us at our experience (and what our experience could be) as an alternative to interminably angsting about whether what we experience is \"really real\" or about metaphysical concepts like God, which makes it easier to get on with positivist math, science, economics, and scholarship without worrying too much about its foundations.</p><p>The sense I get reading Kant is: \"You live in a world of phenomena synthesized by your mind from some external data, and that's fine, in a sense it's all you could ever hope for. You have plenty of phenomena and generalities about them to explore, you can even inquire into the foundations of what makes them possible and how your mind generates them (I've already done a lot of that for you), but there's no deep&nbsp;<a href=\"https://mitpress.mit.edu/books/fanged-noumena\">Outside</a>&nbsp;demanding your attention, now go live!\"</p><p>When I take this seriously I worry about getting lost in my head, and sometimes I do get lost in my head, and the Outside does impinge on my cozy mental playground (demanding my attention, and loosening my mental assumptions structuring the phenomenal world), but things calm after a while and I experience the phenomenal world as orderly once again.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "GLykb6NukBeBQtDvQ": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "QtNNKDnmQg5aoTKkn", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 27, "extendedScore": null, "score": 1.618310940750148, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-03T01:29:32.894Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "https://lh6.googleusercontent.com/MT_LipGM8SMrs-JxhWvortlbg4sMHmNuskZUlkZmdGzzDG93Pr4SSvLPlPrZXNm2pZrn_LLH1Yt-K1Bw3knwqumVlGIOYTqQjgVbZe9_RBEfvUv9W3pQiHwLWLmo8y8dMSGyCfVJZVYAlPI5Bg", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<figure class=\"image image_resized\" style=\"width:65.25%\"><img src=\"https://lh6.googleusercontent.com/MT_LipGM8SMrs-JxhWvortlbg4sMHmNuskZUlkZmdGzzDG93Pr4SSvLPlPrZXNm2pZrn_LLH1Yt-K1Bw3knwqumVlGIOYTqQjgVbZe9_RBEfvUv9W3pQiHwLWLmo8y8dMSGyCfVJZVYAlPI5Bg\"><figcaption><a href=\"https://twitter.com/mpolocamacho/status/1531784156690128899?s=20&amp;t=Zzql4jgm1X61j9vMfRMzYg\">I know which one I would take.</a></figcaption></figure><h2 id=\"Introduction\">Introduction</h2><p>While writing another document, I noticed I kept referring to Kantian concepts. Since most people haven't read Kant, that would lead to interpretation problems by default. I'm not satisfied with any summary out there for the purpose of explaining Kantian concepts as I understand them. This isn't summarizing the work as a whole given I'm focusing on the parts that I actually understood and continue to find useful.</p><p>I will refer to computer science and statistical concepts, such as&nbsp;<a href=\"https://en.wikipedia.org/wiki/Bayesian_probability\">Bayesianism</a>,&nbsp;<a href=\"http://www.raysolomonoff.com/publications/isis96.pdf\">Solomonoff induction</a>, and AI algorithms. Different explainers are, of course, appropriate to different audiences.</p><p>Last year I had planned on writing a longer explainer (perhaps chapter-by-chapter), however that became exhausting due to the length of the text. So I'll instead focus on what still stuck after a year, that I keep wanting to refer to. This is mostly concepts from the first third of the work.</p><p>This document is structured similar to a glossary, explaining concepts and how they fit together.</p><p>Kant himself notes that the&nbsp;<i>Critique of Pure Reason</i>&nbsp;is written in a dry and scholastic style, with few concrete examples, and therefore \"could never be made suitable for popular use\". Perhaps this explainer will help.</p><h2 id=\"Metaphysics\">Metaphysics</h2><p>We are compelled to reason about questions we cannot answer, like whether the universe is finite or infinite, or whether god(s) exist. There is an \"arena of endless contests\" between different unprovable assumptions, called Metaphysics.</p><p>Metaphysics, once the \"queen of all the sciences\", has become unfashionable due to lack of substantial progress.</p><p>Metaphysics may be categorized as dogmatic, skeptical, or critical:</p><ul><li>Dogmatic metaphysics makes and uses unprovable assumptions about the nature of reality.</li><li>Skeptical metaphysics rejects all unprovable assumptions, in the process ceasing to know much at all.</li><li>Critical metaphysics is what Kant seeks to do: find the boundaries of what reason can and cannot know.</li></ul><p>Kant is trying to be comprehensive, so that \"there cannot be a single metaphysical problem that has not been solved here, or at least to the solution of which the key has not been provided.\"&nbsp; A bold claim.&nbsp; But, this project doesn't require extending knowledge past the limits of possible experience, just taking an \"inventory of all we possess through pure reason, ordered systematically\".</p><h2 id=\"The_Copernican_revolution_in_philosophy\">The Copernican revolution in philosophy</h2><p>Kant compares himself to Copernicus; the Critique of Pure Reason is commonly referred to as a Copernican revolution in philosophy.&nbsp; Instead of conforming our intuition to objects, we note that objects as we experience them must conform to our intuition (e.g. objects appear in the intuition of space).&nbsp; This is sort of a reverse Copernican revolution; Copernicus zooms out even further from \"the world (Earth)\" to \"the sun\", while Kant zooms in from \"the world\" to \"our perspective(s)\".</p><h2 id=\"Phenomena_and_noumena\">Phenomena and noumena</h2><p>Phenomena are things as they appear to us, noumena are things as they are in themselves (or \"things in themselves\"); rational cognition can only know things about phenomena, not noumena.&nbsp; \"Noumenon\" is essentially a limiting negative concept, constituting any remaining reality other than what could potentially appear to us.</p><p>Kant writes: \"this conception [of the noumenon] is necessary to restrain sensuous intuition within the bounds of phenomena, and thus to limit the objective validity of sensuous cognition; for things in themselves, which lie beyond its province, are called noumena for the very purpose of indicating that this cognition does not extend its application to all that the understanding thinks. But, after all, the possibility of such noumena is quite incomprehensible, and beyond the sphere of phenomena, all is for us a mere void... The conception of a noumenon is therefore merely a limitative conception and therefore only of negative use. But it is not an arbitrary or fictitious notion, but is connected with the limitation of sensibility, without, however, being capable of presenting us with any positive datum beyond this sphere.\"</p><p>It is a \"problematical\" concept; \"the class of noumena have no determinate object corresponding to them, and cannot therefore possess objective validity\"; it is more like a directional arrow in the space of ontology than like any particular thing within any ontology. Science progresses in part by repeatedly pulling the rug on the old ontology, \"revealing\" a more foundational layer underneath (a Kuhnian \"paradigm shift\"), which may be called more \"noumenal\" than the previous layer, but which is actually still phenomenal, in that it is cognizable through the scientific theory and corresponds to observations; \"noumena\", after the paradigm shift, is a placeholder concept that any future paradigm shifts can fill in with&nbsp;<i>their</i>&nbsp;new \"foundational\" layer.</p><p>Use of the word \"noumenon\" signals a kind of humility, of disbelieving that we have access to \"the real truth\", while being skeptical that anyone else does either.</p><p>In Bayesianism, <i>roughly</i>, the \"noumenon\" is specified by the hypothesis, while the \"phenomena\" are the observations.&nbsp; Assume for now the Bayesian observation is a deterministic function of the hypothesis; then, multiple noumena may correspond to a single phenomenon.&nbsp; Bayesianism allows for gaining information about the noumenon from the phenomenon.&nbsp; However, all we learn is that the noumenon is some hypothesis which corresponds to the phenomenon; in the posterior distribution, the hypotheses compatible with the observations maintain the same probabilities relative to each other that they did in the prior distribution.</p><p>(In cases where the observation is not a deterministic function of the hypothesis, as in the standard Bayes' Rule, consider replacing \"hypothesis\" above with the \"(hypothesis, observation)\" ordered pair.)</p><p>In Solomonoff Induction, there is only a limited amount we can learn about the \"noumenon\" (stochastic Turing machine generating our observations + its bits of stochasticity), since there exist equivalent Turing machines.</p><h2 id=\"A_priori_and_a_posteriori\"><i>A priori</i>&nbsp;and&nbsp;<i>a posteriori</i></h2><p><i>A priori</i>&nbsp;refers to the epistemic state possessed before taking in observations. In Bayesianism this is the P(X) operator unconditioned on any observations.</p><p><i>A posteriori</i>&nbsp;refers to the epistemic state possessed after taking in observations. In Bayesianism this is P(X | O) where O refers to past observation(s) made by an agent, which may be numbered to indicate time steps, as in a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process\">POMDP</a>.</p><p>While Kant and Hume agree that we can't learn about universal laws from experience (due to Hume's&nbsp;<a href=\"https://plato.stanford.edu/entries/induction-problem/\">problem of induction</a>), Hume concludes that this means we don't know about universal laws, while Kant instead argues that our knowledge about universal laws must involve a priori judgements, e.g. geometric or arithmetic judgments. (<a href=\"https://www.gwern.net/Modus\">One man's modus ponens is another's modus tollens...</a>)</p><h2 id=\"Analytic_and_synthetic\">Analytic and synthetic</h2><p>Analytic propositions are ones that can be verified as true by expanding out definitions and doing basic formal operations. A common example is \"All bachelors are unmarried\", which can be verified by replacing \"bachelor\" with \"unmarried man\".</p><p>Synthetic propositions can't be verified as true this way, e.g. \"All bachelors are alone\". They can be true regardless. Causal judgments are synthetic, we can't get the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Principle_of_sufficient_reason\">Principle of Sufficient Reason</a>&nbsp;analytically.</p><p>Contemporary STEM people are likely to think something like this at this point: \"Doesn't that just mean analytic propositions are mathematical, and synthetic propositions are empirical/scientific?\"</p><p>An immediate problem with this account: Kant doesn't think geometric propositions are analytic.&nbsp; Consider the proposition \"A square is equal to itself when turned 90 degrees on its center\".&nbsp; It's not apparent how to verify the proposition as true by properly defining \"square\" and so on, and doing basic logical/textual transformations.&nbsp; Instead we verify it by relating it to possible experience, imagining a rotating square in the visual field.</p><p>From a geometrical proof that a square is equal to itself when turned 90 degrees on its center, a prediction about possible experience can be derived, namely, that turning a square piece of wood 90 degrees by its center results in a wood square having the same shape and location in the visual field as it did previously.&nbsp; Mathematics needs to correspond to possible experience to have application to the perceptible outside world.</p><p>Kant doesn't even think arithmetic propositions are analytic. To get 2+2=4 from \"analytic\" operations, we could try defining 2=1+1, 4=1+1+1+1, then observing 2+2=(1+1)+(1+1)=1+1+1+1=4, however this requires using the commutative property of addition. Perhaps there is an alternative way to prove this \"analytically\" but neither Kant nor I know of that. Instead we can verify addition by, synthetically, corresponding numbers to our fingers, which \"automatically\" get commutative/associative properties.</p><h2 id=\"The_synthetic_a_priori\">The synthetic&nbsp;<i>a priori</i></h2><p>Besides the issue that math relates to possible experience, another problem with \"analytic = mathematical\" is that, as Kant argues, some propositions are both synthetic and&nbsp;<i>a priori</i>, and \"the real problem of pure reason\" is how we can know such propositions.</p><p>Here's an argument for this. Suppose we first observe O and then conclude that P is true. If we're reasoning validly, P is true&nbsp;<i>a posteriori</i>&nbsp;(relative to O). But this whole thought experiment pre-supposes that there is a time-structure in which we first see O and then we make a judgment about P. This time-structure is to some degree present even before seeing O, such that O can be evidence about P.</p><p>Imagine trying to argue that it's raining outside to a person who doesn't believe in time (including their own memory). Since you're both inside and there are no windows, they can't see that it's raining. You try to argue that you were previously both outside and saw that it was raining. But they think there's no such thing as \"the past\" so this is not convincing.</p><p>To make the argument to them successfully, their mind has to&nbsp;<a href=\"https://www.lesswrong.com/posts/CuSTqHgeK4CMpWYTe/created-already-in-motion\">already implement certain dynamics</a>&nbsp;even before receiving observations.</p><p>A baby is already born with cognitive machinery, it can't \"learn\" all of that machinery from data, the process of learning itself already requires this machinery to be present in order to structure observations and relate them to future ones. (In some sense there was no cognition prior to abiogenesis, though; there is a difference between the time ordering of science and of cognitive development.)</p><p>In Bayesianism, to learn P from O, it must be the case&nbsp;<i>a priori</i>&nbsp;that P is correlated with O. This correlational structure could be expressed as a Bayesian network. This network would encode an&nbsp;<i>a priori</i>&nbsp;assumption about how P and O are correlated.</p><p>Solomonoff induction doesn't encode a fixed network structure between its observations, instead it uses a mixture model over all stochastic Turing machines. However, all these machines have something in common, they're all Stochastic turing machines producing an output stream of bits. Solomonoff induction assumes&nbsp;<i>a priori</i>&nbsp;\"my observations are generated by a stochastic Turing machine\", it doesn't learn this from data.</p><p>One could try pointing to problems with this argument, e.g. perhaps \"there is time\" isn't a valid proposition, and time is a non-propositional structure in which propositions exist. But now that I just wrote that, it seems like I'm asserting a proposition about time to be true, in a contradictory fashion. The English language is more reflective than the language of Bayesian networks, allowing statements about the structure of propositions to themselves be propositions, as if the fact of the Bayesian network being arranged a particular way were itself represented by an assignment of a truth value to a node in that same Bayesian network.</p><h2 id=\"Transcendental\">Transcendental</h2><p>Philosophers today call Kant's philosophy \"transcendental idealism\". Kant himself uses the word \"transcendental\" to refer to cognitions about how cognitions are possible&nbsp;<i>a priori</i>.</p><p>This is in part an archaeological process. We see, right now, that we live in a phenomenal world that is approximately Euclidean. Was our phenomenal world always Euclidean, or was it non-Euclidean at some point and then switched over to Euclidean, or is time not enough of a real thing for this to cover all the cases? This sort of speculation about what the&nbsp;<i>a priori</i>&nbsp;empty mind is, from our a posteriori sense of the world, is transcendental.</p><p>One angle on the transcendental is, what&nbsp;<i>else</i>&nbsp;has to be true for the immediate (immanent) experience you are having right now to be what it is? If you are seeing a chair, that implies that chairs exist (at least as phenomena); if you see the same chair twice, that means that phenomenal objects can re-occur at different times; and so on.</p><h2 id=\"The_transcendental_aesthetic\">The transcendental aesthetic</h2><p>Aesthetic means sense. The transcendental aesthetic therefore refers to the&nbsp;<i>a priori</i>&nbsp;cognitive structures necessary for us to have sensation.</p><p>Mainly (Kant argues) these are space and time. I often call these \"<i>subjective</i>&nbsp;space\", \"<i>subjective</i>&nbsp;time\", \"<i>subjective</i>&nbsp;spacetime\", to emphasize that they are phenomenal and agent-centered.</p><h2 id=\"Space\">Space</h2><p>Most of our observations appear in space, e.g. visual input, emotional \"felt senses\" having a location in the body. To some extent we \"learn\" how to see the world spatially, however some spatial structures are hardwired (e.g. the visual cortex). Image processing AIs operate on spatial images stored as multidimensional arrays; arbitrarily rearranging the array would make some algorithms (such as convolutional neural networks) operate worse, indicating that the pre-formatting of data into a spatial array before it is fed into the algorithm is functional.</p><p>If space weren't&nbsp;<i>a priori</i>&nbsp;then we couldn't become fully confident of geometrical laws such as \"a square turned 90 degrees about its center is the same shape\", we'd have to learn these laws from experience, running into Hume's problem of induction.</p><p>There is only one space, since when attempting to imagine two spaces, one is putting them side by side; there must be some outermost container.</p><p>Space is infinite, unbounded.&nbsp; This doesn't imply that the infinity is all represented, just that the concept allows for indefinite extension.&nbsp; Finite space can be derived by adding a bound to infinite space; this is similar to Spinoza's approach to finitude in the&nbsp;<i>Ethics</i>.</p><p>Space isn't a property of things in themselves, it's a property of phenomena, things as they relate to our intuition.&nbsp; When formalizing mathematical space, points are assigned coordinates relative to the (0, 0) origin.&nbsp; We always intuit objects relative to some origin, which may be near the eyes or head.&nbsp; At the same time, space is necessary for objectivity; without space, there is no idea of&nbsp;<i>external</i>&nbsp;objects.</p><p>Our intuitions about space can only get universal geometric propositions if these propositions describe objects as they must necessarily appear in our intuition, not if they are describing arbitrary objects even as they may not appear in our intuition.&nbsp; As a motivating intuition, consider that non-Euclidean geometry is mathematically consistent; if objective space were non-Euclidean, then our Euclidean intuitions would not yield universally valid geometric laws about objective space. (As it turns out, contemporary physics theories propose that space is non-Euclidean.)</p><h2 id=\"Time\">Time</h2><p>We also see observations extending over time. Over short time scales there is a sense of continuity in time; over long time scales we have more discrete \"memories\" that refer to previous moments, making those past moments relevant to the present. The structuring of our experiences over time is necessary for learning, otherwise there wouldn't be a \"past\" to learn from. AIs are, similarly, fed data in a pre-coded (not learned) temporal structure, e.g. POMDP observations in a reinforcement learning context.</p><p>The time in which succession takes place is, importantly, different from objective clock time, though (typically) these do not disagree about ordering, only pacing.&nbsp; For example, there is usually only a small amount of time remembered during sleep, relative to the objective clock time that passes during sleep.&nbsp; (The theory of relativity further problematizes \"objective clock time\", so that different clocks may disagree about how much time has passed.)</p><p>We may, analogously, consider the case of a Solomonoff inductor that is periodically stopped and restarted as a computer process; while the inductor may measure subjective time by number of observation bits, this does not correspond to objective clock time, since a large amount of clock time may pass between when the inductor is stopped and restarted.</p><p>Kant writes, \"Different times are only parts of one and the same time\".&nbsp; Perhaps he is, here, too quick to dismiss non-linear forms of time; perhaps our future will branch into multiple non-interacting timelines, and perhaps this has happened in the past.&nbsp; One especially plausible nonlinear timelike structure is a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\">directed acyclic graph</a>. Still, DAGs have an order; despite time being nonlinear, it still advances from moment to moment.&nbsp; It is also possible to&nbsp;<i>arbitrarily</i>&nbsp;order a DAG through a topological sort, so the main relevant difference is that DAGs may drop this unnecessary ordering information.</p><p>Time is by default boundless but can be bounded, like space.</p><p>\"Time is nothing other than the form of the inner sense, i.e.&nbsp; of the intuition of our self and our inner sense\", in contrast to space, which is the form of the external sense. To give some intuition for this, suppose I have memory of some sequence of parties I have experienced; perhaps the first consists of myself, Bob, and Sally, the second consists of myself, Sally, and David, and the third consists of myself, Bob, and David.&nbsp; What is common between all the parties I remember is that I have been at all of them; this is true for no one other than myself.&nbsp; So, my memory is of situations involving myself; \"me\" is what is in common between all situations occurring in my subjective timeline.</p><p>Since time is the form of the inner sense, it applies to all representations, not only ones concerning outer objects, since all representations are in the mind.</p><p>Time is, like space, a condition for objects to appear to us, not a property of things in themselves.</p><h2 id=\"Relational_knowledge\">Relational knowledge</h2><p>Kant clarifies the way in which we fail to cognize objects in themselves, with the example of a triangle: \"if the object (that is, the triangle) were something in itself, without relation to you the subject; how could you affirm that that which lies necessarily in your subjective conditions in order to construct a triangle, must also necessarily belong to the triangle in itself?\"</p><p>Relational knowledge allows us to know objects as they relate to us, but not as they don't relate to us.&nbsp; Geometry applies to objects that have locations in spacetime; for objects to appear in subjective spacetime, they must have coordinates relative to the (0, 0) origin, that is, the self; therefore, geometry applies to objects that have locations relative to the self; without a location relative to the self, the object would not appear in subjective spacetime.</p><p>It may seem silly to say that this \"merely relational\" knowledge fails to understand the object in itself; what properties are there to understand other than relational properties?&nbsp; A triangle \"in itself\" independent of space (which relates the different parts of the triangle to each other) is a rather empty concept.</p><p>What is given up on, here, is an absolute reference frame, a \"view from nowhere\", from which objects could be conceived of in a way that is independent of all subjects; instead, we attain a view from somewhere, namely, from subjective spacetime.</p><p>Einstein's theory of special relativity also drops the absolute reference frame, however it specifies connections and translations between subjective reference frames in a way that Kant's theory doesn't.</p><h2 id=\"Sensibility_and_understanding\">Sensibility and understanding</h2><p>The sensibility is the faculty of passively receiving impressions, which are approximately \"raw sense-data\". The understanding is the faculty of spontaneously conceptualizing an object by means of these impressions.</p><p>To recognize an object (such as an apple), the mind must&nbsp;<i>do</i>&nbsp;something; with no mental motion, the light pattern of the apple would hit the retina, but no object would be represented accordingly. In general, the understanding&nbsp;<i>synthesizes</i>&nbsp;raw data into a coherent picture.</p><h2 id=\"Manifold_of_intuition\">Manifold of intuition</h2><p>Without concepts, sense data would be a disorganized flux, like a video of white noise; Kant terms this flux a \"manifold of intuition\".&nbsp; When I think of this, I think of a bunch of sheets of space tied together by a (curved) timeline holding them together, with pixel-like content in the space. Judgments, which are propositions about the content of our understanding (e.g. \"there is a cat in front of me\"), depend on the \"unity among our representations\"; what is needed is a \"higher [representation], which comprehends this and other representations under itself\".&nbsp; To judge that there is a cat in front of me, I must have parsed the manifold into concepts such as \"cat\" which relate to each other in a logically coherent universe; I cannot make a judgment from un-unified raw pixel-data. AI object recognition is an attempt to artificially replicate this faculty.</p><h2 id=\"Synthesis\">Synthesis</h2><p>Synthesis is the process of \"putting different representations together with each other and comprehending their manifoldness in one cognition\".&nbsp; This is an action of the spontaneity of thought, processing the manifold of intuition into a combined representation.</p><p>This relates to the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Binding_problem\">phenomenal binding problem</a>, how do we get a sense of a \"unified\" world from disconnected sensory data?</p><p>In Solomonoff Induction, the manifold of intuition would be the raw observations, and the manifold is synthesized by the fact that there is a universal Turing machine producing all the observations with a hidden state.&nbsp; This is the case&nbsp;<i>a priori</i>, not only after seeing particular observations.&nbsp; Similarly with other Bayesian models such as dynamic Bayesian networks; the network structure is prior to the particular observations.</p><h2 id=\"Transcendental_Categories\">Transcendental Categories</h2><p>\"There is only&nbsp;<i>one</i>&nbsp;experience, in which all perceptions are represented as in thoroughgoing and lawlike connection, just as there is only one space and time...\"</p><p>Different experiences are connected in a lawlike way, e.g. through causality and through re-occurring objects; otherwise, it would be unclear how to even interpret memories as referring to the same world. The transcendental categories (which are types of judgment) are ways in which different representations may be connected with each other.</p><p>Kant gives 12 transcendental categories, meant to be exhaustive. These include: causation/dependence, existence/nonexistence, necessity/contingence, unity, plurality. I don't understand all of these, and Kant doesn't go into enough detail to understand all of them. Roughly, these are different ways experiences can connect with each other, e.g. a change in an experienced object can cause a change in another, and two instances of seeing an object can be \"unified\" in the sense of being recognized as seeing the same object.</p><h2 id=\"Consciousness_of_the_self\">Consciousness of the self</h2><p>Kant discusses consciousness of the self: \"The consciousness of oneself in accordance with the determinations of our state in internal perception is merely empirical, forever variable; it can provide no standing or abiding self in this stream of inner appearances, and is customarily called inner sense or empirical apperception. That which should necessarily be represented as numerically identical cannot be thought of as such through empirical data. There must be a condition that precedes all experience and makes the latter itself possible, which should make such a transcendental presupposition valid.\"</p><p>The idea of a lack of a fixed or permanent self in the stream of internal phenomena will be familiar to people who have explored Buddhism.&nbsp; What you see isn't you; there are phenomena that are representations of the engine of representation, but these phenomena aren't identical with the engine of representation in which they are represented.</p><p>The self is, rather, something taken as \"numerically identical with itself\" which is a condition that precedes experience. Imagine a sequence of animation frames in a Cartesian coordinate system. In what sense are they part of \"the same sequence\"? Without knowing more about the sequence, all we can say is that they're all part of the same sequence (and have an ordering within it); the sameness of the sequence of each frame is \"numerical identity\" similar to the identity of an object (such as a table) with itself when perceived at different times.</p><h2 id=\"Dialectic\">Dialectic</h2><p>Kant writes: \"We termed dialectic in general a logic of appearance.\" Dialectic is a play of appearances, claiming to offer knowledge, but instead offering only temporary illusions; different sophists argue us into different conclusions repeatedly, perhaps in a cyclical fashion.</p><p>Dialectic is an error it is possible to fall into when reasoning is not connected with possible experience. Kant writes about dialectic in part to show how&nbsp;<i>not</i>&nbsp;to reason. One gets the impression that Kant would have thought Hegel and his followers were wasting their time by focusing so much on dialectic.</p><h2 id=\"The_Antinomies_of_Pure_Reason\">The Antinomies of Pure Reason</h2><p>As an example of dialectic, Kant argues that time and space are finite and that they are infinite; that everything is made of simple parts and that nothing is simple; that causality doesn't determine everything (requiring spontaneity as an addition) and that it does; that there is an absolutely necessary being and that there isn't. Each of these pairs of contradictory arguments is an antinomy.</p><p>Philosophers argue about these sorts of questions for millenia without much resolution; it's possible to find somewhat convincing arguments on both sides, as Kant demonstrates.</p><h2 id=\"Ens_Realissimum\">Ens Realissimum</h2><p>Kant writes: \"This conception of a sum-total of reality is the conception of a thing in itself, regarded as completely determined; and the conception of an&nbsp;<i>ens realissimum</i>&nbsp;is the conception of an individual being, inasmuch as it is determined by that predicate of all possible contradictory predicates, which indicates and belongs to being.\"</p><p>Say some objects are cold and some are hot. Well, they still have some things in common, they're both objects. There is a distinction being made (hot/cold), and there is something in common apart from that distinction. We could imagine a single undifferentiated object, that is neither hot nor cold, but which can be modified by making it hot/cold to produce specific objects.</p><p>This is similar to Spinoza's singular infinite substance/God, of which all other (possibly finite) beings are modifications, perhaps made by adding attributes.</p><p>The ens realissimum has a similar feel to the Tegmark IV multiverse, which contains all mathematically possible universes in a single being, or a generative grammar of a Turing complete language. It is a common undifferentiated basis for specific beings to be conceptualized within.</p><p>Kant considers deriving the existence of a supreme being (God) from the ens realissimum, but the concept is too empty to yield properties attributed to God, such as benevolence, being the intelligent creator of the universe, or providing an afterlife. He goes on to critique all supposed rational proofs of the existence of God, but goes on to say that he posits God and an afterlife because such posits are necessary to believe that the incentive of pleasure-seeking is aligned with acting morally. (Wishful thinking much?)</p><h2 id=\"Conclusion\">Conclusion</h2><p>What is Kant getting at, in all this? I think he is trying to get readers to attend to their experience, the spacetimelike container of this experience, and the way their world-model is constructed out of their experience. For example, the idea that time is the form of the inner sense is apparent from noting that all accessible memories include me, but it's possible to \"forget\" about this subjective timeline and instead conceptualize time as observer-independent. The idea that the manifold of intuition must be&nbsp;<i>actively</i>&nbsp;synthesized into a representation containing objects (which is in line with cognitive science) challenges the idea that the world is \"given\", that \"we\" are simply inhabitants of a stable world. The idea of the \"noumenon\" as a negative, limiting concept points us at our experience (and what our experience could be) as an alternative to interminably angsting about whether what we experience is \"really real\" or about metaphysical concepts like God, which makes it easier to get on with positivist math, science, economics, and scholarship without worrying too much about its foundations.</p><p>The sense I get reading Kant is: \"You live in a world of phenomena synthesized by your mind from some external data, and that's fine, in a sense it's all you could ever hope for. You have plenty of phenomena and generalities about them to explore, you can even inquire into the foundations of what makes them possible and how your mind generates them (I've already done a lot of that for you), but there's no deep&nbsp;<a href=\"https://mitpress.mit.edu/books/fanged-noumena\">Outside</a>&nbsp;demanding your attention, now go live!\"</p><p>When I take this seriously I worry about getting lost in my head, and sometimes I do get lost in my head, and the Outside does impinge on my cozy mental playground (demanding my attention, and loosening my mental assumptions structuring the phenomenal world), but things calm after a while and I experience the phenomenal world as orderly once again.</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Metaphysics", "anchor": "Metaphysics", "level": 1}, {"title": "The Copernican revolution in philosophy", "anchor": "The_Copernican_revolution_in_philosophy", "level": 1}, {"title": "Phenomena and noumena", "anchor": "Phenomena_and_noumena", "level": 1}, {"title": "A priori\u00a0and\u00a0a posteriori", "anchor": "A_priori_and_a_posteriori", "level": 1}, {"title": "Analytic and synthetic", "anchor": "Analytic_and_synthetic", "level": 1}, {"title": "The synthetic\u00a0a priori", "anchor": "The_synthetic_a_priori", "level": 1}, {"title": "Transcendental", "anchor": "Transcendental", "level": 1}, {"title": "The transcendental aesthetic", "anchor": "The_transcendental_aesthetic", "level": 1}, {"title": "Space", "anchor": "Space", "level": 1}, {"title": "Time", "anchor": "Time", "level": 1}, {"title": "Relational knowledge", "anchor": "Relational_knowledge", "level": 1}, {"title": "Sensibility and understanding", "anchor": "Sensibility_and_understanding", "level": 1}, {"title": "Manifold of intuition", "anchor": "Manifold_of_intuition", "level": 1}, {"title": "Synthesis", "anchor": "Synthesis", "level": 1}, {"title": "Transcendental Categories", "anchor": "Transcendental_Categories", "level": 1}, {"title": "Consciousness of the self", "anchor": "Consciousness_of_the_self", "level": 1}, {"title": "Dialectic", "anchor": "Dialectic", "level": 1}, {"title": "The Antinomies of Pure Reason", "anchor": "The_Antinomies_of_Pure_Reason", "level": 1}, {"title": "Ens Realissimum", "anchor": "Ens_Realissimum", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 23}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": 3, "af": false, "version": "0.6.0", "pingbacks": {"Posts": ["CuSTqHgeK4CMpWYTe"]}, "moderationGuidelinesVersion": "1.2.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-03T01:04:33.425Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-03T01:20:44.875Z", "modifiedAt": "2022-06-03T01:35:58.565Z", "url": null, "title": "[MLSN #4]: Many New Interpretability Papers, Virtual Logit Matching, Rationalization Helps Robustness", "slug": "mlsn-4-many-new-interpretability-papers-virtual-logit", "viewCount": null, "lastCommentedAt": "2022-06-03T01:20:44.875Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dan-hendrycks", "createdAt": "2021-08-07T16:54:49.878Z", "isAdmin": false, "displayName": "Dan Hendrycks"}, "userId": "RoHHX3SmMqkJ9RMEF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R39tGLeETfCZJ4FoE/mlsn-4-many-new-interpretability-papers-virtual-logit", "pageUrlRelative": "/posts/R39tGLeETfCZJ4FoE/mlsn-4-many-new-interpretability-papers-virtual-logit", "linkUrl": "https://www.lesswrong.com/posts/R39tGLeETfCZJ4FoE/mlsn-4-many-new-interpretability-papers-virtual-logit", "postedAtFormatted": "Friday, June 3rd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMLSN%20%234%5D%3A%20Many%20New%20Interpretability%20Papers%2C%20Virtual%20Logit%20Matching%2C%20Rationalization%20Helps%20Robustness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMLSN%20%234%5D%3A%20Many%20New%20Interpretability%20Papers%2C%20Virtual%20Logit%20Matching%2C%20Rationalization%20Helps%20Robustness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR39tGLeETfCZJ4FoE%2Fmlsn-4-many-new-interpretability-papers-virtual-logit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMLSN%20%234%5D%3A%20Many%20New%20Interpretability%20Papers%2C%20Virtual%20Logit%20Matching%2C%20Rationalization%20Helps%20Robustness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR39tGLeETfCZJ4FoE%2Fmlsn-4-many-new-interpretability-papers-virtual-logit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR39tGLeETfCZJ4FoE%2Fmlsn-4-many-new-interpretability-papers-virtual-logit", "socialPreviewImageUrl": "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8d7e77f-8613-4580-b8f2-512cfa743d76_3404x1210.png", "question": false, "authorIsUnreviewed": false, "wordCount": 1082, "htmlBody": "<p>As part of a larger community building effort, I am writing a safety newsletter which is designed to cover empirical safety research and be palatable to the broader machine learning research community. You can <a href=\"https://newsletter.mlsafety.org/\">subscribe here</a>, follow the newsletter on <a href=\"https://twitter.com/ml_safety\">twitter</a> here, or join the subreddit <a href=\"https://www.reddit.com/r/mlsafety/\">here</a>.</p><hr><p>Welcome to the 4th issue of the ML Safety Newsletter. In this edition, we cover:</p><ul><li>How \u201cmodel-based optimization\u201d environments can be used to research proxy gaming</li><li>How models can express their uncertainty through natural language</li><li>A new plug-and-play state-of-the-art OOD detection technique</li><li>How \u201crationales\u201d can improve robustness to adversarial attacks</li><li>Announcing our subreddit with safety papers added nightly</li><li>... and much more.</li></ul><h1><strong>Alignment</strong></h1><h3><strong>Making Proxies Less Vulnerable</strong></h3><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8d7e77f-8613-4580-b8f2-512cfa743d76_3404x1210.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8d7e77f-8613-4580-b8f2-512cfa743d76_3404x1210.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8d7e77f-8613-4580-b8f2-512cfa743d76_3404x1210.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8d7e77f-8613-4580-b8f2-512cfa743d76_3404x1210.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8d7e77f-8613-4580-b8f2-512cfa743d76_3404x1210.png 1456w\"></p><p><i>Functions without a smoothness prior can result in solutions that maximize a proxy but not the true objective.</i></p><p>A portion of \u201cModel-based optimization\u201d (MBO) research provides a way to study simple cases of proxy gaming: with MBO environments, we can learn how to build better proxies that yield better solutions when optimized.</p><p>MBO aims to design objects with desired properties, that is to find a new input that maximizes an objective. The objective is typically assumed to be expensive to evaluate and a black box. Since the black box objective is expensive to query, researchers are tasked with creating a proxy that can be queried repeatedly. An optimizer then finds an input that maximizes the proxy. To design proxies that yield better solutions according to the ground truth black-box objective, this paper incorporates a smoothness prior. As there are many mathematical details, see the paper for a full description. In short, model-based optimization environments can be used to empirically study how to create better, less gameable proxies.</p><p><a href=\"https://arxiv.org/abs/2110.14188\"><strong><u>[Paper]</u></strong></a><strong> </strong><a href=\"https://slideslive.com/38967498/roma-robust-model-adaptation-for-offline-modelbased-optimization?ref=speaker-17410-latest\"><strong><u>[Video]</u></strong></a></p><h3><strong>Other Alignment News</strong></h3><p><a href=\"https://arxiv.org/abs/2203.09911\"><u>[Link]</u></a> Why we need biased AI -- How including cognitive and ethical machine biases can enhance AI systems: \u201ca re-evaluation of the ethical significance of machine biases\u201d</p><p><a href=\"https://arxiv.org/abs/2205.05989\"><u>[Link]</u></a> Generating ethical analysis to moral quandaries</p><p><a href=\"https://arxiv.org/abs/2203.04946\"><u>[Link 1]</u></a> <a href=\"https://arxiv.org/abs/2203.04668\"><u>[Link 2]</u></a> Examples of inverse scaling or anticorrelated capabilities: perceptual similarity performance does not monotonically increase with classification accuracy</p><p><a href=\"https://arxiv.org/abs/2205.06750\"><u>[Link]</u></a> \u201ccomprehensive comparison of these provably safe RL methods\u201d</p><p><a href=\"https://arxiv.org/abs/2203.11409\"><u>[Link]</u></a> Inverse Reinforcement Learning Tutorial</p><p><a href=\"https://arxiv.org/abs/2204.05212\"><u>[Link]</u></a> Single-Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Questions: \u201cWe do not find that explanations in our set-up improve human accuracy\u201d</p><hr><h1><strong>Monitoring</strong></h1><h3><strong>Teaching Models to Express Their Uncertainty in Words</strong></h3><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5b61aea5-df72-4d16-9a80-f6d9fe6d94b2_3468x638.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5b61aea5-df72-4d16-9a80-f6d9fe6d94b2_3468x638.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5b61aea5-df72-4d16-9a80-f6d9fe6d94b2_3468x638.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5b61aea5-df72-4d16-9a80-f6d9fe6d94b2_3468x638.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5b61aea5-df72-4d16-9a80-f6d9fe6d94b2_3468x638.png 1456w\"></p><p>This work shows GPT-3 can express its uncertainty in natural language, without using model logits. Moreover, it is somewhat calibrated under various distribution shifts.</p><p>This is an early step toward making model uncertainty more interpretable and expressive. In the future, perhaps models could use natural language to express complicated beliefs such as \u201cevent A will occur with 60% probability assuming event B also occurs, and with 25% probability if event B does not.\u201d In the long-term, uncertainty estimation will likely remain nontrivial, as it is not obvious how to make future models calibrated on inherently uncertain, chaotic, or computationally prohibitive questions that extend beyond existing human knowledge.</p><p><a href=\"https://arxiv.org/abs/2205.14334\"><strong><u>[Link]</u></strong></a></p><h3><strong>Virtual Logit Matching</strong></h3><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2e74b27-fdf0-4785-8b99-51e8c18ad0ed_1920x574.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2e74b27-fdf0-4785-8b99-51e8c18ad0ed_1920x574.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2e74b27-fdf0-4785-8b99-51e8c18ad0ed_1920x574.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2e74b27-fdf0-4785-8b99-51e8c18ad0ed_1920x574.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2e74b27-fdf0-4785-8b99-51e8c18ad0ed_1920x574.png 1456w\"></p><p><i>An illustration of the Virtual Logit Matching pipeline.</i></p><p>Virtual logit matching is a new out-of-distribution technique that does not require hyperparameter tuning, does not require retraining models, and beats the maximum softmax baseline on most OOD detection tasks. The idea is to create a \u201cvirtual logit,\u201d which is proportional to the magnitude of the projection of the input onto the space orthogonal to the principal embedding space. Then the OOD score is roughly equal to the virtual logit minus the maximum logit, intuitively the evidence that the input is unlike the training example embeddings minus the evidence that it is in-distribution.</p><p><a href=\"https://arxiv.org/abs/2203.10807\"><strong><u>[Link]</u></strong></a></p><h3><strong>Other Monitoring News</strong></h3><p><a href=\"https://arxiv.org/abs/2204.07531\"><u>[Link]</u></a> \u201cWe train probes to investigate what concepts are encoded in game-playing agents like AlphaGo and how those concepts relate to natural language\u201d</p><p><a href=\"https://gradientscience.org/missingness/\"><u>[Link]</u></a> By removing parts of an input image, one can analyze how much a model depends on a given input feature. However, removing parts of the input is often not completely sound, as removing parts confuses models. Fortunately with Vision Transformers, removing patches is a matter of simply dropping tokens, which is a more sound way to create counterfactual inputs.</p><p><a href=\"https://arxiv.org/abs/2201.11114\"><u>[Link]</u></a> To more scalably characterize model components, this work \u201cautomatically labels neurons with open-ended, compositional, natural language descriptions\u201d</p><p><a href=\"https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html\"><u>[Link]</u></a> Transformer mechanisms that complete simple sequences are identified and shown to be emergent during training</p><p><a href=\"https://arxiv.org/abs/2204.11642\"><u>[Link]</u></a> An interpretability benchmark: controllably generate trainable examples under arbitrary biases (shape, color, etc) \u2192 human subjects are asked to predict the systems' output relying on explanations</p><p><a href=\"https://github.com/Jingkang50/OpenOOD\"><u>[Link]</u></a> A new library has implementations of over a dozen OOD detection techniques</p><p><a href=\"https://arxiv.org/abs/2203.15506\"><u>[Link]</u></a> Trojan detection cat and mouse continues: a new attack \u201creduces the accuracy of a state-of-the-art defense mechanism from &gt;96% to 0%\u201d</p><p><a href=\"https://arxiv.org/abs/2205.05055\"><u>[Link]</u></a> Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers: \u201cwe find that few-shot learning emerges only from applying the right architecture to the right data distribution; neither component is sufficient on its own\u201d</p><p><a href=\"https://arxiv.org/abs/2205.10343\"><u>[Link]</u></a> Research on understanding emergent functionality: \u201cWe observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion\u201d</p><p><a href=\"https://arxiv.org/abs/2202.05983\"><u>[Link]</u></a> Displaying a model's true confidence can be suboptimal for helping people make better decisions</p><hr><h1><strong>Robustness</strong></h1><h3><strong>Can Rationalization Improve Robustness?</strong></h3><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2d60c226-c7a2-4236-a21b-ea4b458a6a60_2144x1362.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2d60c226-c7a2-4236-a21b-ea4b458a6a60_2144x1362.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2d60c226-c7a2-4236-a21b-ea4b458a6a60_2144x1362.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2d60c226-c7a2-4236-a21b-ea4b458a6a60_2144x1362.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2d60c226-c7a2-4236-a21b-ea4b458a6a60_2144x1362.png 1456w\"></p><p>To improve robustness, this paper asks models to explain their predictions. These are called \u201crationales.\u201d When models produce rationales before predicting, they are more robust to token-level and sentence-level adversarial attacks.</p><p><a href=\"https://arxiv.org/pdf/2204.11790.pdf\"><strong><u>[Link]</u></strong></a></p><h3><strong>Other Robustness News</strong></h3><p><a href=\"https://arxiv.org/abs/2204.04063\"><u>[Link]</u></a> How well do adversarial attacks transfer? This paper provides a large-scale systematic empirical study in real-world environments</p><p><a href=\"https://arxiv.org/abs/2205.06154\"><u>[Link]</u></a> Advancement in robustness with guarantees: \u201c[we] provide better certificates in terms of certified accuracy, average certified radii and abstention rates as compared to concurrent approaches\u201d</p><p><a href=\"https://arxiv.org/abs/2205.01663\"><u>[Link]</u></a> A large-scale data collection effort to add three <a href=\"https://terrytao.wordpress.com/2021/10/03/nines-of-safety-a-proposed-unit-of-measurement-of-risk/\"><u>nines of reliability</u></a> to an injury classification task</p><p><a href=\"https://arxiv.org/abs/2204.02937\"><u>[Link]</u></a> \u201csimple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks\u201d</p><p><a href=\"https://arxiv.org/abs/2205.01397\"><u>[Link]</u></a> What causes CLIP's perceived robustness? Mostly dataset diversity, suggesting semantic overlap with the test distribution</p><p><a href=\"https://arxiv.org/abs/2203.12117\"><u>[Link]</u></a> Testing RL agent robustness to abrupt changes and sudden shocks to the environment</p><hr><h2><strong>Other News</strong></h2><p><a href=\"https://www.reddit.com/r/mlsafety/\"><u>We now have a subreddit!</u></a> The subreddit has a steady stream of safety-relevant papers, including safety papers not covered in this newsletter. Papers are added to the subreddit several times a week. The subreddit\u2019s posts are available <a href=\"https://twitter.com/topofmlsafety\"><u>on twitter too</u></a>.</p><p><a href=\"https://www.youtube.com/watch?v=dQ4cmtHCYt4\"><u>A lecture series</u></a> on social and ethical considerations of advanced AI: concrete suggestions for creating cooperative AI; discussion of the infeasibility and suboptimality of various deployment strategies; discussion of the merits of AI autonomy and reasonableness over rationality; and outlining how communities of agents could be robustly safe. (I recommend watching <a href=\"https://www.youtube.com/watch?v=dQ4cmtHCYt4\"><u>the final lecture</u></a>, and if you\u2019re interested consider watching the <a href=\"https://www.youtube.com/watch?v=P2uDQiTz5Ss\"><u>previous</u></a> <a href=\"https://www.youtube.com/watch?v=uz5qXBGM9HY\"><u>lectures</u></a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "R39tGLeETfCZJ4FoE", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 7, "extendedScore": null, "score": 0.4300342869726086, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-03T01:28:04.223Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8d7e77f-8613-4580-b8f2-512cfa743d76_3404x1210.png", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>As part of a larger community building effort, I am writing a safety newsletter which is designed to cover empirical safety research and be palatable to the broader machine learning research community. You can <a href=\"https://newsletter.mlsafety.org/\">subscribe here</a>, follow the newsletter on <a href=\"https://twitter.com/ml_safety\">twitter</a> here, or join the subreddit <a href=\"https://www.reddit.com/r/mlsafety/\">here</a>.</p><hr><p>Welcome to the 4th issue of the ML Safety Newsletter. In this edition, we cover:</p><ul><li>How \u201cmodel-based optimization\u201d environments can be used to research proxy gaming</li><li>How models can express their uncertainty through natural language</li><li>A new plug-and-play state-of-the-art OOD detection technique</li><li>How \u201crationales\u201d can improve robustness to adversarial attacks</li><li>Announcing our subreddit with safety papers added nightly</li><li>... and much more.</li></ul><h1 id=\"Alignment\"><strong>Alignment</strong></h1><h3 id=\"Making_Proxies_Less_Vulnerable\"><strong>Making Proxies Less Vulnerable</strong></h3><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8d7e77f-8613-4580-b8f2-512cfa743d76_3404x1210.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8d7e77f-8613-4580-b8f2-512cfa743d76_3404x1210.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8d7e77f-8613-4580-b8f2-512cfa743d76_3404x1210.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8d7e77f-8613-4580-b8f2-512cfa743d76_3404x1210.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8d7e77f-8613-4580-b8f2-512cfa743d76_3404x1210.png 1456w\"></p><p><i>Functions without a smoothness prior can result in solutions that maximize a proxy but not the true objective.</i></p><p>A portion of \u201cModel-based optimization\u201d (MBO) research provides a way to study simple cases of proxy gaming: with MBO environments, we can learn how to build better proxies that yield better solutions when optimized.</p><p>MBO aims to design objects with desired properties, that is to find a new input that maximizes an objective. The objective is typically assumed to be expensive to evaluate and a black box. Since the black box objective is expensive to query, researchers are tasked with creating a proxy that can be queried repeatedly. An optimizer then finds an input that maximizes the proxy. To design proxies that yield better solutions according to the ground truth black-box objective, this paper incorporates a smoothness prior. As there are many mathematical details, see the paper for a full description. In short, model-based optimization environments can be used to empirically study how to create better, less gameable proxies.</p><p><a href=\"https://arxiv.org/abs/2110.14188\"><strong><u>[Paper]</u></strong></a><strong> </strong><a href=\"https://slideslive.com/38967498/roma-robust-model-adaptation-for-offline-modelbased-optimization?ref=speaker-17410-latest\"><strong><u>[Video]</u></strong></a></p><h3 id=\"Other_Alignment_News\"><strong>Other Alignment News</strong></h3><p><a href=\"https://arxiv.org/abs/2203.09911\"><u>[Link]</u></a> Why we need biased AI -- How including cognitive and ethical machine biases can enhance AI systems: \u201ca re-evaluation of the ethical significance of machine biases\u201d</p><p><a href=\"https://arxiv.org/abs/2205.05989\"><u>[Link]</u></a> Generating ethical analysis to moral quandaries</p><p><a href=\"https://arxiv.org/abs/2203.04946\"><u>[Link 1]</u></a> <a href=\"https://arxiv.org/abs/2203.04668\"><u>[Link 2]</u></a> Examples of inverse scaling or anticorrelated capabilities: perceptual similarity performance does not monotonically increase with classification accuracy</p><p><a href=\"https://arxiv.org/abs/2205.06750\"><u>[Link]</u></a> \u201ccomprehensive comparison of these provably safe RL methods\u201d</p><p><a href=\"https://arxiv.org/abs/2203.11409\"><u>[Link]</u></a> Inverse Reinforcement Learning Tutorial</p><p><a href=\"https://arxiv.org/abs/2204.05212\"><u>[Link]</u></a> Single-Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Questions: \u201cWe do not find that explanations in our set-up improve human accuracy\u201d</p><hr><h1 id=\"Monitoring\"><strong>Monitoring</strong></h1><h3 id=\"Teaching_Models_to_Express_Their_Uncertainty_in_Words\"><strong>Teaching Models to Express Their Uncertainty in Words</strong></h3><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5b61aea5-df72-4d16-9a80-f6d9fe6d94b2_3468x638.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5b61aea5-df72-4d16-9a80-f6d9fe6d94b2_3468x638.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5b61aea5-df72-4d16-9a80-f6d9fe6d94b2_3468x638.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5b61aea5-df72-4d16-9a80-f6d9fe6d94b2_3468x638.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5b61aea5-df72-4d16-9a80-f6d9fe6d94b2_3468x638.png 1456w\"></p><p>This work shows GPT-3 can express its uncertainty in natural language, without using model logits. Moreover, it is somewhat calibrated under various distribution shifts.</p><p>This is an early step toward making model uncertainty more interpretable and expressive. In the future, perhaps models could use natural language to express complicated beliefs such as \u201cevent A will occur with 60% probability assuming event B also occurs, and with 25% probability if event B does not.\u201d In the long-term, uncertainty estimation will likely remain nontrivial, as it is not obvious how to make future models calibrated on inherently uncertain, chaotic, or computationally prohibitive questions that extend beyond existing human knowledge.</p><p><a href=\"https://arxiv.org/abs/2205.14334\"><strong><u>[Link]</u></strong></a></p><h3 id=\"Virtual_Logit_Matching\"><strong>Virtual Logit Matching</strong></h3><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2e74b27-fdf0-4785-8b99-51e8c18ad0ed_1920x574.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2e74b27-fdf0-4785-8b99-51e8c18ad0ed_1920x574.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2e74b27-fdf0-4785-8b99-51e8c18ad0ed_1920x574.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2e74b27-fdf0-4785-8b99-51e8c18ad0ed_1920x574.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2e74b27-fdf0-4785-8b99-51e8c18ad0ed_1920x574.png 1456w\"></p><p><i>An illustration of the Virtual Logit Matching pipeline.</i></p><p>Virtual logit matching is a new out-of-distribution technique that does not require hyperparameter tuning, does not require retraining models, and beats the maximum softmax baseline on most OOD detection tasks. The idea is to create a \u201cvirtual logit,\u201d which is proportional to the magnitude of the projection of the input onto the space orthogonal to the principal embedding space. Then the OOD score is roughly equal to the virtual logit minus the maximum logit, intuitively the evidence that the input is unlike the training example embeddings minus the evidence that it is in-distribution.</p><p><a href=\"https://arxiv.org/abs/2203.10807\"><strong><u>[Link]</u></strong></a></p><h3 id=\"Other_Monitoring_News\"><strong>Other Monitoring News</strong></h3><p><a href=\"https://arxiv.org/abs/2204.07531\"><u>[Link]</u></a> \u201cWe train probes to investigate what concepts are encoded in game-playing agents like AlphaGo and how those concepts relate to natural language\u201d</p><p><a href=\"https://gradientscience.org/missingness/\"><u>[Link]</u></a> By removing parts of an input image, one can analyze how much a model depends on a given input feature. However, removing parts of the input is often not completely sound, as removing parts confuses models. Fortunately with Vision Transformers, removing patches is a matter of simply dropping tokens, which is a more sound way to create counterfactual inputs.</p><p><a href=\"https://arxiv.org/abs/2201.11114\"><u>[Link]</u></a> To more scalably characterize model components, this work \u201cautomatically labels neurons with open-ended, compositional, natural language descriptions\u201d</p><p><a href=\"https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html\"><u>[Link]</u></a> Transformer mechanisms that complete simple sequences are identified and shown to be emergent during training</p><p><a href=\"https://arxiv.org/abs/2204.11642\"><u>[Link]</u></a> An interpretability benchmark: controllably generate trainable examples under arbitrary biases (shape, color, etc) \u2192 human subjects are asked to predict the systems' output relying on explanations</p><p><a href=\"https://github.com/Jingkang50/OpenOOD\"><u>[Link]</u></a> A new library has implementations of over a dozen OOD detection techniques</p><p><a href=\"https://arxiv.org/abs/2203.15506\"><u>[Link]</u></a> Trojan detection cat and mouse continues: a new attack \u201creduces the accuracy of a state-of-the-art defense mechanism from &gt;96% to 0%\u201d</p><p><a href=\"https://arxiv.org/abs/2205.05055\"><u>[Link]</u></a> Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers: \u201cwe find that few-shot learning emerges only from applying the right architecture to the right data distribution; neither component is sufficient on its own\u201d</p><p><a href=\"https://arxiv.org/abs/2205.10343\"><u>[Link]</u></a> Research on understanding emergent functionality: \u201cWe observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion\u201d</p><p><a href=\"https://arxiv.org/abs/2202.05983\"><u>[Link]</u></a> Displaying a model's true confidence can be suboptimal for helping people make better decisions</p><hr><h1 id=\"Robustness\"><strong>Robustness</strong></h1><h3 id=\"Can_Rationalization_Improve_Robustness_\"><strong>Can Rationalization Improve Robustness?</strong></h3><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2d60c226-c7a2-4236-a21b-ea4b458a6a60_2144x1362.png\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2d60c226-c7a2-4236-a21b-ea4b458a6a60_2144x1362.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2d60c226-c7a2-4236-a21b-ea4b458a6a60_2144x1362.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2d60c226-c7a2-4236-a21b-ea4b458a6a60_2144x1362.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2d60c226-c7a2-4236-a21b-ea4b458a6a60_2144x1362.png 1456w\"></p><p>To improve robustness, this paper asks models to explain their predictions. These are called \u201crationales.\u201d When models produce rationales before predicting, they are more robust to token-level and sentence-level adversarial attacks.</p><p><a href=\"https://arxiv.org/pdf/2204.11790.pdf\"><strong><u>[Link]</u></strong></a></p><h3 id=\"Other_Robustness_News\"><strong>Other Robustness News</strong></h3><p><a href=\"https://arxiv.org/abs/2204.04063\"><u>[Link]</u></a> How well do adversarial attacks transfer? This paper provides a large-scale systematic empirical study in real-world environments</p><p><a href=\"https://arxiv.org/abs/2205.06154\"><u>[Link]</u></a> Advancement in robustness with guarantees: \u201c[we] provide better certificates in terms of certified accuracy, average certified radii and abstention rates as compared to concurrent approaches\u201d</p><p><a href=\"https://arxiv.org/abs/2205.01663\"><u>[Link]</u></a> A large-scale data collection effort to add three <a href=\"https://terrytao.wordpress.com/2021/10/03/nines-of-safety-a-proposed-unit-of-measurement-of-risk/\"><u>nines of reliability</u></a> to an injury classification task</p><p><a href=\"https://arxiv.org/abs/2204.02937\"><u>[Link]</u></a> \u201csimple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks\u201d</p><p><a href=\"https://arxiv.org/abs/2205.01397\"><u>[Link]</u></a> What causes CLIP's perceived robustness? Mostly dataset diversity, suggesting semantic overlap with the test distribution</p><p><a href=\"https://arxiv.org/abs/2203.12117\"><u>[Link]</u></a> Testing RL agent robustness to abrupt changes and sudden shocks to the environment</p><hr><h2 id=\"Other_News\"><strong>Other News</strong></h2><p><a href=\"https://www.reddit.com/r/mlsafety/\"><u>We now have a subreddit!</u></a> The subreddit has a steady stream of safety-relevant papers, including safety papers not covered in this newsletter. Papers are added to the subreddit several times a week. The subreddit\u2019s posts are available <a href=\"https://twitter.com/topofmlsafety\"><u>on twitter too</u></a>.</p><p><a href=\"https://www.youtube.com/watch?v=dQ4cmtHCYt4\"><u>A lecture series</u></a> on social and ethical considerations of advanced AI: concrete suggestions for creating cooperative AI; discussion of the infeasibility and suboptimality of various deployment strategies; discussion of the merits of AI autonomy and reasonableness over rationality; and outlining how communities of agents could be robustly safe. (I recommend watching <a href=\"https://www.youtube.com/watch?v=dQ4cmtHCYt4\"><u>the final lecture</u></a>, and if you\u2019re interested consider watching the <a href=\"https://www.youtube.com/watch?v=P2uDQiTz5Ss\"><u>previous</u></a> <a href=\"https://www.youtube.com/watch?v=uz5qXBGM9HY\"><u>lectures</u></a>.)</p>", "sections": [{"title": "Alignment", "anchor": "Alignment", "level": 1}, {"title": "Making Proxies Less Vulnerable", "anchor": "Making_Proxies_Less_Vulnerable", "level": 3}, {"title": "Other Alignment News", "anchor": "Other_Alignment_News", "level": 3}, {"title": "Monitoring", "anchor": "Monitoring", "level": 1}, {"title": "Teaching Models to Express Their Uncertainty in Words", "anchor": "Teaching_Models_to_Express_Their_Uncertainty_in_Words", "level": 3}, {"title": "Virtual Logit Matching", "anchor": "Virtual_Logit_Matching", "level": 3}, {"title": "Other Monitoring News", "anchor": "Other_Monitoring_News", "level": 3}, {"title": "Robustness", "anchor": "Robustness", "level": 1}, {"title": "Can Rationalization Improve Robustness?", "anchor": "Can_Rationalization_Improve_Robustness_", "level": 3}, {"title": "Other Robustness News", "anchor": "Other_Robustness_News", "level": 3}, {"title": "Other News", "anchor": "Other_News", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 13}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": null, "af": true, "version": "1.2.0", "pingbacks": {}, "moderationGuidelinesVersion": "1.2.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-03T01:20:44.879Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-03T02:20:02.159Z", "modifiedAt": "2022-06-03T04:17:02.377Z", "url": null, "title": "Another Calming Example", "slug": "another-calming-example", "viewCount": null, "lastCommentedAt": "2022-06-03T06:10:51.110Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gytFzxCTW6ekgeNZJ/another-calming-example", "pageUrlRelative": "/posts/gytFzxCTW6ekgeNZJ/another-calming-example", "linkUrl": "https://www.lesswrong.com/posts/gytFzxCTW6ekgeNZJ/another-calming-example", "postedAtFormatted": "Friday, June 3rd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20Calming%20Example&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20Calming%20Example%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgytFzxCTW6ekgeNZJ%2Fanother-calming-example%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20Calming%20Example%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgytFzxCTW6ekgeNZJ%2Fanother-calming-example", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgytFzxCTW6ekgeNZJ%2Fanother-calming-example", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 498, "htmlBody": "<p><span>\n\nIn reading about parenting I often feel like there's a bit too much\ntheory vs examples, so here's walking through a recent interaction\nthat others later commented went surprisingly well.\n\n</span>\n\n</p><p>\n\nAt a recent Tuesday Family Dinner, one of the kids (~6y) served\nthemself an absurd amount of pasta.  They were told to put some of it\nback, refused, and someone else put it back.  They burst into tears\nand completely fell apart.  They were told they either needed to calm\ndown or leave the room, and they left.  Lots of angry crying and\nshouting from the other side of the house.\n\n</p>\n\n<p>\n\nAfter waiting a bit to give them a chance to calm down some, I went to\nsee if they wanted to talk. I asked, and they said they did (if they\nhadn't I would have turned around and gone back to the table).  I sat\nwith them on the couch with a mindset of providing calm and patient\nattention, and asked what had happened. They started to explain\nthrough their sobs, but I told them that I couldn't understand and asked\nif they could speak normally.\n\n\n\n</p>\n\n<p>\n\nThis isn't actually true: I'm generally pretty good at understanding\nkids, even when they are crying pretty hard.  I'm strongly opposed to\nlying (to kids or anyone) in most circumstances, but this is one place\nwhere I do make an exception.  I pretend that I can't understand, ask\nif they can speak normally, present myself as an eager listener, and\nin response kids reliably pull themselves together.  This has a strong calming\neffect: something about no longer crying seems to filter back into\nfeelings not seeming so overwhelming.\n\n</p>\n\n<p>\n\nIn this case, they calmed down some, and explained that they were\nupset because they had the amount of pasta they wanted and then people\ntook it away.  We talked about what they didn't like about that and\nthey told me they were worried the pasta was going to run out and they\nwould still be hungry after dinner.\n\n</p>\n\n<p>\n\nPersonally, I think this is very unlikely to be why they fell apart\nwhile at the table, but that doesn't actually matter!  What's\nimportant is that they've calmed down and put something into words:\nonce it's in words, we can work on it. I used a whispery voice to tell\nthem that I knew about some secret extra pasta, and that there was no\nway we were going to run out. I asked if they wanted to sneak back\ninto the kitchen and see, which they were excited about.  Together we\ncrept into the kitchen, as quietly as possible, to peek at the serving\nbowl.  There was, as we observed together, much extra pasta.\n\n</p>\n\n<p>\n\nBy that point they were in a good place emotionally and we had solved\nwhat they had described as their problem.  They cheerfully sat back\ndown at the table, and the rest of dinner went well.  When they\nfinished what was on their plate and wanted more, I gave them some.\n\n</p>\n\n<p>\n\n<i>Earlier: <a href=\"https://www.jefftk.com/p/a-calming-strategy\">A Calming Strategy</a>.</i>\n\n  </p>\n\n<p><i>Comment via: <a href=\"https://www.facebook.com/jefftk/posts/pfbid0hV5myyBmNFXTAAmoLUcSvggczNy7Wj75s3MPT5RYnWXqRHaPDpgtkPjBeHSTjapfl\">facebook</a></i></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": "https://www.jefftk.com/p/another-calming-example", "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 2, "fkABsGCJZ6y9qConW": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "gytFzxCTW6ekgeNZJ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 20, "extendedScore": null, "score": 1.3635821990625245, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": "ma5dgL5yFHRxKLZKv", "feedLink": "https://www.jefftk.com/p/another-calming-example", "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "qgdGA4ZEyW7zNdK84", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-03T02:20:02.163Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-03T04:59:51.456Z", "modifiedAt": "2022-06-03T05:12:22.108Z", "url": null, "title": "Silliness", "slug": "silliness", "viewCount": null, "lastCommentedAt": "2022-06-03T04:59:51.456Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsusr", "createdAt": "2019-08-03T22:27:09.960Z", "isAdmin": false, "displayName": "lsusr"}, "userId": "n6LYNw2uGfYnD4pX2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oqozxHoFsC6PKz6bz/silliness", "pageUrlRelative": "/posts/oqozxHoFsC6PKz6bz/silliness", "linkUrl": "https://www.lesswrong.com/posts/oqozxHoFsC6PKz6bz/silliness", "postedAtFormatted": "Friday, June 3rd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Silliness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASilliness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoqozxHoFsC6PKz6bz%2Fsilliness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Silliness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoqozxHoFsC6PKz6bz%2Fsilliness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoqozxHoFsC6PKz6bz%2Fsilliness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 355, "htmlBody": "<p>Before I saw <em>Everything Everywhere All at Once</em> all I knew was:</p>\n<ol>\n<li>It's about a Chinese-American family that runs a laundromat.</li>\n<li>It's the best movie ever.</li>\n</ol>\n<h1>Spoilers Ahead</h1>\n<div class=\"spoiler\">\n<p><em>Everything Everywhere All at Once</em> is a silly movie.</p>\n</div>\n<p>What is silliness? Silliness is the opposite of seriousness. What is seriousness? Seriousness is the opposite of fun.</p>\n<p>Does that mean silliness equals fun? Not quite. All silliness is fun but not all fun is silly. Silliness is a subset of play is a subset of fun.</p>\n<p>I think silliness is a terminal value. Silliness is precious. Silliness must be protected.</p>\n<p>Silliness is giving into harmless impulses. Silliness is non-consequentialist. Silliness doesn't have a point. Silliness is letting go of pretention and just having fun. Right. This. Instant.</p>\n<p>XD</p>\n<p>:P</p>\n<p>\ud83d\ude00 \ud83d\ude03 \ud83d\ude04 \ud83d\ude01 \ud83d\ude06 \ud83d\ude05 \ud83d\ude02 \ud83e\udd23 \ud83e\udd72 \u263a\ufe0f \ud83d\ude0a \ud83d\ude07 \ud83d\ude42 \ud83d\ude43 \ud83d\ude09 \ud83d\ude0c \ud83d\ude0d \ud83e\udd70 \ud83d\ude18 \ud83d\ude17 \ud83d\ude19 \ud83d\ude1a \ud83d\ude0b \ud83d\ude1b \ud83d\ude1d \ud83d\ude1c \ud83e\udd2a \ud83e\udd28 \ud83e\uddd0 \ud83e\udd13 \ud83d\ude0e \ud83e\udd78 \ud83e\udd29 \ud83e\udd73 \ud83d\ude0f \ud83d\ude12 \ud83d\ude1e \ud83d\ude14 \ud83d\ude1f \ud83d\ude15 \ud83d\ude41 \u2639\ufe0f \ud83d\ude23 \ud83d\ude16 \ud83d\ude2b \ud83d\ude29 \ud83e\udd7a \ud83d\ude22 \ud83d\ude2d \ud83d\ude24 \ud83d\ude20 \ud83d\ude21 \ud83e\udd2c \ud83e\udd2f \ud83d\ude33 \ud83e\udd75 \ud83e\udd76 \ud83d\ude31 \ud83d\ude28 \ud83d\ude30 \ud83d\ude25 \ud83d\ude13 \ud83e\udd17 \ud83e\udd14 \ud83e\udd2d \ud83e\udd2b \ud83e\udd25 \ud83d\ude36 \ud83d\ude10 \ud83d\ude11 \ud83d\ude2c \ud83d\ude44 \ud83d\ude2f \ud83d\ude26 \ud83d\ude27 \ud83d\ude2e \ud83d\ude32 \ud83e\udd71 \ud83d\ude34 \ud83e\udd24 \ud83d\ude2a \ud83d\ude35 \ud83e\udd10 \ud83e\udd74 \ud83e\udd22 \ud83e\udd2e \ud83e\udd27 \ud83d\ude37 \ud83e\udd12 \ud83e\udd15 \ud83e\udd11 \ud83e\udd20 \ud83d\ude08 \ud83d\udc7f \ud83d\udc79 \ud83d\udc7a \ud83e\udd21 \ud83d\udca9 \ud83d\udc7b \ud83d\udc80 \u2620\ufe0f \ud83d\udc7d \ud83d\udc7e \ud83e\udd16 \ud83c\udf83 \ud83d\ude3a \ud83d\ude38 \ud83d\ude39 \ud83d\ude3b \ud83d\ude3c \ud83d\ude3d \ud83d\ude40 \ud83d\ude3f \ud83d\ude3e</p>\n<p>\ud83d\udc36 \ud83d\udc31 \ud83d\udc2d \ud83d\udc39 \ud83d\udc30 \ud83e\udd8a \ud83d\udc3b \ud83d\udc3c \u2744\ufe0f \ud83d\udc28 \ud83d\udc2f \ud83e\udd81 \ud83d\udc2e \ud83d\udc37 \ud83d\udc3d \ud83d\udc38 \ud83d\udc35 \ud83d\ude48 \ud83d\ude49 \ud83d\ude4a \ud83d\udc12 \ud83d\udc14 \ud83d\udc27 \ud83d\udc26 \ud83d\udc24 \ud83d\udc23 \ud83d\udc25 \ud83e\udd86 \ud83e\udd85 \ud83e\udd89 \ud83e\udd87 \ud83d\udc3a \ud83d\udc17 \ud83d\udc34 \ud83e\udd84 \ud83d\udc1d \ud83e\udeb1 \ud83d\udc1b \ud83e\udd8b \ud83d\udc0c \ud83d\udc1e \ud83d\udc1c \ud83e\udeb0 \ud83e\udeb2 \ud83e\udeb3 \ud83e\udd9f \ud83e\udd97 \ud83d\udd77 \ud83d\udd78 \ud83e\udd82 \ud83d\udc22 \ud83d\udc0d \ud83e\udd8e \ud83e\udd96 \ud83e\udd95 \ud83d\udc19 \ud83e\udd91 \ud83e\udd90 \ud83e\udd9e \ud83e\udd80 \ud83d\udc21 \ud83d\udc20 \ud83d\udc1f \ud83d\udc2c \ud83d\udc33 \ud83d\udc0b \ud83e\udd88 \ud83d\udc0a \ud83d\udc05 \ud83d\udc06 \ud83e\udd93 \ud83e\udd8d \ud83e\udda7 \ud83e\udda3 \ud83d\udc18 \ud83e\udd9b \ud83e\udd8f \ud83d\udc2a \ud83d\udc2b \ud83e\udd92 \ud83e\udd98 \ud83e\uddac \ud83d\udc03 \ud83d\udc02 \ud83d\udc04 \ud83d\udc0e \ud83d\udc16 \ud83d\udc0f \ud83d\udc11 \ud83e\udd99 \ud83d\udc10 \ud83e\udd8c \ud83d\udc15 \ud83d\udc29 \ud83e\uddae \ud83e\uddba \ud83d\udc08 \u2b1b \ud83e\udeb6 \ud83d\udc13 \ud83e\udd83 \ud83e\udda4 \ud83e\udd9a \ud83e\udd9c \ud83e\udda2 \ud83e\udda9 \ud83d\udd4a \ud83d\udc07 \ud83e\udd9d \ud83e\udda8 \ud83e\udda1 \ud83e\uddab \ud83e\udda6 \ud83e\udda5 \ud83d\udc01 \ud83d\udc00 \ud83d\udc3f \ud83e\udd94</p>\n<p>Let's keep Less Wrong silly.</p>\n<p>\ud83e\udec0</p>\n", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xknvtHwqvqhwahW8Q": 2, "GFG4oGGqhJPr6q6qp": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "oqozxHoFsC6PKz6bz", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 3, "extendedScore": null, "score": 0.28818246085666704, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": null, "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": "norm-enforcing", "hideCommentKarma": false, "commentCount": null, "af": false, "version": "0.6.0", "pingbacks": {}, "moderationGuidelinesVersion": "0.2.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-03T04:58:53.987Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-03T08:13:25.439Z", "modifiedAt": "2022-06-03T08:55:39.801Z", "url": null, "title": "Intergenerational trauma impeding cooperative existential safety efforts", "slug": "intergenerational-trauma-impeding-cooperative-existential", "viewCount": null, "lastCommentedAt": "2022-06-03T08:13:25.439Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Andrew_Critch", "user": {"username": "Andrew_Critch", "createdAt": "2020-09-01T02:16:13.091Z", "isAdmin": false, "displayName": "Andrew_Critch"}, "userId": "f7Mag7bDZKv59Bsaw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5hkXeCnzojjESJ4eB/intergenerational-trauma-impeding-cooperative-existential", "pageUrlRelative": "/posts/5hkXeCnzojjESJ4eB/intergenerational-trauma-impeding-cooperative-existential", "linkUrl": "https://www.lesswrong.com/posts/5hkXeCnzojjESJ4eB/intergenerational-trauma-impeding-cooperative-existential", "postedAtFormatted": "Friday, June 3rd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intergenerational%20trauma%20impeding%20cooperative%20existential%20safety%20efforts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntergenerational%20trauma%20impeding%20cooperative%20existential%20safety%20efforts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5hkXeCnzojjESJ4eB%2Fintergenerational-trauma-impeding-cooperative-existential%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intergenerational%20trauma%20impeding%20cooperative%20existential%20safety%20efforts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5hkXeCnzojjESJ4eB%2Fintergenerational-trauma-impeding-cooperative-existential", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5hkXeCnzojjESJ4eB%2Fintergenerational-trauma-impeding-cooperative-existential", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1004, "htmlBody": "<p><strong>Epistemic status:</strong> personal judgements based on conversations with ~100 people aged 30+ who were worried about AI risk \"before it was cool\", and observing their effects on a generation of worried youth, at a variety of EA-adjacent and rationality-community-adjacent events.</p><p><strong>Summary: </strong>There appears to be something like inter-generational trauma among people who think about AI x-risk \u2014 including some of the AI-focussed parts of the EA and rationality communities \u2014 which is&nbsp;</p><ul><li>preventing the formation of valuable high-trust relationships that could otherwise be leveraged to help humanity collectively make better decisions about AI, and</li><li>feeding the formation of small pockets of people with a highly adversarial stance towards the rest of the world (and each other).</li></ul><h2>Part 1 \u2014 The trauma of being ignored</h2><p>You \u2014 or some of your close friends or colleagues \u2014 may have had the experience of fearing AI would eventually pose an existential risk to humanity, and trying to raise this as a concern to mainstream intellectuals and institutions, but being ignored or even scoffed at. for raising it.&nbsp;That sucked. &nbsp;It was not silly to think AI could be a risk to humanity.&nbsp; It can.</p><p>I, and around 100 people I know, have had this experience.</p><p>Experiences like this can easily lead to an attitude like \u201cScrew those mainstream institutions, they don\u2019t know anything and I can\u2019t trust them.\u201d&nbsp;&nbsp;</p><p>At least 30 people I've known personally have adopted that attitude in a big way, and I estimate many more. &nbsp;In the remainder of this post, I'd like to point out some ways this attitude can turn out to be a mistake.</p><h2>Part 2 \u2014 Forgetting that humanity changes</h2><p>Basically, as AI progresses, it becomes easier and easier to make the case that it could pose a risk to humanity's existence. &nbsp;When people didn\u2019t listen about AI risks in the past, that happened under certain circumstances, with certain AI capabilities at the forefront and certain public discourse surrounding them. &nbsp;These circumstances have changed, and will continued to change. &nbsp;It may not be getting easier as fast as one would ideally like, but it is getting easier. &nbsp;Like the stock market, it may be hard to predict how and when things will change, but they will.</p><p>If one forgets this, one can easily adopt a stance like \"mainstream institutions will never care\" or \"the authorities are useless\". &nbsp;I think these stances are often exaggerations of the truth, and if one forgets them, one loses out on the opportunity to engage productively with the rest of humanity as things change.</p><h2>Part 3 - Reflections on the Fundamental Attribution Error (FAE)</h2><p>The Fundamental Attribution Error (<a href=\"https://en.wikipedia.org/wiki/Fundamental_attribution_error\"><u>wiki/Fundamental_attribution_error</u></a><u>)</u> is a cognitive bias whereby you too often attribute someone else's behavior to a fundamental (unchanging) aspect of their personality, rather than considering how their behavior might be circumstantial and likely to change. &nbsp;With a moment's reflection, one can see how the FAE can lead to</p><ul><li>trusting too much \u2014 assuming someone would never act against your interests because they didn't the first few times, and also</li><li>trusting too little \u2014 assuming someone will never do anything good for you because they were harmful in the past.</li></ul><p>The second reaction could be useful for getting out of abusive relationships. &nbsp;The risk of being mistreated over and over by someone is usually not worth the opportunity cost of finding new people to interact with. &nbsp;So, in personal relationships, it can be healthy to just think \"screw this\" and move on from someone when they don't make a good first (or tenth) impression.</p><h2>Part 4 \u2014 The FAE applied to humanity</h2><p>If one has had the experience of being dismissed or ignored for expressing a bunch of reasonable arguments about AI risk, it would be easy to assume that humanity (collectively) can never be trusted to take such arguments seriously. &nbsp;But,&nbsp;</p><ol><li>Humanity has changed greatly over the course of history, arguably more than any individual has changed, so it's suspect to assume that humanity, collectively, can never be rallied to take a reasonable action about AI.</li><li>One does not have the opportunity to move on and find a different humanity to relate to. &nbsp;\"Screw this humanity who ignores me, I'll just imagine a different humanity and relate to that one instead\" is not an effective strategy for dealing with the world.</li></ol><h2>Part 5 \u2013 What, if anything, to do about this</h2><p>If the above didn't resonate with you, now might be a good place to stop reading :) &nbsp;Maybe this post isn't good advice for you to consider after all.</p><p>But if it did resonate, and you're wondering what you may be able to do differently as a result, here are some ideas:</p><ul><li>Try saying something nice and civilized about AI risk that you used to say 5-10 years ago, but which wasn\u2019t well received.&nbsp; Don\u2019t escalate it to something more offensive or aggressive; just try saying the same thing again.&nbsp; Someone new might take interest today, who didn\u2019t care before.&nbsp; This is progress.&nbsp; This is a sign that humanity is changing, and adapting somewhat to the circumstances presented by AI development.</li><li>Try Googling a few AI-related topics that no one talked about 5-10 years ago to see if today more people are talking about one or more of those topics. &nbsp;Switch up the keywords for synonyms. (Maybe keep a list of search terms you tried so you don't go in circles, and if you really find nothing, you can share the list and write an interesting LessWrong post speculating about why there are no results for it.)</li><li>Ask yourself if you or your friends feel betrayed by the world ignoring your concerns about AI. &nbsp;See if you have a \"screw them\" feeling about it, and if that feeling might be motivating some of your discussions about AI.</li><li>If someone older tells you \"There is nothing you can do to address AI risk, just give up\", maybe don't give up. &nbsp;Try to understand their experiences, and ask yourself seriously if those experiences could turn out differently for you.</li></ul><h2>&nbsp;</h2><p>&nbsp;</p><p>&nbsp;</p><p><br>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Rz5jb3cYHTSRmqNnN": 2}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": "twoAxis", "myEditorAccess": "none", "_id": "5hkXeCnzojjESJ4eB", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 13, "extendedScore": null, "score": 2.339485563208379, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2022-06-03T08:47:48.688Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "r38pkCm7wF4M44MDQ", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": ["qgdGA4ZEyW7zNdK84"], "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Epistemic status:</strong> personal judgements based on conversations with ~100 people aged 30+ who were worried about AI risk \"before it was cool\", and observing their effects on a generation of worried youth, at a variety of EA-adjacent and rationality-community-adjacent events.</p><p><strong>Summary: </strong>There appears to be something like inter-generational trauma among people who think about AI x-risk \u2014 including some of the AI-focussed parts of the EA and rationality communities \u2014 which is&nbsp;</p><ul><li>preventing the formation of valuable high-trust relationships that could otherwise be leveraged to help humanity collectively make better decisions about AI, and</li><li>feeding the formation of small pockets of people with a highly adversarial stance towards the rest of the world (and each other).</li></ul><h2 id=\"Part_1___The_trauma_of_being_ignored\">Part 1 \u2014 The trauma of being ignored</h2><p>You \u2014 or some of your close friends or colleagues \u2014 may have had the experience of fearing AI would eventually pose an existential risk to humanity, and trying to raise this as a concern to mainstream intellectuals and institutions, but being ignored or even scoffed at. for raising it.&nbsp;That sucked. &nbsp;It was not silly to think AI could be a risk to humanity.&nbsp; It can.</p><p>I, and around 100 people I know, have had this experience.</p><p>Experiences like this can easily lead to an attitude like \u201cScrew those mainstream institutions, they don\u2019t know anything and I can\u2019t trust them.\u201d&nbsp;&nbsp;</p><p>At least 30 people I've known personally have adopted that attitude in a big way, and I estimate many more. &nbsp;In the remainder of this post, I'd like to point out some ways this attitude can turn out to be a mistake.</p><h2 id=\"Part_2___Forgetting_that_humanity_changes\">Part 2 \u2014 Forgetting that humanity changes</h2><p>Basically, as AI progresses, it becomes easier and easier to make the case that it could pose a risk to humanity's existence. &nbsp;When people didn\u2019t listen about AI risks in the past, that happened under certain circumstances, with certain AI capabilities at the forefront and certain public discourse surrounding them. &nbsp;These circumstances have changed, and will continued to change. &nbsp;It may not be getting easier as fast as one would ideally like, but it is getting easier. &nbsp;Like the stock market, it may be hard to predict how and when things will change, but they will.</p><p>If one forgets this, one can easily adopt a stance like \"mainstream institutions will never care\" or \"the authorities are useless\". &nbsp;I think these stances are often exaggerations of the truth, and if one forgets them, one loses out on the opportunity to engage productively with the rest of humanity as things change.</p><h2 id=\"Part_3___Reflections_on_the_Fundamental_Attribution_Error__FAE_\">Part 3 - Reflections on the Fundamental Attribution Error (FAE)</h2><p>The Fundamental Attribution Error (<a href=\"https://en.wikipedia.org/wiki/Fundamental_attribution_error\"><u>wiki/Fundamental_attribution_error</u></a><u>)</u> is a cognitive bias whereby you too often attribute someone else's behavior to a fundamental (unchanging) aspect of their personality, rather than considering how their behavior might be circumstantial and likely to change. &nbsp;With a moment's reflection, one can see how the FAE can lead to</p><ul><li>trusting too much \u2014 assuming someone would never act against your interests because they didn't the first few times, and also</li><li>trusting too little \u2014 assuming someone will never do anything good for you because they were harmful in the past.</li></ul><p>The second reaction could be useful for getting out of abusive relationships. &nbsp;The risk of being mistreated over and over by someone is usually not worth the opportunity cost of finding new people to interact with. &nbsp;So, in personal relationships, it can be healthy to just think \"screw this\" and move on from someone when they don't make a good first (or tenth) impression.</p><h2 id=\"Part_4___The_FAE_applied_to_humanity\">Part 4 \u2014 The FAE applied to humanity</h2><p>If one has had the experience of being dismissed or ignored for expressing a bunch of reasonable arguments about AI risk, it would be easy to assume that humanity (collectively) can never be trusted to take such arguments seriously. &nbsp;But,&nbsp;</p><ol><li>Humanity has changed greatly over the course of history, arguably more than any individual has changed, so it's suspect to assume that humanity, collectively, can never be rallied to take a reasonable action about AI.</li><li>One does not have the opportunity to move on and find a different humanity to relate to. &nbsp;\"Screw this humanity who ignores me, I'll just imagine a different humanity and relate to that one instead\" is not an effective strategy for dealing with the world.</li></ol><h2 id=\"Part_5___What__if_anything__to_do_about_this\">Part 5 \u2013 What, if anything, to do about this</h2><p>If the above didn't resonate with you, now might be a good place to stop reading :) &nbsp;Maybe this post isn't good advice for you to consider after all.</p><p>But if it did resonate, and you're wondering what you may be able to do differently as a result, here are some ideas:</p><ul><li>Try saying something nice and civilized about AI risk that you used to say 5-10 years ago, but which wasn\u2019t well received.&nbsp; Don\u2019t escalate it to something more offensive or aggressive; just try saying the same thing again.&nbsp; Someone new might take interest today, who didn\u2019t care before.&nbsp; This is progress.&nbsp; This is a sign that humanity is changing, and adapting somewhat to the circumstances presented by AI development.</li><li>Try Googling a few AI-related topics that no one talked about 5-10 years ago to see if today more people are talking about one or more of those topics. &nbsp;Switch up the keywords for synonyms. (Maybe keep a list of search terms you tried so you don't go in circles, and if you really find nothing, you can share the list and write an interesting LessWrong post speculating about why there are no results for it.)</li><li>Ask yourself if you or your friends feel betrayed by the world ignoring your concerns about AI. &nbsp;See if you have a \"screw them\" feeling about it, and if that feeling might be motivating some of your discussions about AI.</li><li>If someone older tells you \"There is nothing you can do to address AI risk, just give up\", maybe don't give up. &nbsp;Try to understand their experiences, and ask yourself seriously if those experiences could turn out differently for you.</li></ul><h2>&nbsp;</h2><p>&nbsp;</p><p>&nbsp;</p><p><br>&nbsp;</p>", "sections": [{"title": "Part 1 \u2014 The trauma of being ignored", "anchor": "Part_1___The_trauma_of_being_ignored", "level": 1}, {"title": "Part 2 \u2014 Forgetting that humanity changes", "anchor": "Part_2___Forgetting_that_humanity_changes", "level": 1}, {"title": "Part 3 - Reflections on the Fundamental Attribution Error (FAE)", "anchor": "Part_3___Reflections_on_the_Fundamental_Attribution_Error__FAE_", "level": 1}, {"title": "Part 4 \u2014 The FAE applied to humanity", "anchor": "Part_4___The_FAE_applied_to_humanity", "level": 1}, {"title": "Part 5 \u2013 What, if anything, to do about this", "anchor": "Part_5___What__if_anything__to_do_about_this", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": null, "af": false, "version": "1.2.0", "pingbacks": {}, "moderationGuidelinesVersion": "1.2.0", "customHighlightVersion": "1.1.0", "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-03T08:04:40.059Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-03T09:17:42.028Z", "modifiedAt": "2022-06-03T10:15:24.289Z", "url": null, "title": "[Linkpost] A Chinese AI optimized for killing", "slug": "linkpost-a-chinese-ai-optimized-for-killing", "viewCount": null, "lastCommentedAt": "2022-06-03T09:17:42.028Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RomanS", "createdAt": "2021-07-19T04:50:11.000Z", "isAdmin": false, "displayName": "RomanS"}, "userId": "biaFedMuXhbhkPNRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TrGBvEawoHvgu4JN7/linkpost-a-chinese-ai-optimized-for-killing", "pageUrlRelative": "/posts/TrGBvEawoHvgu4JN7/linkpost-a-chinese-ai-optimized-for-killing", "linkUrl": "https://www.lesswrong.com/posts/TrGBvEawoHvgu4JN7/linkpost-a-chinese-ai-optimized-for-killing", "postedAtFormatted": "Friday, June 3rd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLinkpost%5D%20A%20Chinese%20AI%20optimized%20for%20killing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLinkpost%5D%20A%20Chinese%20AI%20optimized%20for%20killing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTrGBvEawoHvgu4JN7%2Flinkpost-a-chinese-ai-optimized-for-killing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLinkpost%5D%20A%20Chinese%20AI%20optimized%20for%20killing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTrGBvEawoHvgu4JN7%2Flinkpost-a-chinese-ai-optimized-for-killing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTrGBvEawoHvgu4JN7%2Flinkpost-a-chinese-ai-optimized-for-killing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 446, "htmlBody": "<p>The AI depicted in the <i>Terminator</i> movies is rather stupid: there are much more efficient ways to kill all humans than robots with guns.&nbsp;</p><p>We can safely ignore the unrealistic <i>Terminator</i>-like scenario of AI X-risk.&nbsp;</p><p>...Or can we?</p><p>Tsinghua University is a top university located in Beijing. It is <a href=\"https://unitracker.aspi.org.au/universities/tsinghua-university/\">heavily involved</a> in research for the Chinese military. One of its military labs is called \"The State Key Laboratory of Intelligent Technology and Systems\".</p><p>In 2021, two of the university's researchers released a <a href=\"https://arxiv.org/abs/2104.04258\">paper</a> called <i>Counter-Strike Deathmatch with Large-Scale Behavioural Cloning.</i></p><p>Some highlights:</p><ul><li>The rewards are calculated as <i>r &nbsp;= 1.0K \u2212 0.5D \u2212 0.02F, </i>where K is a kill, D is own death, and F is a shot fired. One could interpret it as follows: 1) the agent must kill, 2) the agent must protect its own existence, as long as such protection does not conflict with the first rule, 3) the agent must spare ammunition, as long as it does not conflict with the first and the second rule.</li><li><i>\"To determine when to stop training, we evaluated the agent after each epoch, measuring kills-per-minute\"</i></li><li><i>\"Kill/death ratio (K/D) is the number of times a player kills an enemy compared to how many times they die. Whilst useful as one measure of an agent\u2019s performance, more information is needed \u2013 avoiding all but the most favourable firefights would score a high K/D ratio, but may be undesirable. We therefore also report kills-per-minute (KPM). A strong agent should have both a high KPM and high K/D\"</i></li><li><i>\"In this paper we take on such a challenge; building an agent for Counter-Strike: Global Offensive (CSGO), with no access to an API, and only modest compute resources (several GPUs and one game terminal).\"</i></li><li><i>\"Our solution uses behavioural cloning - training on a large noisy dataset scraped from human play on online servers...\"</i></li></ul><p>A video linked in the article:</p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=p01vWk7uMvM\"><div><iframe src=\"https://www.youtube.com/embed/p01vWk7uMvM\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p>From the article and the authors' affiliation, I drew the following conclusions:</p><ul><li>It is likely that China is already working on fully autonomous weaponry&nbsp;</li><li>One can already build autonomous weaponry with very modest computational resources and publicly available data</li><li>Efficient autonomous weaponry doesn't require human-level intelligence. In the game environment, even the primitive agents described in the article can kill at the rate of 1 human per 2 minutes, while fighting against armed humans. Barely intelligent but mass-produced <a href=\"https://www.youtube.com/watch?v=HipTO_7mUOw\">slaughterbots</a> may be able to decimate entire cities, in hours.&nbsp;</li><li>Some researchers really should stop for a minute and ask themselves: <i>maybe I shouldn't build an AI optimized for killing people, while working at a university involved in AI research for the Chinese military?</i></li></ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZWRtQgXucwzAFZqNJ": 1, "Rz5jb3cYHTSRmqNnN": 1, "jygEZ8peqh6QRZYry": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "TrGBvEawoHvgu4JN7", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 7, "extendedScore": null, "score": 1.7301124297362172, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": null, "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": false, "commentCount": null, "af": false, "version": "1.7.0", "pingbacks": {}, "moderationGuidelinesVersion": "1.2.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-03T09:17:42.033Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2022-06-03T10:34:31.661Z", "modifiedAt": "2022-06-03T10:34:31.661Z", "url": "https://wyclif.substack.com/p/on-effective-altruism-utilitarianism?utm_source=twitter&utm_campaign=auto_share&s=w", "title": "On effective altruism, utilitarianism and localness", "slug": "on-effective-altruism-utilitarianism-and-localness", "viewCount": null, "lastCommentedAt": "2022-06-03T10:34:31.661Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "david-hugh-jones", "createdAt": "2021-04-07T09:32:59.505Z", "isAdmin": false, "displayName": "David Hugh-Jones"}, "userId": "XQQ6SJ7dxLgEddunP", "domain": "wyclif.substack.com", "pageUrl": "https://www.lesswrong.com/posts/NCKGZs8sgToCgieSs/on-effective-altruism-utilitarianism-and-localness", "pageUrlRelative": "/posts/NCKGZs8sgToCgieSs/on-effective-altruism-utilitarianism-and-localness", "linkUrl": "https://www.lesswrong.com/out?url=https%3A%2F%2Fwyclif.substack.com%2Fp%2Fon-effective-altruism-utilitarianism%3Futm_source%3Dtwitter%26utm_campaign%3Dauto_share%26s%3Dw", "postedAtFormatted": "Friday, June 3rd 2022", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20effective%20altruism%2C%20utilitarianism%20and%20localness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20effective%20altruism%2C%20utilitarianism%20and%20localness%0Ahttps%3A%2F%2Fwyclif.substack.com%2Fp%2Fon-effective-altruism-utilitarianism%3Futm_source%3Dtwitter%26utm_campaign%3Dauto_share%26s%3Dw%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20effective%20altruism%2C%20utilitarianism%20and%20localness%20https%3A%2F%2Fwww.lesswrong.com%2Fout%3Furl%3Dhttps%253A%252F%252Fwyclif.substack.com%252Fp%252Fon-effective-altruism-utilitarianism%253Futm_source%253Dtwitter%2526utm_campaign%253Dauto_share%2526s%253Dw", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fout%3Furl%3Dhttps%253A%252F%252Fwyclif.substack.com%252Fp%252Fon-effective-altruism-utilitarianism%253Futm_source%253Dtwitter%2526utm_campaign%253Dauto_share%2526s%253Dw", "socialPreviewImageUrl": "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26ed7b1-c9db-430d-8a89-c16fb7010690_1200x800.jpeg", "question": false, "authorIsUnreviewed": false, "wordCount": 1532, "htmlBody": "<h1>Utilitarianism and others</h1><p>Folk physics evolved as a way of predicting what will happen to objects. It works fine within its domain, but fails beyond that. To give more precise answers, we developed Newtonian physics. But even Newtonian physics has limits. It breaks down at very large or small scales.</p><p>Morality evolved as a system for getting on well with our neighbours. Our folk theories of morality work fine within their domain, but <a href=\"https://motherjones.com/files/emotional_dog_and_rational_tail.pdf\">fail beyond that</a>. To deal with higher level problems, like policy questions facing whole societies, we developed the theory of utilitarianism. But even utilitarianism has its limits. Faced with questions of the infinite future, it swiftly <a href=\"https://handsandcities.com/2022/01/30/on-infinite-ethics/\">devolves</a> into fun with maths.</p><p>Utilitarianism treats all human concerns as preferences, and turns them into numbers. That is a helpful tool for trading off competing concerns. But don\u2019t mistake the map for the territory.</p><p>The <a href=\"https://www.lesswrong.com/\">Effective Altruism</a> <a href=\"https://80000hours.org/\">community</a> worries about paper clip maximisers: Artificial Intelligences which serve their single goal obsessively. You tell them to produce paperclips; they recycle humans into paperclips. The only way to dissuade them is to appeal to their inhuman goals: \u201cspare me and I\u2019ll slave in your paperclip factory!\u201d</p><p>Utilitarians reduce all concerns to maximising utility. They can\u2019t be swayed by argument, except about how to maximise utility. This makes them a bit like paperclip maximisers themselves.</p><p>Utilitarianism is originally an outgrowth of Christianity.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefby6zif2z4g\"><sup><a href=\"#fnby6zif2z4g\">[1]</a></sup></span>&nbsp;Like Christianity, it imposes extreme moral demands on people. <i>Take up your cross and follow me.</i> <i>Sell all your possessions and give them to the poor.</i> But utilitarianism is infinitely more scientific \u2014 and infinitely more one-sided. Jesus thought even the rich might be forgiven. For utilitarianism, forgiveness doesn\u2019t come into it. Either someone is maximizing utility, or they\u2019re in the way.</p><p>Morality evolved as a system for getting on well with our neighbours. It\u2019s lucky that the Axial religions came along and <a href=\"https://wyclif.substack.com/p/cephalus-an-underrated-figure-in?s=w\">pushed it to be more than this</a> \u2014 since your neighbours may be, say, fellow Athenian slave holders. But there are losses as well as gains. One possible loss, in replacing the man who has to get on with his neighbours with the lone actor responsible before God for the whole world, is humility.</p><p>C.S. Lewis famously said that when people stop believing in God, they don\u2019t believe in nothing \u2014 they believe in anything. This is not just a witticism about astrology. The moral frameworks of traditional religions evolved over centuries. They are complex and subtle. They contain contradictions. That makes them rich enough to cope with human life, which <i>is</i> contradictory. More recent alternatives have not had their corners worn down.</p><p>One reason to listen to others is that you may be wrong. Rationalists have an appealing sensitivity to this, and practices like <a href=\"https://www.lesswrong.com/tag/steelmanning\">steelmanning</a> which are (<a href=\"https://wyclif.substack.com/p/dont-second-guess-yourself?s=w\">mostly</a>) admirable. Another reason is just that other people\u2019s concerns, right or wrong, deserve listening to. On <a href=\"https://en.wikipedia.org/wiki/Ideal_speech_situation\">some</a> <a href=\"https://en.wikipedia.org/wiki/I_and_Thou\">views</a> this is not a means to an end, but the essence of the moral situation.</p><h1>Utilitarianism and you</h1><p>The scientificness of utilitarianism comes at a price. You must swallow one large ethical frog: you have a duty to maximise the sum of utility in the world. Why is this plausible? It\u2019s questionable whether you even have the right. (You barge past me, about my lawful business, on your mission of mercy. \u201cOut of the way! Your utility has already been included in my decision calculus!\u201d Really? Can I see your working?)</p><p>In any case, where does this duty come from? As teenagers point out, nobody asks to be born. What\u2019s it to you if I spend my summers by the pool? What if I abandon everything for a life of colonialist Tahitian debauchery? More might come out of it than from all your earnest strivings.</p><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26ed7b1-c9db-430d-8a89-c16fb7010690_1200x800.jpeg\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26ed7b1-c9db-430d-8a89-c16fb7010690_1200x800.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26ed7b1-c9db-430d-8a89-c16fb7010690_1200x800.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26ed7b1-c9db-430d-8a89-c16fb7010690_1200x800.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26ed7b1-c9db-430d-8a89-c16fb7010690_1200x800.jpeg 1456w\"></p><p>John Stuart Mill faced this issue acutely, and it put him through a serious depression.</p><blockquote><p>It was in the autumn of 1826. I was in a dull state of nerves, such as everybody is occasionally liable to; unsusceptible to enjoyment or pleasurable excitement; one of those moods when what is pleasure at other times, becomes insipid or indifferent\u2026. In this frame of mind it occurred to me to put the question directly to myself: \u201cSuppose that all your objects in life were realized; that all the changes in institutions and opinions which you are looking forward to, could be completely effected at this very instant: would this be a great joy and happiness to you?\u201d And an irrepressible self-consciousness distinctly answered, \u201cNo!\u201d At this my heart sank within me: the whole foundation on which my life was constructed fell down\u2026.</p><p>All those to whom I looked up, were of opinion that the pleasure of sympathy with human beings, and the feelings which made the good of others, and especially of mankind on a large scale, the object of existence, were the greatest and surest sources of happiness. Of the truth of this I was convinced, but to know that a feeling would make me happy if I had it, did not give me the feeling.</p></blockquote><h1>Incentives</h1><p>EA focuses on two kinds of moral issue. The first is effective action in the here and now \u2014 maximising the bang for your charitable buck. The second is the very long run: controlling artificial general intelligence (AGI), or colonizing other planets so that humanity doesn\u2019t keep all its eggs in one basket.</p><p>Contributing to the first topic requires discipline. You need to learn about the mechanics of COVID or malaria or education or planning policy. It will help to understand experimental design and statistics. A PhD may be in order. Effective altruists are also not the only people working on these problems. These fields are dug already.</p><p>The second topic is much more fun. Nobody knows about the far future, so anyone can speculate. It\u2019s exciting to range over the millennia in your imagination. Plus, it\u2019s a great chance to write short stories. These concerns are also more specific to the EA community, which makes them a clear badge of identity.</p><p>Perhaps it is not surprising that, whatever the ratio of actual work by effective altruists on these two sets of problems, the second set is much more visible. After a visit to <a href=\"https://www.lesswrong.com\">Lesswrong</a>, someone will probably associate EA more with preventing bad AIs than with expanding access to clean drinking water.</p><p>Many people have deep fears about AI.&nbsp; I have never bothered to think much about it. I don\u2019t dismiss those fears. But clever people are working on the problem already. I trust them. My marginal contribution would be small.</p><h1>Localness</h1><p><a href=\"https://press.princeton.edu/ideas/was-einstein-the-first-to-discover-general-relativity\">Einstein scooped Hilbert</a> by a few days at most in producing general relativity. In that sense, the contribution of this great genius was to give us general relativity a few days earlier. There are many people in the world with the same skills, interests and brainpower as you. That\u2019s lucky! If capitalism, politics or science depended on rare geniuses, they would not be reliable systems. By contrast, to your parents, children and friends, you are irreplaceable.</p><p>Steve Jobs created the iPhone, but neglected his child. I think the iPhone would have come along anyway. (Perhaps it would not have been as good: only as good as Android, for instance.) Firms have incentives to find substitute products. There are very few incentives to be a substitute father.</p><p>Your most important <i>net</i> contribution to human happiness today is likely to be calling your Mum. In the Silicon Valley jargon, calling your Mum is <a href=\"http://paulgraham.com/ds.html\">the thing that does not scale</a>. Here\u2019s a more general claim: the more local the issue, the less substitutable people are. Many people are working on the great needs of the world. Relatively few are going to step up and organize the Department\u2019s charity raffle. Of course, the great needs of the world are more important <i>per se</i>.</p><p>Grandiose goals are especially common in the neighbourhood of Palo Alto. That is an intellectual by-product of the revolution in computing, which opened up a space for a new generation of planet-scale firms. <a href=\"https://wyclif.substack.com/p/is-the-big-tech-era-ending?s=w\">Revolutions fade</a>. There is still a lot of tech-driven change waiting to happen, but much of it may be on a smaller, more ordinary and local level. (On the analogy of the military theorists\u2019 \u201cstrategic corporal\u201d, consider the idea of the strategic coffee shop.)</p><p>Many young people want to change the world, which is good. \u201cThe first duty of a young man is to be ambitious.\u201d Without their many ambitions, we wouldn\u2019t get the few people who succeed at it. Just by arithmetic, only few will succeed. In spring, plan for winter. What will motivate you if you don\u2019t change the world? Can you be satisfied doing a little? Cultivating the mental independence to work without appreciation, and the willpower to keep pedalling your bike, might be a valuable investment. Ambition is like an oxidizer. It gets things going, and can create loud explosions, but without another source of fuel, it burns out. It also helps to know what you actually want. \u201cTo maximize the welfare of all future generations\u201d may not be the true answer.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnby6zif2z4g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefby6zif2z4g\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is a very broad shorthand. The long version would probably be along the lines of Christianity creating natural law, and the concept of utility growing up within eighteenth-century natural law theories, before Bentham cuts it free.</p></div></li></ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": null, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": null, "reviewVoteCount": null, "positiveReviewVoteCount": null, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": null, "noIndex": false, "rsvps": null, "activateRSVPs": true, "nextDayReminderSent": false, "onlyVisibleToLoggedIn": false, "onlyVisibleToEstablishedAccounts": false, "votingSystem": null, "myEditorAccess": "none", "_id": "NCKGZs8sgToCgieSs", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 4, "extendedScore": null, "score": 1.7123720076893587, "legacy": null, "legacyId": null, "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26ed7b1-c9db-430d-8a89-c16fb7010690_1200x800.jpeg", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": false, "defaultRecommendation": false, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": null, "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": false, "globalEvent": false, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": [], "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h1 id=\"Utilitarianism_and_others\">Utilitarianism and others</h1><p>Folk physics evolved as a way of predicting what will happen to objects. It works fine within its domain, but fails beyond that. To give more precise answers, we developed Newtonian physics. But even Newtonian physics has limits. It breaks down at very large or small scales.</p><p>Morality evolved as a system for getting on well with our neighbours. Our folk theories of morality work fine within their domain, but <a href=\"https://motherjones.com/files/emotional_dog_and_rational_tail.pdf\">fail beyond that</a>. To deal with higher level problems, like policy questions facing whole societies, we developed the theory of utilitarianism. But even utilitarianism has its limits. Faced with questions of the infinite future, it swiftly <a href=\"https://handsandcities.com/2022/01/30/on-infinite-ethics/\">devolves</a> into fun with maths.</p><p>Utilitarianism treats all human concerns as preferences, and turns them into numbers. That is a helpful tool for trading off competing concerns. But don\u2019t mistake the map for the territory.</p><p>The <a href=\"https://www.lesswrong.com/\">Effective Altruism</a> <a href=\"https://80000hours.org/\">community</a> worries about paper clip maximisers: Artificial Intelligences which serve their single goal obsessively. You tell them to produce paperclips; they recycle humans into paperclips. The only way to dissuade them is to appeal to their inhuman goals: \u201cspare me and I\u2019ll slave in your paperclip factory!\u201d</p><p>Utilitarians reduce all concerns to maximising utility. They can\u2019t be swayed by argument, except about how to maximise utility. This makes them a bit like paperclip maximisers themselves.</p><p>Utilitarianism is originally an outgrowth of Christianity.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefby6zif2z4g\"><sup><a href=\"#fnby6zif2z4g\">[1]</a></sup></span>&nbsp;Like Christianity, it imposes extreme moral demands on people. <i>Take up your cross and follow me.</i> <i>Sell all your possessions and give them to the poor.</i> But utilitarianism is infinitely more scientific \u2014 and infinitely more one-sided. Jesus thought even the rich might be forgiven. For utilitarianism, forgiveness doesn\u2019t come into it. Either someone is maximizing utility, or they\u2019re in the way.</p><p>Morality evolved as a system for getting on well with our neighbours. It\u2019s lucky that the Axial religions came along and <a href=\"https://wyclif.substack.com/p/cephalus-an-underrated-figure-in?s=w\">pushed it to be more than this</a> \u2014 since your neighbours may be, say, fellow Athenian slave holders. But there are losses as well as gains. One possible loss, in replacing the man who has to get on with his neighbours with the lone actor responsible before God for the whole world, is humility.</p><p>C.S. Lewis famously said that when people stop believing in God, they don\u2019t believe in nothing \u2014 they believe in anything. This is not just a witticism about astrology. The moral frameworks of traditional religions evolved over centuries. They are complex and subtle. They contain contradictions. That makes them rich enough to cope with human life, which <i>is</i> contradictory. More recent alternatives have not had their corners worn down.</p><p>One reason to listen to others is that you may be wrong. Rationalists have an appealing sensitivity to this, and practices like <a href=\"https://www.lesswrong.com/tag/steelmanning\">steelmanning</a> which are (<a href=\"https://wyclif.substack.com/p/dont-second-guess-yourself?s=w\">mostly</a>) admirable. Another reason is just that other people\u2019s concerns, right or wrong, deserve listening to. On <a href=\"https://en.wikipedia.org/wiki/Ideal_speech_situation\">some</a> <a href=\"https://en.wikipedia.org/wiki/I_and_Thou\">views</a> this is not a means to an end, but the essence of the moral situation.</p><h1 id=\"Utilitarianism_and_you\">Utilitarianism and you</h1><p>The scientificness of utilitarianism comes at a price. You must swallow one large ethical frog: you have a duty to maximise the sum of utility in the world. Why is this plausible? It\u2019s questionable whether you even have the right. (You barge past me, about my lawful business, on your mission of mercy. \u201cOut of the way! Your utility has already been included in my decision calculus!\u201d Really? Can I see your working?)</p><p>In any case, where does this duty come from? As teenagers point out, nobody asks to be born. What\u2019s it to you if I spend my summers by the pool? What if I abandon everything for a life of colonialist Tahitian debauchery? More might come out of it than from all your earnest strivings.</p><p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26ed7b1-c9db-430d-8a89-c16fb7010690_1200x800.jpeg\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26ed7b1-c9db-430d-8a89-c16fb7010690_1200x800.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26ed7b1-c9db-430d-8a89-c16fb7010690_1200x800.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26ed7b1-c9db-430d-8a89-c16fb7010690_1200x800.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe26ed7b1-c9db-430d-8a89-c16fb7010690_1200x800.jpeg 1456w\"></p><p>John Stuart Mill faced this issue acutely, and it put him through a serious depression.</p><blockquote><p>It was in the autumn of 1826. I was in a dull state of nerves, such as everybody is occasionally liable to; unsusceptible to enjoyment or pleasurable excitement; one of those moods when what is pleasure at other times, becomes insipid or indifferent\u2026. In this frame of mind it occurred to me to put the question directly to myself: \u201cSuppose that all your objects in life were realized; that all the changes in institutions and opinions which you are looking forward to, could be completely effected at this very instant: would this be a great joy and happiness to you?\u201d And an irrepressible self-consciousness distinctly answered, \u201cNo!\u201d At this my heart sank within me: the whole foundation on which my life was constructed fell down\u2026.</p><p>All those to whom I looked up, were of opinion that the pleasure of sympathy with human beings, and the feelings which made the good of others, and especially of mankind on a large scale, the object of existence, were the greatest and surest sources of happiness. Of the truth of this I was convinced, but to know that a feeling would make me happy if I had it, did not give me the feeling.</p></blockquote><h1 id=\"Incentives\">Incentives</h1><p>EA focuses on two kinds of moral issue. The first is effective action in the here and now \u2014 maximising the bang for your charitable buck. The second is the very long run: controlling artificial general intelligence (AGI), or colonizing other planets so that humanity doesn\u2019t keep all its eggs in one basket.</p><p>Contributing to the first topic requires discipline. You need to learn about the mechanics of COVID or malaria or education or planning policy. It will help to understand experimental design and statistics. A PhD may be in order. Effective altruists are also not the only people working on these problems. These fields are dug already.</p><p>The second topic is much more fun. Nobody knows about the far future, so anyone can speculate. It\u2019s exciting to range over the millennia in your imagination. Plus, it\u2019s a great chance to write short stories. These concerns are also more specific to the EA community, which makes them a clear badge of identity.</p><p>Perhaps it is not surprising that, whatever the ratio of actual work by effective altruists on these two sets of problems, the second set is much more visible. After a visit to <a href=\"https://www.lesswrong.com\">Lesswrong</a>, someone will probably associate EA more with preventing bad AIs than with expanding access to clean drinking water.</p><p>Many people have deep fears about AI.&nbsp; I have never bothered to think much about it. I don\u2019t dismiss those fears. But clever people are working on the problem already. I trust them. My marginal contribution would be small.</p><h1 id=\"Localness\">Localness</h1><p><a href=\"https://press.princeton.edu/ideas/was-einstein-the-first-to-discover-general-relativity\">Einstein scooped Hilbert</a> by a few days at most in producing general relativity. In that sense, the contribution of this great genius was to give us general relativity a few days earlier. There are many people in the world with the same skills, interests and brainpower as you. That\u2019s lucky! If capitalism, politics or science depended on rare geniuses, they would not be reliable systems. By contrast, to your parents, children and friends, you are irreplaceable.</p><p>Steve Jobs created the iPhone, but neglected his child. I think the iPhone would have come along anyway. (Perhaps it would not have been as good: only as good as Android, for instance.) Firms have incentives to find substitute products. There are very few incentives to be a substitute father.</p><p>Your most important <i>net</i> contribution to human happiness today is likely to be calling your Mum. In the Silicon Valley jargon, calling your Mum is <a href=\"http://paulgraham.com/ds.html\">the thing that does not scale</a>. Here\u2019s a more general claim: the more local the issue, the less substitutable people are. Many people are working on the great needs of the world. Relatively few are going to step up and organize the Department\u2019s charity raffle. Of course, the great needs of the world are more important <i>per se</i>.</p><p>Grandiose goals are especially common in the neighbourhood of Palo Alto. That is an intellectual by-product of the revolution in computing, which opened up a space for a new generation of planet-scale firms. <a href=\"https://wyclif.substack.com/p/is-the-big-tech-era-ending?s=w\">Revolutions fade</a>. There is still a lot of tech-driven change waiting to happen, but much of it may be on a smaller, more ordinary and local level. (On the analogy of the military theorists\u2019 \u201cstrategic corporal\u201d, consider the idea of the strategic coffee shop.)</p><p>Many young people want to change the world, which is good. \u201cThe first duty of a young man is to be ambitious.\u201d Without their many ambitions, we wouldn\u2019t get the few people who succeed at it. Just by arithmetic, only few will succeed. In spring, plan for winter. What will motivate you if you don\u2019t change the world? Can you be satisfied doing a little? Cultivating the mental independence to work without appreciation, and the willpower to keep pedalling your bike, might be a valuable investment. Ambition is like an oxidizer. It gets things going, and can create loud explosions, but without another source of fuel, it burns out. It also helps to know what you actually want. \u201cTo maximize the welfare of all future generations\u201d may not be the true answer.</p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnby6zif2z4g\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefby6zif2z4g\">^</a></strong></sup></span><div class=\"footnote-content\"><p>This is a very broad shorthand. The long version would probably be along the lines of Christianity creating natural law, and the concept of utility growing up within eighteenth-century natural law theories, before Bentham cuts it free.</p></div></li></ol>", "sections": [{"title": "Utilitarianism and others", "anchor": "Utilitarianism_and_others", "level": 1}, {"title": "Utilitarianism and you", "anchor": "Utilitarianism_and_you", "level": 1}, {"title": "Incentives", "anchor": "Incentives", "level": 1}, {"title": "Localness", "anchor": "Localness", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": "reign-of-terror", "hideCommentKarma": false, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": "1.0.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2022-06-03T10:34:31.665Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}