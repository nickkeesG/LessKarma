{"results": [{"createdAt": null, "postedAt": "2010-10-11T08:33:26.600Z", "modifiedAt": null, "url": null, "title": "Video: Getting Things Done Author at DO Lectures", "slug": "video-getting-things-done-author-at-do-lectures", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JamesAndrix", "createdAt": "2009-02-27T06:51:02.535Z", "isAdmin": false, "displayName": "JamesAndrix"}, "userId": "WCXG9t74Aw8sYLkyR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mL7Q4A6XgCkTzxc8Z/video-getting-things-done-author-at-do-lectures", "pageUrlRelative": "/posts/mL7Q4A6XgCkTzxc8Z/video-getting-things-done-author-at-do-lectures", "linkUrl": "https://www.lesswrong.com/posts/mL7Q4A6XgCkTzxc8Z/video-getting-things-done-author-at-do-lectures", "postedAtFormatted": "Monday, October 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Video%3A%20Getting%20Things%20Done%20Author%20at%20DO%20Lectures&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVideo%3A%20Getting%20Things%20Done%20Author%20at%20DO%20Lectures%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmL7Q4A6XgCkTzxc8Z%2Fvideo-getting-things-done-author-at-do-lectures%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Video%3A%20Getting%20Things%20Done%20Author%20at%20DO%20Lectures%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmL7Q4A6XgCkTzxc8Z%2Fvideo-getting-things-done-author-at-do-lectures", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmL7Q4A6XgCkTzxc8Z%2Fvideo-getting-things-done-author-at-do-lectures", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 28, "htmlBody": "<p>If nothing else, this is a distillation of him spending a lot of time analyzing how people ineffectively manage their time.</p>\n<p>Link:</p>\n<p>http://www.dolectures.com/speakers/speakers-2010/david-allen</p>\n<p>I expect to watch this two more times.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mL7Q4A6XgCkTzxc8Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 6.337706088938223e-07, "legacy": true, "legacyId": "3705", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-11T09:01:27.508Z", "modifiedAt": null, "url": null, "title": "Ben Goertzel: What Would It Take to Move Rapidly Toward Beneficial Human-Level AGI?", "slug": "ben-goertzel-what-would-it-take-to-move-rapidly-toward", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:23.633Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YZy4iwF5xi5FC2dMf/ben-goertzel-what-would-it-take-to-move-rapidly-toward", "pageUrlRelative": "/posts/YZy4iwF5xi5FC2dMf/ben-goertzel-what-would-it-take-to-move-rapidly-toward", "linkUrl": "https://www.lesswrong.com/posts/YZy4iwF5xi5FC2dMf/ben-goertzel-what-would-it-take-to-move-rapidly-toward", "postedAtFormatted": "Monday, October 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ben%20Goertzel%3A%20What%20Would%20It%20Take%20to%20Move%20Rapidly%20Toward%20Beneficial%20Human-Level%20AGI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABen%20Goertzel%3A%20What%20Would%20It%20Take%20to%20Move%20Rapidly%20Toward%20Beneficial%20Human-Level%20AGI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYZy4iwF5xi5FC2dMf%2Fben-goertzel-what-would-it-take-to-move-rapidly-toward%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ben%20Goertzel%3A%20What%20Would%20It%20Take%20to%20Move%20Rapidly%20Toward%20Beneficial%20Human-Level%20AGI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYZy4iwF5xi5FC2dMf%2Fben-goertzel-what-would-it-take-to-move-rapidly-toward", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYZy4iwF5xi5FC2dMf%2Fben-goertzel-what-would-it-take-to-move-rapidly-toward", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://multiverseaccordingtoben.blogspot.com/2010/10/what-would-it-take-to-move-rapidly.html\">http://multiverseaccordingtoben.blogspot.com/2010/10/what-would-it-take-to-move-rapidly.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YZy4iwF5xi5FC2dMf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 6.337771760190164e-07, "legacy": true, "legacyId": "3706", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-11T10:06:25.516Z", "modifiedAt": null, "url": null, "title": "Fields Medalists on School Mathematics", "slug": "fields-medalists-on-school-mathematics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:15.096Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MsTu3dqf7BnEupoW4/fields-medalists-on-school-mathematics", "pageUrlRelative": "/posts/MsTu3dqf7BnEupoW4/fields-medalists-on-school-mathematics", "linkUrl": "https://www.lesswrong.com/posts/MsTu3dqf7BnEupoW4/fields-medalists-on-school-mathematics", "postedAtFormatted": "Monday, October 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fields%20Medalists%20on%20School%20Mathematics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFields%20Medalists%20on%20School%20Mathematics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMsTu3dqf7BnEupoW4%2Ffields-medalists-on-school-mathematics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fields%20Medalists%20on%20School%20Mathematics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMsTu3dqf7BnEupoW4%2Ffields-medalists-on-school-mathematics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMsTu3dqf7BnEupoW4%2Ffields-medalists-on-school-mathematics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 785, "htmlBody": "<p>Most people form their impressions of math from their school mathematics courses. The vast majority of school mathematics courses distort the nature of mathematical practice and so have led to widespread misconceptions about the nature of mathematical practice. There's a long history of high caliber mathematicians finding their experiences with school mathematics alienating or irrelevant. I think this should be better known. Here I've collected some relevant quotes.</p>\n<p>I'd like to write some Less Wrong articles diffusing common misconceptions about mathematical practice but am not sure how to frame these hypothetical articles. I'd welcome any suggestions.</p>\n<p><em>Acknowledgment </em>- I obtained some of these quotations from a collection of mathematician quotations compiled by my colleague Laurens Gunnarsen.</p>\n<p><a id=\"more\"></a>In <a href=\"http://www.ias.ac.in/resonance/Dec1996/pdf/Dec1996Reflections.pdf\">Reflections Around the Ramanujan Centenary</a> Fields Medalist <a href=\"http://en.wikipedia.org/wiki/Atle_Selberg\">Atle Selberg</a> said:</p>\n<blockquote>\n<p>I have talked with many others who became mathematicians, about the mathematics they learned in school. Most of them were not particularly inspired by it but started reading on their own, outside of school by some accident or other, as I myself did.</p>\n</blockquote>\n<p>In his autobiography <a href=\"http://en.wikipedia.org/wiki/Ferdinand_Eisenstein\">Ferdinand Eisenstein</a> wrote about how he found his primary school mathematical education tortuous:</p>\n<blockquote>\n<p>During the first years [of elementary school] I acquired my education in the fundamentals: I still remember the torture of completing endless multiplication examples. &nbsp;From this, you might conclude, erroneously, that I lacked mathematical ability, merely because I showed little inclination for calculating. &nbsp;In fact the mechanical, always repetitive nature of the procedures annoyed me, and indeed, I am still disgusted with calculations lacking any purpose, while if there was something new to discover, requiring thought and reasoning, I would spare no pains.</p>\n</blockquote>\n<p>There is some overlap between Eisenstein's early school experience and the experience that Fields Medalist <a href=\"http://en.wikipedia.org/wiki/William_Thurston\">William Thurston</a> describes in his essay in Mariana Cook's book <a href=\"http://press.princeton.edu/titles/8860.html\">Mathematicians: An Outer View of the Inner World</a>:</p>\n<blockquote>\n<p>I've loved mathematics all my life, although I often doubted that mathematics would turn out to be my life's focus even when others thought it obvious. &nbsp;I hated much of what was taught as mathematics in my early schooling, and I often received poor grades. &nbsp;I now view many of these early lessons as anti-math: they actively tried to discourage independent thought. &nbsp;One was supposed to follow an established pattern with mechanical precision, put answers inside boxes, and \"show your work,\" that is, reject mental insights and alternative approaches. &nbsp;My attention is more inward than that of most people: it can be resistant to being captured and directed externally. Exercises like these mathematics lessons were excruciatingly boring and painful (whether or not I had \"mastered the material\").</p>\n</blockquote>\n<p>Thurston's quote points to the personal nature of mathematical practice. This is echoed by Fields Medalist <a href=\"http://en.wikipedia.org/wiki/Alain_Connes\">Alain Connes</a> in <a href=\"http://books.google.com/books?id=GMNxNVt-M8kC&amp;lpg=PP1&amp;ots=jLUDdWOIqF&amp;dq=The%20Unravelers%3A%20Mathematical%20Snapshots&amp;pg=PP1#v=onepage&amp;q&amp;f=false\">The Unravelers: Mathematical Snapshots</a></p>\n<blockquote>\n<p>...for me, one starts to become a mathematician more or less through an act of rebellion. In what sense? In the sense that the future mathematician will start to think about a certain problem, and he will notice that, in fact, what he has read in the literature, what he has read in books, doesn't correspond to his personal vision of the problem. Naturally, this is very often the result of ignorance, but that is not important so long as his arguments are based on personal intuition and, of course, on proof. So it doesn't matter, because in this way he'll learn that in mathematics there is no supreme authority! A twelve-year-old pupil can very well oppose his teacher if he finds a proof of what he argues, and that differentiates mathematics from other disciplines, where the teacher can easily hide behind knowledge that the pupil doesn't have. A child of five can say, \"Daddy, there isn't any biggest number\" and can be certain of it, not because he read it in a book but because he has found a proof in his mind...</p>\n</blockquote>\n<p>In <em>R&eacute;coltes et Semailles</em> Fields Medalist <a href=\"http://en.wikipedia.org/wiki/Alexander_Grothendieck\">Alexander Grothendieck</a> describes an experience of the type that Alain Connes mentions:</p>\n<blockquote>\n<p>I can still recall the first \"mathematics essay\", and that the teacher gave it a bad mark. It was to be a proof of \"three cases in which triangles were congruent.\" My proof wasn't the official one in the textbook he followed religiously. All the same, I already knew that my proof was neither more nor less convincing than the one in the book, and that it was in accord with the traditional spirit of \"gliding this figure over that one.\" It was self-evident that this man was unable or unwilling to think for himself in judging the worth of a train of reasoning. He needed to lean on some authority, that of a book which he held in his hand. It must have made quite an impression on me that I can now recall it so clearly.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MsTu3dqf7BnEupoW4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 16, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "3707", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-11T10:45:52.139Z", "modifiedAt": null, "url": null, "title": "Great Mathematicians on Precocity, Speed and Math Competitions", "slug": "great-mathematicians-on-precocity-speed-and-math", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q8s8qtCmnYmxJscmX/great-mathematicians-on-precocity-speed-and-math", "pageUrlRelative": "/posts/q8s8qtCmnYmxJscmX/great-mathematicians-on-precocity-speed-and-math", "linkUrl": "https://www.lesswrong.com/posts/q8s8qtCmnYmxJscmX/great-mathematicians-on-precocity-speed-and-math", "postedAtFormatted": "Monday, October 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Great%20Mathematicians%20on%20Precocity%2C%20Speed%20and%20Math%20Competitions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGreat%20Mathematicians%20on%20Precocity%2C%20Speed%20and%20Math%20Competitions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq8s8qtCmnYmxJscmX%2Fgreat-mathematicians-on-precocity-speed-and-math%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Great%20Mathematicians%20on%20Precocity%2C%20Speed%20and%20Math%20Competitions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq8s8qtCmnYmxJscmX%2Fgreat-mathematicians-on-precocity-speed-and-math", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq8s8qtCmnYmxJscmX%2Fgreat-mathematicians-on-precocity-speed-and-math", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1204, "htmlBody": "<p>In a <a href=\"http://www.claymath.org/interviews/tao.php\">2003 interview</a>, Terence Tao answered the question</p>\n<blockquote>\n<p><em> What advice would you give to young people starting out in math (i.e. high school students and young researchers)? </em></p>\n</blockquote>\n<p>by saying</p>\n<blockquote>\n<p>Well, I guess they should be warned that their impressions of what professional mathematics is may be quite different from the reality. In elementary school I had the vague idea that professional mathematicians spent their time computing digits of pi, for instance, or perhaps devising and then solving Math Olympiad style problems.</p>\n</blockquote>\n<p>In <a href=\"http://www.jstor.org/pss/3609929\">The Case against the Mathematical Tripos</a> mathematician <a href=\"http://en.wikipedia.org/wiki/G._H._Hardy\">GH Hardy</a> wrote</p>\n<p>It has often been said that Tripos mathematics was a collection of elaborate futilities, and the accusation is broadly true. My own opinion is that this is the inevitable result, in a mathematical examination, of high standards and traditions. The examiner is not allowed to content himself with testing the competence and the knowledge of the candidates; his instructions are to provide a test of more than that, of initiative, imagination, and even of some sort of originality. And as there is only one test of originality in mathematics, namely the accomplishment of original work, and as it is useless to ask a youth of twenty-two to perform original research under examination conditions, the examination necessarily degenerates into a kind of game, and instruction for it into initiation into a series of stunts and tricks.</p>\n<p>In <a href=\"http://www.amazon.com/Map-My-Life-Goro-Shimura/dp/0387797149\">The Map of My Life</a> mathematician <a href=\"http://en.wikipedia.org/wiki/Goro_Shimura\">Goro Shimura</a> wrote of his experience teaching at a cram school</p>\n<blockquote>\n<p>I discovered that many of the exam problems were artificial and required some clever tricks. I avoided such types, and chose more straightforward problems, which one could solve with standard techniques and basic knowledge. There is a competition called the Mathematical Olympic, in which a competitor is asked to solve some problems, which are difficult and of the type I avoided. Though such a competition may have its raison d&rsquo;&circ;etre, I think those younger people who are seriously interested in mathematics will lose nothing by ignoring it.</p>\n</blockquote>\n<p>In his <a href=\"http://www.claymath.org/Popular_Lectures/Andrew_Wiles/\">lecture</a> at the 2001 International Mathematics Olympiad, <a href=\"http://en.wikipedia.org/wiki/Andrew_Wiles\">Andrew Wiles</a> said</p>\n<blockquote>\n<p>Let me then welcome you not only to this event but also to the greater world of mathematics in what many of us believe is now a golden age. However let me also warn you &mdash; whatever the route you have taken so far, the real challenges of mathematics are still before you. I hope to give you a glimpse of this. What then distinguishes the mathematics we professional mathematicians do from the mathematical problems you have faced in the last week? The two principal differences I believe are of scale and novelty. First of scale: in a mathematics contest such as the one you have just entered, you are competing against time and against each other. While there have been periods, notably in the thirteenth, fourteenth and fifteenth centuries when mathematicians would engage in timed duels with each other, nowadays this is not the custom. In fact time is very much on your side. However the transition from a sprint to a marathon requires a new kind of stamina and a profoundly different test of character. We admire someone who can win a gold medal in five successive Olympics games not so much for the raw talent as for the strength of will and determination to pursue a goal over such a sustained period of time. Real mathematical theorems will require the same stamina whether you measure the effort in months or in years [...] The second principal difference is one of novelty [...] Let me stress that creating new mathematics is a quite different occupation from solving problems in a contest. Why is this? Because you don't know for sure what you are trying to prove or indeed whether it is true.</p>\n</blockquote>\n<p>In his <a href=\"http://arxiv.org/abs/math/0503081\">Mathematical Education</a> essay, Fields Medalist William Thurston said</p>\n<blockquote>\n<p>Related to precociousness is the popular tendency to think of mathematics as a race or as an athletic competition. There are widespread high school math leagues: teams from regional high schools meet periodically and are given several problems, with an hour or so to solve them.</p>\n<p><br /> There are also state, national and international competitions. These competitions are fun, interesting, and educationally effective for the people who are successful in them. But they also have a downside. The competitions reinforce the notion that either you &lsquo;have good math genes&rsquo;, or you do not. They put an emphasis on being quick, at the expense of being deep and thoughtful. They emphasize questions which are puzzles with some hidden trick, rather than more realistic problems where a systematic and persistent approach is important. This discourages many people who are not as quick or as practiced, but might be good at working through problems when they have the time to think through them. Some of the best performers on the contests do become good mathematicians, but there are also many top mathematicians who were not so good on contest math.</p>\n<p><br />Quickness is helpful in mathematics, but it is only one of the qualities which is helpful. For people who do not become mathematicians, the skills of contest math are probably even less relevant. These contests are a bit like spelling bees. There is some connection between good spelling and good writing, but the winner of the state spelling bee does not necessarily have the talent to become a good writer, and some fine writers are not good spellers. If there was a popular confusion between good spelling and good writing, many potential writers would be unnecessarily discouraged.</p>\n</blockquote>\n<p>In Recoltes et Semailles, Fields Medalist <a href=\"http://en.wikipedia.org/wiki/Alexander_Grothendieck\">Alexander Grothendieck</a> wrote</p>\n<blockquote>\n<p>Since then I&rsquo;ve had the chance  in the world of mathematics that bid  me welcome, to meet quite a number  of people, both among my &ldquo;elders&rdquo;  and among young people in my general  age group who were more brilliant,  much more &lsquo;gifted&rsquo; than I was. I  admired the facility with which they  picked up, as if at play, new ideas,  juggling them as if familiar with  them from the cradle&ndash;while for myself I  felt clumsy, even oafish,  wandering painfully up an arduous track, like a  dumb ox faced with an  amorphous mountain of things I had to learn (so I  was assured) things I  felt incapable of understanding the essentials or  following through to  the end. Indeed, there was little about me that  identified the kind of  bright student who wins at prestigious  competitions or assimilates  almost by sleight of hand, the most  forbidding subjects.</p>\n<p>In fact, most of these comrades  who I gauged to be more brilliant  than I have gone on to become  distinguished mathematicians. Still from  the perspective or thirty or  thirty five years, I can state that their  imprint upon the mathematics  of our time has not been very profound.  They&rsquo;ve done all things, often  beautiful things in a context that was  already set out before them,  which they had no inclination to disturb.  Without being aware of it,  they&rsquo;ve remained prisoners of those  invisible and despotic circles which  delimit the universe of a certain  milieu in a given era. To have broken  these bounds they would have to  rediscover in themselves that  capability which was their birthright, as  it was mine: The capacity to  be alone.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q8s8qtCmnYmxJscmX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "3708", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-11T11:50:46.004Z", "modifiedAt": null, "url": null, "title": "Great Mathematicians on Math Competitions and \"Genius\"", "slug": "great-mathematicians-on-math-competitions-and-genius", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:24.857Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EdFDwjsLNpgtTMJAp/great-mathematicians-on-math-competitions-and-genius", "pageUrlRelative": "/posts/EdFDwjsLNpgtTMJAp/great-mathematicians-on-math-competitions-and-genius", "linkUrl": "https://www.lesswrong.com/posts/EdFDwjsLNpgtTMJAp/great-mathematicians-on-math-competitions-and-genius", "postedAtFormatted": "Monday, October 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Great%20Mathematicians%20on%20Math%20Competitions%20and%20%22Genius%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGreat%20Mathematicians%20on%20Math%20Competitions%20and%20%22Genius%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdFDwjsLNpgtTMJAp%2Fgreat-mathematicians-on-math-competitions-and-genius%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Great%20Mathematicians%20on%20Math%20Competitions%20and%20%22Genius%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdFDwjsLNpgtTMJAp%2Fgreat-mathematicians-on-math-competitions-and-genius", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdFDwjsLNpgtTMJAp%2Fgreat-mathematicians-on-math-competitions-and-genius", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1851, "htmlBody": "<p>As I mentioned in <a href=\"/r/discussion/lw/2uz/fields_medalists_on_school_mathematics/\">Fields Medalists on School Mathematics</a>, school mathematics usually gives a heavily distorted picture of mathematical practice. It's common for bright young people to participate in math competitions, an activity which is closer to that of mathematical practice. Unfortunately, while math competitions may be more representative of mathematical practice than school mathematics, math competitions are themselves greatly misleading. Furthermore, they've become tied to a misleading mythological conception of \"genius.\" I've collected relevant quotations below.</p>\n<p><em>Acknowledgment&nbsp; </em>- I obtained some of these quotations from a collection of mathematician quotations compiled by my colleague Laurens Gunnarsen.</p>\n<p><a id=\"more\"></a></p>\n<p>In a <a href=\"http://www.claymath.org/interviews/tao.php\">2003 interview</a>, Fields Medalist <a href=\"http://en.wikipedia.org/wiki/Terence_Tao\">Terence Tao</a> answered the question</p>\n<blockquote>\n<p><em> What advice would you give to young people starting out in math (i.e. high school students and young researchers)? </em></p>\n</blockquote>\n<p>by saying</p>\n<blockquote>\n<p>Well, I guess they should be warned that their impressions of what professional mathematics is may be quite different from the reality. In elementary school I had the vague idea that professional mathematicians spent their time computing digits of pi, for instance, or perhaps devising and then solving Math Olympiad style problems.</p>\n</blockquote>\n<p>In <a href=\"http://www.jstor.org/pss/3609929\">The Case against the Mathematical Tripos</a> mathematician <a href=\"http://en.wikipedia.org/wiki/G._H._Hardy\">GH Hardy</a> wrote</p>\n<blockquote>\n<p>It has often been said that Tripos mathematics was a collection of elaborate futilities, and the accusation is broadly true. My own opinion is that this is the inevitable result, in a mathematical examination, of high standards and traditions. The examiner is not allowed to content himself with testing the competence and the knowledge of the candidates; his instructions are to provide a test of more than that, of initiative, imagination, and even of some sort of originality. And as there is only one test of originality in mathematics, namely the accomplishment of original work, and as it is useless to ask a youth of twenty-two to perform original research under examination conditions, the examination necessarily degenerates into a kind of game, and instruction for it into initiation into a series of stunts and tricks.</p>\n</blockquote>\n<p>In <a href=\"http://www.amazon.com/Map-My-Life-Goro-Shimura/dp/0387797149\">The Map of My Life</a> mathematician <a href=\"http://en.wikipedia.org/wiki/Goro_Shimura\">Goro Shimura</a> wrote of his experience teaching at a cram school</p>\n<blockquote>\n<p>I discovered that many of the exam problems were artificial and required some clever tricks. I avoided such types, and chose more straightforward problems, which one could solve with standard techniques and basic knowledge. There is a competition called the Mathematical Olympic, in which a competitor is asked to solve some problems, which are difficult and of the type I avoided. Though such a competition may have its raison d'&ecirc;tre, I think those younger people who are seriously interested in mathematics will lose nothing by ignoring it.</p>\n</blockquote>\n<p>In his <a href=\"http://www.claymath.org/Popular_Lectures/Andrew_Wiles/\">lecture</a> at the 2001 International Mathematics Olympiad, <a href=\"http://en.wikipedia.org/wiki/Andrew_Wiles\">Andrew Wiles</a> gave further description of how math competitions are unrepresentative of mathematical practice</p>\n<blockquote>\n<p>Let me then welcome you not only to this event but also to the greater world of mathematics in what many of us believe is now a golden age. However let me also warn you &mdash; whatever the route you have taken so far, the real challenges of mathematics are still before you. I hope to give you a glimpse of this. What then distinguishes the mathematics we professional mathematicians do from the mathematical problems you have faced in the last week? The two principal differences I believe are of scale and novelty. First of scale: in a mathematics contest such as the one you have just entered, you are competing against time and against each other. While there have been periods, notably in the thirteenth, fourteenth and fifteenth centuries when mathematicians would engage in timed duels with each other, nowadays this is not the custom. In fact time is very much on your side. However the transition from a sprint to a marathon requires a new kind of stamina and a profoundly different test of character. We admire someone who can win a gold medal in five successive Olympics games not so much for the raw talent as for the strength of will and determination to pursue a goal over such a sustained period of time. Real mathematical theorems will require the same stamina whether you measure the effort in months or in years [...]</p>\n<p>The second principal difference is one of novelty [...] Let me stress that creating new mathematics is a quite different occupation from solving problems in a contest. Why is this? Because you don't know for sure what you are trying to prove or indeed whether it is true.</p>\n</blockquote>\n<p>In his <a href=\"http://arxiv.org/abs/math/0503081\">Mathematical Education</a> essay, Fields Medalist <a href=\"http://en.wikipedia.org/wiki/William_Thurston\">William Thurston</a> said</p>\n<blockquote>\n<p>Related to precociousness is the popular tendency to think of mathematics as a race or as an athletic competition. There are widespread high school math leagues: teams from regional high schools meet periodically and are given several problems, with an hour or so to solve them.</p>\n<p>There are also state, national and international competitions. These competitions are fun, interesting, and educationally effective for the people who are successful in them. But they also have a downside. The competitions reinforce the notion that either you &lsquo;have good math genes&rsquo;, or you do not. They put an emphasis on being quick, at the expense of being deep and thoughtful. They emphasize questions which are puzzles with some hidden trick, rather than more realistic problems where a systematic and persistent approach is important. This discourages many people who are not as quick or as practiced, but might be good at working through problems when they have the time to think through them. Some of the best performers on the contests do become good mathematicians, but there are also many top mathematicians who were not so good on contest math.</p>\n<p><br />Quickness is helpful in mathematics, but it is only one of the qualities which is helpful. For people who do not become mathematicians, the skills of contest math are probably even less relevant. These contests are a bit like spelling bees. There is some connection between good spelling and good writing, but the winner of the state spelling bee does not necessarily have the talent to become a good writer, and some fine writers are not good spellers. If there was a popular confusion between good spelling and good writing, many potential writers would be unnecessarily discouraged.</p>\n</blockquote>\n<p>In his book <a href=\"http://www.amazon.com/Mathematics-Short-Introduction-Timothy-Gowers/dp/0192853619\">Mathematics: A Very Short Introduction</a>, Fields Medalist <a href=\"http://en.wikipedia.org/wiki/Timothy_Gowers\">Timothy Gowers</a> writes</p>\n<blockquote>\n<p>While the negative portrayal of mathematicians may be damaging, by putting off people who would otherwise enjoy the subject and be good at it, the damage done by the word genius is more insidious and possibly greater. Here is a rough and ready definition of genius: somebody who can do easily, and at a young age, something that almost nobody else can do except after years of practice, if at all. The achievements of geniuses have some sort of magic quality about them - it is as if their brains work not just more efficiently than ours, but in a completely different way. Every year or two a mathematics undergraduate arrives at Cambridge who regularly manages to solve a in a few minutes problems that take most people, including those who are supposed to be teaching them, several hours or more. When faced with such a person, all one can do is stand back and admire.</p>\n<p>And yet, these extraordinary people are not always the most successful research mathematicians. If you want to solve a problem that other professional mathematicians have tried and failed to solve before you, then, of the many qualities you will need, genius as I have defined it is neither necessary nor sufficient. To illustrate with an extreme example, Andrew Wiles, who (at the age of just over forty) proved Fermat's Last Theorem (which states that if x, y, z, and n are all positive integers and n is greater than 2, then x<sup>n</sup> + y<sup>n</sup> cannot equal z<sup>n</sup>) and thereby solved the world's most famous unsolved mathematics problem, is undoubtedly very clever, but he is not a genius in my sense.</p>\n<p>How, you might ask, could he possibly have done what he did without some sort of mysterious extra brainpower? The answer is that, remarkable though his achievement was, it is not so remarkable as to defy explanation. I do not know precisely what enabled him to succeed, but he would have needed great courage, determination, and patience, a wide knowledge of some very difficult work done by others, the good fortune to be in the right mathematical area at the right time, and an exceptional strategic ability.</p>\n<p>This last quality is, ultimately, more important than freakish mental speed: the most profound contributions to mathematics are often made by tortoises rather than hares. As mathematicians develop, they learn various tricks of the trade, partly from the work of other mathematicians and partly as a result of many hours spent thinking about mathematics. What determines whether they can use their expertise to solve notorious problems is, in large measure, a matter of careful planning: attempting problems that are likely to be fruitful, knowing when to give up a line of thought (a difficult judgement to make), being able to sketch broad outlines of arguments before, just occasionally, managing to fill in the details. This demands a level of maturity which is by no means incompatible with genius but which does not always accompany it.</p>\n</blockquote>\n<p>In <a href=\"http://terrytao.wordpress.com/career-advice/does-one-have-to-be-a-genius-to-do-maths/\">Does one have to be a genius to do maths?</a> Terence Tao concurs with Gowers and expands on the same theme.</p>\n<hr />\n<p><em></em>Fields Medalist <a href=\"http://en.wikipedia.org/wiki/Alexander_Grothendieck\">Alexander Grothendieck</a> describes his own relevant experience in <em>R&eacute;coltes et Semailles</em> <em><em></em></em></p>\n<blockquote>\n<p>Since then I&rsquo;ve had the chance in the world of mathematics that bid me welcome, to meet quite a number of people, both among my &ldquo;elders&rdquo; and among young people in my general age group who were more brilliant, much more &lsquo;gifted&rsquo; than I was. I admired the facility with which they picked up, as if at play, new ideas, juggling them as if familiar with them from the cradle&ndash;while for myself I felt clumsy, even oafish, wandering painfully up an arduous track, like a dumb ox faced with an amorphous mountain of things I had to learn (so I was assured) things I felt incapable of understanding the essentials or following through to the end. Indeed, there was little about me that identified the kind of bright student who wins at prestigious competitions or assimilates almost by sleight of hand, the most forbidding subjects.</p>\n<p>In fact, most of these comrades who I gauged to be more brilliant than I have gone on to become distinguished mathematicians. Still from the perspective or thirty or thirty five years, I can state that their imprint upon the mathematics of our time has not been very profound. They&rsquo;ve done all things, often beautiful things in a context that was already set out before them, which they had no inclination to disturb. Without being aware of it, they&rsquo;ve remained prisoners of those invisible and despotic circles which delimit the universe of a certain milieu in a given era. To have broken these bounds they would have to rediscover in themselves that capability which was their birthright, as it was mine: The capacity to be alone.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "x3zyEPFaJANB2BHmP": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EdFDwjsLNpgtTMJAp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 32, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "3709", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MsTu3dqf7BnEupoW4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-11T14:01:58.960Z", "modifiedAt": null, "url": null, "title": "The Dark Arts - Preamble", "slug": "the-dark-arts-preamble", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:59.679Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Aurini", "createdAt": "2009-03-19T04:39:40.233Z", "isAdmin": false, "displayName": "Aurini"}, "userId": "5fNCGeJcDQCjxEjnD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aPCKiEd2G8H3kkdnN/the-dark-arts-preamble", "pageUrlRelative": "/posts/aPCKiEd2G8H3kkdnN/the-dark-arts-preamble", "linkUrl": "https://www.lesswrong.com/posts/aPCKiEd2G8H3kkdnN/the-dark-arts-preamble", "postedAtFormatted": "Monday, October 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Dark%20Arts%20-%20Preamble&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Dark%20Arts%20-%20Preamble%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaPCKiEd2G8H3kkdnN%2Fthe-dark-arts-preamble%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Dark%20Arts%20-%20Preamble%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaPCKiEd2G8H3kkdnN%2Fthe-dark-arts-preamble", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaPCKiEd2G8H3kkdnN%2Fthe-dark-arts-preamble", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1670, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-CA</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin:0cm; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-bidi-font-family:\"Times New Roman\";} --> <!--[endif]--></p>\n<p class=\"MsoNormal\">I&rsquo;d like to tell you all a story.</p>\n<p class=\"MsoNoSpacing\">Once upon a time I was working for a charity &ndash; a major charity &ndash; going door-to-door to raise money while pretending it wasn&rsquo;t sales.</p>\n<p class=\"MsoNoSpacing\">This story happened on my last day working there. <span>&nbsp;</span>I didn&rsquo;t know that at the time; I wouldn&rsquo;t find out until the following morning when my boss called me up to fire me, but I knew it was coming.<span>&nbsp; </span>For weeks I&rsquo;d been fed up with the job, milking it for the last few dollars I could pull out, hating every minute of it but needing the money.<span>&nbsp; </span>The Sudden Career Readjustment would come as a relief.</p>\n<p class=\"MsoNoSpacing\">So on that day, my last day, I was moving slowly.<span>&nbsp; </span>I knocked on one particular door and there was no response.<span>&nbsp; </span>I had little desire to walk to the next one, however, and there was an interesting spider who&rsquo;d built its web below the doorbell.<span>&nbsp; </span>I tapped its belly with the tip of my pen, and it reacted with aggression &ndash; trying to envenom and ensnare the tip of my ballpoint.<span>&nbsp; </span>I must have been playing with it for a good minute or so when the door suddenly opened.</p>\n<p class=\"MsoNoSpacing\">A distraught woman stood before me.<span>&nbsp; </span>After a brief period of Relating I launched into my pitch.<a id=\"more\"></a>&ldquo;So you&rsquo;re probably wondering why there&rsquo;s a bald weirdo at your door?<span>&nbsp; </span>Actually I&rsquo;m just coming around with <em>Major Charity</em><sup>1</sup> on an emergency campaign.<span>&nbsp; </span>You&rsquo;ve heard of us, right?<span>&nbsp; </span>Brilliant!<span>&nbsp; </span>So obviously you&rsquo;ve thought of getting involved, right?<span>&nbsp; </span>That&rsquo;s awesome!<span>&nbsp; </span>You see, the reason I&rsquo;m coming around is for these guys &ndash; some of our emergency cases...&rdquo;<sup>2</sup></p>\n<p class=\"MsoNoSpacing\">I handed her the pictures of the Developing World Children (yeah, it was one of those charities).<span>&nbsp; </span>She took them, a wistful look on her face.</p>\n<p class=\"MsoNoSpacing\">&ldquo;Oh God, don&rsquo;t show me these.<span>&nbsp; </span>I&rsquo;m such a Rescuer.&rdquo;</p>\n<p class=\"MsoNoSpacing\">&ldquo;Rescuer?<span>&nbsp; </span>Do you have a Rescue Dog?&rdquo; [Where I&rsquo;m from, abused animals brought into a new home are called &lsquo;Rescue Dogs&rsquo;.]</p>\n<p class=\"MsoNoSpacing\">&ldquo;No, I...&rdquo;</p>\n<p class=\"MsoNoSpacing\">&ldquo;You mean your personality?<span>&nbsp; </span>You care about people, don&rsquo;t you?&rdquo;</p>\n<p class=\"MsoNoSpacing\">She nodded slowly.<span>&nbsp; </span>Her face began to crumble.</p>\n<p class=\"MsoNoSpacing\">&ldquo;I&rsquo;m sorry &ndash; I can&rsquo;t look at these children,&rdquo; she handed back the photographs, &ldquo;Not right now.<span>&nbsp; </span>I&rsquo;ve been crying all day and I just can&rsquo;t deal with those emotions...&rdquo;</p>\n<p class=\"MsoNoSpacing\">I took back the children, a look of honest sympathy on my face.<span>&nbsp; </span>The Demon Wheel began spinning.<span>&nbsp; </span>I could see that she was on the verge of crying again.<span>&nbsp; </span>My gut told me that her father had recently died, but the actual cause didn&rsquo;t matter.<span>&nbsp; </span>I could discover that information.<span>&nbsp; </span>The upcoming dialogue played itself out in my mind...</p>\n<p class=\"MsoNoSpacing\">\"Oh jeez, what happened?<span>&nbsp; </span>Oh my god, seriously..?&rdquo; Head tilted as an Alpha confidant enough for Beta behaviour, looking down and shaking, &ldquo;I&rsquo;m lucky enough to have never been through that.<span>&nbsp; </span>Were the two of you close?&rdquo; As she talks I nod, prompting her until she breaks out in tears.<span>&nbsp; </span>I put down my binder and step into her house, embracing her as she cries on my shoulder.</p>\n<p class=\"MsoNoSpacing\">She sniffles.</p>\n<p class=\"MsoNoSpacing\">&ldquo;I&rsquo;m sorry... sorry to do this to you.&rdquo;</p>\n<p class=\"MsoNoSpacing\">&ldquo;No, don&rsquo;t be.<span>&nbsp; </span>Listen... Mary, is it?<span>&nbsp; </span>What you&rsquo;re going through is normal.<span>&nbsp; </span>It&rsquo;s nothing to be ashamed of&hellip;&rdquo;<span>&nbsp; </span>Cue personal anecdote, then pause for a beat. &ldquo;Listen, about the <em>Major Charity</em> thing; this is something you&rsquo;ve always wanted to do, isn&rsquo;t it?<span>&nbsp; </span>Yeah, I can tell.<span>&nbsp; </span>You&rsquo;re a caring person, after all.<span>&nbsp; </span>I tell you what: we&rsquo;ll get you set up with this little boy &ndash; he&rsquo;s from Ecuador, and we&rsquo;re trying to get him eating a healthy diet.<span>&nbsp; </span>We&rsquo;re going to make you his super hero today.<span>&nbsp; </span>And then you&rsquo;ll know &ndash; Mary, you&rsquo;ll know that even at your darkest moment, you still have the strength in you to save a life.</p>\n<p class=\"MsoNoSpacing\">&ldquo;And you know what else?&rdquo; I reach out to touch her arm, &ldquo;Tonight you&rsquo;re going to sleep like a baby knowing that you did this.<span>&nbsp; </span>So you go and get your Credit Card and I&rsquo;ll start filling out the form.&rdquo;</p>\n<p class=\"MsoNoSpacing\">*<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>*<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>*</p>\n<p class=\"MsoNoSpacing\">I could have done it.<span>&nbsp; </span>I could have got that child sponsored.<span>&nbsp; </span>I could have kept my job, and Mary could have stopped crying that evening.<span>&nbsp; </span>She&rsquo;d have thanked me for coming by, and after I left she would have cuddled on the couch with her new Sponsor Child, tears drying as she found hope in the world.</p>\n<p class=\"MsoNoSpacing\">But I didn&rsquo;t do it.<span>&nbsp; </span>Instead I apologized for interrupting her grief, and left.</p>\n<p class=\"MsoNoSpacing\">Because I am not a Meat Fucker.</p>\n<p class=\"MsoNoSpacing\">*<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>*<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>*</p>\n<p class=\"MsoNoSpacing\">All my life, I&rsquo;ve had this bad habit.<span>&nbsp; </span>No matter how hard I try and kick it, there it is: Honesty.<span>&nbsp; </span>I can&rsquo;t tell you how many times it&rsquo;s dug me into a hole.<span>&nbsp; </span>As far as concepts go, it&rsquo;s about as foolish and utopian as Truth and Justice, and I <em>know</em> that, but I just can&rsquo;t seem to let it go.<span>&nbsp; </span>That&rsquo;s a large part of the reason I left Mary alone to her tears &ndash; backed off, rather than digging into her psyche to recalibrate a few clusters of neuron.</p>\n<p class=\"MsoNoSpacing\">The other half is my status as a card-carrying (union-dues-paid-in-full) Anarchist.<span>&nbsp; </span>The way I look at things, the only time you can justify using the Jedi Mind Trick on somebody is when your ethics would stand clean with murdering them as well.</p>\n<p class=\"MsoNoSpacing\">Sending Storm Troopers on a Wild Droid Chase is one thing; scamming Waddo out of a distributor cap for your CGI Space Plane is another.</p>\n<p class=\"MsoNoSpacing\">When you take advantage of the Dark Arts, you&rsquo;re not simply tricking people into giving you what you want; you&rsquo;re making them <em>want to give it to you.<span>&nbsp; </span></em>You&rsquo;re hacking into their brain and inserting a <a title=\"The Moral Void\" href=\"/lw/rr/the_moral_void/\">Murder Pill</a>; afterwards they will literally thank you for doing so (the only sponsor I ever met who wasn&rsquo;t glad that I&rsquo;d come by was the lady whose 6 year old daughter I primed into wanting it).<span>&nbsp; </span>In ninety percent of the situations where the Dark Arts are useful or possible, you can&rsquo;t do it out of spite; when you realign someone&rsquo;s desires to match your own they <em>want</em> to do what you want them to do.<span>&nbsp; </span></p>\n<p class=\"MsoNoSpacing\">And yet there&rsquo;s no clear distinction between using these skills and regular social interaction.<span>&nbsp; </span>Manipulation works best when you&rsquo;re sincere about it.<span>&nbsp; </span>Ethically speaking it&rsquo;s a grey, wavy line.</p>\n<p class=\"MsoNoSpacing\">The thing is, we all like to be Sold, Led, Dominated; if I walk into Subway, and I ask the kid at the counter to give me his Best Submarine Sandwich, I want him to <em>tell</em> me what I want, and make me love it after it&rsquo;s paid for.<span>&nbsp; </span>The last thing he should do is say that &ldquo;They&rsquo;re all good!&rdquo; and make me <a title=\"Harmful Options\" href=\"/lw/x2/harmful_options/\">regret</a> the [(5 breads)x(16 meats)x(2<sup>12</sup> Toppings)-1] subs that I didn&rsquo;t get.<sup>3</sup> Retail is the Dark Arts Done Right (usually).<span>&nbsp; </span>The Sales Lady figures out what I want, uses her expertise to find the best fit, and then kills the cognitive dissonance that could ruin my enjoyment of the product; &ldquo;You really pull off that colour.<span>&nbsp; </span>Seriously, that jacket looks great on you &ndash; you see how these lines naturally compliment your shoulders?<span>&nbsp; </span>Of course you can!&rdquo;</p>\n<p class=\"MsoNoSpacing\">Sexual dynamics are similar; if somebody&rsquo;s drinking in public at 2 in the morning it&rsquo;s because they&rsquo;re on the market.<span>&nbsp; </span>Let&rsquo;s say a &lsquo;faithful wife&rsquo; goes to the club one weekend while her husband is out of town, and she has a few drinks with a bunch of college boys she just met.<span>&nbsp; </span>One of them happens to be a PUA.<span>&nbsp; </span>When it comes to things like date rape drugs, or taking advantage of a person who&rsquo;s sloppy-drunk there <em>is</em> a clear line in the sand.<span>&nbsp; </span>But in this hypothetical the woman&rsquo;s relatively sober.<span>&nbsp; </span>It&rsquo;s just that the young rake is so damned <em>charming!</em></p>\n<p class=\"MsoNoSpacing\">Meanwhile her husband&rsquo;s having a few pints at the hotel bar with Sheila from accounting, and she just keeps making eyes at him&hellip;</p>\n<p class=\"MsoNoSpacing\">Neither Sheila nor the PUA is responsible for the ensuing infidelity.<span>&nbsp; </span>If the husband and wife didn&rsquo;t want it in the first place, they would have never availed themselves to the temptation.<span>&nbsp; </span>If, on the other hand, you meet somebody at a Neighbourhood Watch meeting, and spend the next three months seducing them&hellip; that&rsquo;s when you&rsquo;ve got to start questioning your ethics.<span>&nbsp; </span>Anybody is going to be vulnerable at some time or another.</p>\n<p class=\"MsoNoSpacing\">While the Dark Arts are a Power, it&rsquo;s how you use them that matters, like any other tool.<span>&nbsp; </span>I&rsquo;m running mind-games on people, but I usually won&rsquo;t; I&rsquo;m also good at fighting, but I don&rsquo;t assault people for no reason.<span>&nbsp; </span>I find both concepts repulsive.</p>\n<p class=\"MsoNoSpacing\">That&rsquo;s the end of my moralizing on the matter.<span>&nbsp; </span>The upcoming series is going to be purely descriptive in nature, exploring different strategies for manipulating others.<span>&nbsp; </span>I&rsquo;ll provide tactical <a title=\"OKCupid dating profile tactics\" href=\"/lw/2tw/draft_rational_dating_less_wrongers_on_okcupid/\">examples</a> showing how these strategies can be put into play, but for the most part each battlefield is unique; these are broader methods that apply across the board.<span>&nbsp; </span>What you do with these techniques is up to you.</p>\n<p class=\"MsoNoSpacing\">As for defence&hellip; I don&rsquo;t think I&rsquo;ll have much to say about that.<span>&nbsp; </span>When done properly, the victim doesn&rsquo;t realize it until it&rsquo;s already over, and by then it doesn&rsquo;t matter.<span>&nbsp; </span>You&rsquo;re aware that the AI manipulated you into opening the box, but you&rsquo;re going to open it anyways because that&rsquo;s your new utility function.<span>&nbsp; </span>It&rsquo;s like a game of <a title=\"Boom shaka laka!\" href=\"http://www.urbandictionary.com/define.php?term=roshambo\">Roshambo</a>, or when you&rsquo;re thinking about joining Facebook: the only way to win is not to play.</p>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<p class=\"MsoNoSpacing\"><strong>Endnotes</strong></p>\n<p class=\"MsoNoSpacing\" style=\"margin-left: 36pt; text-indent: -18pt;\"><span><span>1.<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span>Major Charity&rsquo;s methods of acquiring funding don&rsquo;t have any bearing on whether or not it&rsquo;s an effective charity.<span>&nbsp; </span>Whether or not the money going overseas actually makes a difference is a question I cannot answer.</p>\n<p class=\"MsoNoSpacing\" style=\"margin-left: 36pt; text-indent: -18pt;\"><span><span>2.<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span>The repetition here is intentional.<span>&nbsp; </span>I was trying to prime key concepts.</p>\n<p class=\"MsoNoSpacing\" style=\"margin-left: 36pt; text-indent: -18pt;\"><span><span>3.<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span>My theory as to what is going on with these sub places and their myriad of options: the target is not a new customers, those people are going to be intimidated by all the choices, and the restaurants know that.<span>&nbsp; </span>Rather, it is to provide &lsquo;fresh&rsquo; options so that their current customers don&rsquo;t get bored and go elsewhere.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XYHzLjwYiqpeqaf4c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aPCKiEd2G8H3kkdnN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 85, "baseScore": 59, "extendedScore": null, "score": 0.000105, "legacy": true, "legacyId": "3710", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 140, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K9JSM7d7bLJguMxEp", "CtSS6SkHhLBvdodTY", "yeCZb6zkS9bvuLbqa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-11T14:17:09.201Z", "modifiedAt": null, "url": null, "title": "Hazing as Counterfactual Mugging?", "slug": "hazing-as-counterfactual-mugging", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:04.994Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SilasBarta", "createdAt": "2009-03-01T00:03:34.864Z", "isAdmin": false, "displayName": "SilasBarta"}, "userId": "zDPSZfarhLM7Gehug", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EdRHRbTuzqWCFsKDy/hazing-as-counterfactual-mugging", "pageUrlRelative": "/posts/EdRHRbTuzqWCFsKDy/hazing-as-counterfactual-mugging", "linkUrl": "https://www.lesswrong.com/posts/EdRHRbTuzqWCFsKDy/hazing-as-counterfactual-mugging", "postedAtFormatted": "Monday, October 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hazing%20as%20Counterfactual%20Mugging%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHazing%20as%20Counterfactual%20Mugging%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdRHRbTuzqWCFsKDy%2Fhazing-as-counterfactual-mugging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hazing%20as%20Counterfactual%20Mugging%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdRHRbTuzqWCFsKDy%2Fhazing-as-counterfactual-mugging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdRHRbTuzqWCFsKDy%2Fhazing-as-counterfactual-mugging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 327, "htmlBody": "<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">In the interest of making decision theory problems more relevant, I thought I'd propose a real-life version of <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">counterfactual mugging</a>.<span style=\"mso-spacerun: yes;\">&nbsp; </span>This is discussed in Drescher's <em>Good and Real</em>, and many places before.<span style=\"mso-spacerun: yes;\">&nbsp; </span>I will call it the Hazing Problem by comparison to <a href=\"http://en.wikipedia.org/wiki/Hazing\">this practice</a> (possibly NSFW &ndash; this is hazing, folks, not Disneyland).</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">The problem involves a timewise sequence of agents who each decide whether to \"haze\" (abuse) the next agent.<span style=\"mso-spacerun: yes;\">&nbsp; </span>(They cannot impose any penalty on previous agent.)<span style=\"mso-spacerun: yes;\">&nbsp; </span>For all agents n, here is their preference ranking:</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">1) not be hazed by n-1</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">2) be hazed by n-1, and haze n+1</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">3) be hazed by n-1, do NOT haze n+1</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">or, less formally:</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">1) not be hazed</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">2) haze and be hazed</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">3) be hazed, but stop the practice</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">The problem is: you have been hazed by n-1.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Should you haze n+1?</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Like in counterfactual mugging, the average agent has lower utility by conditioning on having been hazed, no matter how big the utility difference between 2) and 3) is.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Also, it involves you having to make a choice from within a \"losing\" part of the \"branching\", which has implications for the other branches.</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">You might object the choice of whether to haze is not random, as Omega&rsquo;s coinflip is in CM; however, there are deterministic phrasings of CM, and your own epistemic limits blur the distinction.</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\"><a href=\"http://wiki.lesswrong.com/wiki/Decision_theory#Common_abbreviations\">UDT</a> sees optimality in returning not-haze unconditionally.<span style=\"mso-spacerun: yes;\">&nbsp; </span>CDT reasons that its having been hazed is fixed, and so hazes.<span style=\"mso-spacerun: yes;\">&nbsp; </span>I *think* EDT would choose to haze because it would prefer to learn that, having been hazed, they hazed n+1, but I'm not sure about that.</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">I also think that TDT chooses not-haze, although this is questionable since I'm claiming this is isomorphic to CM.<span style=\"mso-spacerun: yes;\">&nbsp; </span>I would think TDT reasons that, \"If n's regarded it as optimal to not haze despite having been hazed, then I would not be in a position of having been hazed, so I zero out the disutility of choosing not-haze.\"</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%; mso-pagination: none; mso-layout-grid-align: none;\"><span style=\"font-size: 10pt; line-height: 150%; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Thoughts on the similarity and usefulness of the comparison?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YpHkTW27iMFR2Dkae": 1, "5f5c37ee1b5cdee568cfb1b6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EdRHRbTuzqWCFsKDy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 6.338511876344654e-07, "legacy": true, "legacyId": "3711", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-11T14:58:40.358Z", "modifiedAt": null, "url": null, "title": "Great Mathematicians on the Value of Intuition in Mathematics", "slug": "great-mathematicians-on-the-value-of-intuition-in", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TJL96Q9xJmtYc5CYv/great-mathematicians-on-the-value-of-intuition-in", "pageUrlRelative": "/posts/TJL96Q9xJmtYc5CYv/great-mathematicians-on-the-value-of-intuition-in", "linkUrl": "https://www.lesswrong.com/posts/TJL96Q9xJmtYc5CYv/great-mathematicians-on-the-value-of-intuition-in", "postedAtFormatted": "Monday, October 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Great%20Mathematicians%20on%20the%20Value%20of%20Intuition%20in%20Mathematics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGreat%20Mathematicians%20on%20the%20Value%20of%20Intuition%20in%20Mathematics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTJL96Q9xJmtYc5CYv%2Fgreat-mathematicians-on-the-value-of-intuition-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Great%20Mathematicians%20on%20the%20Value%20of%20Intuition%20in%20Mathematics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTJL96Q9xJmtYc5CYv%2Fgreat-mathematicians-on-the-value-of-intuition-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTJL96Q9xJmtYc5CYv%2Fgreat-mathematicians-on-the-value-of-intuition-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 516, "htmlBody": "<p>There is a widespread misconception among educated laypeople that mathematics is primarily about logic and proof. No serious mathematician would deny that logical rigor has played an essential role in the development of mathematics. But the essential role that intuition plays in mathematical progress is little known. The asymmetry between the high level of awareness of the importance of logical rigor and the low level of awareness of the importance of intuition has led to educated laypeople to have a heavily distorted view of mathematics. Below I've collected some quotes from great mathematicians about the value of intuition in mathematics. I welcome any references to counterbalancing quotes from people of similar caliber<a id=\"more\"></a></p>\n<p>In <a href=\"http://www.marco-learningsystems.com/pages/kline/johnny/johnny-chapt4.html\">Why Johnny Can't Add</a>, mathematician and historian <a href=\"http://en.wikipedia.org/wiki/Morris_Kline\">Morris Kline</a> quotes <a href=\"http://en.wikipedia.org/wiki/Felix_Klein\">Felix Klein</a> saying</p>\n<blockquote>\n<p>You can often hear from non\u2010mathematicians, especially from philosophers, that mathematics consists exclusively in drawing conclusions from clearly stated premises; and that, in this process, it makes no difference what these premises signify, whether they are true or false, provided only that they do not contradict one another. But a person who has done productive mathematical work will talk quite differently. In fact those persons are thinking only of the crystallized form into which finished mathematical theories are finally cast. The investigator himself, however, in mathematics, as in every other science, does not work in this rigorous deductive fashion. On the contrary, he makes essential use of his fantasy and proceeds inductively, aided by heuristic expedients. One can give numerous examples of mathematicians who have discovered theorems of the greatest importance which they were unable to prove. Should one, then, refuse to recognize this as a great accomplishment and, in deference to the above definition, insist that this is not mathematics, and that only the successors who supply polished proofs are doing real mathematics? After all, it is an arbitrary thing how the word is to be used, but no judgment of value can deny that the inductive work of the person who first announces the theorem is at least as valuable as the deductive work of the one who first proves it. For both are equally necessary, and the discovery is the presupposition of the later conclusion.</p>\n</blockquote>\n<p>According to <a href=\"http://sunsite.ubc.ca/DigitalMathArchive/Langlands/pdf/harish2-ps.pdf\">an obituary by Robert Langlands</a>, mathematician <a href=\"http://en.wikipedia.org/wiki/Harish-Chandra\">Harish-Chandra</a> said</p>\n<blockquote>\n<p>In mathematics we agree that clear thinking is very important, but fuzzy thinking is just as important as clear thinking</p>\n</blockquote>\n<p>As <a href=\"http://www.jstor.org/pss/2975040\">reported</a> by <a href=\"http://en.wikipedia.org/wiki/Hermann_Weyl\">Hermann Weyl</a>, while lecturing on <a href=\"http://en.wikipedia.org/wiki/Bernhard_Riemann\">Bernhard Riemann</a> Felix Klein said</p>\n<blockquote>\n<p>Undoubtedly, the capstone of every mathematical theory is a convincing proof of all its assertions. Undoubtedly, mathematics inculpates itself when it foregoes convincing proofs. But the mystery of brilliant productivity will always be the posing of new questions, the anticipation of new theorems that make accessible valuable results and connections. Without the creation of new viewpoints, without the statement of new aims, mathematics would soon exhaust itself in the rigor of its logical proofs and begin to stagnate as its substance vanishes. Thus, in a sense, mathematics has been most advanced by those who have distinguished themselves by <span class=\"il\">intuition</span> than by rigorous proofs.</p>\n</blockquote>\n<p>In his <a href=\"http://www.springerlink.com/content/qu4038q8lw449312/\">essay on Mathematical Creation</a> Henri Poincare wrote</p>\n<p>Numerous mathematicians</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TJL96Q9xJmtYc5CYv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "3712", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-11T21:48:00.240Z", "modifiedAt": null, "url": null, "title": "Poker Playing", "slug": "poker-playing", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:30.639Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mWXxsBLngfgH6iRdT/poker-playing", "pageUrlRelative": "/posts/mWXxsBLngfgH6iRdT/poker-playing", "linkUrl": "https://www.lesswrong.com/posts/mWXxsBLngfgH6iRdT/poker-playing", "postedAtFormatted": "Monday, October 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Poker%20Playing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APoker%20Playing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmWXxsBLngfgH6iRdT%2Fpoker-playing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Poker%20Playing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmWXxsBLngfgH6iRdT%2Fpoker-playing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmWXxsBLngfgH6iRdT%2Fpoker-playing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 610, "htmlBody": "<p>http://www.washingtonpost.com/wp-dyn/content/article/2010/10/01/AR2010100105833_pf.html (one page link; original: http://www.washingtonpost.com/wp-dyn/content/article/2010/10/01/AR2010100105833.html) is a recent Washington Post article on young poker player Steven Silverman. It's interesting.</p>\n<p>Even more interesting are his comments and other poker players' comments on Hacker News: http://news.ycombinator.com/item?id=1777385</p>\n<p>Some selected comments:</p>\n<blockquote>\n<p>&nbsp;</p>\n</blockquote>\n<blockquote>\n<p><span style=\"color: #000000;\">It doesn't matter one jot  how well you play at your best, what matters is how well you play when  you're stressed and upset and ill and exhausted and it's 2am and the  fish whose money you need to take is calling your mother a whore. That's  the real job of playing professional poker - you're on a tightrope  where an hour of perfect poker earns you $80 but an hour on tilt can  lose you $800. The skills that allow you to turn pro are learned as an  amateur, but there's a whole other set of skills required to stick it  out without losing your mind. I had the former, but not the latter. I  walked away from poker because I am absolutely certain it would have  killed me.</span></p>\n</blockquote>\n<p><span style=\"color: #000000;\">-- jdietrich, http://news.ycombinator.com/item?id=1778364</span></p>\n<p><span class=\"comment\">\n<blockquote>\n<p><span style=\"color: #000000;\">Losing streaks are like  nothing you can understand until you've been a professional poker  player. Imagine if you went to work, performed flawlessly, far better  than all of your coworkers, and instead of getting paid your bank  account got debited and your boss told you \"you really sucked today\".  Then that happens every day for a month.</span></p>\n<p><span style=\"color: #000000;\">Humans  (and especially poker players, who study probabilistic decision making  as a hobby) are conditioned through life to believe outcomes are a  direct result of actions. Do a good job at something and work hard at  it, you succeed. Do a bad job at something and slack off, you fail.</span></p>\n<p><span style=\"color: #000000;\">While  this is true in poker in the long run, the long run can be a long time.  I broke even for a year at one point, and that's at cash games.  Meanwhile donkeys without the slightest clue what they were doing were  lucking ass backward into multimillion-dollar tournament prizes.</span></p>\n<p><span style=\"color: #000000;\">You  can't even fathom what this does to you emotionally until you've lived  through it. It engenders self-doubt, which gets you off your game, which  probably makes you play worse, which you know you're doing but aren't  sure exactly how, or what to do about it, which in turn prolongs the  losing streak. There is no end to it.</span></p>\n</blockquote>\n<p><span style=\"color: #000000;\">-- matmaroon, http://news.ycombinator.com/item?id=1778597</span></p>\n<blockquote>\n<p><span class=\"comment\">\n<p><span style=\"color: #000000;\">I had a girlfriend for  most of that time who went pro.  She was pretty good, too, but she had  that lack of respect for money in spades (heh).  She'd spend a whole day  grinding out $3000 in the $40-80 game and then lose $20,000 in an hour  at the Pai Gow table.</span></p>\n<p><span style=\"color: #000000;\">One day she  decided to skip the Pai Gow and went to the mall instead.  Bought all  kinds of designer clothes and perfumes and electronic gadgets.  She  practically filled up the car.  When she came home there was this  shocked look on her face.  I asked her what the problem was and she said  \"Look at all the stuff you can buy for only $5000!  I'm so used to  using money as ammunition I forgot you can trade it for <em>stuff</em>.\"</span></p>\n</span></p>\n</blockquote>\n<p><span style=\"color: #000000;\">-- tsotha, http://news.ycombinator.com/item?id=1778909</span></p>\n<blockquote>\n<p><span class=\"comment\">\n<p><span style=\"color: #000000;\">It's strange how poker  works but I imagine Buddhism and poker to be the most complementary  religion-to-profession complement on the planet.</span></p>\n</span></p>\n</blockquote>\n<p><span style=\"color: #000000;\">-- InfinityX0, http://news.ycombinator.com/item?id=1777624</span></p>\n<blockquote>\n<p><span style=\"color: #000000;\">After  that I got a lot better, jumped limits, and ended up meeting  DeathDonkey in a 100/200 game at Commerce and again realized I was  outclassed.</span></p>\n<p><span style=\"color: #000000;\">I'd say I owe them a debt of gratitude, but I'm pretty sure I already paid it at the tables.</span></p>\n</blockquote>\n<p><span style=\"color: #000000;\">-- bapadna, http://news.ycombinator.com/item?id=1778094 (included because it is funny)<br /></span></p>\n<p><span style=\"color: #000000;\">Wikipedia on 'tilt': http://en.wikipedia.org/wiki/Tilt_%28poker%29</span></p>\n<p><span style=\"color: #000000;\">On Isildur1: http://en.wikipedia.org/wiki/Isildur1#Career<br /></span></p>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mWXxsBLngfgH6iRdT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 24, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "3714", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-11T22:50:35.892Z", "modifiedAt": null, "url": null, "title": "Draft: Reasons to Use Informal Probabilities", "slug": "draft-reasons-to-use-informal-probabilities", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:09.328Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uMXY23xYc85RbQvFm/draft-reasons-to-use-informal-probabilities", "pageUrlRelative": "/posts/uMXY23xYc85RbQvFm/draft-reasons-to-use-informal-probabilities", "linkUrl": "https://www.lesswrong.com/posts/uMXY23xYc85RbQvFm/draft-reasons-to-use-informal-probabilities", "postedAtFormatted": "Monday, October 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Draft%3A%20Reasons%20to%20Use%20Informal%20Probabilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADraft%3A%20Reasons%20to%20Use%20Informal%20Probabilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMXY23xYc85RbQvFm%2Fdraft-reasons-to-use-informal-probabilities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Draft%3A%20Reasons%20to%20Use%20Informal%20Probabilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMXY23xYc85RbQvFm%2Fdraft-reasons-to-use-informal-probabilities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMXY23xYc85RbQvFm%2Fdraft-reasons-to-use-informal-probabilities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1243, "htmlBody": "<blockquote>\n<p>If I roll 15 fair 6-sided dice, take the ones that rolled 4 or higher, roll them again, and sum up all the die rolls... what is the probability that I drop at least one die on the floor?</p>\n</blockquote>\n<p>There are two different ways of using probability. When we think of probability, we normally think of neat statistics problems where you start with numbers, do some math, and end with a number. After all, if we don't have any numbers to start with, we can't use a proven formula from a textbook; and if we don't use a proven formula from a textbook, our answer can't be right, can it? But there's another way of using probability that's more general: a probability is just an estimate, produced by the best means available, even if that's a guess produced by mere intuition. To distinguish these two types, let's call the former kind *formal probabilities*, and the latter kind *informal probabilities*.</p>\n<p>An informal probability summarizes your state of knowledge, no matter how much or how little knowledge that is. You can make an informal probability in a second, based on your present level of confidence, or spend time making it more precise by looking for details, anchors, reference classes. It is perfectly valid to assign probabilities to things you don't have numbers for, to things you're completely ignorant about, to things that are too complex for you to model, and to things that are poorly defined or underspecified. Giving a probability estimate does not require *any* minimum amount of thought, evidence, or calculation. Giving an informal probability&nbsp; is not a claim that any relevant mathematical calculation has been done, nor that any calculation is even possible.</p>\n<p>I present here the case for assigning informal probabilities, as often as is practical. If any statement crosses your mind that seems especially important, you should put a number on it. Routinely putting probabilities on things has significant benefits, even if they aren't very accurate, even if you don't use them in calculations, and even if you don't share them. The process of assigning probabilities to things tends to prompt useful observations and clarify thinking; it eases the transition into formal calculation when you discover you need it, and provides a sanity check on formal probabilities; having used probabilities makes it easier to diagnose mistakes later; and using probabilities lets you quantify, not just confidence, but also the strength and usefulness of pieces of evidence, and the expected value of avenues of investigation. Finally, practice at generating probabilities makes you better at it.</p>\n<p><a id=\"more\"></a>The first thing to notice is that informal probabilities are much more broadly applicable than formal probabilities are. A formal probability requires more information and more work; in particular, you need to start with relevant numbers; but for most routine questions, you just don't have that data and it wouldn't be worth gathering anyways.&nbsp; For example, it's worth estimating the informal probability that you'll like a dish before ordering it at a restaurant, but producing a formal probability would require a taste test, which is far outside the realm of practicality.</p>\n<p>Assigning informal probabilities clarifies thinking, by forcing you to ask [the fundamental question](http://lesswrong.com/lw/24c/the_fundamental_question/): What do I believe, and why do I believe it? Sometimes, the reason turns out not to be very good, and you ought to assign a low probability. That's important to notice. Sometimes the reason is solid, but tracking it down leads you to something else that's important. That's good, too. Coming up with probabilities also pushes you to look for reference classes and examples. You can still ask these things without using probability, but trying to produce a probability gives guidance and motivation that greatly increases the chance that you'll actually remember to ask these questions when you need to. Informal probabilities also ease the transition into formal calculation when you need it; you can fill in an expected-utility calculation or other formula with estimates, then look for better numbers if the decision is close enough.</p>\n<p>Probabilities are easier to remember than informal notions of confidence. This is important if you catch a mistake and need to go back and figure out where you went wrong; you want to be able to point to a specific thought you had and say, \"this was wrong in light of the evidence I had at the time\", or \"I should've updated this when I found out X\". Unfortunately, memories of degrees of confidence tend to come back badly distorted, unless they're crystallized somehow. Worse, they tend to come back consistently biased towards whatever would be judged correct now, which makes them useless or worse.&nbsp; Numbers crystallize those memories, making them usable and enabling you to retrace steps</p>\n<p>Quantifying confidence also enables us to quantify the strength of evidence - that is, how much a piece of information *changes* our confidence. For example, a piece of evidence that changes our probability estimate from 0.2 to 0.8 is a likelihood ratio of 4:1, or 2 bits of evidence. Assigning before-and-after-evidence probabilities to a statement forces you to consider just how good a piece of evidence it is; and this makes certain mistakes less likely. It's less tempting to round weak arguments off to zero, or to respond emotionally to an argument without judging its actual significance, if you're in the habit of putting numbers on that significance. But keep on mind that there is not one true value for the strength of a piece of evidence; it depends what you already know. For example, an argument that's a duplicate of one you've already updated on has no value at all</p>\n<p>Finally, assigning probabilities to things is a skill like any other, which means it improves with practice. Estimating probabilities and writing them down&nbsp; enables us to calibrate our intuitions. Even if you don't write anything down, just noticing every time you put a .99 on something that turns out to be false is a big improvement over no calibration at all.</p>\n<p>I know of only one caveat: You shouldn't share every probability you produce, unless you're very clear about where it came from. People who're used to only seeing formal probabilities may assume that you have more information than you really do, or that you're trying to misrepresent the information you have.</p>\n<p>To help overcome any internal resistance to giving informal probabilities, I have here a list of probability Fermi problems. A Fermi problem asks for only a rough estimate - an order of magnitude - and it does not include enough information for a precise answer. So too with these problems, which contain just enough information for an estimate. Answer quickly (ten seconds per question at most). Don't do any calculations except very simple ones in your head. Don't worry about all the missing details that could affect the answer. The goal is to be quick, since speed is the main obstacle to using probability routinely.</p>\n<p>1. A car is white.<br />2. A car is a white, ten year old Ford with a dent on the rear right door<br />3. A ten-mile car trip will involve a collision.<br />4. A building is residential.<br />5. A person is below the age of 20.<br />6. A word in a book contaains a typo.<br />7. Your arm will spontaneously transform into a blue tentacle today.<br />8. A purse contains exactly 71 coins.<br />9. 76297 is a prime number.</p>\n<p>I also suggest making some predictions on <a href=\"http://www.predictionbook.com\">PredictionBook</a> and taking a <a href=\"/lw/1f8/test_your_calibration/\">calibration quiz</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uMXY23xYc85RbQvFm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 6.339715925213182e-07, "legacy": true, "legacyId": "3715", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dciuB5nTG2o9PzJWe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-11T23:06:47.143Z", "modifiedAt": null, "url": null, "title": "FAI vs network security", "slug": "fai-vs-network-security", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:15.930Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "snarles", "createdAt": "2009-06-01T03:48:38.132Z", "isAdmin": false, "displayName": "snarles"}, "userId": "YsmFaM5MdsDW8GNop", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fTTzXMvwmPE7fF5SP/fai-vs-network-security", "pageUrlRelative": "/posts/fTTzXMvwmPE7fF5SP/fai-vs-network-security", "linkUrl": "https://www.lesswrong.com/posts/fTTzXMvwmPE7fF5SP/fai-vs-network-security", "postedAtFormatted": "Monday, October 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20FAI%20vs%20network%20security&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFAI%20vs%20network%20security%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTTzXMvwmPE7fF5SP%2Ffai-vs-network-security%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=FAI%20vs%20network%20security%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTTzXMvwmPE7fF5SP%2Ffai-vs-network-security", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfTTzXMvwmPE7fF5SP%2Ffai-vs-network-security", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<p>All plausible scenarios of AGI disaster involve the AGI gaining access to resources \"outside the box.\" &nbsp;Therefore there are two ways of preventing AGI disaster: one is preventing AGI, which is the \"FAI route\", and the other is preventing the possibility of rogue AGI gaining control of too many external resources--the \"network security route.\" &nbsp;It seems to me that this network security route--an international initiative to secure networks and computing resources against cyber attacks--is the more realistic solution for preventing AGI disaster. &nbsp;Network security prevents against intentional human-devised attacks as well as the possibility of rogue AGI--therefore such measures are easier to motivate and therefore more likely to be implemented successfully. &nbsp;Also, the development of FAI theory does not prevent the creation of unfriendly AIs. &nbsp;This is not to say that FAI should not be pursued at all, but it can hardly be claimed that development of FAI is of top priority (as it has been stated a few times by users of this site).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fTTzXMvwmPE7fF5SP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -14, "extendedScore": null, "score": -1.9e-05, "legacy": true, "legacyId": "3716", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-11T23:09:59.714Z", "modifiedAt": null, "url": null, "title": "Copyright should be abolished.", "slug": "copyright-should-be-abolished", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:57.721Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "snarles", "createdAt": "2009-06-01T03:48:38.132Z", "isAdmin": false, "displayName": "snarles"}, "userId": "YsmFaM5MdsDW8GNop", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WhwBjppwrGEv4yiq9/copyright-should-be-abolished", "pageUrlRelative": "/posts/WhwBjppwrGEv4yiq9/copyright-should-be-abolished", "linkUrl": "https://www.lesswrong.com/posts/WhwBjppwrGEv4yiq9/copyright-should-be-abolished", "postedAtFormatted": "Monday, October 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Copyright%20should%20be%20abolished.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACopyright%20should%20be%20abolished.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWhwBjppwrGEv4yiq9%2Fcopyright-should-be-abolished%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Copyright%20should%20be%20abolished.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWhwBjppwrGEv4yiq9%2Fcopyright-should-be-abolished", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWhwBjppwrGEv4yiq9%2Fcopyright-should-be-abolished", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": null, "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WhwBjppwrGEv4yiq9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -17, "extendedScore": null, "score": 6.339761419814092e-07, "legacy": true, "legacyId": "3717", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": null, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-12T02:00:43.474Z", "modifiedAt": null, "url": null, "title": "Morality and relativistic vertigo", "slug": "morality-and-relativistic-vertigo", "viewCount": null, "lastCommentedAt": "2019-05-20T19:05:49.597Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ydnDN8S6H4fHPpmZ2/morality-and-relativistic-vertigo", "pageUrlRelative": "/posts/ydnDN8S6H4fHPpmZ2/morality-and-relativistic-vertigo", "linkUrl": "https://www.lesswrong.com/posts/ydnDN8S6H4fHPpmZ2/morality-and-relativistic-vertigo", "postedAtFormatted": "Tuesday, October 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Morality%20and%20relativistic%20vertigo&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMorality%20and%20relativistic%20vertigo%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FydnDN8S6H4fHPpmZ2%2Fmorality-and-relativistic-vertigo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Morality%20and%20relativistic%20vertigo%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FydnDN8S6H4fHPpmZ2%2Fmorality-and-relativistic-vertigo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FydnDN8S6H4fHPpmZ2%2Fmorality-and-relativistic-vertigo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1106, "htmlBody": "<!-- Morality and relativistic vertigo -->\n<p>tl;dr: <em>Relativism bottoms-out in realism by objectifying relations between subjective notions.  This should be communicated using concrete examples that show its practical importance.  It implies in particular that morality should think about science, and science should think about morality.</em></p>\n<p>Sam Harris attacks moral uber-relativism when he asserts that <a href=\"http://www.ted.com/talks/sam_harris_science_can_show_what_s_right.html\">\"Science can answer moral questions\"</a>.  Countering the counterargument that morality is <em>too imprecise</em> to be treated by science, he makes an excellent comparison: \"healthy\" is not a precisely defined concept, but no one is crazy enough to utter that <em>medicine cannot answer questions of health.</em></p>\n<p>What needs adding to his presentation (which is worth seeing, though I don't entirely agree with it) is what I consider the strongest concise argument <em>in favor</em> of science's moral relevance: that morality is <em>relative</em> simply means that the task of science is to examine <em>absolute relations between morals.</em> For example, suppose you uphold the following two moral claims:</p>\n<ol>\n<li> \"Teachers should be allowed to physically punish their students.\" </li>\n<li>\"Children should be raised not to commit violence against others.\" </li>\n</ol>\n<p>First of all, note that questions of <em>causality</em> are significantly more <a href=\"http://bayes.cs.ucla.edu/BOOK-2K/\">accessible to science</a> than <a href=\"http://www.google.com/#hl=en&amp;expIds=17259,22713&amp;sugexp=ldymls&amp;xhr=t&amp;q=%22no+causation+without+manipulation%22&amp;cp=35&amp;pf=p&amp;sclient=psy&amp;site=&amp;source=hp&amp;aq=f&amp;aqi=&amp;aql=&amp;oq=%22no+causation+without+manipulation%22&amp;gs_rfai=&amp;pbx=1&amp;fp=5a996d56de453056\">people before 2000</a> thought was possible.   Now suppose a cleverly designed, <a href=\"http://en.wikipedia.org/wiki/Instrumental_variable\">non-invasive causal analysis</a> found that physically punishing children, frequently or infrequently, <em>causes</em> them to be more likely to commit criminal violence as adults.  Would you find this discovery <em>irrelevant</em> to your adherence to these morals?  Absolutely not.  You would reflect and realize that you needed to prioritize them in some way.  Most would prioritize the second one, but in any case, science will have made a valid impact.</p>\n<p>So although either of the two morals is purely subjective on its own, <strong>how these morals interrelate is a question of objective fact.</strong> Though perhaps obvious, this idea has some seriously persuasive consequences and is not be taken lightly.  Why?</p>\n<p>First of all, you might <em>change your morals</em> in response to them not relating to each other in the way you expected.  <strong>Ideas parse differently when they relate differently.</strong> \"Teachers should be allowed to physically punish their students\" might never feel the same to you after you find out it causes adult violence.  Even if it originally felt like a terminal (fundamental) value, your prioritization of (2) might make (1) slowly fade out of your mind over time.  In <a href=\"/lw/il/hindsight_bias/\">hindsight</a>, you might just see it as an old, misinformed instrumental value that was never in fact terminal.</p>\n<p>Second, as we increase the number of morals under consideration, the number of relations for science to consider grows rapidly, as (n<sup>2</sup>-n)/2: <strong>we have many more moral relations than morals themselves</strong>.  Suddenly the old disjointed list of untouchable maxims called \"morals\" fades into the background, and we see a throbbing circulatory system of moral relations, objective questions and answers without which no person can competently reflect on her own morality.  A highly prevalent moral like \"human suffering is undesirable\" looks like a major organ: important on its own to a lot of people, and lots of connections in and out for science to examine.</p>\n<h3><span style=\"text-decoration: underline;\">Treating relativistic vertigo</span></h3>\n<p>To my best recollection, I have never heard the phrase \"it's all relative\" used to an effect that didn't involve stopping people from thinking.  When the topic of conversation &mdash; morality, belief, success, rationality, or what have you &mdash; is suddenly revealed or claimed to <em>depend on a context</em>, people find it disorienting, often to the point of feeling the entire discourse has been and will continue to be \"meaningless\" or \"arbitrary\".  Once this happens, it can be very difficult to persuade them to keep thinking, let alone thinking <em>productively</em>&hellip;  <a id=\"more\"></a></p>\n<p>To rebuke this sort of conceptual nihilism, it's natural to respond with analogies to other relative concepts that are clearly useful to think about:</p>\n<p>\"Position, momentum, and energy are only relatively defined as numbers, but we don't abandon scientific study of those, do we?\"</p>\n<p>While an important observation, this inevitably evokes the \"But that's different\" analogy-immune response.  The real cure is in understanding explicitly <em>what to do</em> with relative notions:</p>\n<blockquote>If belief is subjective, let us examine objective relations between beliefs. <br />If morality is relative, let us examine absolute relations between morals. <br />If beauty is in the eye of the beholder, let us examine the eyes of the beholders. <br />&hellip;</blockquote>\n<p>To use one of these lines of argument effectively &mdash; and it can be <em>very</em> effective &mdash; one should follow up immediately with a <em>specific example in the case you're talking about</em>.  Don't let the conversation drift in abstraction.  If you're talking about morality, there is no shortage of objective moral relations that science can handle, so you can pick one at random to show how easy and common it is:</p>\n<ul>\n<li>\"Birth control should be discouraged.\" <br />\"Teen pregnancy / the spread of STDs is undesirable.\" <br />Question: Does promoting the use of condoms increase or decrease teen pregnancy rates / the spread of STDs? &nbsp; </li>\n<li>\"Masturbation should be frowned upon.\" <br />\"Married couples should do their best not to cheat on each other.\" <br />Question: Does masturbation increase or decrease adulterous impulses over time? &nbsp; </li>\n<li>\"Gay couples should not be allowed to adopt children.\" <br />\"Children should not be raised in psychologically damaging environments.\" <br />Question: What are the psychological effects of being raised by gay parents? </li>\n</ul>\n<p>I'm not advocating here any of these particular moral claims, nor any particular resolution between them, but simply that the answer to the given question &mdash; and many other relevant ones &mdash; puts you in a much better position to reflect on these issues.   <strong>Your opinion after you know the answer is more valuable than before.</strong></p>\n<p>\"But of course science can answer <em>some</em> moral questions... the point is that it can't answer <em>all</em> of them.  It can't tell us <em>ultimately</em> what is good or evil.\"</p>\n<p>No.  That is <em>not</em> the point.  The point is whether you want teachers to beat their students.  Do you?  Well, science can help you decide.  And more importantly, once you do, it should help you in leading others to the same conclusion.</p>\n<p>A lesson from history: What happens when you examine objective relations between subjective beliefs?  You get probability theory&hellip; Bayesian updating&hellip; we know this story; it started around 200 years ago, and it ends well.</p>\n<p>Now it's morality's turn.</p>\n<blockquote>Between the subjective and the subjective lies the objective. <br />Relative does not mean structureless. <br />It does not mean arbitrary. <br />It does not mean meaningless. <br />Let us not discard the compass along with the map.</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ydnDN8S6H4fHPpmZ2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 60, "extendedScore": null, "score": 0.000113, "legacy": true, "legacyId": "3718", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fkM9XsNvXdYH6PPAx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-12T05:49:15.498Z", "modifiedAt": null, "url": null, "title": "Vanity and Ambition in Mathematics", "slug": "vanity-and-ambition-in-mathematics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:37.014Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CmkvFtw5vvyD5DyDo/vanity-and-ambition-in-mathematics", "pageUrlRelative": "/posts/CmkvFtw5vvyD5DyDo/vanity-and-ambition-in-mathematics", "linkUrl": "https://www.lesswrong.com/posts/CmkvFtw5vvyD5DyDo/vanity-and-ambition-in-mathematics", "postedAtFormatted": "Tuesday, October 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Vanity%20and%20Ambition%20in%20Mathematics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVanity%20and%20Ambition%20in%20Mathematics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCmkvFtw5vvyD5DyDo%2Fvanity-and-ambition-in-mathematics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Vanity%20and%20Ambition%20in%20Mathematics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCmkvFtw5vvyD5DyDo%2Fvanity-and-ambition-in-mathematics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCmkvFtw5vvyD5DyDo%2Fvanity-and-ambition-in-mathematics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1446, "htmlBody": "<p>In my time in the mathematical community I've formed the subjective impression that it's noticeably less common for mathematicians of the highest caliber to engage in status games than members of the general population do. This impression is consistent with the modesty that comes across in the writings of such mathematicians. I record some relevant quotations below and then discuss interpretations of the situation.</p>\n<p><em>Acknowledgment </em>- I learned of the Hironaka interview quoted below from my colleague Laurens Gunnarsen.</p>\n<p><em>Edited 10/12/10 to remove the first portion of the Hironaka quote which didn't capture the phenomenon that I'm trying to get at here.</em></p>\n<p><a id=\"more\"></a>In a <a href=\"http://www.ams.org/notices/200509/fea-hironaka.pdf\">2005 Interview for the Notices of the AMS</a>, one of the reasons that Fields Medalist <a href=\"http://en.wikipedia.org/wiki/Heisuke_Hironaka\">Heisuke Hironaka</a> says</p>\n<blockquote>\n<p>By the way, Mori is a genius. I am not. So that is a big difference! Mori was a student when I was a visiting professor at Kyoto University. I gave lectures in Kyoto, and Mori wrote notes, which were published in a book. He was really amazing. My lectures were terrible, but when I looked at his notes, it was all there! Mori is a discoverer. He finds new things that people never imagined.</p>\n</blockquote>\n<p>(I'll note in passing that the sense of the \"genius\" that Hironaka is using here is probably different than the sense of \"genius\" that Gowers uses in <a href=\"http://www.amazon.com/Mathematics-Short-Introduction-Timothy-Gowers/dp/0192853619\">Mathematics: A Very Short Introduction</a>.)</p>\n<p>In his <a href=\"http://www.sunsite.ubc.ca/DigitalMathArchive/Langlands/hida/hida-ps.pdf\">review of Haruzo Hida&rsquo;s p-adic automorphic forms on Shimura varieties</a> the originator of the Langlands program <a href=\"http://en.wikipedia.org/wiki/Robert_Langlands\">Robert Langlands</a> wrote</p>\n<blockquote>\n<p>So ill-equipped as I am in many ways &ndash; although not in all &ndash; my first, indeed my major task was to take bearings. The second is, bearings taken, doubtful or not, to communicate them at least to an experienced reader and, in so far as this is possible, even to an inexperienced one. For lack of time and competence I accomplished neither task satisfactorily. So, although I have made a real effort, this review is not the brief, limpid yet comprehensive, account of the subject, revealing its manifold possibilities, that I would have liked to write and that it deserves. The review is imbalanced and there is too much that I had to leave obscure, too many possibly premature intimations. A reviewer with greater competence, who saw the domain whole and, in addition, had a command of the detail would have done much better.</p>\n</blockquote>\n<p>For context, it's worthwhile to note that Langlands' own work is used in an essential way in Hida's book.</p>\n<p>The <a href=\"http://www.ams.org/notices/201003/rtx100300391p.pdf\">2009 Abel Prize Interview</a> with <a href=\"http://en.wikipedia.org/wiki/Mikhail_Gromov_%28mathematician%29\">Mikhail Gromov</a> contains the following questions and answers:</p>\n<blockquote>\n<p>Raussen and Skau: Can you remember when and how you became aware of your exceptional mathematical talent?</p>\n<p>Gromov: I do not think I am exceptional. Accidentally, things happened, and I have qualities that you can appreciate. I guess I never thought in those terms.</p>\n<p>[...]</p>\n<p>Raussen and Skau: Is there one particular theorem or result you are the most proud of?</p>\n<p>Gromov: Yes. It is my introduction of pseudoholomorphic curves, unquestionably. Everything else was just understanding what was already known and to make it look like a new kind of discovery.</p>\n</blockquote>\n<p>In <a href=\"http://mathoverflow.net/users/9062/bill-thurston\">his MathOverflow self-summary</a>, <a href=\"http://en.wikipedia.org/wiki/William_Thurston\">William Thurston</a> wrote</p>\n<blockquote>\n<p>Mathematics is a process of staring hard enough with enough perseverance at at the fog of muddle and confusion to eventually break through to improved clarity. I'm happy when I can admit, at least to myself, that my thinking is muddled, and I try to overcome the embarrassment that I might reveal ignorance or confusion. Over the years, this has helped me develop clarity in some things, but I remain muddled in many others. I enjoy questions that seem honest, even when they admit or reveal confusion, in preference to questions that appear designed to project sophistication.</p>\n</blockquote>\n<hr />\n<p>I interpret the above quotations (and many others by similar such people) to point to a markedly lower than usual interest in status. As <a href=\"/r/discussion/lw/2vb/vanity_and_ambition_in_mathematics/2sas?c=1\">JoshuaZ points out</a>, one <em>could</em> instead read the quotations as counter-signaling, but such an interpretation feels like a stretch to me. I doubt that in practice such remarks serve as an effective counter-signal. More to the point, there's a compelling alternate explanation for why one would see lower than usual levels of status signaling among mathematicians of the highest caliber. Gromov hints at this in the aforementioned interview:</p>\n<blockquote>\n<p>Raussen and Skau: We are surprised that you are so modest by playing down your own achievements. Maybe your ideas are na&iacute;ve, as you yourself say; but to get results from these ideas, that requires some ingenuity, doesn&rsquo;t it?</p>\n<p>Gromov: It is not that I am terribly modest. I don&rsquo;t think I am a complete idiot. Typically when you do mathematics you don&rsquo;t think about yourself. A friend of mine was complaining that anytime he had a good idea he became so excited about how smart he was that he could not work afterwards. So naturally, I try not to think about it.</p>\n</blockquote>\n<p>In <em>R&eacute;coltes et Semailles</em>, <a href=\"http://en.wikipedia.org/wiki/Alexander_Grothendieck\">Alexander Grothendieck</a> offered a more detailed explanation:</p>\n<blockquote>\n<p>The truth of the matter is that it is universally the case that, in the real motives of the scientist, of which he himself is often unaware in his work, vanity and ambition will play as large a role as they do in all other professions. The forms that these assume can be in turn subtle or grotesque, depending on the individual. Nor do I exempt myself. Anyone who reads this testimonial will have to agree with me.</p>\n<p>It is also the case that the most totally consuming ambition is powerless to make or to demonstrate the simplest mathematical discovery - even as it is powerless (for example) to \"score\" (in the vulgar sense). Whether one is male or female, that which allows one to 'score' is not ambition, the desire to shine, to exhibit one's prowess, sexual in this case. Quite the contrary!</p>\n<p>What brings success in this case is the acute perception of the presence of something strong, very real and at the same time very delicate. Perhaps one can call it \"beauty\", in its thousand-fold aspects. That someone is ambitious doesn't mean that one cannot also feel the presence of beauty in them; but it is not the attribute of ambition which evokes this feeling....</p>\n<p>The first man to discover and master fire was just like you and me. He was neither a hero nor a demi-god. Once again like you and me he had experienced the sting of anguish, and applied the poultice of vanity to anaesthetize that sting. But, at the moment at which he first \"knew\" fire he had neither fear nor vanity. That is the truth at the heart of all heroic myth. The myth itself becomes insipid, nothing but a drug, when it is used to conceal the true nature of things.</p>\n<p>[...]</p>\n<p>In our acquisition of knowledge of the Universe (whether mathematical or otherwise) that which renovates the quest is nothing more nor less than complete innocence. It is in this state of <em>complete innocence</em> that we receive everything from the moment of our birth. Although so often the object of our contempt and of our private fears, it is always in us. It alone can unite humility with boldness so as to allow us to penetrate to the heart of things, or allow things to enter us and taken possession of us.</p>\n<p>This unique power is in no way a privilege given to \"exceptional talents\" - persons of incredible brain power (for example), who are better able to manipulate, with dexterity and ease, an enormous mass of data, ideas and specialized skills. Such gifts are undeniably valuable, and certainly worthy of envy from those who (like myself) were not so endowed at birth,\" far beyond the ordinary\".</p>\n<p>Yet it is not these gifts, nor the most determined ambition combined with irresistible will-power, that enables one to surmount the \"invisible yet formidable boundaries\" that encircle our universe. Only innocence can surmount them, which mere knowledge doesn't even take into account, in those moments when we find ourselves able to listen to things, totally and intensely absorbed in child play.</p>\n</blockquote>\n<p>The amount of focus on the subject itself which is required to do mathematical research of the highest caliber is very high. It's plausible that the focuses entailed by vanity and ambition are detrimental to subject matter focus. If this is true (as I strongly suspect to be the case based on my own experience, my observations of others, the remarks of colleagues, and the remarks of eminent figures like Gromov and Grothendieck), aspiring mathematicians would do well to work to curb their ambition and vanity and increase their attraction to mathematics for its own sake.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CmkvFtw5vvyD5DyDo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 6.340697999320955e-07, "legacy": true, "legacyId": "3719", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-12T08:51:28.673Z", "modifiedAt": null, "url": null, "title": "Swords and Armor: A Game Theory Thought Experiment", "slug": "swords-and-armor-a-game-theory-thought-experiment", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:31.781Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nick012000", "createdAt": "2010-07-12T11:00:38.790Z", "isAdmin": false, "displayName": "nick012000"}, "userId": "yCSrMSsugCiSiqAY2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3pnhkrfpj4rZkfqN2/swords-and-armor-a-game-theory-thought-experiment", "pageUrlRelative": "/posts/3pnhkrfpj4rZkfqN2/swords-and-armor-a-game-theory-thought-experiment", "linkUrl": "https://www.lesswrong.com/posts/3pnhkrfpj4rZkfqN2/swords-and-armor-a-game-theory-thought-experiment", "postedAtFormatted": "Tuesday, October 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Swords%20and%20Armor%3A%20A%20Game%20Theory%20Thought%20Experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASwords%20and%20Armor%3A%20A%20Game%20Theory%20Thought%20Experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3pnhkrfpj4rZkfqN2%2Fswords-and-armor-a-game-theory-thought-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Swords%20and%20Armor%3A%20A%20Game%20Theory%20Thought%20Experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3pnhkrfpj4rZkfqN2%2Fswords-and-armor-a-game-theory-thought-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3pnhkrfpj4rZkfqN2%2Fswords-and-armor-a-game-theory-thought-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 107, "htmlBody": "<p>Note: this image does not belong to me; I found it on 4chan. It presents an interesting exercise, though, so I'm posting it here for the enjoyment of the Less Wrong community.</p>\n<p><img style=\"vertical-align: baseline;\" src=\"http://i48.photobucket.com/albums/f222/nick012000/swordsvsarmor.png\" alt=\"\" width=\"676\" height=\"676\" /></p>\n<p>For the sake of this thought experiment, assume that all characters have the same amount of HP, which is sufficiently large that random effects can be treated as being equal to their expected values. There are no NPC monsters, critical hits, or other mechanics; gameplay consists of two PCs getting into a duel, and fighting until one or the other loses. The winner is fully healed afterwards.</p>\n<p>Which sword and armor combination do you choose, and why?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3pnhkrfpj4rZkfqN2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 20, "extendedScore": null, "score": 6.341125525952685e-07, "legacy": true, "legacyId": "3720", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-12T15:46:32.585Z", "modifiedAt": null, "url": null, "title": "Before you start solving a problem", "slug": "before-you-start-solving-a-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:14.654Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "xamdam", "createdAt": "2009-10-26T15:17:24.328Z", "isAdmin": false, "displayName": "xamdam"}, "userId": "8DhacFwGJLZX6nJvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/djiZdBCPcw4kRJS6t/before-you-start-solving-a-problem", "pageUrlRelative": "/posts/djiZdBCPcw4kRJS6t/before-you-start-solving-a-problem", "linkUrl": "https://www.lesswrong.com/posts/djiZdBCPcw4kRJS6t/before-you-start-solving-a-problem", "postedAtFormatted": "Tuesday, October 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Before%20you%20start%20solving%20a%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABefore%20you%20start%20solving%20a%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdjiZdBCPcw4kRJS6t%2Fbefore-you-start-solving-a-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Before%20you%20start%20solving%20a%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdjiZdBCPcw4kRJS6t%2Fbefore-you-start-solving-a-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdjiZdBCPcw4kRJS6t%2Fbefore-you-start-solving-a-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 530, "htmlBody": "<p>(While this is a general discussion, I have \"doing well on interview questions\" as an instrumental goal; the discussion below is somewhat skewed due to that).</p>\n<p>I noticed one of the common failures to solving problems (especially under time constraints) is trying to solve the problem prematurely. There are multiple causes for this; awareness of some of them might reduce the chance of falling into failure mode, others (at least one) I do not have a solution to, and a procedural solution might not exist other than the magic of experience.</p>\n<p>Here is my list of the first kind (awareness-helps group):</p>\n<ol>\n<li>Jumping into the problem before completely understanding it: this could be due to perceived time pressure (e.g. test, interview). This *could* be rational, depending on the \"test score\" function, but could be a serious failure mode if done due to stress.</li>\n<li>Using a cached solution instead of trying to solve the problem. The statement of the problem can trigger \"cached thoughts\" despite (possibly intentionally, in interview) being more subtly more difficult than a well known problem. In one instance I actually misread the statement of the problem because it sounded like one I knew of before. </li>\n<li>Another problem with a cached solution, even if it is the correct one for the problem at hand, is that you might believe that you know it without actually doing the \"retrieve from disk\"; consequences might be looking bad when asked follow-up questions on an interview or inability to build on the problem if it's a part of a greater structure.</li>\n<li>Besides cognitive mechanics, there might be a desire to blurt out a cached solution because it makes you look knowledgeable. A status claim might be instrumentally useful (\"this looks like a min-spanning tree algorithm!\"), as long as you properly calibrate your level of confidence and don't fall for the trap.</li>\n</ol>\n<p>This brings me to the last failure mode which I do not have a solution for (which is why I am posting ;). If I avoid the traps above, I should have a pretty good understanding of the problem. I think this is a kind of crucial point, as I by definition, do <strong>not know what to do next</strong>. This uncertainty is scary and might push me into trying to <strong>immediately </strong>solve it, very similar to 1 above. While you might be able to avoid acting on this by being emotionally reflective (which has the instrumental side effect of appearing more confident) I still do not know what exactly should be done next. Giving some time for unconscious processing seems necessary even on a smallish (interview-question-size) problems, but how much time? And what should I be doing in this time? Meditation? Drawing the problem? Trying to solve sub-problems? Writing down lists of whatever-comes-to-mind? I can use the time constraint-expected size of communicating the solution (in proper format, e.g. C++ code) as an upper bound; but there is a moment when I have to sigh (optional) and take a shot at solution. I do not have anything better to go by than gut feel here.</p>\n<p>(Even after the plunge, there is a chance of getting stuck, which is where <a href=\"/lw/2ui/discuss_metathinking_skills/\">Meta-thinking skills</a> come in)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "djiZdBCPcw4kRJS6t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 6.342099549339445e-07, "legacy": true, "legacyId": "3722", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7tbGrvKi7TnyBpnci"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-12T16:19:15.521Z", "modifiedAt": null, "url": null, "title": "In which I fantasize about drugs", "slug": "in-which-i-fantasize-about-drugs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:55.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gg9rMscx687RXFGg3/in-which-i-fantasize-about-drugs", "pageUrlRelative": "/posts/gg9rMscx687RXFGg3/in-which-i-fantasize-about-drugs", "linkUrl": "https://www.lesswrong.com/posts/gg9rMscx687RXFGg3/in-which-i-fantasize-about-drugs", "postedAtFormatted": "Tuesday, October 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20which%20I%20fantasize%20about%20drugs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20which%20I%20fantasize%20about%20drugs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgg9rMscx687RXFGg3%2Fin-which-i-fantasize-about-drugs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20which%20I%20fantasize%20about%20drugs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgg9rMscx687RXFGg3%2Fin-which-i-fantasize-about-drugs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgg9rMscx687RXFGg3%2Fin-which-i-fantasize-about-drugs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>We operate like this: the \"overseer process\" tells the brain, using blunt instruments like chemicals, that we need to find something to eat, somewhere to sleep or someone to mate with. Then the brain follows orders. Unfortunately the orders we receive from the \"overseer\" are often wrong, even though they were right in the ancestral environment. It seems the easiest way to improve humans isn't to augment their brains - it's to send them better orders, e.g. using drugs. Here's a list of fantasy brain-affecting drugs that I would find useful, even though they don't seem to do anything complicated except affecting \"overseer\" chemistry:</p>\n<p>1) A drug against unrequited love, aka \"infatuation\" or 'limerence\".</p>\n<p>2) A drug that makes you become restless and want to exercise.</p>\n<p>3) A drug that puts you in the state of random creativity that you normally experience just before falling asleep.</p>\n<p>4) A drug that puts you in the optimal PUA \"state\".</p>\n<p>5) A drug that boosts your feeling of curiosity. Must be great for doing math or science.</p>\n<p>Anything else?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gg9rMscx687RXFGg3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 13, "extendedScore": null, "score": 6.342174047756565e-07, "legacy": true, "legacyId": "3723", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-12T17:04:25.670Z", "modifiedAt": null, "url": null, "title": "Of the Qran and its stylistic resources: deconstructing the persuasiveness Draft", "slug": "of-the-qran-and-its-stylistic-resources-deconstructing-the", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:16.258Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raw_Power", "createdAt": "2010-09-10T23:59:43.621Z", "isAdmin": false, "displayName": "Raw_Power"}, "userId": "kwSqcED9qTanFyNWG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yqhMMnn7fprW8pQn6/of-the-qran-and-its-stylistic-resources-deconstructing-the", "pageUrlRelative": "/posts/yqhMMnn7fprW8pQn6/of-the-qran-and-its-stylistic-resources-deconstructing-the", "linkUrl": "https://www.lesswrong.com/posts/yqhMMnn7fprW8pQn6/of-the-qran-and-its-stylistic-resources-deconstructing-the", "postedAtFormatted": "Tuesday, October 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Of%20the%20Qran%20and%20its%20stylistic%20resources%3A%20deconstructing%20the%20persuasiveness%20Draft&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOf%20the%20Qran%20and%20its%20stylistic%20resources%3A%20deconstructing%20the%20persuasiveness%20Draft%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyqhMMnn7fprW8pQn6%2Fof-the-qran-and-its-stylistic-resources-deconstructing-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Of%20the%20Qran%20and%20its%20stylistic%20resources%3A%20deconstructing%20the%20persuasiveness%20Draft%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyqhMMnn7fprW8pQn6%2Fof-the-qran-and-its-stylistic-resources-deconstructing-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyqhMMnn7fprW8pQn6%2Fof-the-qran-and-its-stylistic-resources-deconstructing-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<p>(It's my first time posting an article, so please go easy on me.)</p>\n<p>I wonder if anyone ever fully analysed the Qran and all the resources  it uses to tug at the feelings of the reader? It is a remarkably  persuasive (if not at all <em>convincing</em>)&nbsp; book, even if I say so myself as an ex Muslim. I've started  recognizing some patterns since I started reading this site, but I'd  like to know if there is a full-blown, complete, exhaustive  deconstruction of that book, that is not dripped in islamophobia,  ethnocentrism, and other common failures I have seen in Western  theologians when applied to Islam. Not a book about \"How the Qran is  evil\" or \"How the Qran is Wrong\" or \"How IT'S A FAAAKE\" but \"How,  precisely, it manipulates you\". Can anyone here point me towards such a  work?</p>\n<p>And where is the markup help in this blog? I can't seem to find it  and it frustrates the hell out of me when I'm commenting usual posts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jKAkRrhnHedfowNYy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yqhMMnn7fprW8pQn6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 4, "extendedScore": null, "score": 6.342282349022898e-07, "legacy": true, "legacyId": "3725", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-12T17:46:15.603Z", "modifiedAt": null, "url": null, "title": "Help: Which concepts are controversial on LW", "slug": "help-which-concepts-are-controversial-on-lw", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:08.898Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NjqnfBC8HRhTbx75E/help-which-concepts-are-controversial-on-lw", "pageUrlRelative": "/posts/NjqnfBC8HRhTbx75E/help-which-concepts-are-controversial-on-lw", "linkUrl": "https://www.lesswrong.com/posts/NjqnfBC8HRhTbx75E/help-which-concepts-are-controversial-on-lw", "postedAtFormatted": "Tuesday, October 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%3A%20Which%20concepts%20are%20controversial%20on%20LW&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%3A%20Which%20concepts%20are%20controversial%20on%20LW%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjqnfBC8HRhTbx75E%2Fhelp-which-concepts-are-controversial-on-lw%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%3A%20Which%20concepts%20are%20controversial%20on%20LW%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjqnfBC8HRhTbx75E%2Fhelp-which-concepts-are-controversial-on-lw", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjqnfBC8HRhTbx75E%2Fhelp-which-concepts-are-controversial-on-lw", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 334, "htmlBody": "<p>I'm eager to improve the list <a href=\"/lw/2un/references_resources_for_lesswrong/\">References &amp; Resources for LessWrong</a>. I recently introduced a new label with the somewhat playful name Memetic Hazard. It is meant to mark resources that include ideas which might be controversial, bogus or which are works of fiction and therefore shouldn't be taken at face value.</p>\n<p>I should explain that the reason that some controversial concepts are listed in the first place is that I felt that I frequently encountered those concepts in some rather fanciful discussions and posts. Those posts and discussions attract attention as they are some of the more exciting and fictional content on LW. I had to look them up myself once and want to give new readers a companion guide to learn about the very concepts and their status within the community.</p>\n<p>I might also turn the<em> Key Concepts</em> section into just <em>Concepts</em> with a <em>Controversial</em> subcategory.</p>\n<p>The trigger for this discussion post was a <a href=\"/lw/2un/references_resources_for_lesswrong/2sdq?c=1\">recent comment by <span class=\"comment-author\"><strong></strong></span>rwallace</a>:</p>\n<blockquote>\n<p style=\"padding-left: 30px;\"><em>I thought quantum suicide is not controversial since MWI is obviously correct?</em></p>\n<p>I agree MWI is solid, I'm not suggesting that be flagged. But it does  not in any way imply quantum suicide; the latter is somewhere between  fringe and crackpot, and a proven memetic hazard with at least one  recorded death to its credit.</p>\n<p style=\"padding-left: 30px;\"><em>And the AI section? Well, the list is supposed to reflect the  opinions hold in the LW community, especially by EY and the SIAI. I'm  trying my best to do so and by that standard, how controversial is AI  going FOOM etc.?</em></p>\n<p>Well, AI go FOOM etc is again somewhere in the area between fringe  and crackpot, as judged by people who actually know about the subject.  If the list were specifically supposed to represent the opinions of the  SIAI, then it would belong on the SIAI website, not on LW.</p>\n</blockquote>\n<p>So my question, are <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">AI going FOOM</a> and <a href=\"http://en.wikipedia.org/wiki/Quantum_suicide_and_immortality\">Quantum suicide</a> considered controversial concepts in this community? And should any other content on the list potentially be marked controversial?</p>\n<p>Thank you!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NjqnfBC8HRhTbx75E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "3726", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TNHQLZK5pHbxdnz4e"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-13T09:12:21.578Z", "modifiedAt": null, "url": null, "title": "Beauty in Mathematics", "slug": "beauty-in-mathematics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:15.172Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DTfuFX7ozwLP4SgvK/beauty-in-mathematics", "pageUrlRelative": "/posts/DTfuFX7ozwLP4SgvK/beauty-in-mathematics", "linkUrl": "https://www.lesswrong.com/posts/DTfuFX7ozwLP4SgvK/beauty-in-mathematics", "postedAtFormatted": "Wednesday, October 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beauty%20in%20Mathematics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeauty%20in%20Mathematics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTfuFX7ozwLP4SgvK%2Fbeauty-in-mathematics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beauty%20in%20Mathematics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTfuFX7ozwLP4SgvK%2Fbeauty-in-mathematics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTfuFX7ozwLP4SgvK%2Fbeauty-in-mathematics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2226, "htmlBody": "<p>Serious mathematicians are often drawn toward the subject and motivated by a powerful aesthetic response to mathematical stimuli. In his essay on <a href=\"http://www.ias.ac.in/resonance/Feb200/pdf/Feb2000Reflections.pdf\">Mathematical Creation</a>, <a href=\"http://en.wikipedia.org/wiki/Henri_Poincar%C3%A9\">Henri Poincare</a> wrote</p>\n<blockquote>\n<p>It may be surprising to see emotional sensibility invoked &agrave; propos of mathematical demonstrations which, it would seem, can interest only the intellect. This would be to forget the feeling of mathematical beauty, of the harmony of numbers and forms, of geometric elegance. This is a true aesthetic feeling that all real mathematicians know, and surely it belongs to emotional sensibility.</p>\n</blockquote>\n<p>The prevalence and extent of the feeling of mathematical beauty among mathematicians is not well known. In this article I'll describe some of the reasons for this and give examples of the phenomenon. I've excised many of the quotations in this article from the extensive collection of quotations compiled by my colleague Laurens Gunnarsen.<a id=\"more\"></a></p>\n<p>There's an inherent difficulty in discussing mathematical beauty which is that as in all artistic endeavors, aesthetic judgments are subjective and vary from person to person. As Robert Langlands said in his recent essay <a href=\"http://publications.ias.edu/sites/default/files/ND.pdf\">Is there beauty in mathematical theories?</a></p>\n<blockquote>\n<p>I appreciate, as do many, that there is bad architecture, good architecture and great architecture just as there is bad, good, and great music or bad, good and great literature but neither my education, nor my experience nor, above all, my innate abilities allow me to distinguish with any certainty one from the other. Besides the boundaries are fluid and uncertain. With mathematics, my topic in this lecture, the world at large is less aware of these distinctions and, even among mathematicians, there are widely different perceptions of the merits of this or that achievement, this or that contribution.</p>\n</blockquote>\n<p>Even when they are <em>personally</em> motivated by what they find beautiful, mathematicians tend to deemphasize beauty in professional discourse, preferring to rely on more objective criteria. Without such a practice, the risk of <a href=\"/lw/dr/generalizing_from_one_example\">generalizing from one example</a> and confusing one's own immediate aesthetic preferences with what's in the interest of the mathematical community and broader society would be significant. In the same essay Langlands said</p>\n<blockquote>\n<p>Harish-Chandra and Chevalley were certainly not alone in perceiving their goal as the revelation of God&rsquo;s truths, which we might interpret as beauty, but mathematicians largely use a different criterion when evaluating the efforts of their colleagues. The degree of the difficulties to be overcome, thus of the effort and imagination necessary to the solution of a problem, is much more likely than aesthetic criteria to determine their esteem for the solution, and any theory that permits it. This is probably wise, since aesthetic criteria are seldom uniform and often difficult to apply. The search for beauty quickly lapses in less than stern hands into satisfaction with the meretricious.</p>\n</blockquote>\n<p>The asymmetry between personal motivations and professional discourse gives rise to the possibility that outside onlookers might misunderstand the motivations of mathematicians and consequently misunderstand the nature of mathematical practice.</p>\n<p>Aside from this, another reason why outside onlookers are frequently mislead is the high barrier to entry to advanced mathematics. In his article <a href=\"http://www.springerlink.com/content/e033665007637p16/\">Mathematics: art and science</a>, <a href=\"http://en.wikipedia.org/wiki/Armand_Borel\">Armand Borel</a> wrote:</p>\n<blockquote>I [have] already mentioned the idea of mathematics as an art, a poetry of ideas. With that as a starting point, one would conclude that, in order for one to appreciate mathematics, to enjoy it, one needs a unique feeling for the intellectual elegance and beauty of ideas in a very special world of thought. It is not surprising that this can hardly be shared with nonmathematicians: Our poems are written in a highly specialized language, the mathematical language; although it is expressed in many of the more familiar languages, it is nevertheless unique and translatable into no other language; unfortunately, these poems can only be understood in the original. The resemblance to an art is clear. One must also have a certain education for the appreciation of music or painting, which is to say one must learn a certain language.<br /></blockquote>\n<p>I think that Borel's statement about the inaccessibility of mathematics to non-mathematicians is too strong. For a counterpoint, in a reference to be added, <a href=\"/rg/wiki/Jean-Pierre_Serre\">Jean-Pierre Serre</a> said</p>\n<blockquote>\n<p>I&rsquo;ve always loved mathematics. My earliest memory, which goes back to the beginning of elementary school, is of learning the multiplication table. When one loves to play, one tries to understand the reason. All my mathematics is like this, but a bit more complicated.</p>\n</blockquote>\n<p>In his aforementioned essay Langlands wrote</p>\n<blockquote>\n<p>Initially perhaps there is no gangue, not even problems, perhaps just a natural, evolutionary conditioned delight in elementary arithmetic &ndash; the manipulation of numbers, even of small numbers &ndash; or in basic geometric shapes &ndash; triangles, rectangles, regular polygons.</p>\n</blockquote>\n<p>and then after reviewing the history of algebraic numbers:</p>\n<blockquote>\n<p>Does mathematical beauty or pleasure require such an accumulation of concepts and detail? Does music? Does architecture? Does literature? The answer is certainly &ldquo;no&rdquo; in all cases. On the other hand, the answer to the question whether mathematical beauty or pleasure admits such an accumulation and whether the beauty achieved is then of a different nature is, in my view, &ldquo;yes&rdquo;. This response is open to dispute, as it would be for the other domains.</p>\n</blockquote>\n<p>This can be viewed as a reconciliation of Borel's statement and Serre's statement.</p>\n<p>I'll proceed to give some more specific examples of aesthetic reactions. In the spirit of the quotations from Langlands above, the remarks quoted below should be interpreted as expressions of personal preferences and experiences rather than as statements about the objective nature of reality. All the same, since human preferences are correlated, knowing about the personal preferences of others does provide useful information about what one might personally find attractive.</p>\n<p>Furthermore, as Roger Penrose wrote in his article <em>The Role of Aesthetics in Pure and Applied Mathematical Research</em>, the ultimate justification for pursuing mathematics for its own sake is aesthetic:</p>\n<blockquote>\n<p>How, in fact, does one decide which things in mathematics are important and which are not? Ultimately, the criteria have to be aesthetic ones. There are other values in mathematics, such as depth, generality, and utility. But these are not so much ends in themselves. Their significance would seem to rest on the values of the other things to which they relate. The ultimate values seem simply to be aesthetic; that is, artistic values such as one has in music or painting or any other art form.<em><br /></em></p>\n</blockquote>\n<hr />\n<p>In <a href=\"http://books.google.com/books?id=q6eSjV-0egUC&amp;lpg=PA225&amp;ots=rhW_vimTOv&amp;dq=Oscar%20Zariski%20bewitched%20me.%20When%20he%20spoke%20the%20words%20%E2%80%9Calgebraic%20variety%E2%80%9D%20there%20was%20a%20certain%20resonance%20in%20his%20voice%20that%20said%20distinctly%20that%20he%20was%20looking%20into%20a%20secret%20garden.%20I%20immediately%20wanted%20to%20be%20able%20to%20do%20this%20too.%20It%20led%20me%20to%2025%20years%20of%20struggling%20to%20make%20this%20world%20tangible%20and%20visible.&amp;pg=PA225#v=onepage&amp;q=Oscar%20Zariski%20bewitched%20me.%20When%20he%20spoke%20the%20words%20%E2%80%9Calgebraic%20variety%E2%80%9D%20there%20was%20a%20certain%20resonance%20in%20his%20voice%20that%20said%20distinctly%20that%20he%20was%20looking%20into%20a%20secret%20garden.%20I%20immediately%20wanted%20to%20be%20able%20to%20do%20this%20too.%20It%20led%20me%20to%2025%20years%20of%20struggling%20to%20make%20this%20world%20tangible%20and%20visible.&amp;f=false\">an autobiography</a>, <a href=\"http://en.wikipedia.org/wiki/David_Mumford\">David Mumford</a> wrote</p>\n<blockquote>\n<p>At Harvard, a classmate said \"Come with me to hear Professor Zariski's first lecture, even though we won't understand a word\" and Oscar Zariski bewitched me. When he spoke the words 'algebraic variety,' there was a certain resonance in his voice that said distinctly that he was looking into a secret garden. I immediately wanted to be able to do this too. It led me to 25 yeras of stuggling to make this world tangilbe and visible. Especially, I became obsessed with a kind of passion flower in this garden, the moduli spaces of Riemann. I was always trying to find new angles from which I could see them better.</p>\n</blockquote>\n<p>In his essay in <a href=\"http://press.princeton.edu/titles/8860.html\">Mathematicians: An Outer View of the Inner World</a>, Don Zagier wrote</p>\n<blockquote>\n<p>I like explicit, hands-on formulas. To me they have a beauty of their own. They can be deep or not. As an example, imagine you have a series of numbers such that if you add 1 to any number you will get the product of its left and right neighbors. Then this series will repeat itself at every fifth step! For instance, if you start with 3, 4 then the sequence continues: 3, 4, 5/3, 2/3, 1, 3, 4, 5/3, etc. The difference between a mathematician and a nonmathematician is not just being able to discover something like this, but to care about it and to be curious about why it's true, what it means, and what other things in mathematics it might be connected with. In this particular case, the statement itself turns out to be connected with a myriad of deep topics in advanced mathematics: hyperbolic geometry, algebraic K-theory, the Schrodinger equation of quantum mechanics, and certain models of quantum field theory. I find this kind of connection between very elementary and very deep mathematics overwhelmingly beautiful. Some mathematicians find formulas and special cases less interesting and care only about understanding the deep underlying reasons. Of course that is the final goal, but the examples let you see things for a particular problem differently, and anyway it's good to have different approaches and different types of mathematicians.</p>\n</blockquote>\n<p>In <em>Recoltes et Semailles</em> <a href=\"http://en.wikipedia.org/wiki/Alexander_Grothendieck\">Alexander Grothendieck</a> wrote about his subjective experience of his transition to algebraic geometry following a successful early career in analysis:</p>\n<blockquote>\n<p><img src=\"file:///Users/jonahsinick/Library/Caches/TemporaryItems/moz-screenshot-2.png\" alt=\"\" />The year 1955 marked a critical departure in my work in mathematics: that of my passage from \"analysis\" to \"geometry\". I well recall the power of my emotional response (very subjective naturally); it was as if I'd fled the harsh arid steppes to find myself suddenly transported to a kind of \"promised land\" of superabundant richness, multiplying out to infinity wherever I placed my hand in it, either to search or to gather... This impression, of overwhelming riches has continued to be confirmed and grow in substance and depth down to the present day. (*)</p>\n<p>(*) The phrase \"superabundant richness\" has this nuance: it refers to the situation in which the impressions and sensations raised in us through encounter with something whose splendor, grandeur or beauty are out of the ordinary, are so great as to totally submerge us, to the point that the urge to express whatever we are feeling is obliterated.</p>\n</blockquote>\n<p>On rare occasions I've been fortunate to experience the \"superabundant richness\" that Grothendieck describes in connection with mathematics. I've quoted a reflective piece that I wrote about a year ago about such an experience from November 2008:</p>\n<blockquote>\n<p>I was in Steve Ullom's course on class field theory, finally understanding the statements of the theorems. I had been intrigued by class field theory ever since I encountered David Cox's book titled \"<a href=\"http://www.amazon.com/Primes-Form-Multiplication-Mathematics-Wiley-Interscience/dp/0471190799\" target=\"_blank\">Primes of the form x<sup>2</sup> + ny<sup>2</sup></a>\"&nbsp; in 2004 or so, but I was not able to form a mental picture of the subject from Cox's presentation. <br /> <br />I was initially drawn toward algebraic number theory primarily by its reputation rather than out of a love for the subject. By this I don't mean that I was motivated by careerism, but rather that I knew that the subject was a favorite of some of the greatest historical mathematicians and I had seen the Nova video on Fermat's Last Theorem while in high school in which the mathematicians interviewed (in particular Wiles, Mazur, Ribet and Shimura) seemed fascinated by the subject&nbsp; - I figured that if I stuck with it for long enough I would be so struck as well.&nbsp; <br /> <br />It took me a long time to come to a genuine appreciation of the subject. I don't think that this is uncommon - the early manifestations of the subject are somewhat obscure, and I don't think it an accident that it wasn't until the late 1800's that it became mainstream despite having a pedigree stretching back very far. And even today, few expositions highlight the essential points. <br /> <br />Anyway, in Steve Ullom's course I finally \"got it\" - both on a semantic level and why so many people might have been attracted to the subject. Sometime in November I revisited Silverman and Tate's <a href=\"http://www.amazon.com/Rational-Points-Elliptic-Undergraduate-Mathematics/dp/0387978259\" target=\"_blank\"><em>Rational Points on Elliptic Curves</em></a> and looked at the last chapter on complex multiplication. I knew that the theory of complex multiplication came highly recommended, Kronecker citing its development as his \"dearest dream of youth\" and Hilbert having said something like \"The theory of complex multiplication of elliptic curves is not only the most beautiful part of mathematics but of all science.\" I had seen glimmerings of what made it interesting from Cox's book, but again, I had not been able to understand the subject from his exposition.<br /> <br />With the background of the course that I was taking, reading Silverman and Tate I was able to understand the instance of complex multiplication that they worked out and was totally bewitched to learn how the elliptic curve y<sup>2</sup> = x<sup>3</sup> + x organizes all finite abelian extensions of Q(i) in a very coherent way. <br /> <br />For the next several weeks I was in a plane of existence different both from what which I'm accustomed to and from that of the people surrounding me. Naturally it's difficult to describe such an experience in words and all the more so retrospectively. It was a state of great inner focus and tranquility. I was filled with a sense of limitless possibility. I was simultaneously able to acknowledge the problems in the human world around me while also not being discouraged by them in the least.&nbsp; There are certainly echoes of the Buddhist conception of enlightenment in my feeling. My state brought to mind a famous poem by the Chinese poet Li Bai:<br /> <br /><em>Question and Answer in the Mountains<br /><br />They ask me why I live in the green mountains.<br />I smile and don't reply; my heart's at ease.<br />Peach blossoms flow downstream, leaving no trace -<br /> And there are other earths and skies than these<br /></em></p>\n</blockquote>\n<p><img src=\"file:///Users/jonahsinick/Library/Caches/TemporaryItems/moz-screenshot.png\" alt=\"\" /><img src=\"file:///Users/jonahsinick/Library/Caches/TemporaryItems/moz-screenshot-1.png\" alt=\"\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DTfuFX7ozwLP4SgvK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": null}], "voteCount": 15, "baseScore": 20, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "3721", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-13T09:16:26.583Z", "modifiedAt": null, "url": null, "title": "Gandhi, murder pills, and mental illness", "slug": "gandhi-murder-pills-and-mental-illness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:35.231Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "erratio", "createdAt": "2010-06-29T09:32:42.768Z", "isAdmin": false, "displayName": "erratio"}, "userId": "ty7er2ZYEnPYALnnJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SdkAesHBt4tsivEKe/gandhi-murder-pills-and-mental-illness", "pageUrlRelative": "/posts/SdkAesHBt4tsivEKe/gandhi-murder-pills-and-mental-illness", "linkUrl": "https://www.lesswrong.com/posts/SdkAesHBt4tsivEKe/gandhi-murder-pills-and-mental-illness", "postedAtFormatted": "Wednesday, October 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Gandhi%2C%20murder%20pills%2C%20and%20mental%20illness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGandhi%2C%20murder%20pills%2C%20and%20mental%20illness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdkAesHBt4tsivEKe%2Fgandhi-murder-pills-and-mental-illness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Gandhi%2C%20murder%20pills%2C%20and%20mental%20illness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdkAesHBt4tsivEKe%2Fgandhi-murder-pills-and-mental-illness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdkAesHBt4tsivEKe%2Fgandhi-murder-pills-and-mental-illness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 320, "htmlBody": "<p>Gandhi is the perfect pacifist, utterly committed to not bringing about harm to his fellow beings. If a murder pill existed such that it would make murder seem ok without changing any of your other values, Gandhi would refuse to take it on the grounds that he doesn't want his future self to go around doing things that his current self isn't comfortable with. Is there anything you could say to Gandhi that could convince him to take the pill? If a serial killer was hiding under his bed waiting to ambush him, would it be ethical to force him to take it so that he would have a chance to save his own life? If for some convoluted reason he was the only person who could kill the researcher about to complete uFAI, would it be ethical to force him to take the pill so that he'll go and save us all from uFAI?</p>\n<p>&nbsp;</p>\n<p>Charlie is very depressed, utterly certain that life is meaningless and terrible and not going to improve anytime between now and the heat death of the universe. He would kill himself but even that seems pointless. If a magic pill existed that would get rid of depression permanently and without side effects, he would refuse it on the grounds that he doesn't want his future self to go around with a delusion (that everything is fine) which his current self knows to be false. Is there anything you could say to Charlie that could convince him to take it? Would it be ethical to force him to take the pill?</p>\n<p>&nbsp;</p>\n<p>Note: I'm aware of the conventional wisdom for dealing with mental illness, and generally subscribe to it myself. I'm more interested in why people intuitively feel that there's a difference between these two situations, whether there are arguments that could be used to change someone's terminal values, or as a rationale for forcing a change on them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qQMEMrXioExa4uhTB": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SdkAesHBt4tsivEKe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 18, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "3727", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-13T14:04:06.988Z", "modifiedAt": null, "url": null, "title": "Open Thread", "slug": "open-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:38.756Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AManWkjkCqZcRY3tp/open-thread", "pageUrlRelative": "/posts/AManWkjkCqZcRY3tp/open-thread", "linkUrl": "https://www.lesswrong.com/posts/AManWkjkCqZcRY3tp/open-thread", "postedAtFormatted": "Wednesday, October 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAManWkjkCqZcRY3tp%2Fopen-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAManWkjkCqZcRY3tp%2Fopen-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAManWkjkCqZcRY3tp%2Fopen-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 18, "htmlBody": "<p>This is an experiment to see whether people would like an open thread for small topics and conversation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AManWkjkCqZcRY3tp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 6.345240201990499e-07, "legacy": true, "legacyId": "3728", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-13T14:07:31.195Z", "modifiedAt": null, "url": null, "title": "Is there evolutionary selection for female orgasms?", "slug": "is-there-evolutionary-selection-for-female-orgasms", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:20.927Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5DFz85BgjdEYBpWCh/is-there-evolutionary-selection-for-female-orgasms", "pageUrlRelative": "/posts/5DFz85BgjdEYBpWCh/is-there-evolutionary-selection-for-female-orgasms", "linkUrl": "https://www.lesswrong.com/posts/5DFz85BgjdEYBpWCh/is-there-evolutionary-selection-for-female-orgasms", "postedAtFormatted": "Wednesday, October 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20there%20evolutionary%20selection%20for%20female%20orgasms%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20there%20evolutionary%20selection%20for%20female%20orgasms%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5DFz85BgjdEYBpWCh%2Fis-there-evolutionary-selection-for-female-orgasms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20there%20evolutionary%20selection%20for%20female%20orgasms%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5DFz85BgjdEYBpWCh%2Fis-there-evolutionary-selection-for-female-orgasms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5DFz85BgjdEYBpWCh%2Fis-there-evolutionary-selection-for-female-orgasms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 270, "htmlBody": "<p>&gt;<strong>Elisabeth Lloyd:</strong> I don&rsquo;t actually know. I think that it&rsquo;s at a very problematic intersection of topics. I mean, you&rsquo;re taking the intersection of human evolution, women, sexuality &ndash; once you take that intersection you&rsquo;re bound to kind of get a disaster. More than that, when evolutionists have looked at this topic, I think that they&rsquo;ve had quite a few items on their agenda, including telling the story about human origins that bolsters up the family, monogamy, a certain view of female sexuality that&rsquo;s complimentary to a certain view of male sexuality. And all of those items have been on their agenda and it&rsquo;s quite visible in their explanations.<br /><br /><strong>&gt;Natasha Mitchell:</strong> I guess it&rsquo;s perplexed people partly, too, because women don&rsquo;t need an orgasm to become pregnant, and so the question is: well, what&rsquo;s its purpose? Well, is its purpose to give us pleasure so that we have sex, so that we can become pregnant, according to the classic evolutionary theories?<br /><br /><strong>&gt;Elisabeth Lloyd:</strong> The problem is even worse than it appears at first because not only is orgasm not necessary on the female side to become pregnant, there isn&rsquo;t even any evidence that orgasm makes any difference at all to fertility, or pregnancy rate, or reproductive success. It seems intuitive that a female orgasm would motivate females to engage in intercourse which would naturally lead to more pregnancies or help with bonding or something like that, but the evidence simply doesn&rsquo;t back that up.</p>\n<p><a href=\"http://www.abc.net.au/cgi-bin/common/printfriendly.pl?http://www.abc.net.au/rn/science/mind/stories/s1382386.htm\">The whole discussion.</a> It backs my theory that using evolution to explain current traits seriously tempts people to make things up.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5DFz85BgjdEYBpWCh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 6.345248196839406e-07, "legacy": true, "legacyId": "3729", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-13T20:32:08.085Z", "modifiedAt": null, "url": null, "title": "A Fundamental Question of Group Rationality", "slug": "a-fundamental-question-of-group-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:23.245Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "steven0461", "createdAt": "2009-02-27T16:16:38.980Z", "isAdmin": false, "displayName": "steven0461"}, "userId": "cn4SiEmqWbu7K9em5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S2r9BjZZFTHJpr6DA/a-fundamental-question-of-group-rationality", "pageUrlRelative": "/posts/S2r9BjZZFTHJpr6DA/a-fundamental-question-of-group-rationality", "linkUrl": "https://www.lesswrong.com/posts/S2r9BjZZFTHJpr6DA/a-fundamental-question-of-group-rationality", "postedAtFormatted": "Wednesday, October 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Fundamental%20Question%20of%20Group%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Fundamental%20Question%20of%20Group%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS2r9BjZZFTHJpr6DA%2Fa-fundamental-question-of-group-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Fundamental%20Question%20of%20Group%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS2r9BjZZFTHJpr6DA%2Fa-fundamental-question-of-group-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS2r9BjZZFTHJpr6DA%2Fa-fundamental-question-of-group-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<p>What do you believe because others believe it, even though your own evidence and reasoning (\"impressions\") point the other way?</p>\n<p>(Note that answers like \"quantum chromodynamics\" don't count, except in the unlikely case that you've seriously tried to do your own physics, and it suggested the mainstream was wrong, and that's what you would have believed if not for it being the mainstream.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zv7v2ziqexSn5iS9v": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S2r9BjZZFTHJpr6DA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 17, "extendedScore": null, "score": 6.346151792620288e-07, "legacy": true, "legacyId": "3730", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-14T00:03:26.507Z", "modifiedAt": null, "url": null, "title": "Standing Desks and Hunter-Gatherers", "slug": "standing-desks-and-hunter-gatherers", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:16.563Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XeTWxqS3LZm45ABNj/standing-desks-and-hunter-gatherers", "pageUrlRelative": "/posts/XeTWxqS3LZm45ABNj/standing-desks-and-hunter-gatherers", "linkUrl": "https://www.lesswrong.com/posts/XeTWxqS3LZm45ABNj/standing-desks-and-hunter-gatherers", "postedAtFormatted": "Thursday, October 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Standing%20Desks%20and%20Hunter-Gatherers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStanding%20Desks%20and%20Hunter-Gatherers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXeTWxqS3LZm45ABNj%2Fstanding-desks-and-hunter-gatherers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Standing%20Desks%20and%20Hunter-Gatherers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXeTWxqS3LZm45ABNj%2Fstanding-desks-and-hunter-gatherers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXeTWxqS3LZm45ABNj%2Fstanding-desks-and-hunter-gatherers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<p>I recently&nbsp;started&nbsp;using a standing desk and found it increases my productivity perhaps because my mostly caveman brain \"thinks\" that I will usually stand when facing&nbsp;cognitively&nbsp;challenging tasks, but I will sit when I want to relax and save energy.</p>\n<p>Are there other ways we can attempt to increase our&nbsp;cognitive powers&nbsp;by taking into account that many of the genes which influence human emotion and cognition&nbsp;were selected for to make our ancestors better hunter-gatherers?</p>\n<p>Edited because of Reisqui's comment.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XeTWxqS3LZm45ABNj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 6.346648324210838e-07, "legacy": true, "legacyId": "3731", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-14T00:39:09.365Z", "modifiedAt": null, "url": null, "title": "LW favorites", "slug": "lw-favorites", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:17.468Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sPNbbezfkxiNF8qn7/lw-favorites", "pageUrlRelative": "/posts/sPNbbezfkxiNF8qn7/lw-favorites", "linkUrl": "https://www.lesswrong.com/posts/sPNbbezfkxiNF8qn7/lw-favorites", "postedAtFormatted": "Thursday, October 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20favorites&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20favorites%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsPNbbezfkxiNF8qn7%2Flw-favorites%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20favorites%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsPNbbezfkxiNF8qn7%2Flw-favorites", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsPNbbezfkxiNF8qn7%2Flw-favorites", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>What's still on your mind (in a positive way) a year or more after it was posted?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sPNbbezfkxiNF8qn7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 6.346732252823458e-07, "legacy": true, "legacyId": "3732", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-14T12:49:24.851Z", "modifiedAt": null, "url": null, "title": "1993 AT&T \"You Will\" ads", "slug": "1993-at-and-t-you-will-ads", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:15.353Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nick012000", "createdAt": "2010-07-12T11:00:38.790Z", "isAdmin": false, "displayName": "nick012000"}, "userId": "yCSrMSsugCiSiqAY2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uRcXEcyQYgAaZJvms/1993-at-and-t-you-will-ads", "pageUrlRelative": "/posts/uRcXEcyQYgAaZJvms/1993-at-and-t-you-will-ads", "linkUrl": "https://www.lesswrong.com/posts/uRcXEcyQYgAaZJvms/1993-at-and-t-you-will-ads", "postedAtFormatted": "Thursday, October 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%201993%20AT%26T%20%22You%20Will%22%20ads&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A1993%20AT%26T%20%22You%20Will%22%20ads%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuRcXEcyQYgAaZJvms%2F1993-at-and-t-you-will-ads%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=1993%20AT%26T%20%22You%20Will%22%20ads%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuRcXEcyQYgAaZJvms%2F1993-at-and-t-you-will-ads", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuRcXEcyQYgAaZJvms%2F1993-at-and-t-you-will-ads", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>Here's something I found while wasting time on Youtube today. Sort of surprising how close they got to the truth, though of course the aesthetics are all wrong, and AT&amp;T wasn't the company who brought them about.<a href=\"http://www.youtube.com/watch?v=TZb0avfQme8&amp;feature=grec_index\"><br /></a></p>\n<p><a href=\"http://www.youtube.com/watch?v=TZb0avfQme8&amp;feature=grec_index\">http://www.youtube.com/watch?v=TZb0avfQme8&amp;feature=grec_index</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uRcXEcyQYgAaZJvms", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 6.3484487908577e-07, "legacy": true, "legacyId": "3734", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-14T17:53:57.292Z", "modifiedAt": null, "url": null, "title": "Retirement leads to loss of cognitive abilities", "slug": "retirement-leads-to-loss-of-cognitive-abilities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:35.698Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nick012000", "createdAt": "2010-07-12T11:00:38.790Z", "isAdmin": false, "displayName": "nick012000"}, "userId": "yCSrMSsugCiSiqAY2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZDsxeieoXi6uvWAxg/retirement-leads-to-loss-of-cognitive-abilities", "pageUrlRelative": "/posts/ZDsxeieoXi6uvWAxg/retirement-leads-to-loss-of-cognitive-abilities", "linkUrl": "https://www.lesswrong.com/posts/ZDsxeieoXi6uvWAxg/retirement-leads-to-loss-of-cognitive-abilities", "postedAtFormatted": "Thursday, October 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Retirement%20leads%20to%20loss%20of%20cognitive%20abilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARetirement%20leads%20to%20loss%20of%20cognitive%20abilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZDsxeieoXi6uvWAxg%2Fretirement-leads-to-loss-of-cognitive-abilities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Retirement%20leads%20to%20loss%20of%20cognitive%20abilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZDsxeieoXi6uvWAxg%2Fretirement-leads-to-loss-of-cognitive-abilities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZDsxeieoXi6uvWAxg%2Fretirement-leads-to-loss-of-cognitive-abilities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1126, "htmlBody": "<p>&nbsp;</p>\n<p><a href=\"http://www.nytimes.com/2010/10/12/science/12retire.html?src=me&amp;ref=general\">Here</a>'s something I found after surfing a few links. Given the interest in intelligence augmentation and biological immortality here, I figured you guys would find it useful to know; I wasn't particularly planning on retiring (given biological immortality or uploading technologies), but if this is true, I definitely won't be for as long as I can avoid it.</p>\n<blockquote>\n<p>The two economists call their paper &ldquo;Mental Retirement,&rdquo; and their argument has intrigued behavioral researchers. Data from the United States, England and 11 other European countries suggest that the earlier people retire, the more quickly their memories decline.</p>\n<p>The implication, the economists and others say, is that there really seems to be something to the &ldquo;use it or lose it&rdquo; notion &mdash; if people want to preserve their memories and reasoning abilities, they may have to keep active.</p>\n<p>&ldquo;It&rsquo;s incredibly interesting and exciting,&rdquo; said Laura L. Carstensen, director of the Center on Longevity at <span class=\"meta-org\">Stanford University</span>. &ldquo;It suggests that work actually provides an important component of the environment that keeps people functioning optimally.&rdquo;</p>\n<p>While not everyone is convinced by the new analysis, published recently in The Journal of Economic Perspectives, a number of leading researchers say the study is, at least, a tantalizing bit of evidence for a hypothesis that is widely believed but surprisingly difficult to demonstrate.</p>\n<p>Researchers repeatedly find that retired people as a group tend to do less well on <span class=\"meta-classifier\">cognitive tests</span> than people who are still working. But, they note, that could be because people whose memories and thinking skills are declining may be more likely to retire than people whose cognitive skills remain sharp.</p>\n<p>And research has failed to support the premise that mastering things like memory exercises, crossword puzzles and games like Sudoku carry over into real life, improving overall functioning.</p>\n<p>&ldquo;If you do crossword puzzles, you get better at crossword puzzles,&rdquo; said Lisa Berkman, director of the Center for Population and Development Studies at Harvard. &ldquo;If you do Sudoku, you get better at Sudoku. You get better at one narrow task. But you don&rsquo;t get better at cognitive behavior in life.&rdquo;</p>\n<p>The study was possible, explains one of its authors, Robert Willis, a professor of economics at the <span class=\"meta-org\">University of Michigan</span>, because the National Institute on Aging began a large study in the United States nearly 20 years ago. Called the Health and Retirement Study, it surveys more than 22,000 Americans over age 50 every two years, and administers memory tests.</p>\n<p>That led European countries to start their own surveys, using similar questions so the data would be comparable among countries. Now, Dr. Willis said, Japan and South Korea have begun administering the survey to their populations. China is planning to start doing a survey next year. And India and several countries in Latin America are starting preliminary work on their own surveys.</p>\n<p>&ldquo;This is a new approach that is only possible because of the development of comparable data sets around the world.&rdquo; Dr. Willis said.</p>\n<p>The memory test looks at how well people can recall a list of 10 nouns immediately and 10 minutes after they heard them. A perfect score is 20, meaning all 10 were recalled each time. Those tests were chosen for the surveys because memory generally declines with age, and this decline is associated with diminished ability to think and reason.</p>\n<p>People in the United States did best, with an average score of 11. Those in Denmark and England were close behind, with scores just above 10. In Italy, the average score was around 7, in France it was 8, and in Spain it was a little more than 6.</p>\n<p>Examining the data from the various countries, Dr. Willis and his colleague Susann Rohwedder, associate director of the RAND Center for the Study of Aging in Santa Monica, Calif., noticed that there are large differences in the ages at which people retire.</p>\n<p>In the United States, England and Denmark, where people retire later, 65 to 70 percent of men were still working when they were in their early 60s. In France and Italy, the figure is 10 to 20 percent, and in Spain it is 38 percent.</p>\n<p>Economic incentives produce the large differences in retirement age, Dr. Rohwedder and Dr. Willis report. Countries with earlier retirement ages have tax policies, pension, disability and other measures that encourage people to leave the work force at younger ages.</p>\n<p>The researchers find a straight-line relationship between the percentage of people in a country who are working at age 60 to 64 and their performance on memory tests. The longer people in a country keep working, the better, as a group, they do on the tests when they are in their early 60s.</p>\n<p>The study cannot point to what aspect of work might help people retain their memories. Nor does it reveal whether different kinds of work might be associated with different effects on memory tests. And, as Dr. Berkman notes, it has nothing to say about the consequences of staying in a physically demanding job that might lead to disabilities. &ldquo;There has to be an out for people who face physical disabilities if they continue,&rdquo; she said.</p>\n<p>And of course not all work is mentally stimulating. But, Dr. Willis said, work has other aspects that might be operating.</p>\n<p>&ldquo;There is evidence that social skills and personality skills &mdash; getting up in the morning, dealing with people, knowing the value of being prompt and trustworthy &mdash; are also important,&rdquo; he said. &ldquo;They go hand in hand with the work environment.&rdquo;</p>\n<p>But Hugh Hendrie, an emeritus <span class=\"meta-classifier\">psychology</span> professor at <span class=\"meta-org\">Indiana University</span> School of Medicine, is not convinced by the paper&rsquo;s conclusions.</p>\n<p>&ldquo;It&rsquo;s a nice approach, a very good study,&rdquo; he said. But, he said, there are many differences among countries besides retirement ages. The correlations do not prove causation. They also, he added, do not prove that there is a clinical significance to the changes in scores on memory tests.</p>\n<p>All true, said Richard Suzman, associate director for behavioral and social research at the National Institute on Aging.</p>\n<p>Nonetheless, he said, &ldquo;it&rsquo;s a strong finding; it&rsquo;s a big effect.&rdquo;</p>\n<p>If work does help maintain cognitive functioning, it will be important to find out what aspect of work is doing that, Dr. Suzman said. &ldquo;Is it the social engagement and interaction or the cognitive component of work, or is it the aerobic component of work?&rdquo; he asked. &ldquo;Or is it the absence of what happens when you retire, which could be increased TV watching?&rdquo;</p>\n<p>&ldquo;It&rsquo;s quite convincing, but it&rsquo;s not the complete story,&rdquo; Dr. Suzman said. &ldquo;This is an opening shot. But it&rsquo;s got to be followed up.&rdquo;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZDsxeieoXi6uvWAxg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 6e-06, "legacy": true, "legacyId": "3735", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-14T23:31:49.777Z", "modifiedAt": null, "url": null, "title": "A 50-minute introduction to probability", "slug": "a-50-minute-introduction-to-probability-1", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:16.320Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "5954HqXu7aiBr9pZo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kdn9E7F64yd5wg8WH/a-50-minute-introduction-to-probability-1", "pageUrlRelative": "/posts/kdn9E7F64yd5wg8WH/a-50-minute-introduction-to-probability-1", "linkUrl": "https://www.lesswrong.com/posts/kdn9E7F64yd5wg8WH/a-50-minute-introduction-to-probability-1", "postedAtFormatted": "Thursday, October 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%2050-minute%20introduction%20to%20probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%2050-minute%20introduction%20to%20probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkdn9E7F64yd5wg8WH%2Fa-50-minute-introduction-to-probability-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%2050-minute%20introduction%20to%20probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkdn9E7F64yd5wg8WH%2Fa-50-minute-introduction-to-probability-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkdn9E7F64yd5wg8WH%2Fa-50-minute-introduction-to-probability-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<html><head></head><body></body></html>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kdn9E7F64yd5wg8WH", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "3738", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-15T01:04:57.530Z", "modifiedAt": null, "url": null, "title": "Lifelogging: the recording device", "slug": "lifelogging-the-recording-device", "viewCount": null, "lastCommentedAt": "2020-05-13T22:52:10.201Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o6BfKyWQC6yv8G7bc/lifelogging-the-recording-device", "pageUrlRelative": "/posts/o6BfKyWQC6yv8G7bc/lifelogging-the-recording-device", "linkUrl": "https://www.lesswrong.com/posts/o6BfKyWQC6yv8G7bc/lifelogging-the-recording-device", "postedAtFormatted": "Friday, October 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lifelogging%3A%20the%20recording%20device&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALifelogging%3A%20the%20recording%20device%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo6BfKyWQC6yv8G7bc%2Flifelogging-the-recording-device%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lifelogging%3A%20the%20recording%20device%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo6BfKyWQC6yv8G7bc%2Flifelogging-the-recording-device", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo6BfKyWQC6yv8G7bc%2Flifelogging-the-recording-device", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 401, "htmlBody": "<p>The old idea of <a href=\"http://en.wikipedia.org/wiki/Lifelogging\">lifelogging</a> <em>seems</em> to be a reality now. It has the potential to be quite useful, and not just in distant contrived scenarios like <a href=\"/lw/1ay/is_cryonics_necessary_writing_yourself_into_the/\">cryonics or being recreated by an AI</a>.</p>\n<p>One of the classic objections was that we couldn&rsquo;t afford to store the many gigabytes - possibly <em>hundreds</em> of gigabytes a year! - such a practice would generate, but <a href=\"http://forre.st/storage#sata\">right now</a> you can buy 1 terabyte for &lt;$50. And there&rsquo;s no end in sight to whatever Moore&rsquo;s law has been governing hard-drives over the past decade or two.</p>\n<p>But how is one to record it? That seems to be the rub. All the storage space we could want, all sorts of new formats like WebM or Dirac or x264 to store the videos in - but what camera generates the data in the first place?</p>\n<p>We don&rsquo;t care about sleep time, so we don&rsquo;t need any more than 16 hours or so of recording a day. We can probably get away with 12. Even 8 might be enough (to record yourself on the job - or off). An encoded compressed video might be 1 megabyte a minute or 60 megabytes an hour, but let&rsquo;s be generous and assume 15x worse than that, or about 1 gigabyte an hour. So perhaps 16 gigabytes.</p>\n<p>16 gigabytes of Flash costs <a href=\"http://forre.st/storage#flash\">$40</a> or less. So that&rsquo;s not an issue either.</p>\n<p>And presumably optics and microprocessors are very cheap given the incredible popularity of web cameras, digital cameras, digital camcorders and whatnot over the last decade.</p>\n<p>But for all that, I can&rsquo;t seem to find a mini-camcorder which will record even 8 hours and be a useful lifelogger!</p>\n<ul>\n<li><a href=\"http://www.amazon.com/dp/B00400O8PO\">Looxcie</a> costs an absurd $200, and has no more than 4 hours battery life</li>\n<li><a href=\"http://www.woot.com/Forums/ViewPost.aspx?PostID=4184544\">Flip MinoHD</a> costs a far more reasonable $70 but only gets 2 hours of battery life; the other <a href=\"http://www.theflip.com/en-us/Products/specs.aspx\">Flips</a> do little better</li>\n<li>the <a href=\"http://www.ucorder.com/IRDC250.html\">IRDC250 uCorder</a> is $90, possibly better video than the Looxcie, and perfect - except for its 2 hour battery life</li>\n<li>the <a href=\"http://www.chinavasion.com/product_info.php/pName/videoclipper-handsfree-mini-video-camera-clipon/\">Video Clipper</a> is similar to the uCorder but claims better battery life &amp; to be just $44</li>\n</ul>\n<p>Am I wrong? Are there existing products? It seems to me that it ought to be perfectly possible to take something like the uCorder, slap in $110 of batteries, and get it up to 8 or 12 hours&rsquo; life. But I have yet to find such a thing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"QZn3ujmgnvStbuLEc": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o6BfKyWQC6yv8G7bc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 14, "extendedScore": null, "score": 6.350178585859178e-07, "legacy": true, "legacyId": "3739", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iRMBxyrKubkwhxccq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-15T05:39:18.919Z", "modifiedAt": null, "url": null, "title": "Melbourne Less Wrong Meetup for November", "slug": "melbourne-less-wrong-meetup-for-november", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:17.196Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Patrick", "createdAt": "2009-02-27T08:09:44.663Z", "isAdmin": false, "displayName": "Patrick"}, "userId": "KC7mjSorWj2XsdL3v", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QFoBXL3P6Sf5LGqyP/melbourne-less-wrong-meetup-for-november", "pageUrlRelative": "/posts/QFoBXL3P6Sf5LGqyP/melbourne-less-wrong-meetup-for-november", "linkUrl": "https://www.lesswrong.com/posts/QFoBXL3P6Sf5LGqyP/melbourne-less-wrong-meetup-for-november", "postedAtFormatted": "Friday, October 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Melbourne%20Less%20Wrong%20Meetup%20for%20November&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMelbourne%20Less%20Wrong%20Meetup%20for%20November%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQFoBXL3P6Sf5LGqyP%2Fmelbourne-less-wrong-meetup-for-november%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Melbourne%20Less%20Wrong%20Meetup%20for%20November%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQFoBXL3P6Sf5LGqyP%2Fmelbourne-less-wrong-meetup-for-november", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQFoBXL3P6Sf5LGqyP%2Fmelbourne-less-wrong-meetup-for-november", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 33, "htmlBody": "<p>Hot on the heels of October's meetup, I present to you the details for November's meetup!</p>\n<p><strong>Date: Friday, November 5th</strong></p>\n<p><strong>Place: <a href=\"http://foursquare.com/venue/368509\">Don Tojo</a></strong></p>\n<p><strong>Time: 6-9pm</strong></p>\n<p>Please comment to say if you're attending, and also to suggest activities.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QFoBXL3P6Sf5LGqyP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 6.350824008985378e-07, "legacy": true, "legacyId": "3740", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Hot on the heels of October's meetup, I present to you the details for November's meetup!</p>\n<p><strong id=\"Date__Friday__November_5th\">Date: Friday, November 5th</strong></p>\n<p><strong id=\"Place__Don_Tojo\">Place: <a href=\"http://foursquare.com/venue/368509\">Don Tojo</a></strong></p>\n<p><strong id=\"Time__6_9pm\">Time: 6-9pm</strong></p>\n<p>Please comment to say if you're attending, and also to suggest activities.</p>", "sections": [{"title": "Date: Friday, November 5th", "anchor": "Date__Friday__November_5th", "level": 1}, {"title": "Place: Don Tojo", "anchor": "Place__Don_Tojo", "level": 1}, {"title": "Time: 6-9pm", "anchor": "Time__6_9pm", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-15T06:41:34.772Z", "modifiedAt": null, "url": null, "title": "You don't need barefoot shoes to start walking differently. ", "slug": "you-don-t-need-barefoot-shoes-to-start-walking-differently", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:28.063Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FvaPCfZLXv5uhZ5wz/you-don-t-need-barefoot-shoes-to-start-walking-differently", "pageUrlRelative": "/posts/FvaPCfZLXv5uhZ5wz/you-don-t-need-barefoot-shoes-to-start-walking-differently", "linkUrl": "https://www.lesswrong.com/posts/FvaPCfZLXv5uhZ5wz/you-don-t-need-barefoot-shoes-to-start-walking-differently", "postedAtFormatted": "Friday, October 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20don't%20need%20barefoot%20shoes%20to%20start%20walking%20differently.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20don't%20need%20barefoot%20shoes%20to%20start%20walking%20differently.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFvaPCfZLXv5uhZ5wz%2Fyou-don-t-need-barefoot-shoes-to-start-walking-differently%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20don't%20need%20barefoot%20shoes%20to%20start%20walking%20differently.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFvaPCfZLXv5uhZ5wz%2Fyou-don-t-need-barefoot-shoes-to-start-walking-differently", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFvaPCfZLXv5uhZ5wz%2Fyou-don-t-need-barefoot-shoes-to-start-walking-differently", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>I bought into the hype and decided that I was going to get a pair of Vibrams. My intention was not to use them as running shoes, but as everyday walking shoes. Then my girlfriend told me that I wasn't allowed, that they were too hideous to be worn in public. In almost two years together, this is the only thing that she has forbidden me from doing, and I regularly do completely ridiculous things so I deferred to her judgement. I thought about getting barefoot dress shoes but $150 seemed excessive.</p>\n<p>I then decided that I didn't need fancy shoes to stop walking heel first. I started wearing a pair of casual brown slip-on shoes with a fair amount of cushioning but little support. From the start, I thought it felt good to actually walk on the balls of my feet.</p>\n<p>It took three weeks for my feet to stop hurting, but now I naturally walk on the balls of my feet. You can do the same thing. It will probably be easier in a light pair of shoes rather than a clunky pair of dress shoes or boots.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FvaPCfZLXv5uhZ5wz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -4, "extendedScore": null, "score": 6.3509705015789e-07, "legacy": true, "legacyId": "3741", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-15T11:17:33.191Z", "modifiedAt": null, "url": null, "title": "Picking your battles", "slug": "picking-your-battles", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:19.220Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SeventhNadir", "createdAt": "2010-04-18T04:11:11.485Z", "isAdmin": false, "displayName": "SeventhNadir"}, "userId": "adAXuo6KKGxTap3SN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4DFiJKeGc5XFGCcSX/picking-your-battles", "pageUrlRelative": "/posts/4DFiJKeGc5XFGCcSX/picking-your-battles", "linkUrl": "https://www.lesswrong.com/posts/4DFiJKeGc5XFGCcSX/picking-your-battles", "postedAtFormatted": "Friday, October 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Picking%20your%20battles&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APicking%20your%20battles%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4DFiJKeGc5XFGCcSX%2Fpicking-your-battles%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Picking%20your%20battles%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4DFiJKeGc5XFGCcSX%2Fpicking-your-battles", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4DFiJKeGc5XFGCcSX%2Fpicking-your-battles", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 303, "htmlBody": "<p>I think that raising the sanity waterline is a worthwhile goal, but picking your battles is absolutely necessary. It doesn't matter how formidable your argument is if you're arguing in the comments of a youtube video, you've lost by default. So where is the line in the&nbsp; sand? Where would you feel compelled to take action, and to what lengths would you go to? What price would you be willing to pay?</p>\n<p>I'm a psychology student, third year and currently doing a unit called \"cultural psychology\". The lecturer has advanced notions of \"multiple truths\" and how \"reality is socially constructed\". To quote him directly in regards to this:</p>\n<p><em>\"There is a tendency for those who believe in one reality to use the physical world as a basis for argument, while those who believe in multiple realities use the social world. Even in physics we have 'reality' changing as you get closer to the speed of light, and the laws of physics don't apply prior to the big bang. These are fairly extreme situations. In this course we are dealing with social realities and the point is that different cultures operate in worlds that can be quite different. To see this purely as a perspective risks the dominant social grouping seeing their reality as the true reality, and others as having a different perspective on that reality. The assumption that cultures can have different realities places every on a level playing field with a dominant culture calling all the shots.\"</em></p>\n<p>You can see in the last line the conclusion he wants his premises to support. The exercise is not to pick his argument apart, find all the holes and write a crushing riposte (although you can if you're so inclined).</p>\n<p>&nbsp;</p>\n<p>The question is, if the goal is to raise the sanity waterline, is this a battle worth picking?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4DFiJKeGc5XFGCcSX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 6.351619872726399e-07, "legacy": true, "legacyId": "3742", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-15T13:13:25.322Z", "modifiedAt": null, "url": null, "title": "Human performance, psychometry, and baseball statistics", "slug": "human-performance-psychometry-and-baseball-statistics", "viewCount": null, "lastCommentedAt": "2019-04-19T14:33:57.256Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Craig_Heldreth", "createdAt": "2010-06-14T23:30:28.110Z", "isAdmin": false, "displayName": "Craig_Heldreth"}, "userId": "hhKowsjZBQSyBE6c5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e8qF4w56P62DXimnE/human-performance-psychometry-and-baseball-statistics", "pageUrlRelative": "/posts/e8qF4w56P62DXimnE/human-performance-psychometry-and-baseball-statistics", "linkUrl": "https://www.lesswrong.com/posts/e8qF4w56P62DXimnE/human-performance-psychometry-and-baseball-statistics", "postedAtFormatted": "Friday, October 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Human%20performance%2C%20psychometry%2C%20and%20baseball%20statistics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHuman%20performance%2C%20psychometry%2C%20and%20baseball%20statistics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe8qF4w56P62DXimnE%2Fhuman-performance-psychometry-and-baseball-statistics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Human%20performance%2C%20psychometry%2C%20and%20baseball%20statistics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe8qF4w56P62DXimnE%2Fhuman-performance-psychometry-and-baseball-statistics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe8qF4w56P62DXimnE%2Fhuman-performance-psychometry-and-baseball-statistics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1826, "htmlBody": "<p><em><span style=\"color: #000000;\">I. Performance levels and age</span></em></p>\n<p>Human ambition for achievement in modest measure gives meaning to our lives, unless one is an existentialist pessimist like Schopenhauer who taught that life with all its suffering and cruelty simply should not be. Psychologists study our achievements under a number of different descriptions--testing for IQ, motivation, creativity, others. As part of my current career transition, I have been examining my own goals closely, and have recently read a fair amount on these topics which are variable in their evidence.<br /><br />A useful collection of numerical data on the subject of human performance is the collection of Major League Baseball player performance statistics--the batting averages, number home runs, runs batted in, slugging percentage--of the many thousands of participants in the hundred years since detailed statistical records have been kept and studied by the players, journalists, and fans of the sport. The advantage of examining issues like these from the angle of Major League Baseball player performance statistics is the enormous sample size of accurately measured and archived data.<br /><br />The current senior authority in this field is <a href=\"http://en.wikipedia.org/wiki/Bill_James\">Bill James</a>, who now works for the Boston Red Sox; for the first twenty-five years of his activity as a baseball statistician James was not employed by any of the teams. It took him a long time to find a hearing for his views on the inside of the industry, although the fans started buying his books as soon as he began writing them.<br /><br />In one of the early editions of his <span style=\"font-style: italic;\">Baseball Abstract</span>, James discussed the biggest fallacies that managers and executives held regarding the achievements of baseball players. He was adamant about the most obvious misunderstood fact of player performance: it is sharply peaked at age 27 and decreases rapidly, so rapidly that only the very best players were still useful at the age of 35. He was able to observe only one executive that seemed to intuit this--a man whose sole management strategy was to trade everybody over the age of 30 for the best available player under the age of 30 he could acquire.<a id=\"more\"></a><br /><br />There is a fair amount of more formal academic research on this issue. It is described in the literature as the field of <span style=\"font-style: italic;\">Age and Achievement</span>. The dean of the psychologists studying Age and Achievement is Dean Simonton. A decent overview of their findings is <a href=\"http://www.thefreelibrary.com/Age+and+Achievement%3A+Talent+Development+Across+the+Life+Span.%28Review%29-a074571891\">here</a>. This is a meta-study of hundreds of individual studies. Many fields and many metrics are sampled. There is one repeated finding. Performance starts low at a young age and steadily increases along a curve which bears a resemblance to a Gaussian bell-shaped curve, peaks, and then declines. The decline is not as rapid as the rise (it is not a symmetric bell shape; it is steeply inclining from the left to the peak and gently declining form the peak to the right), but it is inevitably seen everywhere. The age of peak achievement varies, depending on the field. Baseball players peak at 27 (the curves from the psychology publications look exactly like the curve published by Bill James in his Abstract), business executives peak at 60, and physicists peak at age 35. Shakespearian actors peak late and rock stars peak early. These are statistical results and individual outliers abound. You, the individual physicist, may not be over the hill at 40, but this is the way to bet.<br /><br />My hometown major league baseball franchise, the Houston Astros, recently had this empirical law verified for themselves in real time, and the hard way. They invested the bulk of their payroll budget on three players: Miguel Tejada, Carlos Lee, and Lance Berkman. All three were over the age of 30, i.e., definitely into their decline phase. When their performance declined more rapidly than predicted, the team lost many more games than they were planning for. They had a contending team's payroll and big plans, but now Tejada and Berkman are gone and they are rebuilding. In an attempt to cut losses, they traded their (prime-age) star pitcher for young players.<br /><br />A recent post on <span style=\"font-style: italic;\">Hacker News</span>, <a href=\"http://news.ycombinator.com/item?id=1641763\">Silicon Valley's Dark Secret: It's all about Age</a>, generated 120 comments of heated discussion about institutional age discrimination and the unappreciated value of experience. The consensus view expressed there is young programmers have to advance into management or become unemployable near age 50.<br /><br />It could perhaps be seen as an example of Evolutionary Biology. We are in an ecosystem. The ecosystem selects for fitness. What is sometimes misunderstood is the ecosystem does not select for absolute fitness, but for fitness specific to a niche. If the available niches in this \"ecosystem\" are for 40 year-old-brains, and there aren't any niches for 50 year-old-brains, then some fully fit brains (in an absolute sense) are going to be out of employment opportunities. Faced with a system like this, the job seeker may have to be clever at finding ever narrower niches to squeeze themself into.<br /><br />One of the moderators at Hacker News, Paul Graham, is a software startup venture capitalist. He is accused in the thread of unconcealed age discrimination--that he will not invest in entrepreneurs over 38, and claiming that nobody over 25 will ever learn Lisp. If you are a forty-year-old physicist and you want to learn Lisp and get venture capital funding for your business plan--well, good luck with that!<br /><br /><em>II. Time to mastery</em></p>\n<p>This leads directly into my second topic within my larger subject of human performance, psychometry, and baseball statistics. Learning curves and estimated time for mastery. To continue with the above example, assuming you want to master Lisp, how much of your time should you plan to allocate for the task? K. Anders Ericson is the author of the <a href=\"http://www.coachingmanagement.nl/The%20Making%20of%20an%20Expert.pdf\">relevant research findings</a>. At a crude level of approximation, something like that takes ten thousand hours. This is a result I was first exposed to many years ago in the context of Buddhist meditation, in an Esalen conference presented by Helen Palmer (mostly known for her work on the <a href=\"http://www.amazon.com/Enneagram-Understanding-Yourself-Others-Your/dp/0062506838/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1283454312&amp;sr=1-1\">Eneagram</a>). She reported that to become skilled at Zen meditation requires ten thousand hours of practice. In the University of Wisconsin brain imaging meditation <a href=\"http://www.plosone.org/article/info:doi/10.1371/journal.pone.0001897\">study</a>, the subjects were Tibetan monks who had all logged a minimum of ten thousand hours of practice. The ten thousand hours of practice requirement was also reported popularly by Malcom Gladwell in his best-selling book <span style=\"font-style: italic;\">Outliers</span>. Another take on this: <a href=\"http://norvig.com/21-days.html\">Teach Yourself Programming in Ten Years</a>. Ten thousand hours of 40-hour-weeks is five years, not ten; the number is not precise, but the idea is consistent that ambitious projects take a daunting amount of time.<br /><br />One of my dance teachers was fond of reminding me that practice does not make perfect. Only perfect practice can make you perfect. For most of us even that is an exaggeration. I think we can reliably predict that ten thousand hours of very good practice will make you very good if you first possess an average or above-average amount of raw aptitude..<br /><br /><em>III. Distribution of performance across a population, replacement-level player</em><br /><br />The second biggest fallacy among baseball personnel managers, according to Bill James, is they do not understand how ability is distributed amongst professional baseball players. He defines the concept of <span style=\"font-style: italic;\">replacement-level player</span>, and insists the vast majority of the fellows working in the Major Leagues are easily, quickly, replaceable. His reasoning is simple.<br /><br />If you have a random selection of humans and measure nearly any measurable trait--height, weight, speed, strength, reflex time--the frequency plot will be the familiar bell shape Gaussian curve. People playing baseball professionally are an extreme non-random sample. 98% of the left-hand portion of the curve is gone, because none of those people have the physical requirements to get employment playing baseball. The resulting distribution is a truncated Gaussian distribution, with few at the highest levels, and the vast majority of participants of nearly indistinguishable quality. When performance is creamed at stage after stage after stage, little league to high school to college to minor leagues to the majors, almost all the remaining players are excellent and interchangeable.<br /><br />If you are managing a corporation and you only hire candidates with golden resumes you have a truncated Gaussian distribution of talent. If in your evaluation process you shove those people into a Gaussian distribution, Bill James says you are doing it totally wrong. Another common mistake is that managers think there is something magical about \"major league\" talent, that some guys have it (as Thomas Wolfe referred to the \"right stuff\") and some do not, and they mislabel players who could help them win baseball games as not having it, due to the circumstantial variations of where the players have found themselves employed up until now. Organizations that hire top talent and pay high salaries have far more options than they generally presume. Nearly every single person working for your company is easily replaceable.<br /><br />There is a story, possibly apocryphal, about Benoit Mandelbrot and his early preoccupation with financial market data. His questioner thought finance was a fuzzy science and hard scientific data really ought to be much more attractive to his scientific temperament. Mandelbrot explained that the great feature of studying financial data was that there was so much of it, and it was thus endlessly fascinating. Many statisticians have a similar fondness for baseball statistics. It is reliably recorded, unambiguous in definition, and there is so much of it. Many subtle statistics results are best explained in the context of baseball statistics, and there may be unknown statistical theorems sitting in the archives waiting to be extracted by clever statisticians. The wikipedia page on Stein's paradox (first published by Charles Stein in 1956) has a reference to a well-known (well-known to baseball statisticians, anyway) <a href=\"http://www-stat.stanford.edu/~ckirby/brad/other/Article1977.pdf\">article</a> from the May 1977 issue of <span style=\"font-style: italic;\">Scientific American </span>using baseball statistics to illustrate Stein's paradox.<br /><br />After my article was nearly finished, I stumbled upon this \"news\" in the New York Times Sports section:<br /><br /><a href=\"http://www.nytimes.com/2010/10/03/sports/baseball/03hitters.html?_r=1\">Sniffing .300, Hitters Hunker Down on Last Chances</a>. (Here they are presenting research from a couple of economists from U. Pennsylvania's Wharton School of Business. The academic publication is <a href=\"http://faculty.chicagobooth.edu/devin.pope/research/pdf/Psych%20Science%20Submission.pdf\">here</a>.)<br /><br />The preceding should be of interest to anybody who is interested in the subjects of human achievement, psychometry and baseball statistics. My own interest is narrower and the lesson I personally draw is a hybrid from the sequence of lessons here. I have an ambitious scope for the company I am building. Ten thousand hours is close to the limit I am choosing for myself as the point when I will write off these lessons and losses (if they be) and go back to rejoin the American corporation employment market.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"t7t9nW6BtJhfGNSR6": 1, "x3zyEPFaJANB2BHmP": 1, "iNqgMuoewHKMhhXAp": 1, "bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e8qF4w56P62DXimnE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 33, "extendedScore": null, "score": 6.351892549245452e-07, "legacy": true, "legacyId": "3743", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-15T18:04:35.160Z", "modifiedAt": null, "url": null, "title": "Rationality Dojo", "slug": "rationality-dojo", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:21.929Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "freyley", "createdAt": "2009-02-27T18:49:48.880Z", "isAdmin": false, "displayName": "freyley"}, "userId": "LXoeJdNeQDPy32ki6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bnGkuqaBfbgsexbfn/rationality-dojo", "pageUrlRelative": "/posts/bnGkuqaBfbgsexbfn/rationality-dojo", "linkUrl": "https://www.lesswrong.com/posts/bnGkuqaBfbgsexbfn/rationality-dojo", "postedAtFormatted": "Friday, October 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Dojo&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Dojo%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbnGkuqaBfbgsexbfn%2Frationality-dojo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Dojo%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbnGkuqaBfbgsexbfn%2Frationality-dojo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbnGkuqaBfbgsexbfn%2Frationality-dojo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 143, "htmlBody": "<p>Last night, here in Portland (OR), some friends and I got together to try to start Rationality Dojo. We talked about it for a while and came up with exactly 4 exercises that we could readily practice:</p>\n<ol>\n<li>Play <a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\">Paranoid Debating</a></li>\n<li>Play the AI-Box experiment</li>\n<li>Read <a href=\"http://www.fanfiction.net/s/5782108/1/Harry_Potter_and_the_Methods_of_Rationality\">Harry Potter and the Methods of Rationality</a></li>\n<li>Write fanfiction in the style of #3</li>\n</ol>\n<p>We also had a whole bunch of semi-formed ideas about selecting a target (happiness, health) and optimizing it a month at a time. Starting a dojo, in a time before organized martial arts, was surely incredibly difficult. I hope we can accrete exercises rather than require a single sensei to invent the majority of the discipline.  So I've added a <a href=\"http://wiki.lesswrong.com/wiki/Category:Rationality_Dojo\">category</a> to the wiki, and I'm asking here. Do you have ideas or refinements for exercises to fit within rationality dojo?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bnGkuqaBfbgsexbfn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "3744", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-16T02:20:00.095Z", "modifiedAt": null, "url": null, "title": "Discuss: Original Seeing Practices", "slug": "discuss-original-seeing-practices", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:57.766Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "YpTmfmnMjgakwFRQQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mbYSiC4sPuz4vQxyB/discuss-original-seeing-practices", "pageUrlRelative": "/posts/mbYSiC4sPuz4vQxyB/discuss-original-seeing-practices", "linkUrl": "https://www.lesswrong.com/posts/mbYSiC4sPuz4vQxyB/discuss-original-seeing-practices", "postedAtFormatted": "Saturday, October 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Discuss%3A%20Original%20Seeing%20Practices&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADiscuss%3A%20Original%20Seeing%20Practices%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmbYSiC4sPuz4vQxyB%2Fdiscuss-original-seeing-practices%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Discuss%3A%20Original%20Seeing%20Practices%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmbYSiC4sPuz4vQxyB%2Fdiscuss-original-seeing-practices", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmbYSiC4sPuz4vQxyB%2Fdiscuss-original-seeing-practices", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<p><a href=\"/lw/k7/original_seeing/\"></a>What can we practice to help us think <a href=\"/lw/k7/original_seeing/\">original thoughts</a>? How can we see beyond our <a href=\"/lw/k5/cached_thoughts/\">cached responses</a>?</p>\n<p>This is a place to share life patterns, techniques, or exercises that help you (either occasionally or regularly) think new thoughts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mbYSiC4sPuz4vQxyB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 3, "extendedScore": null, "score": 6.353744170751509e-07, "legacy": true, "legacyId": "3640", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SA79JMXKWke32A3hG", "2MD3NMLBPCqPfnfre"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-16T16:00:05.537Z", "modifiedAt": null, "url": null, "title": "Mixed strategy Nash equilibrium", "slug": "mixed-strategy-nash-equilibrium", "viewCount": null, "lastCommentedAt": "2010-10-25T19:41:13.752Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Meni_Rosenfeld", "createdAt": "2010-04-19T15:09:59.043Z", "isAdmin": false, "displayName": "Meni_Rosenfeld"}, "userId": "84ebCjWmavqjgjAfM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YyjYPts5hnqnBmBue/mixed-strategy-nash-equilibrium", "pageUrlRelative": "/posts/YyjYPts5hnqnBmBue/mixed-strategy-nash-equilibrium", "linkUrl": "https://www.lesswrong.com/posts/YyjYPts5hnqnBmBue/mixed-strategy-nash-equilibrium", "postedAtFormatted": "Saturday, October 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mixed%20strategy%20Nash%20equilibrium&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMixed%20strategy%20Nash%20equilibrium%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYyjYPts5hnqnBmBue%2Fmixed-strategy-nash-equilibrium%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mixed%20strategy%20Nash%20equilibrium%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYyjYPts5hnqnBmBue%2Fmixed-strategy-nash-equilibrium", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYyjYPts5hnqnBmBue%2Fmixed-strategy-nash-equilibrium", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1231, "htmlBody": "<p><strong>Inspired by:</strong> <a href=\"/lw/2vc/swords_and_armor_a_game_theory_thought_experiment/\">Swords and Armor: A Game Theory Thought Experiment</a><br /><br />Recently, nick012000 has posted <a href=\"/lw/2vc/swords_and_armor_a_game_theory_thought_experiment/\">Swords and Armor: A Game Theory Thought Experiment</a>. I was disappointed to see many confused replies to this post, even after a complete solution was given by Steve_Rayhawk. I thought someone really ought to post an explanation about mixed strategy Nash equilibria. Then I figured that that someone may as well be me.<br /><br />I will assume readers are familiar with the concepts of a game (a setting with several players, each having a choice of strategies to take and a payoff which depends on the strategies taken by all players) and of a Nash equilibrium (an \"optimal\" assignment of strategies such that, if everyone plays their assigned strategy, no player will have a reason to switch to a different strategy). Some games, like the famous prisoner's dilemma, have a Nash equilibrium in so-called \"pure strategies\" (as opposed to mixed strategies, to be introduced later). Consider, however, the following variant of the <a href=\"http://en.wikipedia.org/wiki/Matching_pennies\">matching pennies</a> game:<br /><br />Player 1 is a general leading an attacking army, and player 2 is the general of the defending army. The attacker can attack from the east or west, and the defender can concentrate his defenses on the east or west. By the time each side learns the strategy of its enemy, it is too late to switch strategies. Attacking where the defenses aren't concentrated gives a great advantage; additionally, due to unspecified tactical circumstances, attacking from the east gives a slight advantage. The sides have no interest in cooperating, so this is a zero-sum game (what one side wins, the other loses).<br /><br />This elaborate description can be summarized in the following payoff matrix (these payoffs are for the attacker; the defender's payoffs are their negatives):</p>\n<table style=\"height: 74px;\" border=\"2\" width=\"180\">\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td align=\"center\">2: East</td>\n<td align=\"center\">2: West</td>\n</tr>\n<tr>\n<td>1: East</td>\n<td align=\"center\">-1</td>\n<td align=\"center\">2</td>\n</tr>\n<tr>\n<td>1: West</td>\n<td align=\"center\">1</td>\n<td align=\"center\">-2</td>\n</tr>\n</tbody>\n</table>\n<p><a id=\"more\"></a></p>\n<p>What strategy should each side play? The attacker can think, \"Overall, going east is advantageous. So I'll go east.\" The defender, anticipating this, might say, \"Surely they will go east, so that's where I'll wait for them.\" But after some deliberation, the attacker will realize this, and say \"They will expect me from the east! I'll surprise them and go west.\" You can see where this is going - the defender will think \"they'll try to be smart and go west; I'll be smarter and be ready for them\", the attacker will think \"I was right the first time, east is the way to go\" and so on, ad infinitum.<br /><br />Indeed, looking at the table does not reveal any obvious Nash equilibrium. Every assignment of strategies, represented as a square in the table, will leave one side preferring the alternative. So, are the sides deadlocked in an infinite recursion of trying to outsmart each other? No. They have the option of choosing a strategy randomly.<br /><br />Suppose the attacker decides to toss a coin, and go east if it lands heads, and west for tails. Suppose also that the defender, with his mastery of psychology, manages to accurately predict the attacker's bizarre behavior. What can he do with this knowledge? He cannot predict the outcome of a coin toss, so he makes do with calculating the expected outcome for each of his (the defender's) strategies. And it can be easily seen that no matter what the defender does, the expectation is 0.<br /><br />Similarly, suppose the defender consults his preferred (P)RNG so that he defends east with probability 2/3, west with probability 1/3, and suppose that the attacker anticipates this. Again, either of the attacker's strategies will give an expected gain of 0.<br /><br />This randomized behavior, which is a combination of strategies from which one is randomly chosen with specified probabilities, is called a \"mixed strategy\". They are typically denoted as a vector listing the probability for choosing each pure strategy, so the defender's suggested strategy is (2/3, 1/3).<br /><br />What we have seen here, is that by a clever choice of mixed strategy, each side can make sure they cannot be outsmarted! By constraining ourselves to rely on randomness for deciding on an action, we denied our opponent the ability to predict it and counter it. If yourself you don't know what you'll do, there's no way your opponent will know. We conclude that sometimes, acting randomly is the rational action to take.<br /><br />As we've seen, for each player we have that if he uses his suggested mixed strategy, then it doesn't matter what the other player will do. The corollary is that if the attacker plays (1/2, 1/2) and the defender plays (2/3, 1/3), then no player will have a reason to switch to a different strategy - this is a Nash equilibrium!<br /><br />Some more points of interest: \"Just\" randomizing won't do - you have to pick the right probabilities. The exact probabilities of the mixed strategy Nash equilibria, and the resulting payoff, depend on the specifics of the payoff matrix. In my example, the defender needs a high probability of defending east to prevent the attacker from exercising his advantage, but the symmetry is such that the attacker chooses with even odds. In games with more strategies per player, an equilibrium mixed strategy may be supported on (have positive probability for) all, several, or one of the pure strategies. If all players apply a Nash equilibrium, then any player can switch to any one of the pure strategies supporting his mixed strategy without changing his expected payoff; switching to any other strategy either decreases the payoff or leaves it unchanged.<br /><br />Perhaps most interesting of all is Nash's theorem, saying that every finite game has a mixed strategy Nash equilibrium! Our original problem, that some games have no equilibrium, is solved completely once we move to mixed strategies.</p>\n<p>One thing should be kept in mind. A Nash equilibrium strategy, much like a minimax strategy, is \"safe\". It makes sure your expected payoff won't be too low no matter how clever your opponent is. But what if you don't want to be safe - what if you want to <strong>win</strong>? If you have good reason to believe you are smarter than your opponent, that he will play a non-equilibrium strategy you'll be able to predict, then go ahead and counter that strategy. Nash equilibria are for smart people facing smarter people.</p>\n<p>In fact, it is possible that some of the comments I called confused earlier were fully aware of these issues and coming from this \"post-Nash\" view. To them I apologize.&nbsp;&nbsp;</p>\n<p>Examples where mixed strategies are crucial are plentiful. I'll give one more - the <a href=\"http://en.wikipedia.org/wiki/Volunteer%27s_dilemma\">volunteer's dilemma</a>. A group of people are about to suffer greatly from some misfortune, unless at least one of them volunteers to take a slightly inconvenient action. They cannot communicate to coordinate the volunteering. If everyone uses the same deterministic strategy, then either all or none will volunteer, neither of which is stable. But they have a mixed strategy equilibrium which, by carefully balancing the risks of needlessly volunteering and having nobody volunteer, leaves everyone with an expected penalty equal to just the inconvenience of volunteering. Not as good as having just one person volunteer, but at least it's stable.</p>\n<p>For further reading, Wikipedia has many articles about game theory and Nash equilibria. Also, Perplexed made <a href=\"/lw/2lm/how_can_we_compare_decision_theories/2gtp?c=1\">this related comment</a> recently.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YyjYPts5hnqnBmBue", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 59, "extendedScore": null, "score": 0.000103, "legacy": true, "legacyId": "3736", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3pnhkrfpj4rZkfqN2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-10-16T16:00:05.537Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-16T16:01:09.279Z", "modifiedAt": null, "url": null, "title": "Monetary Incentives and Performance", "slug": "monetary-incentives-and-performance", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:33.502Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E7dTqbLRhtnDDcHpY/monetary-incentives-and-performance", "pageUrlRelative": "/posts/E7dTqbLRhtnDDcHpY/monetary-incentives-and-performance", "linkUrl": "https://www.lesswrong.com/posts/E7dTqbLRhtnDDcHpY/monetary-incentives-and-performance", "postedAtFormatted": "Saturday, October 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Monetary%20Incentives%20and%20Performance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMonetary%20Incentives%20and%20Performance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE7dTqbLRhtnDDcHpY%2Fmonetary-incentives-and-performance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Monetary%20Incentives%20and%20Performance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE7dTqbLRhtnDDcHpY%2Fmonetary-incentives-and-performance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE7dTqbLRhtnDDcHpY%2Fmonetary-incentives-and-performance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<p>I've been thinking about incorporating my <a href=\"/r/discussion/lw/2vb/vanity_and_ambition_in_mathematics/\">Vanity and Ambition in Mathematics</a> into a top level posting. If possible I would like to situation my remarks and the quotations that I cite with respect to the existing experimental psychology literature. When I've discussed the material in the aforementioned article with people in psychology they've sometimes made reference to recent findings that monetary incentives <em>reduce </em>performance on certain kinds of tasks, perhaps suggesting that intrinsic rather than extrinsic motivation is key for performance on certain kinds of tasks.</p>\n<p>I'll do my own research, but does anybody know of any relevant studies?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E7dTqbLRhtnDDcHpY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 6.355678197565148e-07, "legacy": true, "legacyId": "3748", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CmkvFtw5vvyD5DyDo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-17T13:34:52.398Z", "modifiedAt": null, "url": null, "title": "Re: sub-reddits", "slug": "re-sub-reddits", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:05.338Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hhrv8aAcmkzJxvP58/re-sub-reddits", "pageUrlRelative": "/posts/hhrv8aAcmkzJxvP58/re-sub-reddits", "linkUrl": "https://www.lesswrong.com/posts/hhrv8aAcmkzJxvP58/re-sub-reddits", "postedAtFormatted": "Sunday, October 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Re%3A%20sub-reddits&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARe%3A%20sub-reddits%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhhrv8aAcmkzJxvP58%2Fre-sub-reddits%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Re%3A%20sub-reddits%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhhrv8aAcmkzJxvP58%2Fre-sub-reddits", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhhrv8aAcmkzJxvP58%2Fre-sub-reddits", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 139, "htmlBody": "<p>A while back, I polled the community on <a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/2m5t?c=1\">the possibility of subreddits</a>. Most people said they wanted them, and I said I'd investigate.</p>\n<p>I talked to a couple of people and eventually ended up talking to Tricycle, the developers of this site. They told me about their own proposed solution to the community organization problem, which is this new Discussion section. They said that searching the discussion section by tag was equivalent to a sub-reddit. For example, if you want a sub-reddit on consciousness, the <a href=\"/r/discussion/tag/consciousness/\">discussion consciousness tag search</a> is an amazing imitation</p>\n<p>I told them I wasn't <em>entirely</em> convinced by this and sent some reasons why, but I haven't heard back from them lately and I'm not going keep pursuing this and make a big deal of it unless a large percentage of the people who wanted sub-reddits are unsatisfied.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hhrv8aAcmkzJxvP58", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 6.358726171719918e-07, "legacy": true, "legacyId": "3750", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-17T14:03:46.227Z", "modifiedAt": null, "url": null, "title": "Number bias", "slug": "number-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:25.149Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oFp6JLn8z9uxgdPp8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8gXxT2mZ7RGFumWJY/number-bias", "pageUrlRelative": "/posts/8gXxT2mZ7RGFumWJY/number-bias", "linkUrl": "https://www.lesswrong.com/posts/8gXxT2mZ7RGFumWJY/number-bias", "postedAtFormatted": "Sunday, October 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Number%20bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANumber%20bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8gXxT2mZ7RGFumWJY%2Fnumber-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Number%20bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8gXxT2mZ7RGFumWJY%2Fnumber-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8gXxT2mZ7RGFumWJY%2Fnumber-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<p>The New York Times ran <a href=\"http://www.nytimes.com/2010/10/17/opinion/17gilbert.html?src=me&amp;ref=general\">an editorial</a> about an interesting type of cognitive bias: according to the article, the fact that our system of timekeeping is based on factors of 24, 7, etc. and the fact that we have 10 fingers profoundly influences our way of thinking. As the article explains, this bias is distinct from scope neglect and misunderstanding of probability. Has anyone else heard of this kind of \"number bias\" before? Also, is this an issue that deserves further study on LessWrong?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8gXxT2mZ7RGFumWJY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 6.358795490043001e-07, "legacy": true, "legacyId": "3751", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-17T16:55:51.772Z", "modifiedAt": null, "url": null, "title": "Ranking the \"competition\" based on optimization power", "slug": "ranking-the-competition-based-on-optimization-power", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:19.706Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blogospheroid", "createdAt": "2009-03-17T08:11:01.816Z", "isAdmin": false, "displayName": "blogospheroid"}, "userId": "dgscyYwrDh3u7dE7h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4Z364g6wbazQgFu5z/ranking-the-competition-based-on-optimization-power", "pageUrlRelative": "/posts/4Z364g6wbazQgFu5z/ranking-the-competition-based-on-optimization-power", "linkUrl": "https://www.lesswrong.com/posts/4Z364g6wbazQgFu5z/ranking-the-competition-based-on-optimization-power", "postedAtFormatted": "Sunday, October 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ranking%20the%20%22competition%22%20based%20on%20optimization%20power&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARanking%20the%20%22competition%22%20based%20on%20optimization%20power%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Z364g6wbazQgFu5z%2Franking-the-competition-based-on-optimization-power%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ranking%20the%20%22competition%22%20based%20on%20optimization%20power%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Z364g6wbazQgFu5z%2Franking-the-competition-based-on-optimization-power", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Z364g6wbazQgFu5z%2Franking-the-competition-based-on-optimization-power", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 202, "htmlBody": "<p>Most long term users on Less Wrong understand the concept of optimization power and how a system can be called intelligent if it can restrict the future in significant ways. Now I believe that in this world, only&nbsp;institutions&nbsp;are close to superintelligence in any significant way.&nbsp;</p>\n<p>I believe it is important for us to have atleast some outside idea of which institutions/systems are powerful in today's world so that we can atleast see some outlines of how the increasing optimization power will end up affecting normal people.</p>\n<p>So, my question is - what are the present institutions or systems that you would classify as having the maximum optimization power. Please present your thought behind it if you feel you are mentioning some unknown institution. I am presenting my guesses after the break.</p>\n<p><a id=\"more\"></a></p>\n<p>Blogospheroid's guess list</p>\n<p>&nbsp;</p>\n<ol>\n<li>NSA / US Intelligence and defence community</li>\n<li>Harvard University</li>\n<li>The Chinese Politburo</li>\n<li>Goldman Sachs</li>\n<li>The Kremlin / Russian intelligence and defence community</li>\n<li>Google Inc&nbsp;</li>\n<li>Oracle Inc</li>\n<li>Microsoft Inc</li>\n<li>Murdoch's media empire</li>\n</ol>\n<p>Institutions I found significant outside this list is the Singaporean and Abu Dhabi city governments, very rational and increasing their significance in the world, but highly restricted from fooming because of constraints.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4Z364g6wbazQgFu5z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 0, "extendedScore": null, "score": 6.359201296023316e-07, "legacy": true, "legacyId": "3752", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-17T22:14:17.440Z", "modifiedAt": null, "url": null, "title": "Bayesian Doomsday Argument", "slug": "bayesian-doomsday-argument-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:16.904Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielLC", "createdAt": "2009-12-26T17:34:50.257Z", "isAdmin": false, "displayName": "DanielLC"}, "userId": "3e6zTkDmDpNspRb8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7jYKgyQvL5agiWh48/bayesian-doomsday-argument-0", "pageUrlRelative": "/posts/7jYKgyQvL5agiWh48/bayesian-doomsday-argument-0", "linkUrl": "https://www.lesswrong.com/posts/7jYKgyQvL5agiWh48/bayesian-doomsday-argument-0", "postedAtFormatted": "Sunday, October 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesian%20Doomsday%20Argument&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesian%20Doomsday%20Argument%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7jYKgyQvL5agiWh48%2Fbayesian-doomsday-argument-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesian%20Doomsday%20Argument%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7jYKgyQvL5agiWh48%2Fbayesian-doomsday-argument-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7jYKgyQvL5agiWh48%2Fbayesian-doomsday-argument-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1262, "htmlBody": "<p>First, if you don't already know it, Frequentist Doomsday Argument:<br /><br />There's some number of total humans. There's a 95% chance that you come after the last 5%. There's been about 60 to 120 billion people so far, so there's a 95% chance that the total will be less than 1.2 to 2.4 trillion.<br /><br />I've modified it to be Bayesian.<br /><br />First, find the priors:<br /><br />Do you think it's possible that the total number of sentients that have ever lived or will ever live is less than a googolplex? I'm not asking if you're certain, or even if you think it's likely. Is it more likely than one in infinity? I think it is too. This means that the prior must be normalizable.<br /><br />If we take P(T=n) &prop; 1/n, where T is the total number of people, it can't be normalized, as 1/1 + 1/2 + 1/3 + ... is an infinite sum. If it decreases faster, it can at least be normalized. As such, we can use 1/n as an upper limit.<br /><br />Of course, that's just the limit of the upper tail, so maybe that's not a very good argument. Here's another one:<br /><br />We're not so much dealing with lives as life-years. Year is a pretty arbitrary measurement, so we'd expect the distribution to be pretty close for the majority of it if we used, say, days instead. This would require the 1/n distribution.<br /><br />After that,<br /><br />T = total number of people<br /><br />U = number you are<br /><br />P(T=n) &prop; 1/n<br />U = m<br />P(U=m|T=n) &prop; 1/n<br />P(T=n|U=m) = P(U=m|T=n) * P(T=n) / P(U=m)<br />= (1/n^2) / P(U=m)<br />P(T&gt;n|U=m) = &int;P(T=n|U=m)dn<br />= (1/n) / P(U=m)<br />And to normalize:<br />P(T&gt;m|U=m) = 1<br />= (1/m) / P(U=m)<br />m = 1/P(U=m)<br />P(T&gt;n|U=m) = (1/n)*m<br />P(T&gt;n|U=m) = m/n</p>\n<p>So, the probability of there being a total of 1 trillion people total if there's been 100 billion so far is 1/10.</p>\n<p>There's still a few issues with this. It assumes P(U=m|T=n) &prop; 1/n. This seems like it makes sense. If there's a million people, there's a one-in-a-million chance of being the 268,547th. But if there's also a trillion sentient animals, the chance of being the nth person won't change that much between a million and a billion people. There's a few ways I can amend this.</p>\n<p>First: a = number of sentient animals. P(U=m|T=n) &prop; 1/(a+n). This would make the end result P(T&gt;n|U=m) = (m+a)/(n+a).</p>\n<p>Second: Just replace every mention of people with sentients.</p>\n<p>Third: Take this as a prediction of the number of sentients who aren't humans who have lived so far.</p>\n<p>The first would work well if we can find the number of sentient animals without knowing how many humans there will be. Assuming we don't take the time to terreform every planet we come across, this should work okay.</p>\n<p>The second would work well if we did tereform every planet we came across.</p>\n<p>The third seems a bit wierd. It gives a smaller answer than the other two. It gives a smaller answer than what you'd expect for animals alone. It does this because it combines it for a Doomsday Argument against animals being sentient. You can work that out separately. Just say T is the total number of humans, and U is the total number of animals. Unfortunately, you have to know the total number of humans to work out how many animals are sentient, and vice versa. As such, the combined argument may be more useful. It won't tell you how many of the denizens of planets we colonise will be animals, but I don't think it's actually possible to tell that.</p>\n<p>One more thing, you have more information. You have a lifetime of evidence, some of which can be used in these predictions. The lifetime of humanity isn't obvious. We might make it to the heat death of the universe, or we might just kill each other off in a nuclear or biological war in a few decades. We also might be annihilated by a paperclipper somewhere in between. As such, I don't think the evidence that way is very strong.</p>\n<p>The evidence for animals is stronger. Emotions aren't exclusively intelligent. It doesn't seem animals would have to be that intelligent to be sentient. Even so, how sure can you really be. This is much more subjective than the doomsday part, and the evidence against their sentience is staggering. I think so anyway, how many animals are there at different levels of intelligence?</p>\n<p>Also, there's the priors for total human population so far. I've read estimates vary between 60 and 120 billion. I don't think a factor of two really matters too much for this discussion.</p>\n<p>So, what can we use for these priors?</p>\n<p>Another issue is that this is for all of space and time, not just Earth.</p>\n<p>Consider that you're the mth person (or sentient) from the lineage of a given planet. l(m) is the number of planets with a lineage of at least m people. N is the total number of people ever, n is the number on the average planet, and p is the number of planets.<br /><br />l(m)/N<br />=l(m)/(n*p)<br />=(l(m)/p)/n</p>\n<p>l(m)/p is the portion of planets that made it this far. This increases with n, so this weakens my argument, but only to a limited extent. I'm not sure what that is, though. Instinct is that l(m)/p is 50% when m=n, but the mean is not the median. I'd expect a left-skew, which would make l(m)/p much lower than that. Even so, if you placed it at 0.01%, this would mean that it's a thousand times less likely at that value. This argument still takes it down orders of magnitude than what you'd think, so that's not really that significant.</p>\n<p>Also, a back-of-the-envolope calculation:</p>\n<p>Assume, against all odds, there are a trillion times as many sentient animals as humans, and we happen to be the humans. Also, assume humans only increase their own numbers, and they're at the top percentile for the populations you'd expect. Also, assume 100 billion humans so far.</p>\n<p>n = 1,000,000,000,000 * 100,000,000,000 * 100</p>\n<p>n = 10^12 * 10^11 * 10^2</p>\n<p>n = 10^25</p>\n<p>Here's more what I'd expect:</p>\n<p>Humanity eventually puts up a satilite to collect solar energy. Once they do one, they might as well do another, until they have a dyson swarm. Assume 1% efficiency. Also, assume humans still use their whole bodies instead of being a brain in a vat. Finally, assume they get fed with 0.1% efficiency. And assume an 80-year lifetime.</p>\n<p>n = solar luminosity * 1% / power of a human * 0.1% * lifetime of Sun / lifetime of human</p>\n<p>n = 4 * 10^26 Watts * 0.01 / 100 Watts * 0.001 * 5,000,000,000 years / 80 years</p>\n<p>n = 2.5 * 10^27</p>\n<p>By the way, the value I used for power of a human is after the inefficiencies of digesting.<br /><br />Even with assumptions that extreme, we couldn't use this planet to it's full potential. Granted, that requires mining pretty much the whole planet, but with a dyson sphere you can do that in a week, or two years with the efficiency I gave.<br /><br />It actually works out to about 150 tons of Earth per person. How much do you need to get the elements to make a person?</p>\n<p>Incidentally, I rewrote the article, so don't be surprised if some of the comments don't make sense.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7jYKgyQvL5agiWh48", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -6, "extendedScore": null, "score": 6.35995229188649e-07, "legacy": true, "legacyId": "3753", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-18T06:20:50.425Z", "modifiedAt": null, "url": null, "title": "Was Carl Segan an Agnostic Prophet?", "slug": "was-carl-segan-an-agnostic-prophet", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:20.597Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adsenanim", "createdAt": "2010-08-11T04:54:54.280Z", "isAdmin": false, "displayName": "adsenanim"}, "userId": "oJuRgGFdLgBwc8oQk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eM5x4PAwHrQa96gwD/was-carl-segan-an-agnostic-prophet", "pageUrlRelative": "/posts/eM5x4PAwHrQa96gwD/was-carl-segan-an-agnostic-prophet", "linkUrl": "https://www.lesswrong.com/posts/eM5x4PAwHrQa96gwD/was-carl-segan-an-agnostic-prophet", "postedAtFormatted": "Monday, October 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Was%20Carl%20Segan%20an%20Agnostic%20Prophet%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWas%20Carl%20Segan%20an%20Agnostic%20Prophet%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeM5x4PAwHrQa96gwD%2Fwas-carl-segan-an-agnostic-prophet%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Was%20Carl%20Segan%20an%20Agnostic%20Prophet%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeM5x4PAwHrQa96gwD%2Fwas-carl-segan-an-agnostic-prophet", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeM5x4PAwHrQa96gwD%2Fwas-carl-segan-an-agnostic-prophet", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<p><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;; color: black; font-size: 7.5pt;\">\r\n<p><span style=\"font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black;\"><span style=\"font-size: small;\">I ask that those who want to participate&nbsp;follow these rules:</span></span></p>\r\n<p><span style=\"font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black;\"><span style=\"font-size: small;\">Syllogistic representations&nbsp;are preferred.</span></span></p>\r\n<p><span style=\"font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black;\"><span style=\"font-size: small;\">Anecdotes are welcome, but please limit yourself.</span></span></p>\r\n<p><span style=\"font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black;\"><span style=\"font-size: small;\">Platitudes are self recriminatory.</span></span></p>\r\n<p><span style=\"font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black;\"><span style=\"font-size: small;\">Haiku are considered poetry.</span></span></p>\r\n<p><span style=\"font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black;\"><span style=\"font-size: small;\">Math, while appropriate, may cause confusion.</span></span></p>\r\n<p><span style=\"font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black;\"><span style=\"font-size: small;\">If the argument that you represent is not listed above please try to limit your response for clarity.</span></span></p>\r\n<p><span style=\"font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black;\"><span style=\"font-size: small;\">Those who wish to argue that \"Agnostic\" and \"Prophet\" are incongruent, please understand that \"Prophet\" is understood to mean \"any person that can observe phenomena over time and hazards a guess to what will happen next\".</span></span></p>\r\n<p>&nbsp;</p>\r\n</span></p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eM5x4PAwHrQa96gwD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": -36, "extendedScore": null, "score": 6.361100097905775e-07, "legacy": true, "legacyId": "3756", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-18T14:52:14.393Z", "modifiedAt": null, "url": null, "title": "Evolution just might be chaotic", "slug": "evolution-just-might-be-chaotic", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:16.788Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LZsAdWzhpjSxb5Svj/evolution-just-might-be-chaotic", "pageUrlRelative": "/posts/LZsAdWzhpjSxb5Svj/evolution-just-might-be-chaotic", "linkUrl": "https://www.lesswrong.com/posts/LZsAdWzhpjSxb5Svj/evolution-just-might-be-chaotic", "postedAtFormatted": "Monday, October 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evolution%20just%20might%20be%20chaotic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvolution%20just%20might%20be%20chaotic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZsAdWzhpjSxb5Svj%2Fevolution-just-might-be-chaotic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evolution%20just%20might%20be%20chaotic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZsAdWzhpjSxb5Svj%2Fevolution-just-might-be-chaotic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZsAdWzhpjSxb5Svj%2Fevolution-just-might-be-chaotic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 206, "htmlBody": "<p><a href=\"http://www.newscientist.com/article/mg20827821.000-the-chaos-theory-of-evolution.html?DCMP=OTC-rss&amp;nsref=online-news\">The Chaos Theory of Evolution</a></p>\n<p>:</p>\n<blockquote>Research on animals has come to similarly unexpected conclusions, albeit based on sparser fossil records. For example, palaeontologist Russell Graham at Illinois State Museum has looked at North American mammals and palaeontologist Russell Coope at the University of Birmingham in the UK has examined insects (Annual Review of Ecology and Systematics, vol 10, p 247). Both studies show that most species remain unchanged for hundreds of thousands of years, perhaps longer, and across several ice ages. Species undergo major changes in distribution and abundance, but show no evolution of morphological characteristics despite major environmental changes.  That is not to say that major evolutionary change such as speciation doesn't happen. But recent \"molecular clock\" research suggests the link between speciation and environmental change is weak at best.  Molecular clock approaches allow us to estimate when two closely related modern species split from a common ancestor by comparing their DNA. Most of this work has been carried out in birds, and shows that new species appear more or less continuously, regardless of the dramatic climatic oscillations of the Quaternary or the longer term cooling that preceded it</blockquote>\n<p>The hypothesis is that there's very little possibility of finding patterns in evolution. What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LZsAdWzhpjSxb5Svj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 6.362306921361475e-07, "legacy": true, "legacyId": "3770", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-18T16:55:10.360Z", "modifiedAt": null, "url": null, "title": "Vipassana Meditation: Developing Meta-Feeling Skills", "slug": "vipassana-meditation-developing-meta-feeling-skills", "viewCount": null, "lastCommentedAt": "2019-08-02T19:09:58.095Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "YpTmfmnMjgakwFRQQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NTkBCFJSA4PFBxSM9/vipassana-meditation-developing-meta-feeling-skills", "pageUrlRelative": "/posts/NTkBCFJSA4PFBxSM9/vipassana-meditation-developing-meta-feeling-skills", "linkUrl": "https://www.lesswrong.com/posts/NTkBCFJSA4PFBxSM9/vipassana-meditation-developing-meta-feeling-skills", "postedAtFormatted": "Monday, October 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Vipassana%20Meditation%3A%20Developing%20Meta-Feeling%20Skills&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVipassana%20Meditation%3A%20Developing%20Meta-Feeling%20Skills%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNTkBCFJSA4PFBxSM9%2Fvipassana-meditation-developing-meta-feeling-skills%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Vipassana%20Meditation%3A%20Developing%20Meta-Feeling%20Skills%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNTkBCFJSA4PFBxSM9%2Fvipassana-meditation-developing-meta-feeling-skills", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNTkBCFJSA4PFBxSM9%2Fvipassana-meditation-developing-meta-feeling-skills", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1356, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/lw/2rd/understanding_vipassana_meditation/\">Understanding vipassana meditation</a></p>\n<p><em>I explain how to practice vipassana meditation<sup>1</sup> (a form of Buddhist meditation), giving instructions, advice, and measures of progress. By practicing vipassana one becomes aware of (and exerts control over) the process of affective judgment. This process may underlie important (and subtle) mental patterns of feeling that are responsible for common rationality failures. While I've tried to give helpful information, you should meditate at your own risk; you may experience mental instability or change in undesirable ways.<sup>2 </sup>This is a somewhat brief post containing information I've personally found most helpful on my meditative journey, see other guides (like <a href=\"http://www.urbandharma.org/udharma4/mpe.html\">this one</a>) for more.<br /></em></p>\n<p><em><a id=\"more\"></a></em><strong>Instructions</strong></p>\n<p><em>Decide beforehand how long you will meditate for, and set some kind of alarm to go off at the end of this period. </em></p>\n<p>Start with 10-15 minutes; you can incrementally increase this amount to 30 minutes or an hour. The use of an alarm allows you to meditate without worrying about checking if your time is up.</p>\n<p><em>Go to a quiet location where you feel comfortable. Assume a posture that you can stay in for a while. Do your best not to change your posture during the time period.<br /></em></p>\n<p>AFAICT the posture you choose is not especially important. You can sit Indian style, in a half-lotus position, or in a full-lotus position. You can sit on a pillow or directly on the floor. If none of these positions work you can also sit in a chair. You should be reasonably comfortable (but alert) and stable, and able to breathe naturally. Take care not to aggravate past injuries or cause new ones.</p>\n<p><em>Close your eyes and your mouth. Breath naturally through your nose. Keep your awareness centered on the area below the nostrils and above the upper lip. Neutrally and passively observe the breath passing over this area<sup>3</sup>. </em><em>If you realize your mind has wandered, patiently re-center your awareness. Once you have established some degree of concentration you should be able to \"see through\" thoughts and emotions without getting swept away by them.<br /></em></p>\n<p>You should not regulate your breath. If the breath is deep, simply observe that the breath is deep. If the breath is shallow, simply observe that the breath is shallow. Observe the breath <em>neutrally</em> and <em>passively</em>. Don't <em>associate yourself</em> with the breath.</p>\n<p>The breath should be the center of your awareness, the anchor<sup>4</sup> that you remain attached to regardless of what arises in the mind. Sooner or later you will get \"stuck to\" a train of thought, and lose your awareness of the breath. When you notice that this has happened, patiently re-center your awareness on the breath. Do this without feeling the slightest bit upset or disappointed.</p>\n<p>After practicing for some time (hours, days, or months) you should be able to \"see through\"<sup>5</sup> arising thoughts and emotions without getting \"stuck to\" them. Demanding thoughts and emotions will arise, and by \"seeing through\" them you maintain your observation of the breath as they continue (unattended to) in your peripheral awareness.</p>\n<p><strong>Advice</strong></p>\n<p><em>Meditate every day.</em></p>\n<p>Really. You're trying to change deep mental habits of awareness and feeling, and that requires constant pressure and reinforcement. Consistency is important. Choosing to meditate at the same time and in the same spot can facilitate making it part of your daily routine.</p>\n<p><em>Keep an innocently curious mindset.<br /></em></p>\n<p>Think of meditation as a wonderful opportunity to learn about your mind. It seems reasonable to expect your mind to be able to focus on one object for 5 minutes (or even 1 minute), and the fact that it's so hard for many people is <em>interesting</em>. Re-centering your mind, you might notice that you tend to get de-railed most often by thoughts about some past injustice, or about some future fantasy. When you remain centered on the breath, and are \"seeing through\" the arising thoughts and emotions, you may notice you think much more about some particular thing than you thought you did. Don't <a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Tarski\">be</a> <a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Gendlin\">afraid</a>; unravel parts of yourself to <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">become stronger</a>.</p>\n<p><em>Beware of <a href=\"/lw/2q6/compartmentalization_in_epistemic_and\">wireheading patterns</a>.</em></p>\n<p>These patterns can occur in the form of trying to realize one's idea of what meditation should be (e.g. attempting to repeat a peak experience). This leads to altering one's practice in order to try to match previous expectations. This can be a subtle (and recurring) error, since one's meditation should actually evolve over time and through new experiences. Trying to follow the instructions as straightforwardly as possible, and looking for the manifestation of the benefits in one's life, can help to distinguish between wireheading patterns and genuine growth.</p>\n<p><strong>Measures of progress</strong></p>\n<p><em>Improved concentration.</em></p>\n<p>You find that you are able to focus on tasks for longer periods without getting irritated or distracted.</p>\n<p><em>Less anxiety.</em></p>\n<p>On a coarse level, you get worried or stressed less often about macroscopic events. On a more subtle level, you aren't irritated by formerly annoying bodily experiences (e.g. cold or hunger).</p>\n<p><em>Feeling unusual sensations (during or outside meditation).</em></p>\n<p>You might feel spreading tingling sensations, numbness, muscle twitches, or a host of other surprising things.</p>\n<p><em>Enhanced sensory perception.</em></p>\n<p>You start to notice (and eventually continually become aware of) subtle sensations. You strongly smell trees and flowers when walking down the street, become sensitive to the temperature of the things you touch, etc. This enhanced perception is similar to the sensory sensitivity one experiences when taking psychedelics.</p>\n<p><em>Insights about patterns of feelings</em>.</p>\n<p>You discover that you are perpetuating patterns that hurt you in one way or another. (See <a href=\"/lw/2sv/a_novice_buddhists_humble_experiences/2qsh?c=1\">here</a> for a concrete example)</p>\n<p><em>Experiences of egolessness (during or outside meditation).</em></p>\n<p>You find that you become absorbed in some aspect of your experience; you lose your sense of self and feel that nothing else exists. The first time I strongly experienced this I became absorbed in a sensation that was previously causing extreme pain.</p>\n<p><em>Meditation during daily life.</em></p>\n<p>You begin regulating your awareness and feelings as you do in meditation in the course of daily life.</p>\n<p>&nbsp;</p>\n<p><strong> \n<hr />\n</strong><sup>1</sup><strong> </strong>There is much <a href=\"/r/discussion/lw/2sv/a_novice_buddhists_humble_experiences/2qsw?c=1\">confusion</a> out there about what vipassana is, and how it is related to other forms of Buddhist meditation (like <a href=\"http://en.wikipedia.org/wiki/Anapanasati\">anapanasati</a>). I've made personal decisions about how to use the terms (and what instructions to give) in a way that I think is most clear and conducive to beneficial practice.</p>\n<p><sup>2 </sup><a href=\"http://www.dhamma.org/\">These courses</a> <a href=\"http://www.dhamma.org/en/qanda.shtml\">indicate</a> that they may turn down people with serious emotional problems. I expect that undesirable changes (if they occur) will be slow; you will see them happening and can stop meditating if you so desire. An example of such a change: I now <em>very rarely</em> get sad (didn't shed a tear at my last grandparent's funeral). This doesn't bother me at all, as I generally understand sadness as an indication of my attachment to how someone makes me feel, and not a measure of how much I intrinsically care about them.</p>\n<p><sup>3 </sup>At the start your awareness of the breath will not be very sharp. As it becomes easier to keep your awareness centered you can sharpen your awareness by focusing more precisely on the <em>sensation</em> the breath causes in this area, the <em>touch</em> of the breath, as you inhale and exhale.</p>\n<p><sup>4</sup> I expect that the particular anchor you use isn't important (but I'm not sure). AFAICT in <a href=\"http://www.dhamma.org/\">these courses</a> your anchor is the mental procedure of systematically observing bodily sensations.</p>\n<p><sup>5</sup> <a href=\"http://www.urbandharma.org/udharma4/mpe.html\">This guide</a> has a good description of the difference between \"seeing through\" (being aware of) and \"getting stuck to\" (thinking) a thought:</p>\n<blockquote>\n<p><span style=\"font-size: small;\">There is a difference between being aware of a thought and thinking a thought. That difference is very subtle. It is primarily a matter of feeling or texture. A thought you are simply aware of with bare attention feels light in texture; there is a sense of distance between that thought and the awareness viewing it. It arises lightly like a bubble, and it passes away without necessarily giving rise to the next thought in that chain. Normal conscious thought is much heavier in texture. It is ponderous, commanding, and compulsive. It sucks you in and grabs control of consciousness. By its very nature it is obsessional, and it leads straight to the next thought in the chain, apparently with no gap between them.</span></p>\n</blockquote>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p><span style=\"font-size: small;\">I've created an <a href=\"/r/discussion/lw/2ws/vipassana_open_thread/\">open thread</a> to discuss experiences and problems related to vipassana meditation, and to organize events and retreats.<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AiNyf5iwbpc7mehiX": 1, "xv7Bg5fbF9WppYREi": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NTkBCFJSA4PFBxSM9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 30, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "3749", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to:</strong> <a href=\"/lw/2rd/understanding_vipassana_meditation/\">Understanding vipassana meditation</a></p>\n<p><em>I explain how to practice vipassana meditation<sup>1</sup> (a form of Buddhist meditation), giving instructions, advice, and measures of progress. By practicing vipassana one becomes aware of (and exerts control over) the process of affective judgment. This process may underlie important (and subtle) mental patterns of feeling that are responsible for common rationality failures. While I've tried to give helpful information, you should meditate at your own risk; you may experience mental instability or change in undesirable ways.<sup>2 </sup>This is a somewhat brief post containing information I've personally found most helpful on my meditative journey, see other guides (like <a href=\"http://www.urbandharma.org/udharma4/mpe.html\">this one</a>) for more.<br></em></p>\n<p><em><a id=\"more\"></a></em><strong>Instructions</strong></p>\n<p><em>Decide beforehand how long you will meditate for, and set some kind of alarm to go off at the end of this period. </em></p>\n<p>Start with 10-15 minutes; you can incrementally increase this amount to 30 minutes or an hour. The use of an alarm allows you to meditate without worrying about checking if your time is up.</p>\n<p><em>Go to a quiet location where you feel comfortable. Assume a posture that you can stay in for a while. Do your best not to change your posture during the time period.<br></em></p>\n<p>AFAICT the posture you choose is not especially important. You can sit Indian style, in a half-lotus position, or in a full-lotus position. You can sit on a pillow or directly on the floor. If none of these positions work you can also sit in a chair. You should be reasonably comfortable (but alert) and stable, and able to breathe naturally. Take care not to aggravate past injuries or cause new ones.</p>\n<p><em>Close your eyes and your mouth. Breath naturally through your nose. Keep your awareness centered on the area below the nostrils and above the upper lip. Neutrally and passively observe the breath passing over this area<sup>3</sup>. </em><em>If you realize your mind has wandered, patiently re-center your awareness. Once you have established some degree of concentration you should be able to \"see through\" thoughts and emotions without getting swept away by them.<br></em></p>\n<p>You should not regulate your breath. If the breath is deep, simply observe that the breath is deep. If the breath is shallow, simply observe that the breath is shallow. Observe the breath <em>neutrally</em> and <em>passively</em>. Don't <em>associate yourself</em> with the breath.</p>\n<p>The breath should be the center of your awareness, the anchor<sup>4</sup> that you remain attached to regardless of what arises in the mind. Sooner or later you will get \"stuck to\" a train of thought, and lose your awareness of the breath. When you notice that this has happened, patiently re-center your awareness on the breath. Do this without feeling the slightest bit upset or disappointed.</p>\n<p>After practicing for some time (hours, days, or months) you should be able to \"see through\"<sup>5</sup> arising thoughts and emotions without getting \"stuck to\" them. Demanding thoughts and emotions will arise, and by \"seeing through\" them you maintain your observation of the breath as they continue (unattended to) in your peripheral awareness.</p>\n<p><strong id=\"Advice\">Advice</strong></p>\n<p><em>Meditate every day.</em></p>\n<p>Really. You're trying to change deep mental habits of awareness and feeling, and that requires constant pressure and reinforcement. Consistency is important. Choosing to meditate at the same time and in the same spot can facilitate making it part of your daily routine.</p>\n<p><em>Keep an innocently curious mindset.<br></em></p>\n<p>Think of meditation as a wonderful opportunity to learn about your mind. It seems reasonable to expect your mind to be able to focus on one object for 5 minutes (or even 1 minute), and the fact that it's so hard for many people is <em>interesting</em>. Re-centering your mind, you might notice that you tend to get de-railed most often by thoughts about some past injustice, or about some future fantasy. When you remain centered on the breath, and are \"seeing through\" the arising thoughts and emotions, you may notice you think much more about some particular thing than you thought you did. Don't <a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Tarski\">be</a> <a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Gendlin\">afraid</a>; unravel parts of yourself to <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">become stronger</a>.</p>\n<p><em>Beware of <a href=\"/lw/2q6/compartmentalization_in_epistemic_and\">wireheading patterns</a>.</em></p>\n<p>These patterns can occur in the form of trying to realize one's idea of what meditation should be (e.g. attempting to repeat a peak experience). This leads to altering one's practice in order to try to match previous expectations. This can be a subtle (and recurring) error, since one's meditation should actually evolve over time and through new experiences. Trying to follow the instructions as straightforwardly as possible, and looking for the manifestation of the benefits in one's life, can help to distinguish between wireheading patterns and genuine growth.</p>\n<p><strong id=\"Measures_of_progress\">Measures of progress</strong></p>\n<p><em>Improved concentration.</em></p>\n<p>You find that you are able to focus on tasks for longer periods without getting irritated or distracted.</p>\n<p><em>Less anxiety.</em></p>\n<p>On a coarse level, you get worried or stressed less often about macroscopic events. On a more subtle level, you aren't irritated by formerly annoying bodily experiences (e.g. cold or hunger).</p>\n<p><em>Feeling unusual sensations (during or outside meditation).</em></p>\n<p>You might feel spreading tingling sensations, numbness, muscle twitches, or a host of other surprising things.</p>\n<p><em>Enhanced sensory perception.</em></p>\n<p>You start to notice (and eventually continually become aware of) subtle sensations. You strongly smell trees and flowers when walking down the street, become sensitive to the temperature of the things you touch, etc. This enhanced perception is similar to the sensory sensitivity one experiences when taking psychedelics.</p>\n<p><em>Insights about patterns of feelings</em>.</p>\n<p>You discover that you are perpetuating patterns that hurt you in one way or another. (See <a href=\"/lw/2sv/a_novice_buddhists_humble_experiences/2qsh?c=1\">here</a> for a concrete example)</p>\n<p><em>Experiences of egolessness (during or outside meditation).</em></p>\n<p>You find that you become absorbed in some aspect of your experience; you lose your sense of self and feel that nothing else exists. The first time I strongly experienced this I became absorbed in a sensation that was previously causing extreme pain.</p>\n<p><em>Meditation during daily life.</em></p>\n<p>You begin regulating your awareness and feelings as you do in meditation in the course of daily life.</p>\n<p>&nbsp;</p>\n<p><strong> \n</strong></p><hr><strong>\n</strong><sup>1</sup><strong> </strong>There is much <a href=\"/r/discussion/lw/2sv/a_novice_buddhists_humble_experiences/2qsw?c=1\">confusion</a> out there about what vipassana is, and how it is related to other forms of Buddhist meditation (like <a href=\"http://en.wikipedia.org/wiki/Anapanasati\">anapanasati</a>). I've made personal decisions about how to use the terms (and what instructions to give) in a way that I think is most clear and conducive to beneficial practice.<p></p>\n<p><sup>2 </sup><a href=\"http://www.dhamma.org/\">These courses</a> <a href=\"http://www.dhamma.org/en/qanda.shtml\">indicate</a> that they may turn down people with serious emotional problems. I expect that undesirable changes (if they occur) will be slow; you will see them happening and can stop meditating if you so desire. An example of such a change: I now <em>very rarely</em> get sad (didn't shed a tear at my last grandparent's funeral). This doesn't bother me at all, as I generally understand sadness as an indication of my attachment to how someone makes me feel, and not a measure of how much I intrinsically care about them.</p>\n<p><sup>3 </sup>At the start your awareness of the breath will not be very sharp. As it becomes easier to keep your awareness centered you can sharpen your awareness by focusing more precisely on the <em>sensation</em> the breath causes in this area, the <em>touch</em> of the breath, as you inhale and exhale.</p>\n<p><sup>4</sup> I expect that the particular anchor you use isn't important (but I'm not sure). AFAICT in <a href=\"http://www.dhamma.org/\">these courses</a> your anchor is the mental procedure of systematically observing bodily sensations.</p>\n<p><sup>5</sup> <a href=\"http://www.urbandharma.org/udharma4/mpe.html\">This guide</a> has a good description of the difference between \"seeing through\" (being aware of) and \"getting stuck to\" (thinking) a thought:</p>\n<blockquote>\n<p><span style=\"font-size: small;\">There is a difference between being aware of a thought and thinking a thought. That difference is very subtle. It is primarily a matter of feeling or texture. A thought you are simply aware of with bare attention feels light in texture; there is a sense of distance between that thought and the awareness viewing it. It arises lightly like a bubble, and it passes away without necessarily giving rise to the next thought in that chain. Normal conscious thought is much heavier in texture. It is ponderous, commanding, and compulsive. It sucks you in and grabs control of consciousness. By its very nature it is obsessional, and it leads straight to the next thought in the chain, apparently with no gap between them.</span></p>\n</blockquote>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p><span style=\"font-size: small;\">I've created an <a href=\"/r/discussion/lw/2ws/vipassana_open_thread/\">open thread</a> to discuss experiences and problems related to vipassana meditation, and to organize events and retreats.<br></span></p>", "sections": [{"title": "Advice", "anchor": "Advice", "level": 1}, {"title": "Measures of progress", "anchor": "Measures_of_progress", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "107 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 108, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CNLMxEkx7PqHnnvxC", "DoLQN5ryZ9XkZjq5h", "N99KgncSXewWqkzMA", "uWg8Yy9RGjQxwJEQQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-18T17:01:28.575Z", "modifiedAt": null, "url": null, "title": "Vipassana Meditation Open Thread", "slug": "vipassana-meditation-open-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.930Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "YpTmfmnMjgakwFRQQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uWg8Yy9RGjQxwJEQQ/vipassana-meditation-open-thread", "pageUrlRelative": "/posts/uWg8Yy9RGjQxwJEQQ/vipassana-meditation-open-thread", "linkUrl": "https://www.lesswrong.com/posts/uWg8Yy9RGjQxwJEQQ/vipassana-meditation-open-thread", "postedAtFormatted": "Monday, October 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Vipassana%20Meditation%20Open%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVipassana%20Meditation%20Open%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuWg8Yy9RGjQxwJEQQ%2Fvipassana-meditation-open-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Vipassana%20Meditation%20Open%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuWg8Yy9RGjQxwJEQQ%2Fvipassana-meditation-open-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuWg8Yy9RGjQxwJEQQ%2Fvipassana-meditation-open-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 35, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/2rd/understanding_vipassana_meditation/\">Understanding vipassana meditation</a>, <a href=\"/lw/2w5/vipassana_meditation_developing_metafeeling_skills/\">Vipassana Meditation: Developing Meta-Feeling Skills</a></p>\n<p>This is a place to discuss experiences and problems related to practicing vipassana meditation. This space can also be used to organize meditation events or retreats.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uWg8Yy9RGjQxwJEQQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 4, "extendedScore": null, "score": 6.362611963202563e-07, "legacy": true, "legacyId": "3772", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CNLMxEkx7PqHnnvxC", "NTkBCFJSA4PFBxSM9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-18T18:41:33.876Z", "modifiedAt": null, "url": null, "title": "Rally to Restore Rationality", "slug": "rally-to-restore-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:25.504Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KhuJHHFD6DBiZaBGG/rally-to-restore-rationality", "pageUrlRelative": "/posts/KhuJHHFD6DBiZaBGG/rally-to-restore-rationality", "linkUrl": "https://www.lesswrong.com/posts/KhuJHHFD6DBiZaBGG/rally-to-restore-rationality", "postedAtFormatted": "Monday, October 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rally%20to%20Restore%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARally%20to%20Restore%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKhuJHHFD6DBiZaBGG%2Frally-to-restore-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rally%20to%20Restore%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKhuJHHFD6DBiZaBGG%2Frally-to-restore-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKhuJHHFD6DBiZaBGG%2Frally-to-restore-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 35, "htmlBody": "<p>Hey everyone. If anyone else is heading to Jon Stewart's <a href=\"http://www.rallytorestoresanity.com/\">Rally to Restore Sanity</a> on the National Mall on Oct. 30th, please comment or contact me at pphysics141@gmail.com so we can arrange an LW meetup.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KhuJHHFD6DBiZaBGG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "3773", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-18T21:28:17.651Z", "modifiedAt": null, "url": null, "title": "October 2010 Southern California Meetup", "slug": "october-2010-southern-california-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:20.855Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimmy", "createdAt": "2009-02-27T18:23:27.410Z", "isAdmin": false, "displayName": "jimmy"}, "userId": "JKdbpXHkv9AsuazJ3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HZb5vKcRcXZ62gPGb/october-2010-southern-california-meetup", "pageUrlRelative": "/posts/HZb5vKcRcXZ62gPGb/october-2010-southern-california-meetup", "linkUrl": "https://www.lesswrong.com/posts/HZb5vKcRcXZ62gPGb/october-2010-southern-california-meetup", "postedAtFormatted": "Monday, October 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20October%202010%20Southern%20California%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOctober%202010%20Southern%20California%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZb5vKcRcXZ62gPGb%2Foctober-2010-southern-california-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=October%202010%20Southern%20California%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZb5vKcRcXZ62gPGb%2Foctober-2010-southern-california-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHZb5vKcRcXZ62gPGb%2Foctober-2010-southern-california-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<p>We're having the third SoCal LessWrong meetup this Saturday, the 23rd. It'll be held at <a href=\"http://maps.google.com/maps?hl=en&amp;ie=UTF8&amp;q=18542+Macarthur+Blvd+Irvine+CA,+92612+ihop&amp;fb=1&amp;gl=us&amp;hq=ihop&amp;hnear=18542+MacArthur+Blvd,+Irvine,+CA+92612&amp;cid=0,0,16042522294109526569&amp;ei=uLa8TOG7MoWCsQO1nM2TDw&amp;ved=0CBYQnwIwAA&amp;ll=33.679247,-117.859418&amp;spn=0.009285,0.021136&amp;t=h&amp;z=16&amp;iwloc=A\">this IHOP</a> in Irvine, from 1PM to 8PM, in the upstairs meeting area.</p>\n<p>For those that haven't yet come, the last two were quite successful bringing 13 and 16 people respectively, and there was plenty of intelligent and friendly discussion.</p>\n<p>Make sure to comment if you have suggestions for how to improve on the last one, if you can give/need a ride, or just to say you're coming.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HZb5vKcRcXZ62gPGb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 6.363241826388787e-07, "legacy": true, "legacyId": "3774", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-19T08:22:05.411Z", "modifiedAt": null, "url": null, "title": "Rational Regions? ", "slug": "rational-regions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:21.013Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CMxaxnG3uJSqH6yNJ/rational-regions", "pageUrlRelative": "/posts/CMxaxnG3uJSqH6yNJ/rational-regions", "linkUrl": "https://www.lesswrong.com/posts/CMxaxnG3uJSqH6yNJ/rational-regions", "postedAtFormatted": "Tuesday, October 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Regions%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Regions%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCMxaxnG3uJSqH6yNJ%2Frational-regions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Regions%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCMxaxnG3uJSqH6yNJ%2Frational-regions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCMxaxnG3uJSqH6yNJ%2Frational-regions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p>Are certain areas of the world, specifically within the United States, more or less rational than others? If so, which ones and why? I am trying to determine what parts of the country would be ideal for me to live in the future and any help would be greatly appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CMxaxnG3uJSqH6yNJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 2, "extendedScore": null, "score": 6.364785675126108e-07, "legacy": true, "legacyId": "3776", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-20T03:00:34.608Z", "modifiedAt": null, "url": null, "title": "Help: Is there a quick and dirty way to explain quantum immortality?", "slug": "help-is-there-a-quick-and-dirty-way-to-explain-quantum", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:58.651Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "erratio", "createdAt": "2010-06-29T09:32:42.768Z", "isAdmin": false, "displayName": "erratio"}, "userId": "ty7er2ZYEnPYALnnJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/med7PYExueRxq5uLt/help-is-there-a-quick-and-dirty-way-to-explain-quantum", "pageUrlRelative": "/posts/med7PYExueRxq5uLt/help-is-there-a-quick-and-dirty-way-to-explain-quantum", "linkUrl": "https://www.lesswrong.com/posts/med7PYExueRxq5uLt/help-is-there-a-quick-and-dirty-way-to-explain-quantum", "postedAtFormatted": "Wednesday, October 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%3A%20Is%20there%20a%20quick%20and%20dirty%20way%20to%20explain%20quantum%20immortality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%3A%20Is%20there%20a%20quick%20and%20dirty%20way%20to%20explain%20quantum%20immortality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmed7PYExueRxq5uLt%2Fhelp-is-there-a-quick-and-dirty-way-to-explain-quantum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%3A%20Is%20there%20a%20quick%20and%20dirty%20way%20to%20explain%20quantum%20immortality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmed7PYExueRxq5uLt%2Fhelp-is-there-a-quick-and-dirty-way-to-explain-quantum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmed7PYExueRxq5uLt%2Fhelp-is-there-a-quick-and-dirty-way-to-explain-quantum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<p>I had an incredibly frustrating conversation this morning trying to explain the idea of quantum immortality to someone whose understanding of MWI begins and ends at pop sci fi movies. I think I've identified the main issue that I wasn't covering in enough depth (continuity of identity between near-identical realities) but I was wondering whether anyone has ever faced this problem before, and whether anyone has (or knows where to find) a canned 5 minute explanation of it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "med7PYExueRxq5uLt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 6.367426062345744e-07, "legacy": true, "legacyId": "3777", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-20T13:57:05.340Z", "modifiedAt": null, "url": null, "title": "How do autistic people learn how to read people's emotions?", "slug": "how-do-autistic-people-learn-how-to-read-people-s-emotions", "viewCount": null, "lastCommentedAt": "2022-05-23T01:41:37.274Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eugman", "createdAt": "2009-09-28T01:40:39.582Z", "isAdmin": false, "displayName": "eugman"}, "userId": "rtJy8Y9zXRpjWmeMi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DkjAjtgtjqPpQrWjd/how-do-autistic-people-learn-how-to-read-people-s-emotions", "pageUrlRelative": "/posts/DkjAjtgtjqPpQrWjd/how-do-autistic-people-learn-how-to-read-people-s-emotions", "linkUrl": "https://www.lesswrong.com/posts/DkjAjtgtjqPpQrWjd/how-do-autistic-people-learn-how-to-read-people-s-emotions", "postedAtFormatted": "Wednesday, October 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20do%20autistic%20people%20learn%20how%20to%20read%20people's%20emotions%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20do%20autistic%20people%20learn%20how%20to%20read%20people's%20emotions%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDkjAjtgtjqPpQrWjd%2Fhow-do-autistic-people-learn-how-to-read-people-s-emotions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20do%20autistic%20people%20learn%20how%20to%20read%20people's%20emotions%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDkjAjtgtjqPpQrWjd%2Fhow-do-autistic-people-learn-how-to-read-people-s-emotions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDkjAjtgtjqPpQrWjd%2Fhow-do-autistic-people-learn-how-to-read-people-s-emotions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<p>From my understanding, people on the autism spectrum have difficulty reading people's emotions and general social cues. I'm curious how these people develop these skills and what one can do to improve them. I ask this as a matter of personal interest; while I am somewhat neurotypical, I feel this is an area where I am very lacking.</p>\n<p>(Sidenote: would this be considered an appropriate used of the discussion section?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DkjAjtgtjqPpQrWjd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 6.368980432845439e-07, "legacy": true, "legacyId": "3781", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-22T02:29:18.779Z", "modifiedAt": null, "url": null, "title": "How are critical thinking skills acquired? Five perspectives", "slug": "how-are-critical-thinking-skills-acquired-five-perspectives", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:34.279Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ig4QYxoQHEDcRy8S5/how-are-critical-thinking-skills-acquired-five-perspectives", "pageUrlRelative": "/posts/ig4QYxoQHEDcRy8S5/how-are-critical-thinking-skills-acquired-five-perspectives", "linkUrl": "https://www.lesswrong.com/posts/ig4QYxoQHEDcRy8S5/how-are-critical-thinking-skills-acquired-five-perspectives", "postedAtFormatted": "Friday, October 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20are%20critical%20thinking%20skills%20acquired%3F%20Five%20perspectives&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20are%20critical%20thinking%20skills%20acquired%3F%20Five%20perspectives%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fig4QYxoQHEDcRy8S5%2Fhow-are-critical-thinking-skills-acquired-five-perspectives%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20are%20critical%20thinking%20skills%20acquired%3F%20Five%20perspectives%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fig4QYxoQHEDcRy8S5%2Fhow-are-critical-thinking-skills-acquired-five-perspectives", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fig4QYxoQHEDcRy8S5%2Fhow-are-critical-thinking-skills-acquired-five-perspectives", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 276, "htmlBody": "<p><strong>Link to source</strong>:&nbsp;<a href=\"http://timvangelder.com/2010/10/20/how-are-critical-thinking-skills-acquired-five-perspectives/\">http://timvangelder.com/2010/10/20/how-are-critical-thinking-skills-acquired-five-perspectives/</a><br /><strong>Previous LW discussion of argument mapping</strong>:&nbsp;<a href=\"/lw/16u/argument_maps_improve_critical_thinking/\">Argument Maps Improve Critical Thinking</a>,&nbsp;<a href=\"/lw/1qq/debate_tools_an_experience_report/\">Debate tools: an experience report</a></p>\n<p><a href=\"http://timvangelder.com/2010/10/20/how-are-critical-thinking-skills-acquired-five-perspectives/\"><strong>How are critical thinking skills acquired? Five perspectives</strong></a>:&nbsp;<a href=\"http://timvangelder.com/about/\">Tim van Gelder</a> discusses&nbsp;acquisition of critical thinking skills, suggesting several theories of skill acquisition that don't work, and one with which he and hundreds of his students have had significant success.</p>\n<blockquote>\n<p><span style=\"font-family: verdana, tahoma, arial, sans-serif; font-size: 12px; color: #333333; line-height: 19px;\">In our work in the Reason Project at the University of Melbourne we refined the Practice perspective into what we called the Quality (or Deliberate) Practice Hypothesis. &nbsp; This was based on the foundational work of Ericsson and others who have shown that skill acquisition in general depends on extensive quality practice. &nbsp;We conjectured that this would also be true of critical thinking; i.e. critical thinking skills would be (best) acquired by doing lots and lots of good-quality practice on a wide range of real (or realistic) critical thinking problems. &nbsp; To improve the quality of practice we developed a training program based around the use of argument mapping, resulting in what has been called the LAMP (Lots of Argument Mapping) approach. &nbsp; In a series of rigorous (or rather, as-rigorous-as-possible-under-the-circumstances) studies involving pre-, post- and follow-up testing using a variety of tests, and setting our results in the context of a meta-analysis of hundreds of other studies of critical thinking gains, we were able to establish that critical thinking skills gains could be dramatically accelerated, with students reliably improving 7-8 times faster, over one semester, than they would otherwise have done just as university students. &nbsp; (For some of the detail on the Quality Practice hypothesis and our studies, see&nbsp;</span><span style=\"font-family: verdana, tahoma, arial, sans-serif; font-size: 12px; color: #333333; line-height: 19px;\"><a style=\"text-decoration: none; color: #265e15; border-bottom-color: #996633; border-bottom-width: 1px; border-bottom-style: dashed; padding: 0px; margin: 0px;\" href=\"http://sites.google.com/site/timvangelder/publications-1/cultivating-expertise-in-informal-reasoning\">this paper</a></span><span style=\"font-family: verdana, tahoma, arial, sans-serif; font-size: 12px; color: #333333; line-height: 19px;\">, and&nbsp;</span><span style=\"font-family: verdana, tahoma, arial, sans-serif; font-size: 12px; color: #333333; line-height: 19px;\"><a style=\"text-decoration: none; color: #265e15; border-bottom-color: #996633; border-bottom-width: 1px; border-bottom-style: dashed; padding: 0px; margin: 0px;\" href=\"http://sites.google.com/site/timvangelder/publications-1/does-philosophy-improve-reasoning-skills/AlvarezThesis-Abridged-Chapter5%2CMetaAnalysis.pdf?attredirects=0&amp;d=1\">this chapter</a></span><span style=\"font-family: verdana, tahoma, arial, sans-serif; font-size: 12px; color: #333333; line-height: 19px;\">.)</span></p>\n</blockquote>\n<p><span style=\"font-family: verdana, tahoma, arial, sans-serif; font-size: 12px; color: #333333; line-height: 19px;\">LW has been introduced to argument mapping <a href=\"/lw/16u/argument_maps_improve_critical_thinking/\">before</a>.&nbsp;</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ig4QYxoQHEDcRy8S5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 6.374167951360131e-07, "legacy": true, "legacyId": "3785", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NJ8k2RTwy3ELmwYZT", "dJJYgmaYYFmHoQM4L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-22T11:53:18.401Z", "modifiedAt": null, "url": null, "title": "Does it matter if you don't remember?", "slug": "does-it-matter-if-you-don-t-remember", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:03.882Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexflint", "createdAt": "2009-07-17T10:07:09.115Z", "isAdmin": false, "displayName": "Alex Flint"}, "userId": "ifEGDHySkAejhCFDf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jxDY2EfNp9pHbyb8g/does-it-matter-if-you-don-t-remember", "pageUrlRelative": "/posts/jxDY2EfNp9pHbyb8g/does-it-matter-if-you-don-t-remember", "linkUrl": "https://www.lesswrong.com/posts/jxDY2EfNp9pHbyb8g/does-it-matter-if-you-don-t-remember", "postedAtFormatted": "Friday, October 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20it%20matter%20if%20you%20don't%20remember%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20it%20matter%20if%20you%20don't%20remember%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjxDY2EfNp9pHbyb8g%2Fdoes-it-matter-if-you-don-t-remember%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20it%20matter%20if%20you%20don't%20remember%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjxDY2EfNp9pHbyb8g%2Fdoes-it-matter-if-you-don-t-remember", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjxDY2EfNp9pHbyb8g%2Fdoes-it-matter-if-you-don-t-remember", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 209, "htmlBody": "<p>Does it matter if you experienced pain in the past, but you don't remember? (And there are no other side-effects, etc etc). At one point in <em>Accelerando</em>, Charles Strauss describes children that routinely decapitate and disembowel each other, only to be repaired (bodily and memory-wise) by the friendly local AI. This struck me as awful, but I'm suspicious of my intuition. Note that here I'm assuming pain is a terminal \"bad\" factor in your utility function. You can substitute \"pain\" for whatever you think is bad.&nbsp;I think there are at least two questions here:</p>\n<ol>\n<li>Is it bad for someone to be in pain if they will not remember it in the future? I think yes, because by assumption pain is a terminal \"bad\" node. Being&nbsp;relieved&nbsp;of future painful memories is&nbsp;good, but nowhere near good enough to fully compensate.</li>\n<li>Is it bad to have experienced pain in the past, if you don't remember it? Or, can your utility function coherently include facts about the past, even if they have no causal connection to the present?&nbsp;My intuition here says yes, but I'd be interested in others' thoughts.&nbsp;To make this concrete, imaging that you have a choice between medium pain that you will remember, or extreme pain followed by memory erasure.</li>\n</ol>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jxDY2EfNp9pHbyb8g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 6.375503753884056e-07, "legacy": true, "legacyId": "3789", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-22T11:59:45.211Z", "modifiedAt": null, "url": null, "title": "Church: a language for probabilistic modeling", "slug": "church-a-language-for-probabilistic-modeling", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:33.851Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HCQgQLZGhHynAQZqb/church-a-language-for-probabilistic-modeling", "pageUrlRelative": "/posts/HCQgQLZGhHynAQZqb/church-a-language-for-probabilistic-modeling", "linkUrl": "https://www.lesswrong.com/posts/HCQgQLZGhHynAQZqb/church-a-language-for-probabilistic-modeling", "postedAtFormatted": "Friday, October 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Church%3A%20a%20language%20for%20probabilistic%20modeling&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChurch%3A%20a%20language%20for%20probabilistic%20modeling%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHCQgQLZGhHynAQZqb%2Fchurch-a-language-for-probabilistic-modeling%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Church%3A%20a%20language%20for%20probabilistic%20modeling%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHCQgQLZGhHynAQZqb%2Fchurch-a-language-for-probabilistic-modeling", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHCQgQLZGhHynAQZqb%2Fchurch-a-language-for-probabilistic-modeling", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 431, "htmlBody": "<p>I've been reading about <a href=\"http://www.naviasystems.com/wp-content/uploads/2010/08/Navia-Church.pdf\" target=\"_blank\">Church</a>, which is a new computer language, developed in a prize-winning MIT doctoral <a href=\"http://www.naviasystems.com/wp-content/uploads/2010/08/navia_vkm_dissertation.pdf\" target=\"_blank\">thesis</a>, that's designed to make computers better at modeling probability distributions. &nbsp;</p>\n<p>The idea is that simulations are cheap to run (given a probability distribution, generate an example outcome) but inferred distributions are expensive to run (from a set of data, what was the most likely probability distribution that could have generated it?) This is essentially a Bayesian task, and it's what we want to do to understand, say, which borrowers are likeliest to default, or where terrorists are likely to strike again. &nbsp;It's also the necessary building block of AI. &nbsp;The problem is that the space of probability distributions that can explain the data is very big. &nbsp;Infinitely big in reality, of course, but still exponentially big after discretizing. &nbsp;Also, while the computational complexity of evaluating f(g(x)) is just f + g, the computational complexity of composing two conditional probability distributions B|A and C|B is</p>\n<p>&Sigma;<sub>B</sub>&nbsp;P(C, B|A)</p>\n<p>whose computational time will grow exponentially rather than linearly.</p>\n<p>Church is an attempt to solve this problem. &nbsp;(Apparently it's a practical attempt, because the founders have already started a company, <a href=\"http://www.naviasystems.com/\">Navia Systems</a>, using this structure to build probabilistic computers.) &nbsp;The idea is, instead of describing a probability distribution as a deterministic procedure that evaluates the probabilities of different events, represent them in terms of probabilistic procedures for generating samples from them. &nbsp;That is, a random variable is <em>actually</em>&nbsp;a random variable. &nbsp;This means that repeating a computation will not give the same result each time, because evaluating a random variable doesn't give the same result each time. &nbsp;There's a computational advantage here because it's possible to compose random variables without summing over all possible values.</p>\n<p>Church is based on Lisp. At the lowest level, it replaces Boolean gates with stochastic digital circuits. &nbsp;These circuits are wired together to form Markov chains (the probabilistic counterpart of finite state machines.) &nbsp;At the top, it's possible to define probabilistic procedures for generating samples from recursively defined distributions.</p>\n<p>&nbsp;</p>\n<p>When I saw this paper, I thought it might make an interesting top-level post -- unfortunately, I'm not the one to do it. &nbsp;I don't know enough computer science; it's been ages since I've touched Lisp, and <a href=\"http://en.wikipedia.org/wiki/Lambda_calculus\">lambda calculus </a>is new to me. &nbsp;So this is an open call for volunteers -- any brave Bayesians want to blog about a brand new computer language?</p>\n<p>(As an aside, I think we need more technical posts so we don't spend all our time hissing at each other; how would people feel about seeing summaries of recent research in the neuro/cognitive science/AI-ish cluster?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HCQgQLZGhHynAQZqb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 25, "extendedScore": null, "score": 6.375517838580998e-07, "legacy": true, "legacyId": "3790", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-22T21:49:12.297Z", "modifiedAt": null, "url": null, "title": "Does inclusive fitness theory miss part of the picture?", "slug": "does-inclusive-fitness-theory-miss-part-of-the-picture", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:23.075Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nhamann", "createdAt": "2009-10-15T04:19:24.675Z", "isAdmin": false, "displayName": "nhamann"}, "userId": "t6Fcfj7hMxrdJraSD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QaR6pE2pcjoDm7MmG/does-inclusive-fitness-theory-miss-part-of-the-picture", "pageUrlRelative": "/posts/QaR6pE2pcjoDm7MmG/does-inclusive-fitness-theory-miss-part-of-the-picture", "linkUrl": "https://www.lesswrong.com/posts/QaR6pE2pcjoDm7MmG/does-inclusive-fitness-theory-miss-part-of-the-picture", "postedAtFormatted": "Friday, October 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20inclusive%20fitness%20theory%20miss%20part%20of%20the%20picture%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20inclusive%20fitness%20theory%20miss%20part%20of%20the%20picture%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQaR6pE2pcjoDm7MmG%2Fdoes-inclusive-fitness-theory-miss-part-of-the-picture%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20inclusive%20fitness%20theory%20miss%20part%20of%20the%20picture%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQaR6pE2pcjoDm7MmG%2Fdoes-inclusive-fitness-theory-miss-part-of-the-picture", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQaR6pE2pcjoDm7MmG%2Fdoes-inclusive-fitness-theory-miss-part-of-the-picture", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 872, "htmlBody": "<p>I originally titled this post \"The Less Wrong wiki is wrong about group selection\", because it seemed wildly overconfident about its assertion that group selection is nonsense. The <a href=\"http://wiki.lesswrong.com/wiki/Group_selection\">wiki entry on \"group selection\"</a> currently reads:</p>\n<blockquote>\n<p>People who are unfamiliar with evolutionary theory sometimes propose that a feature of the organism is there <em>for the good of the group</em> - for example, that human <a class=\"external text\" rel=\"nofollow\" href=\"/lw/mk/a_failed_justso_story\">religion is an adaptation to make human groups more cohesive</a>, since religious groups outfight nonreligious groups.</p>\n<p>Postulating group selection is guaranteed to make professional evolutionary biologists roll up their eyes and sigh.</p>\n</blockquote>\n<p>However, it appears that the real problem is not that the wiki is overconfident (that's a problem, but it's only a symptom of the next problem) but that the traditional dogma on the viability of group selection is wrong, or at least overconfident. I make this assertion after stumbling across a paper by <a href=\"http://www.fas.harvard.edu/%7Eped/people/faculty/\">Martin Nowak</a>, <a href=\"http://www.math.harvard.edu/%7Ecorina/me.html\">Corina Tarnita</a>, and <a href=\"http://en.wikipedia.org/wiki/E._O._Wilson\">E. O. Wilson</a> titled <a href=\"http://www.nature.com/nature/journal/v466/n7310/full/nature09205.html\">\"The evolution of eusociality\"</a>, which appeared in Nature in August of this year. I found a PDF of this paper through Google scholar, <a href=\"http://bio.fsu.edu/%7Emiller/HOMEPAGE/docs/Nowak%20et%20al.%202010.pdf\">click here</a>. A blog entry discussing the paper can be found <a href=\"http://plektix.fieldofscience.com/2010/08/eusociality-and-blow-to-kin-selection.html\">here</a> (bias alert: it is written by a postdoc working in Martin Nowak's Evolutionary Dynamics program at Harvard).</p>\n<p>Here's some quotes (bolding is mine):</p>\n<blockquote>\n<p>It has further turned out that selection forces exist in groups that diminish the advantage of close collateral kinship. They include the favouring of raised genetic variability by colony-level selection in the ants <em>Pogonomyrmex occidentalis</em><sup> </sup>and <em>Acromyrmex echinatior</em>&mdash;due, at least in the latter, to disease resistance. The contribution of genetic diversity to disease resistance at the colony level has moreover been established definitively in honeybees. Countervailing forces also include variability in predisposition to worker sub-castes in <em>Pogonomyrmex badius</em>, which may sharpen division of labour and improve colony fitness&mdash;although that hypothesis is yet to be tested. Further, an increase in stability of nest temperature with genetic diversity has been found within nests of honeybees and <em>Formica</em> ants. Other selection forces working against the binding role of close pedigree kinship are the disruptive impact of nepotism within colonies, and the overall negative effects associated with inbreeding. <strong>Most of these countervailing forces act through group selection or, for eusocial insects in particular, through between-colony selection.</strong></p>\n</blockquote>\n<blockquote>\n<p>Yet, considering its position for four decades as the dominant paradigm in the theoretical study of eusociality, the production of inclusive fitness theory must be considered meagre. During the same period, in contrast, empirical research on eusocial organisms has flourished, revealing the rich details of caste, communication, colony life cycles, and other phenomena at both the individual- and colony-selection levels. In some cases social behaviour has been causally linked through all the levels of biological organization from molecule to ecosystem. <strong>Almost none of this progress has been stimulated or advanced by inclusive fitness theory, which has evolved into an abstract enterprise largely on its own</strong></p>\n</blockquote>\n<p>...</p>\n<blockquote>\n<p>The question arises: if we have a theory that works for all cases (standard natural selection theory) and a theory that works only for a small subset of cases (inclusive fitness theory), and if for this subset the two theories lead to identical conditions, then why not stay with the general theory? The question is pressing, because inclusive fitness theory is provably correct only for a small (non-generic) subset of evolutionary models, but the intuition it provides is mistakenly embraced as generally correct.</p>\n</blockquote>\n<p>Check out the paper for more details. Also look at the Supplementary Information if you have access to it. They perform an evolutionary game theoretic analysis, which I am still reading.</p>\n<p>Apparently this theory is not that new. <a href=\"http://www.cogsci.msu.edu/DSS/2006-2007/Wilson/Rethinking_July_20.pdf\">In this 2007 paper</a> by David Sloan Wilson and E. O. Wilson, they argue (I'm just pasting the abstract):</p>\n<blockquote>\n<p>The current foundation of sociobiology is based upon the rejection of group selection in the 1960s and the acceptance thereafter of alternative theories to explain the evolution of cooperative and altruistic behaviors. These events need to be reconsidered in the light of subsequent research. Group selection has become both theoretically plausible and empirically well supported. Moreover, the so-called alternative theories include the logic of multilevel selection within their own frameworks. We review the history and conceptual basis of sociobiology to show why a new consensus regarding group selection is needed and how multilevel selection theory can provide a more solid foundation for sociobiology in the future.</p>\n</blockquote>\n<p>From the other camp, <a href=\"http://www.biology.ed.ac.uk/research/groups/gardner/publications/WestGriffinGardner_2008.pdf\">this</a> seems to be a fairly highly-cited paper from 2008. They concluded:</p>\n<blockquote>\n<p>(a) the arguments about group selection are only continued by a limited number of theoreticians, on the basis of simpli\ufb01ed models that can be dif\ufb01cult to apply to real organisms (see Error 3); (b) theoretical models which make testable predictions tend to be made with kin selection theory (Tables 1 and 2); (c) empirical biologists interested in social evolution measure the kin selection coef\ufb01cient of relatedness rather than the corresponding group selection parameters (Queller &amp; Goodnight, 1989). It is best to think of group selection as a potentially useful, albeit informal, way of conceptualizing some issues, rather than a general evolutionary approach in its own right.</p>\n</blockquote>\n<p>I know (as of yet) very little biology, so I leave the conclusion for readers to discuss. Does anyone have detailed knowledge of the issues here?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QaR6pE2pcjoDm7MmG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "3792", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kX6C2qdngKp4AdEAk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-23T04:10:27.684Z", "modifiedAt": null, "url": null, "title": "Interesting talk on Bayesians and frequentists", "slug": "interesting-talk-on-bayesians-and-frequentists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:20.978Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsteinhardt", "createdAt": "2010-08-05T03:07:27.568Z", "isAdmin": false, "displayName": "jsteinhardt"}, "userId": "EF8W65G6RaXxZjLBX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BW9A6hiwByJH3uEcx/interesting-talk-on-bayesians-and-frequentists", "pageUrlRelative": "/posts/BW9A6hiwByJH3uEcx/interesting-talk-on-bayesians-and-frequentists", "linkUrl": "https://www.lesswrong.com/posts/BW9A6hiwByJH3uEcx/interesting-talk-on-bayesians-and-frequentists", "postedAtFormatted": "Saturday, October 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interesting%20talk%20on%20Bayesians%20and%20frequentists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInteresting%20talk%20on%20Bayesians%20and%20frequentists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBW9A6hiwByJH3uEcx%2Finteresting-talk-on-bayesians-and-frequentists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interesting%20talk%20on%20Bayesians%20and%20frequentists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBW9A6hiwByJH3uEcx%2Finteresting-talk-on-bayesians-and-frequentists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBW9A6hiwByJH3uEcx%2Finteresting-talk-on-bayesians-and-frequentists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 337, "htmlBody": "<p>I recently started watching an interesting lecture by Michael Jordan on Bayesians and frequentists; he's a pretty successful machine learning expert that takes both views in his work. You can watch it here:&nbsp;<a href=\"http://videolectures.net/mlss09uk_jordan_bfway/\">http://videolectures.net/mlss09uk_jordan_bfway/</a>. I found it interesting because his portrayal of frequentism is much different than the standard portrayal on lesswrong. It isn't about whether probabilities are frequencies or beliefs, it's about trying to get a good model versus trying to get rigorous guarantees of performance in a class of scenarios. So I wonder why the meme on lesswrong is that frequentists think probabilities are frequencies; in practice it seems to be more about how you approach a given problem. In fact, frequentists seem more \"rational\", as they're willing to use any tool that solves a problem instead of constraining themselves to methods that obey Bayes' rule.</p>\n<p>In practice, it seems that while Bayes is the main tool for epistemic rationality, instrumental rationality should oftentimes be frequentist at the top level (with epistemic rationality, guided by Bayes, in turn guiding the specific application of a frequentist algorithm).</p>\n<p>For instance, in many cases I should be willing to, once I have a sufficiently constrained search space, try different things until one of the works, without worrying about understanding why the specific thing I did worked (think shooting a basketball, or riffle shuffling a deck of cards). In practice, it seems like epistemic rationality is important for constraining a search space, and after that some sort of online learning algorithm can be applied to find the optimal action from within that search space. Of course, this isn't true when you only get one chance to do something, or extreme precision is required, but this is not often true in everyday life.</p>\n<p>The main point of this thread is to raise awareness of the actual distinction between Bayesians and frequentists, and why it's actually reasonable to be both, since it seems like lesswrong is strongly Bayesian and there isn't even a good discussion of the fact that there are other methods out there.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BW9A6hiwByJH3uEcx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 11, "extendedScore": null, "score": 6.377819288747719e-07, "legacy": true, "legacyId": "3794", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-23T04:45:57.854Z", "modifiedAt": null, "url": null, "title": "Is there a \"percentage fallacy\"?", "slug": "is-there-a-percentage-fallacy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:21.776Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ata", "createdAt": "2009-07-20T22:13:53.102Z", "isAdmin": false, "displayName": "ata"}, "userId": "KppHkGEqTNeDaGJTc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ceyNA9E3yJe9J3CLw/is-there-a-percentage-fallacy", "pageUrlRelative": "/posts/ceyNA9E3yJe9J3CLw/is-there-a-percentage-fallacy", "linkUrl": "https://www.lesswrong.com/posts/ceyNA9E3yJe9J3CLw/is-there-a-percentage-fallacy", "postedAtFormatted": "Saturday, October 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20there%20a%20%22percentage%20fallacy%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20there%20a%20%22percentage%20fallacy%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FceyNA9E3yJe9J3CLw%2Fis-there-a-percentage-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20there%20a%20%22percentage%20fallacy%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FceyNA9E3yJe9J3CLw%2Fis-there-a-percentage-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FceyNA9E3yJe9J3CLw%2Fis-there-a-percentage-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 465, "htmlBody": "<p>A couple years ago, Aaron Swartz blogged about what he called the \"<a href=\"http://www.aaronsw.com/weblog/percentagefallacy\">percentage fallacy</a>\":</p>\n<blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; text-rendering: optimizelegibility; font-size: 1em; line-height: 1.5em; padding: 0px;\">There&rsquo;s one bit of irrationality that seems like it ought to be in behavioral economics introduction but mysteriously isn&rsquo;t. For lack of a better term, let&rsquo;s call it&nbsp;<em>the percentage fallacy</em>. The idea is simple:</p>\n<blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; text-rendering: optimizelegibility; font-size: 1em; line-height: 1.5em; padding: 0px;\">One day I find I need a blender. I see a particularly nice one at the store for $40, so I purchase it and head home. But on the way home, I see the exact same blender on sale at a different store for $20. Now I feel ripped off, so I drive back to the first store, return the blender, drive back to the second store, and buy it for $20.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; text-rendering: optimizelegibility; font-size: 1em; line-height: 1.5em; padding: 0px;\">The next day I find I need a laptop. I see a particularly nice one at the store for $2500, so I purchase it and head home. But on the way home, I see the exact same laptop for $2480. &ldquo;Pff, well, it&rsquo;s only $20,&rdquo; I say, and continue home with the original laptop.</p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; text-rendering: optimizelegibility; font-size: 1em; line-height: 1.5em; padding: 0px;\">I&rsquo;m sure all of you have done something similar &mdash; maybe the issue wasn&rsquo;t having to return something, but spending more time looking for a cheaper model, or fiddling with coupons and rebates, or buying something of inferior quality. But the basic point is consistent: we&rsquo;ll do things to save 50% that we&rsquo;d never do to save 1%.</p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; text-rendering: optimizelegibility; font-size: 1em; line-height: 1.5em; padding: 0px;\">He <a href=\"http://www.aaronsw.com/weblog/percentagefallacy2\">recently followed</a> up with a speculation that this may explain some irrational behaviour normally attributed to hyperbolic discounting:</p>\n<blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; text-rendering: optimizelegibility; font-size: 1em; line-height: 1.5em; padding: 0px;\">In a famous experiment, some people are asked to choose between $100 today or $120 tomorrow. Many choose the first. Meanwhile, some people are asked to choose between $100 sixty days from now or $120 sixty-one days from now. Almost everyone choose the laster. The puzzle is this: why are people willing to sacrifice $20 to avoid waiting a day right now but not in the future?</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; text-rendering: optimizelegibility; font-size: 1em; line-height: 1.5em; padding: 0px;\">The standard explanation is hyperbolic discounting: humans tend to weigh immediate effects much more strongly than distant ones. But I think the actual psychological effect at work here is just&nbsp;<a href=\"http://www.aaronsw.com/weblog/percentagefallacy\">the percentage fallacy</a>. If I ask for the money now, I may have to wait 60 seconds. But if I get it tomorrow I have to wait 143900% more. By contrast, waiting 61 days is only 1.6% worse than waiting 6 days. Why not wait an extra 2% when you get 16% more money for it?</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; text-rendering: optimizelegibility; font-size: 1em; line-height: 1.5em; padding: 0px;\">Has anyone done a test confirming the percentage fallacy? A good test would be to show people treat the $100 vs. $120 tradeoff as equivalent to the $1000 to $1200 tradeoff.</p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; text-rendering: optimizelegibility; font-size: 1em; line-height: 1.5em; padding: 0px;\">Is this a real thing? Is there any such research? Is there existing evidence that does especially support the usual hyperbolic discounting explanation over this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ceyNA9E3yJe9J3CLw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 17, "extendedScore": null, "score": 6.377903446673153e-07, "legacy": true, "legacyId": "3796", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-23T05:14:07.308Z", "modifiedAt": null, "url": null, "title": "The Problem With Trolley Problems", "slug": "the-problem-with-trolley-problems", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:23.372Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lionhearted", "createdAt": "2010-07-29T13:30:07.417Z", "isAdmin": false, "displayName": "lionhearted"}, "userId": "tooJeLNxoeccqGEky", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h22n4nZQd9J2MEZxq/the-problem-with-trolley-problems", "pageUrlRelative": "/posts/h22n4nZQd9J2MEZxq/the-problem-with-trolley-problems", "linkUrl": "https://www.lesswrong.com/posts/h22n4nZQd9J2MEZxq/the-problem-with-trolley-problems", "postedAtFormatted": "Saturday, October 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Problem%20With%20Trolley%20Problems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Problem%20With%20Trolley%20Problems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh22n4nZQd9J2MEZxq%2Fthe-problem-with-trolley-problems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Problem%20With%20Trolley%20Problems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh22n4nZQd9J2MEZxq%2Fthe-problem-with-trolley-problems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh22n4nZQd9J2MEZxq%2Fthe-problem-with-trolley-problems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1014, "htmlBody": "<p>A trolley problem is something that's used increasing often in philosophy to get at people's beliefs and debate on them. Here's an example from Wikipedia:</p>\n<blockquote>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px;\">As before, a trolley is hurtling down a track towards five people. You are on a bridge under which it will pass, and you can stop it by dropping a heavy weight in front of it. As it happens, there is a very fat man next to you - your only way to stop the trolley is to push him over the bridge and onto the track, killing him to save five. Should you proceed?</span></p>\n</blockquote>\n<p>I believe trolley problems are fundamentally flawed - at best a waste of time, and at worst lead to really sloppy thinking. Here's four reasons why:</p>\n<p>1. It assumes perfect information about outcomes.</p>\n<p>2. It ignores the global secondary effects that local choices create.</p>\n<p>3. It ignores real human nature - which would be to freeze and be indecisive.</p>\n<p>4. It usually gives you two choices and no alternatives, and in real life, there's always alternatives.</p>\n<p>First, trolley problems contain <strong>perfect information about outcomes</strong>&nbsp;- which is rarely the case in real life. In real life, you're making choices based on <strong>imperfect information</strong>. You don't know what would happen for sure as a result of your actions.&nbsp;</p>\n<p>Second, everything creates <strong>secondary effects</strong>. If putting people involuntarily in harm's way to save others was an acceptable result, suddenly we'd all have to be really careful in any emergency. Imagine living in a world where anyone would be comfortable ending your life to save other people nearby - you'd have to not only be constantly checking your surroundings, but also constantly on guard against do-gooders willing to push you onto the tracks.</p>\n<p>Third, <strong>it ignores human nature</strong>. Human nature is to freeze up when bad things happen unless you're explicitly trained to react. In real life, most people would freeze or panic instead of react. In order to get over that, first responders, soldiers, medics, police, firefighters go through training. That training includes dealing with questionable circumstances and how to evaluate them, so you don't have a society where your trained personnel act randomly in emergencies.&nbsp;</p>\n<p>Fourth, <strong>it gives you two choices and no alternatives</strong>. I firmly reject this - I think there's almost always alternative ways to get there from here if you open your mind to it. Once you start thinking that your only choice is to push the one guy in front of the trolley or to stand there doing nothing, your mind is closed to all other alternatives.</p>\n<p>At best, this means trolley problems are just a harmless waste of time. But I think they're not just a harmless waste of time.</p>\n<p><strong>I think \"trolley problem\" type thinking is commonly used in real life to advocate and justify bad policy.</strong></p>\n<p>Here's how it goes:</p>\n<p>Activist says, \"We've got to take from this rich fat cat and give it to these poor people, or the poor people will starve and die. If you take the money, the fat cat will buy less cars and yachts, and the poor people will become much more successful and happy.\"</p>\n<p>You'll see all the flaws I described above in that statement.</p>\n<p>First, it assumes perfect information. The activist says that taking more money will lead to less yachts and cars - useless consumption. He doesn't consider that people might first cut their charity budget, or their investment budget, or something else. Higher tax jurisdictions, like Northern Europe, have very low levels of charitable giving. They also have relatively low levels of capital investment.</p>\n<p>Second, it ignores secondary effects. The activist assumes he can milk the cow and the cow won't mind. In reality, people start spending their time on minimizing their tax burden instead of doing productive work. It ripples through society.</p>\n<p>Third, it ignores human nature. Saying \"the fat cat won't miss it\" is false - everyone is loss averse.&nbsp;</p>\n<p>Fourth, the biggest problem of all, it gives two choices and no alternatives. \"Tax the fat cat, or the poor people starve\" - is there no other way to encourage charitable giving? Could we give charity visas where anyone giving $500,000 in philanthropy to the poor can get fast-track residency into the USA? Could we give larger tax breaks to people who choose to take care of distant relatives as a dependent? Are there other ways? Once the debate gets constrained to, \"We must do this, or starvation is the result\" you've got problems.</p>\n<p>And I think that these poor quality thoughts on policy are a direct descendant of trolley problems. It's the same line of thinking - perfect information, ignores secondary effects, ignores human nature, and gives two choices while leaving no other alternatives. That's not real life. That's sloppy thinking.</p>\n<p>Edit:&nbsp;This is being very poorly received so far... well, it was quickly voted up to +3, and now it's down to -2, which means controversial but generally negative reception.</p>\n<p>Do people disagree? I understand trolley problems are an established part of critical thinking on philosophy, however, I think they're flawed and I wanted to highlight those flaws.</p>\n<p>The best counterargument I see right now is that the value of a trolley problem is it reduces everything to just the moral decision. That's an interesting point, however, I think you could come up with better&nbsp;hypotheticals&nbsp;that don't suffer from this flaw. Or perhaps the particular politics example isn't popular? You can substitute in similar arguments for prohibition of alcohol, and perhaps I ought to have done that to make it less controversial. In any event, I welcome discussion and disagreement.</p>\n<p><strong>Questions for you:</strong> I think that trolley problems contain perfect information about outcomes in advance of them happening, ignore secondary effects, ignore human nature, and give artificially false constraints. Do you agree with that part? I think that's pretty much fact. Now, I think that's bad. Agree/disagree there? Okay, finally, I think this kind of thinking seeps over into politics, and it's likewise bad there. Agree/disagree? I know this is a bit of controversial argument since trolley problems are common in philosophy, but I'd encourage you to have a think on what I wrote and agree, disagree, and otherwise discuss.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LMFBzsJaCRADQqw3F": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h22n4nZQd9J2MEZxq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 64, "baseScore": 14, "extendedScore": null, "score": 4.1e-05, "legacy": true, "legacyId": "3797", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 113, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-23T11:42:46.099Z", "modifiedAt": null, "url": null, "title": "Green consumers more likely to steal", "slug": "green-consumers-more-likely-to-steal", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:20.913Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nick012000", "createdAt": "2010-07-12T11:00:38.790Z", "isAdmin": false, "displayName": "nick012000"}, "userId": "yCSrMSsugCiSiqAY2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h29tYzT3HuhWdkQ8m/green-consumers-more-likely-to-steal", "pageUrlRelative": "/posts/h29tYzT3HuhWdkQ8m/green-consumers-more-likely-to-steal", "linkUrl": "https://www.lesswrong.com/posts/h29tYzT3HuhWdkQ8m/green-consumers-more-likely-to-steal", "postedAtFormatted": "Saturday, October 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Green%20consumers%20more%20likely%20to%20steal&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGreen%20consumers%20more%20likely%20to%20steal%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh29tYzT3HuhWdkQ8m%2Fgreen-consumers-more-likely-to-steal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Green%20consumers%20more%20likely%20to%20steal%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh29tYzT3HuhWdkQ8m%2Fgreen-consumers-more-likely-to-steal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh29tYzT3HuhWdkQ8m%2Fgreen-consumers-more-likely-to-steal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 573, "htmlBody": "<p>Here's an article I found today, though it is a few months old; it seems to illustrate an interesting cognitive bias that I don't think I've seen mentioned before that might explain a good bit of hypocrisy people tend to have.</p>\n<blockquote>\n<p>When Al Gore was caught running up huge energy bills at home at the same time as lecturing on the need to save electricity, it turns out that he was only reverting to \"green\" type.</p>\n<p>According to a study, when people feel they have been morally virtuous by saving the planet through their purchases of organic baby food, for example, it leads to the \"licensing [of] selfish and morally questionable behaviour\", otherwise known as \"moral balancing\" or \"compensatory ethics\".</p>\n<p>Do Green Products Make Us Better People is published in the latest edition of the journal Psychological Science. Its authors, Canadian psychologists Nina Mazar and Chen-Bo Zhong, argue that people who wear what they call the \"halo of green consumerism\" are less likely to be kind to others, and more likely to cheat and steal. \"Virtuous acts can license subsequent asocial and unethical behaviours,\" they write. [See footnote].</p>\n<p>The pair found that those in their study who bought green products appeared less willing to share with others a set amount of money than those who bought conventional products. When the green consumers were given the chance to boost their money by cheating on a computer game and then given the opportunity to lie about it &ndash; in other words, steal &ndash; they did, while the conventional consumers did not. Later, in an honour system in which participants were asked to take money from an envelope to pay themselves their spoils, the greens were six times more likely to steal than the conventionals.</p>\n<p>Mazar and&nbsp;Zhong said their study showed that just as exposure to pictures of exclusive restaurants can improve table manners but may not lead to an overall improvement in behaviour, \"green products do not necessarily make for better people\". They added that one motivation for carrying out the study was that, despite the \"stream of research focusing on identifying the 'green consumer'\", there was a lack of understanding into \"how green consumption fits into people's global sense of responsibility and morality and [how it] affects behaviours outside the consumption domain\".</p>\n<p>The pair said their findings surprised them, having thought that just as \"exposure to the Apple logo increased creativity\", according to a recent study, \"given that green products are manifestations of high ethical standards and humanitarian considerations, mere exposure\" to them would \"activate norms of social responsibility and ethical conduct\".</p>\n<p>Dieter Frey, a social psychologist at the University of Munich, said the findings fitted patterns of human behaviour. \"At the moment in which you have proven your credentials in a particular area, you tend to allow yourself to stray elsewhere,\" he said.</p>\n<p>&bull; This footnote was added on 31 March 2010: The study findings above, and the methods used, are challenged by researchers associated with the social <a title=\"More from guardian.co.uk on Psychology\" href=\"http://www.guardian.co.uk/science/psychology\">psychology</a> department at the London School of Economics, the Institute of Ecological Economy Research in Berlin, and the Institute for Perspective Technological Studies in Seville. Their analysis can be found here: <a href=\"http://www.lrcg.co.uk/index.php?option=com_content&amp;task=view&amp;id=44&amp;Itemid=1\">lrcg.co.uk</a></p>\n</blockquote>\n<p><a href=\"http://www.guardian.co.uk/environment/2010/mar/15/green-consumers-more-likely-steal\">Here</a> is a link to the actual article. Thoughts? It seems like a trap that it'd be easy for the folks here to fall into, if they start self-congratulating about being Rational.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h29tYzT3HuhWdkQ8m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 6.37889161084477e-07, "legacy": true, "legacyId": "3798", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-23T16:01:59.956Z", "modifiedAt": null, "url": null, "title": "Three kinds of political similarity", "slug": "three-kinds-of-political-similarity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:21.510Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ubgBqRwHG9u7idAig/three-kinds-of-political-similarity", "pageUrlRelative": "/posts/ubgBqRwHG9u7idAig/three-kinds-of-political-similarity", "linkUrl": "https://www.lesswrong.com/posts/ubgBqRwHG9u7idAig/three-kinds-of-political-similarity", "postedAtFormatted": "Saturday, October 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Three%20kinds%20of%20political%20similarity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThree%20kinds%20of%20political%20similarity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FubgBqRwHG9u7idAig%2Fthree-kinds-of-political-similarity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Three%20kinds%20of%20political%20similarity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FubgBqRwHG9u7idAig%2Fthree-kinds-of-political-similarity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FubgBqRwHG9u7idAig%2Fthree-kinds-of-political-similarity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1235, "htmlBody": "<p>Suppose you wanted to describe the political landscape efficiently. That is, you want to describe the range of existing political positions, and develop some way of evaluating which political positions are \"similar.\" (I'm American, so I'm going to think in terms of the US.) &nbsp;There are three ways you could go about this and they are radically different.</p>\n<p>&nbsp;</p>\n<p><strong>1. &nbsp;Politicians' voting.</strong></p>\n<p>Because we have a two-party system, politicians (and some other political players, like Supreme Court justices) make decisions on a single, left-right axis. &nbsp;You can explain almost all the variation with one dimension: \"More Republican\" or \"More Democrat.\" &nbsp;Generally, if a politician supports one \"Republican\" position, he'll support most of the other \"Republican\" positions. &nbsp;Studies have been done here (too lazy to look them up right now) supporting this phenomenon at all levels of politics.</p>\n<p><strong>2. &nbsp;Individuals' opinions on poll questions.</strong></p>\n<p>Here, we do something different. &nbsp;We ask poll questions about a variety of issues and see what people think. &nbsp;Suddenly, it's not one-dimensional any more. &nbsp;There are people who like low taxes and also like gay marriage. &nbsp;There are people who are anti-abortion and anti-war. &nbsp;The fact that individual opinions don't line up along a left-right axis is what's behind ideas like the Nolan Chart -- although it doesn't imply that the \"real\" way to explain opinions is necessarily two-dimensional. &nbsp;How many dimensions actually do efficiently explain all the different kinds of opinions? &nbsp;I don't know, and that's a statistical puzzle in itself, although I do know of techniques (like multi-dimensional scaling) that purport to estimate the \"right\" number of dimensions.</p>\n<p>One thing we do know is that if we project all the opinions onto a set of dimensions that we like -- for example, the old standby of liberal/conservative and authoritarian/libertarian -- we can start to measure \"political diversity.\" &nbsp;People who describe themselves as Democrats, for example, span a much wider range of views than people who describe themselves as \"Republicans.\" &nbsp;Implicitly, we're putting a Euclidean distance on a high-dimensional space, projected onto a few dimensions.</p>\n<p><strong>3. &nbsp;Logical and causal implications of policies.</strong></p>\n<p>Just as politicians' voting doesn't capture what regular people think, it could also be argued that people's opinions on poll questions also don't capture something important about politics. &nbsp;Namely, some policies logically&nbsp;<em>imply</em>&nbsp;one another. &nbsp;If you support raising spending on the war in Afghanistan for 2011, you must also support continuing the war in Afghanistan in 2011. &nbsp;If you support spending more on X, you must either support less spending on some other thing Y, or support raising taxes, or support running a higher deficit. &nbsp;Simply asking people about their opinions on a variety of questions doesn't capture this structure.</p>\n<p>Additionally, some policies <em>have consequences</em>&nbsp;for other policies. &nbsp;You cannot simultaneously be in favor of reducing carbon emissions, and be in favor of a set of policies that, on net, increase carbon emissions. &nbsp;Or, you can, but you'd have to be either ignorant or confused. &nbsp;</p>\n<p>And some philosophical claims imply policies. &nbsp;If you believe \"Congress should not do anything except the enumerated powers in the Constitution\" then that implies you have to oppose all the things Congress does that are not enumerated powers. &nbsp;</p>\n<p>In part 2, every policy position was just a coordinate in a high-dimensional vector space. &nbsp;Now, in part 3, it's suddenly not so simple. &nbsp;You have a directed graph of implications. &nbsp;It's very hard to get a handle on this graph. But just as this graph is more intractable, it's also more informative. &nbsp;In the part 2 model, a person could easily support a variety of inconsistent positions, and, in fact, people do. &nbsp;In part 3, some of your policy choices are determined by your other policy choices -- not just by <em>correlation</em>&nbsp;but by <em>necessity</em>.</p>\n<p>What can we say about one policy node that implies a lot about other policy nodes? &nbsp;Well, it's very influential. &nbsp;If you could examine a single person's directed graph of the policy universe, with nodes colored red for \"oppose\" and green for \"support,\" then the choke-points, those nodes that imply the choice of color for lots of other nodes, are core beliefs. &nbsp;A pacifist's graph, for example, is heavily influenced by the \"War is wrong\" node, because that logically implies his position on all specific wars.</p>\n<p>Obviously this doesn't have to be binary; you could have degrees of support and opposition, that imply updating the degrees of daughter nodes. &nbsp;Increasing your support of \"End all wars\" should increase your support of all nodes \"End war X,\" but increasing your support of \"Go to war to defend allies\" should decrease your support of \"End war Y\" if an ally was attacked in war Y.</p>\n<p>This gives us a different way of defining which policies are similar to each other. &nbsp;Policies that are close \"cousins\" on a tree are similar; \"End the war in Iraq,\" and \"End the Korean war\" are close because they're both implied by \"End all wars\", but \"End the war in Iraq\" and \"End the war in Afghanistan\" are even closer because they're both implied by \"End the War on Terror\" (which is implied by \"End all wars.\") &nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Three kinds of similarity are not the same</strong></p>\n<p>Points that are close on the left-right spectrum of politicians' decisions are not necessarily all that close in the higher-dimensional space of people's opinions on poll questions. &nbsp;I also hypothesize that points that are closely correlated in people's poll answers are not necessarily close in the part 3 sense of being close cousins on a tree of implications. &nbsp;That is, I would guess that people may tend to hold opinion B whenever they hold opinion A, even when A and B <em>actually</em>&nbsp;have nothing to do with each other.&nbsp;</p>\n<p><strong>How different is your debating partner?</strong></p>\n<p>I think part 3 is a good model for having political conversations -- better than part 1 or part 2. &nbsp;How \"different\" a person's politics feel from your own, once you've had a discussion with him, is not so much a matter of which party he votes for, or what his opinions are on a laundry list of issues. &nbsp;No: a person feels really \"different\" when you and he have opposite opinions on one of your influential nodes, something really far back on the tree. &nbsp;If one of your influential nodes is \"Democracy with universal suffrage is the best form of government\" and you're talking to someone who says, \"Hi, I actually support monarchy,\" then that person is <em>really different</em>&nbsp;from you. &nbsp;A person who agrees with you up until the last branch on the tree is <em>pretty similar</em>&nbsp;to you, no matter how vehemently you disagree about that last branch, because you both can draw upon the same assumptions from farther up the tree.</p>\n<p>The personal lesson is that it's useful to be clear with yourself about levels of difference. &nbsp;People can spend a lot of time arguing or even hating people who are very similar, and completely forget that there's a higher level on the tree. &nbsp;My politics are pretty different from Sarah Palin's; but I'd be Sarah Palin's ally against Louis XIV, I'd be Louis XIV's ally against Aurangzeb, and I'd be Aurangzeb's ally against a giant space squid intent on annihilating Earth -- Aurangzeb may have wanted to get rid of all the Hindus in India, but we've got more in common with each other than with a planeticidal space squid. &nbsp;Squids aside, it's rather ridiculous when people identify their \"greatest enemy\" or \"greatest threat\" as a person with only <em>slightly </em>different political views.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ubgBqRwHG9u7idAig", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "3799", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p>Suppose you wanted to describe the political landscape efficiently. That is, you want to describe the range of existing political positions, and develop some way of evaluating which political positions are \"similar.\" (I'm American, so I'm going to think in terms of the US.) &nbsp;There are three ways you could go about this and they are radically different.</p>\n<p>&nbsp;</p>\n<p><strong id=\"1___Politicians__voting_\">1. &nbsp;Politicians' voting.</strong></p>\n<p>Because we have a two-party system, politicians (and some other political players, like Supreme Court justices) make decisions on a single, left-right axis. &nbsp;You can explain almost all the variation with one dimension: \"More Republican\" or \"More Democrat.\" &nbsp;Generally, if a politician supports one \"Republican\" position, he'll support most of the other \"Republican\" positions. &nbsp;Studies have been done here (too lazy to look them up right now) supporting this phenomenon at all levels of politics.</p>\n<p><strong id=\"2___Individuals__opinions_on_poll_questions_\">2. &nbsp;Individuals' opinions on poll questions.</strong></p>\n<p>Here, we do something different. &nbsp;We ask poll questions about a variety of issues and see what people think. &nbsp;Suddenly, it's not one-dimensional any more. &nbsp;There are people who like low taxes and also like gay marriage. &nbsp;There are people who are anti-abortion and anti-war. &nbsp;The fact that individual opinions don't line up along a left-right axis is what's behind ideas like the Nolan Chart -- although it doesn't imply that the \"real\" way to explain opinions is necessarily two-dimensional. &nbsp;How many dimensions actually do efficiently explain all the different kinds of opinions? &nbsp;I don't know, and that's a statistical puzzle in itself, although I do know of techniques (like multi-dimensional scaling) that purport to estimate the \"right\" number of dimensions.</p>\n<p>One thing we do know is that if we project all the opinions onto a set of dimensions that we like -- for example, the old standby of liberal/conservative and authoritarian/libertarian -- we can start to measure \"political diversity.\" &nbsp;People who describe themselves as Democrats, for example, span a much wider range of views than people who describe themselves as \"Republicans.\" &nbsp;Implicitly, we're putting a Euclidean distance on a high-dimensional space, projected onto a few dimensions.</p>\n<p><strong id=\"3___Logical_and_causal_implications_of_policies_\">3. &nbsp;Logical and causal implications of policies.</strong></p>\n<p>Just as politicians' voting doesn't capture what regular people think, it could also be argued that people's opinions on poll questions also don't capture something important about politics. &nbsp;Namely, some policies logically&nbsp;<em>imply</em>&nbsp;one another. &nbsp;If you support raising spending on the war in Afghanistan for 2011, you must also support continuing the war in Afghanistan in 2011. &nbsp;If you support spending more on X, you must either support less spending on some other thing Y, or support raising taxes, or support running a higher deficit. &nbsp;Simply asking people about their opinions on a variety of questions doesn't capture this structure.</p>\n<p>Additionally, some policies <em>have consequences</em>&nbsp;for other policies. &nbsp;You cannot simultaneously be in favor of reducing carbon emissions, and be in favor of a set of policies that, on net, increase carbon emissions. &nbsp;Or, you can, but you'd have to be either ignorant or confused. &nbsp;</p>\n<p>And some philosophical claims imply policies. &nbsp;If you believe \"Congress should not do anything except the enumerated powers in the Constitution\" then that implies you have to oppose all the things Congress does that are not enumerated powers. &nbsp;</p>\n<p>In part 2, every policy position was just a coordinate in a high-dimensional vector space. &nbsp;Now, in part 3, it's suddenly not so simple. &nbsp;You have a directed graph of implications. &nbsp;It's very hard to get a handle on this graph. But just as this graph is more intractable, it's also more informative. &nbsp;In the part 2 model, a person could easily support a variety of inconsistent positions, and, in fact, people do. &nbsp;In part 3, some of your policy choices are determined by your other policy choices -- not just by <em>correlation</em>&nbsp;but by <em>necessity</em>.</p>\n<p>What can we say about one policy node that implies a lot about other policy nodes? &nbsp;Well, it's very influential. &nbsp;If you could examine a single person's directed graph of the policy universe, with nodes colored red for \"oppose\" and green for \"support,\" then the choke-points, those nodes that imply the choice of color for lots of other nodes, are core beliefs. &nbsp;A pacifist's graph, for example, is heavily influenced by the \"War is wrong\" node, because that logically implies his position on all specific wars.</p>\n<p>Obviously this doesn't have to be binary; you could have degrees of support and opposition, that imply updating the degrees of daughter nodes. &nbsp;Increasing your support of \"End all wars\" should increase your support of all nodes \"End war X,\" but increasing your support of \"Go to war to defend allies\" should decrease your support of \"End war Y\" if an ally was attacked in war Y.</p>\n<p>This gives us a different way of defining which policies are similar to each other. &nbsp;Policies that are close \"cousins\" on a tree are similar; \"End the war in Iraq,\" and \"End the Korean war\" are close because they're both implied by \"End all wars\", but \"End the war in Iraq\" and \"End the war in Afghanistan\" are even closer because they're both implied by \"End the War on Terror\" (which is implied by \"End all wars.\") &nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"Three_kinds_of_similarity_are_not_the_same\">Three kinds of similarity are not the same</strong></p>\n<p>Points that are close on the left-right spectrum of politicians' decisions are not necessarily all that close in the higher-dimensional space of people's opinions on poll questions. &nbsp;I also hypothesize that points that are closely correlated in people's poll answers are not necessarily close in the part 3 sense of being close cousins on a tree of implications. &nbsp;That is, I would guess that people may tend to hold opinion B whenever they hold opinion A, even when A and B <em>actually</em>&nbsp;have nothing to do with each other.&nbsp;</p>\n<p><strong id=\"How_different_is_your_debating_partner_\">How different is your debating partner?</strong></p>\n<p>I think part 3 is a good model for having political conversations -- better than part 1 or part 2. &nbsp;How \"different\" a person's politics feel from your own, once you've had a discussion with him, is not so much a matter of which party he votes for, or what his opinions are on a laundry list of issues. &nbsp;No: a person feels really \"different\" when you and he have opposite opinions on one of your influential nodes, something really far back on the tree. &nbsp;If one of your influential nodes is \"Democracy with universal suffrage is the best form of government\" and you're talking to someone who says, \"Hi, I actually support monarchy,\" then that person is <em>really different</em>&nbsp;from you. &nbsp;A person who agrees with you up until the last branch on the tree is <em>pretty similar</em>&nbsp;to you, no matter how vehemently you disagree about that last branch, because you both can draw upon the same assumptions from farther up the tree.</p>\n<p>The personal lesson is that it's useful to be clear with yourself about levels of difference. &nbsp;People can spend a lot of time arguing or even hating people who are very similar, and completely forget that there's a higher level on the tree. &nbsp;My politics are pretty different from Sarah Palin's; but I'd be Sarah Palin's ally against Louis XIV, I'd be Louis XIV's ally against Aurangzeb, and I'd be Aurangzeb's ally against a giant space squid intent on annihilating Earth -- Aurangzeb may have wanted to get rid of all the Hindus in India, but we've got more in common with each other than with a planeticidal space squid. &nbsp;Squids aside, it's rather ridiculous when people identify their \"greatest enemy\" or \"greatest threat\" as a person with only <em>slightly </em>different political views.&nbsp;</p>", "sections": [{"title": "1. \u00a0Politicians' voting.", "anchor": "1___Politicians__voting_", "level": 1}, {"title": "2. \u00a0Individuals' opinions on poll questions.", "anchor": "2___Individuals__opinions_on_poll_questions_", "level": 1}, {"title": "3. \u00a0Logical and causal implications of policies.", "anchor": "3___Logical_and_causal_implications_of_policies_", "level": 1}, {"title": "Three kinds of similarity are not the same", "anchor": "Three_kinds_of_similarity_are_not_the_same", "level": 1}, {"title": "How different is your debating partner?", "anchor": "How_different_is_your_debating_partner_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "30 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-23T17:15:04.957Z", "modifiedAt": null, "url": null, "title": "Does anyone else find ROT13 spoilers as annoying as I do?", "slug": "does-anyone-else-find-rot13-spoilers-as-annoying-as-i-do", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.851Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nick012000", "createdAt": "2010-07-12T11:00:38.790Z", "isAdmin": false, "displayName": "nick012000"}, "userId": "yCSrMSsugCiSiqAY2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ju2qFSS6qyr2LzjMQ/does-anyone-else-find-rot13-spoilers-as-annoying-as-i-do", "pageUrlRelative": "/posts/Ju2qFSS6qyr2LzjMQ/does-anyone-else-find-rot13-spoilers-as-annoying-as-i-do", "linkUrl": "https://www.lesswrong.com/posts/Ju2qFSS6qyr2LzjMQ/does-anyone-else-find-rot13-spoilers-as-annoying-as-i-do", "postedAtFormatted": "Saturday, October 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20anyone%20else%20find%20ROT13%20spoilers%20as%20annoying%20as%20I%20do%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20anyone%20else%20find%20ROT13%20spoilers%20as%20annoying%20as%20I%20do%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJu2qFSS6qyr2LzjMQ%2Fdoes-anyone-else-find-rot13-spoilers-as-annoying-as-i-do%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20anyone%20else%20find%20ROT13%20spoilers%20as%20annoying%20as%20I%20do%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJu2qFSS6qyr2LzjMQ%2Fdoes-anyone-else-find-rot13-spoilers-as-annoying-as-i-do", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJu2qFSS6qyr2LzjMQ%2Fdoes-anyone-else-find-rot13-spoilers-as-annoying-as-i-do", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<p>Every time I come across one, it annoys me. I have to copy the text, open a new tab to the ROT13 site, paste the text, and click the translate button.</p>\n<p>Compare this to something like a collapsable spoiler button-box where you press a button and it expands and expands a box with the appropriate text underneath it. Even making a [spoiler][/spoiler] tag that gave a black background and equally black text would be better than the current ROT13 solution.</p>\n<p>Was there actually a reason for doing things this way? If so, why not just include the ROT13 translation in the javascript that'd open and close the textbox? Comments? Criticism?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ju2qFSS6qyr2LzjMQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 6.379679659714978e-07, "legacy": true, "legacyId": "3800", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-23T18:50:14.100Z", "modifiedAt": null, "url": null, "title": "Tomatoes", "slug": "tomatoes", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:22.068Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hangedman", "createdAt": "2010-10-13T21:14:13.042Z", "isAdmin": false, "displayName": "hangedman"}, "userId": "ZjApzBYFpbgTkE7ZF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ASeRt6BpaZLpTfFit/tomatoes", "pageUrlRelative": "/posts/ASeRt6BpaZLpTfFit/tomatoes", "linkUrl": "https://www.lesswrong.com/posts/ASeRt6BpaZLpTfFit/tomatoes", "postedAtFormatted": "Saturday, October 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tomatoes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATomatoes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FASeRt6BpaZLpTfFit%2Ftomatoes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tomatoes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FASeRt6BpaZLpTfFit%2Ftomatoes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FASeRt6BpaZLpTfFit%2Ftomatoes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 600, "htmlBody": "<p>Are tomatoes fruits or vegetables?</p>\n<p>I've been reading Eliezer's criticisms of Aristotelian classes as a model for the meaning of words.&nbsp; It occurred to me that this little chestnut is a good illustration of the problem.&nbsp; The best part about this example is that almost everyone has argued either on one side or the other at some point in their lives.&nbsp; One would think that the English speaking world could come to some consensus on such a simple, trivial problem, but still the argument rages on.&nbsp; Fruit or vegetable?</p>\n<p>In my experience, the argument is usually started by the fruit advocate (we'll call him Lemon).&nbsp; \"It's the fruiting body of the plant,\" he says.&nbsp; \"It contains the seeds.\"&nbsp; He argues that the tomato is, <em>by definition</em>, a fruit.</p>\n<p>Bean has never thought of tomatoes as fruits, but when her belief is challenged by Lemon, she's not entirely sure how to respond.&nbsp; She hesitates, then starts slowly -- \"All the things I call fruits are sweet,\" she says.&nbsp; \"Not that tomatoes are bitter, but they're certainly not sweet enough to be fruits.\"&nbsp; Bean is proposing a stricter definition -- fruits are <em>sweet</em> fruiting bodies of plants.&nbsp; But does Bean really think that's the difference between a fruit and a vegetable?</p>\n<p>Not really.&nbsp; Bean learned what these words mean by talking to other people about fruits, vegetables, and tomatoes, and through her cooking and eating.&nbsp; There was never any moment when she said to herself, \"Aha!&nbsp; a tomato is not a fruit!\"&nbsp; This belief is a result of countless minute inferences made over the course of Bean's gustatory life.&nbsp; The definition she proposes is an ad hoc defense of her belief that tomatoes are not fruits, not a <em>real reason</em>.</p>\n<p>Bean's real mistake was to think that she needed to defend her belief that tomatoes are not fruits.&nbsp; Tomatoes are what they are regardless of how they're classified, and most people classify them as fruit or vegetable long before they learn anything about Aristotelian classes or membership tests.&nbsp; The classification is made as the result of a long history of silent inferences from the way parents and peers use those words.&nbsp; The first English dictionary was written in 1604, several hundred years after both \"fruit\" and \"vegetable\" had entered the English vocabulary (right about the same time as \"tomato,\" actually).&nbsp; Before that, Lemon couldn't point to a definition to make his case. He could only rely on his experiences with usage just as Bean does in her rebuttal, and it's not clear why we should privilege one's experience other the other.&nbsp; The meaning of the word is prior to the definition.</p>\n<p>There is a simple solution to the tomato problem, by the way.&nbsp; \"Vegetable\" is any edible plant matter and \"fruit\" is a subclass of \"vegetable.\"&nbsp; All fruits are vegetables, and so tomatoes are both.&nbsp; In the same way, wheat is both a grain and a vegetable.&nbsp; The distinction is made only for convenience -- consider the fact that before electric guitars were invented, there were no \"acoustic guitars\" -- only \"guitars.\"&nbsp; It's not false to describe an acoustic guitar as a guitar, merely imprecise.&nbsp; This points to what I think is a common phenomenon in spoken language which leads to errors in reasoning: a distinction is made between a subclass B and superclass A with the understanding that x in B -&gt; x in A; later, the distinction is maintained but the understanding of the interconnection is lost so that A and B are considered distinct categories -- x in A xor x in B.&nbsp; Can anyone think of any other examples of this kind of error?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ASeRt6BpaZLpTfFit", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 1, "extendedScore": null, "score": 6.379905335391806e-07, "legacy": true, "legacyId": "3801", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-23T23:26:42.124Z", "modifiedAt": null, "url": null, "title": "Evidential Decision Theory and Mass Mind Control", "slug": "evidential-decision-theory-and-mass-mind-control", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:21.068Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielLC", "createdAt": "2009-12-26T17:34:50.257Z", "isAdmin": false, "displayName": "DanielLC"}, "userId": "3e6zTkDmDpNspRb8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eSX2v6L5d9vB4JcC3/evidential-decision-theory-and-mass-mind-control", "pageUrlRelative": "/posts/eSX2v6L5d9vB4JcC3/evidential-decision-theory-and-mass-mind-control", "linkUrl": "https://www.lesswrong.com/posts/eSX2v6L5d9vB4JcC3/evidential-decision-theory-and-mass-mind-control", "postedAtFormatted": "Saturday, October 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evidential%20Decision%20Theory%20and%20Mass%20Mind%20Control&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvidential%20Decision%20Theory%20and%20Mass%20Mind%20Control%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeSX2v6L5d9vB4JcC3%2Fevidential-decision-theory-and-mass-mind-control%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evidential%20Decision%20Theory%20and%20Mass%20Mind%20Control%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeSX2v6L5d9vB4JcC3%2Fevidential-decision-theory-and-mass-mind-control", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeSX2v6L5d9vB4JcC3%2Fevidential-decision-theory-and-mass-mind-control", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 317, "htmlBody": "<p><strong>Required Reading: <a title=\"Evidential Decision Theory\" href=\"/lw/2l5/an_introduction_to_decision_theory/\">Evidential Decision Theory</a></strong></p>\n<p>Let me begin with something similar to <a title=\"Nowcomb's Paradox\" href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Paradox</a>. You're not the guy choosing whether or not to take both boxes. You're the guy who predicts. You're not actually prescient. You can only make an educated guess.<br /><br />You watch the first person play. Let's say they pick one box. You know they're not an ordinary person. They're a lot more philosophical than normal. But that doesn't mean that the knowledge of what they choose is completely useless later on. The later people might be just as weird. Or they might be normal, but they're not completely independent of this outlier. You can use his decision to help predict theirs, if only by a little. What's more, this still works if you're reading through archives and trying to \"predict\" the decisions people have already made in earlier trials.<br /><br />The decision of the player choosing the box affects whether or not the predictor will predict that later, or earlier, people will take the box. According to EDT, one should act in the way that results in the most evidence for what one wants. Since the predictor is completely rational, this means that the player choosing the box effectively changes decisions other people make, or actually changes depending on your interpretation of EDT. One can even affect people's decisions in the past, provided that one doesn't know what they were.<br /><br />In short, the decisions you make affect the decisions other people will make and have made. I'm not sure how much, but there have probably been 50 to 100 billion people. And that's not including the people who haven't been born yet. Even if you only change one in a thousand decisions, that's at least 50 million people.<br /><br />Like I said: <span class=\"posthilit\">mass</span> <span class=\"posthilit\">mind</span> <span class=\"posthilit\">control</span>. Use this power for good.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eSX2v6L5d9vB4JcC3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": -3, "extendedScore": null, "score": 6.380561120633888e-07, "legacy": true, "legacyId": "3802", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cRrM8LPf9waAd4uiL", "6ddcsdA2c2XpNpE5x"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-24T07:29:34.119Z", "modifiedAt": null, "url": null, "title": "LWers on last.fm", "slug": "lwers-on-last-fm", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mindspillage", "createdAt": "2010-09-20T19:45:07.674Z", "isAdmin": false, "displayName": "mindspillage"}, "userId": "kfRYbvqZe8BHtWmPm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G9eTxWCQcMKw4CmHz/lwers-on-last-fm", "pageUrlRelative": "/posts/G9eTxWCQcMKw4CmHz/lwers-on-last-fm", "linkUrl": "https://www.lesswrong.com/posts/G9eTxWCQcMKw4CmHz/lwers-on-last-fm", "postedAtFormatted": "Sunday, October 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LWers%20on%20last.fm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALWers%20on%20last.fm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9eTxWCQcMKw4CmHz%2Flwers-on-last-fm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LWers%20on%20last.fm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9eTxWCQcMKw4CmHz%2Flwers-on-last-fm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9eTxWCQcMKw4CmHz%2Flwers-on-last-fm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>Since several people spent a while comparing musical taste on IRC, I thought it was worth creating a group for those of us on last.fm:</p>\n<p><a href=\"http://www.last.fm/group/Less+Wrong\">Less Wrong</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G9eTxWCQcMKw4CmHz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 6.381706775588582e-07, "legacy": true, "legacyId": "3805", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-24T10:41:55.278Z", "modifiedAt": null, "url": null, "title": "That which can be destroyed by the truth should *not* necessarily be", "slug": "that-which-can-be-destroyed-by-the-truth-should-not", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:24.129Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexflint", "createdAt": "2009-07-17T10:07:09.115Z", "isAdmin": false, "displayName": "Alex Flint"}, "userId": "ifEGDHySkAejhCFDf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TeuytT4osZTyxwGtP/that-which-can-be-destroyed-by-the-truth-should-not", "pageUrlRelative": "/posts/TeuytT4osZTyxwGtP/that-which-can-be-destroyed-by-the-truth-should-not", "linkUrl": "https://www.lesswrong.com/posts/TeuytT4osZTyxwGtP/that-which-can-be-destroyed-by-the-truth-should-not", "postedAtFormatted": "Sunday, October 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20That%20which%20can%20be%20destroyed%20by%20the%20truth%20should%20*not*%20necessarily%20be&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThat%20which%20can%20be%20destroyed%20by%20the%20truth%20should%20*not*%20necessarily%20be%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTeuytT4osZTyxwGtP%2Fthat-which-can-be-destroyed-by-the-truth-should-not%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=That%20which%20can%20be%20destroyed%20by%20the%20truth%20should%20*not*%20necessarily%20be%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTeuytT4osZTyxwGtP%2Fthat-which-can-be-destroyed-by-the-truth-should-not", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTeuytT4osZTyxwGtP%2Fthat-which-can-be-destroyed-by-the-truth-should-not", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 491, "htmlBody": "<p>I've been throwing some ideas around in my head, and I want to throw some of them half-formed into the open for discussion here.</p>\n<p>I want to draw attention to a particular class of decisions that sound much like beliefs.</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td width=\"50%\">\n<p><strong>Belief</strong></p>\n</td>\n<td width=\"50%\">\n<p><strong>Decision</strong></p>\n</td>\n</tr>\n<tr>\n<td>\n<p>There is no personal god that answers prayers. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>\n</td>\n<td>\n<p>I should badger my friend about atheism.</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>Cryonics is a rational course of action.</p>\n</td>\n<td>\n<p>To convince others about cryonics, I should start by explaining that if we exist in the future at all, then we can expect it to be nicer than the present on account of benevolent super-intelligences.</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>There is an objective reality.</p>\n</td>\n<td>\n<p>Postmodernists should be ridiculed and ignored.</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>1+1=2</p>\n</td>\n<td>\n<p>If I encounter a person about to jump unless he is told \"1+1=3\", I should not acquiesce.</p>\n</td>\n</tr>\n</tbody>\n</table>\n<p>I've thrown ideas from a few different bags into the table, and I've perhaps chosen&nbsp;unnecessarily&nbsp;inflammatory&nbsp;examples. There are many arguments to be had about these examples, but the point I want to make is the way in which questions about the best course of action can <em>sound</em>&nbsp;very much like questions about truth. Now this is dangerous because the way in which we chose amongst decisions is <em>radically</em>&nbsp;different from the way in which we chose amongst beliefs. For a start, evaluating decisions <em>always </em>involves evaluating a utility function, whereas evaluating beliefs <em>never </em>does (unless the utility function is explicitly part of the question). By appropriate changes to one's utility function the optimal decision in any given situation can be modified arbitrarily <em>whilst simultaneously leaving all probability assignments to all statements fixed</em>. This should make you immediately suspicious if you <em>ever</em>&nbsp;make a decision without consulting your utility function.&nbsp;<em>There is no simple mapping from beliefs to decisions.</em></p>\n<p>I've noticed various friends and some people on this site making just this mistake.&nbsp;It's as if their love for truth and rational enquiry, <em>which is a great thing in its own right</em>, spills over into a conviction to act in a particular way, which itself is of questionable optimality.</p>\n<p>In recent months there have been several posts on LessWrong about the \"dark arts\", which have mostly concerned using asymmetric knowledge to manipulate people. I like these posts, and I respect the moral stance implied by their name, but I&nbsp;fear that \"dark arts\" is becoming applicable to the much broader case of <em>not</em> acting according to the simple rule that decisions are always good when they sound like true beliefs. I shouldn't need to argue explicitly that there are cases when lying or manipulating constitute good decisions; that would privileged&nbsp;a very particular hypothesis (namely that&nbsp;decisions are always good when they sound like true beliefs).</p>\n<p>This brings be all the way back to the much-loved quotation, \"that which can be destroyed by the truth should be\". Now there are several ways to interpret the quote but at least one interpretation implies the existence of a simple isomorphism from true beliefs to good decisions. Personally, I can think of lots of things that could be destroyed by the truth but should <em>not</em> be.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TeuytT4osZTyxwGtP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 11, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "3806", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I've been throwing some ideas around in my head, and I want to throw some of them half-formed into the open for discussion here.</p>\n<p>I want to draw attention to a particular class of decisions that sound much like beliefs.</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td width=\"50%\">\n<p><strong id=\"Belief\">Belief</strong></p>\n</td>\n<td width=\"50%\">\n<p><strong id=\"Decision\">Decision</strong></p>\n</td>\n</tr>\n<tr>\n<td>\n<p>There is no personal god that answers prayers. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>\n</td>\n<td>\n<p>I should badger my friend about atheism.</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>Cryonics is a rational course of action.</p>\n</td>\n<td>\n<p>To convince others about cryonics, I should start by explaining that if we exist in the future at all, then we can expect it to be nicer than the present on account of benevolent super-intelligences.</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>There is an objective reality.</p>\n</td>\n<td>\n<p>Postmodernists should be ridiculed and ignored.</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>1+1=2</p>\n</td>\n<td>\n<p>If I encounter a person about to jump unless he is told \"1+1=3\", I should not acquiesce.</p>\n</td>\n</tr>\n</tbody>\n</table>\n<p>I've thrown ideas from a few different bags into the table, and I've perhaps chosen&nbsp;unnecessarily&nbsp;inflammatory&nbsp;examples. There are many arguments to be had about these examples, but the point I want to make is the way in which questions about the best course of action can <em>sound</em>&nbsp;very much like questions about truth. Now this is dangerous because the way in which we chose amongst decisions is <em>radically</em>&nbsp;different from the way in which we chose amongst beliefs. For a start, evaluating decisions <em>always </em>involves evaluating a utility function, whereas evaluating beliefs <em>never </em>does (unless the utility function is explicitly part of the question). By appropriate changes to one's utility function the optimal decision in any given situation can be modified arbitrarily <em>whilst simultaneously leaving all probability assignments to all statements fixed</em>. This should make you immediately suspicious if you <em>ever</em>&nbsp;make a decision without consulting your utility function.&nbsp;<em>There is no simple mapping from beliefs to decisions.</em></p>\n<p>I've noticed various friends and some people on this site making just this mistake.&nbsp;It's as if their love for truth and rational enquiry, <em>which is a great thing in its own right</em>, spills over into a conviction to act in a particular way, which itself is of questionable optimality.</p>\n<p>In recent months there have been several posts on LessWrong about the \"dark arts\", which have mostly concerned using asymmetric knowledge to manipulate people. I like these posts, and I respect the moral stance implied by their name, but I&nbsp;fear that \"dark arts\" is becoming applicable to the much broader case of <em>not</em> acting according to the simple rule that decisions are always good when they sound like true beliefs. I shouldn't need to argue explicitly that there are cases when lying or manipulating constitute good decisions; that would privileged&nbsp;a very particular hypothesis (namely that&nbsp;decisions are always good when they sound like true beliefs).</p>\n<p>This brings be all the way back to the much-loved quotation, \"that which can be destroyed by the truth should be\". Now there are several ways to interpret the quote but at least one interpretation implies the existence of a simple isomorphism from true beliefs to good decisions. Personally, I can think of lots of things that could be destroyed by the truth but should <em>not</em> be.</p>", "sections": [{"title": "Belief", "anchor": "Belief", "level": 1}, {"title": "Decision", "anchor": "Decision", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "12 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-24T14:21:40.469Z", "modifiedAt": null, "url": null, "title": "Not exactly the trolley problem", "slug": "not-exactly-the-trolley-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:21.581Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rHsuG5D6hGpTjoDD9/not-exactly-the-trolley-problem", "pageUrlRelative": "/posts/rHsuG5D6hGpTjoDD9/not-exactly-the-trolley-problem", "linkUrl": "https://www.lesswrong.com/posts/rHsuG5D6hGpTjoDD9/not-exactly-the-trolley-problem", "postedAtFormatted": "Sunday, October 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Not%20exactly%20the%20trolley%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANot%20exactly%20the%20trolley%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrHsuG5D6hGpTjoDD9%2Fnot-exactly-the-trolley-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Not%20exactly%20the%20trolley%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrHsuG5D6hGpTjoDD9%2Fnot-exactly-the-trolley-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrHsuG5D6hGpTjoDD9%2Fnot-exactly-the-trolley-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p><a href=\"http://www.news.com.au/travel/news/crocodile-on-a-plane-kills-19/story-e6frfq80-1225942045322\">An unusual incident</a>.  Are you obligated to be on the side of the plane with the crocodile if the other passengers are overbalancing the plane? To push other passengers over to the side with the crocodile?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LMFBzsJaCRADQqw3F": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rHsuG5D6hGpTjoDD9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 6.382684830641566e-07, "legacy": true, "legacyId": "3807", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-24T19:29:57.455Z", "modifiedAt": null, "url": null, "title": "Learning the foundations of math", "slug": "learning-the-foundations-of-math", "viewCount": null, "lastCommentedAt": "2021-07-17T00:44:25.612Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zNCdxFvfbFZggTxDk/learning-the-foundations-of-math", "pageUrlRelative": "/posts/zNCdxFvfbFZggTxDk/learning-the-foundations-of-math", "linkUrl": "https://www.lesswrong.com/posts/zNCdxFvfbFZggTxDk/learning-the-foundations-of-math", "postedAtFormatted": "Sunday, October 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Learning%20the%20foundations%20of%20math&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALearning%20the%20foundations%20of%20math%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNCdxFvfbFZggTxDk%2Flearning-the-foundations-of-math%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Learning%20the%20foundations%20of%20math%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNCdxFvfbFZggTxDk%2Flearning-the-foundations-of-math", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNCdxFvfbFZggTxDk%2Flearning-the-foundations-of-math", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 255, "htmlBody": "<p>I have recently become interested in the foundations of math. I am interested in tracing the fundamentals of math in a path such as: propositional logic -&gt; first order logic -&gt; set theory -&gt; measure theory. Does anyone have any resources (books, webpages, pdfs etc.) they would like to recommend?</p>\n<p>This seems like it would be a popular activity among LWers, so I thought this would be a good place to ask for advice.</p>\n<p>My criteria (feel free to post resources which you think others who stumble across this might be interested in):</p>\n<ul>\n<li>The more basic the starting point the better: I would prefer a resource that defines propositional logic in terms of a context free grammar and an evaluation procedure (don't know if that is possible, but that's the sort of thing I am interested in) to one that just describes propositional logic in English; I would prefer a resource which builds first order logic from propositional logic + some definitions to one that just describes how first order logic works; etc.</li>\n<li>The fewer axioms (perhaps that's not quite the right word) the better. I prefer a resource defines describes propositional logic with just two operators (say negation and conjugation) and then builds the other operators of interest to one that defines it with 5 or 6 operators (I've seen many resources which do this).</li>\n<li>I expect that there are multiple ways to build math from basic building blocks. I am more interested in standard ways than than non-standard ways.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zNCdxFvfbFZggTxDk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "3809", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-25T01:01:48.304Z", "modifiedAt": null, "url": null, "title": "*Cryoburn* by Bujold", "slug": "cryoburn-by-bujold", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:48.316Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yqpjd7mQ8mqr7Cyvn/cryoburn-by-bujold", "pageUrlRelative": "/posts/yqpjd7mQ8mqr7Cyvn/cryoburn-by-bujold", "linkUrl": "https://www.lesswrong.com/posts/yqpjd7mQ8mqr7Cyvn/cryoburn-by-bujold", "postedAtFormatted": "Monday, October 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20*Cryoburn*%20by%20Bujold&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A*Cryoburn*%20by%20Bujold%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyqpjd7mQ8mqr7Cyvn%2Fcryoburn-by-bujold%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=*Cryoburn*%20by%20Bujold%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyqpjd7mQ8mqr7Cyvn%2Fcryoburn-by-bujold", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyqpjd7mQ8mqr7Cyvn%2Fcryoburn-by-bujold", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 275, "htmlBody": "<p>This science fiction novel is probably the best I've seen about the amount of trustworthiness needed to make large-scale cryonics work.</p>\n<p>Minor spoilers: It's set on a planet where large corporations do the freezing and revival and can vote (in politics) as representatives of their frozen clients. Things Go Wrong, and there's even an echo of the current mortgage crisis.</p>\n<p>On the whole, I think Bujold mistrusts any organization which is too large for individual loyalties to make a difference.</p>\n<p>Still, it may be worth thinking about what sort of emotional/governmental/economic system it would take to make cryonics work for a large proportion of the population and/or for long periods-- and remember that for unmodified humans, mere decades are a long time.</p>\n<p>The book is basically unsympathetic to cryonics-- sympathetic presentation of characters who choose not to be frozen, and a mention of a society which is so busy avoiding death that it's forgotten how to live. That last is just sloppy, it's not supported by the text. At least it's in favor of non-atrocious methods of rejuvenation, and she may have a point that their development will be pretty gradual. I'm not sure it's plausible that brain-transplants into clones will be developed well before any aspect of rejuvenation.</p>\n<p>It's a fair-to-middling caper novel, or maybe somewhat better than that. It's better than the two weakest novels in the series (*Cetaganda* and *Diplomatic Immunity*) and not as good as the best (probably *Memory*, *Brothers in Arms*, and *A Civil Campaign*, though I'm also awfully fond of *Komarr* and *Mirror Dance*). I think it would make sense for the most part even if you haven't read other books in the series.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yqpjd7mQ8mqr7Cyvn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 6.384204585277496e-07, "legacy": true, "legacyId": "3813", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-25T01:05:18.965Z", "modifiedAt": null, "url": null, "title": "Help: Building Awesome Personal Organization Systems", "slug": "help-building-awesome-personal-organization-systems", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:57.754Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "YpTmfmnMjgakwFRQQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yKnj937Y3GPp9PyTR/help-building-awesome-personal-organization-systems", "pageUrlRelative": "/posts/yKnj937Y3GPp9PyTR/help-building-awesome-personal-organization-systems", "linkUrl": "https://www.lesswrong.com/posts/yKnj937Y3GPp9PyTR/help-building-awesome-personal-organization-systems", "postedAtFormatted": "Monday, October 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%3A%20Building%20Awesome%20Personal%20Organization%20Systems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%3A%20Building%20Awesome%20Personal%20Organization%20Systems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKnj937Y3GPp9PyTR%2Fhelp-building-awesome-personal-organization-systems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%3A%20Building%20Awesome%20Personal%20Organization%20Systems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKnj937Y3GPp9PyTR%2Fhelp-building-awesome-personal-organization-systems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKnj937Y3GPp9PyTR%2Fhelp-building-awesome-personal-organization-systems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 100, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/2qc/rationality_power_tools\">Rationality Power Tools</a></p>\n<p>I'm looking to use (or make) something that helps me achieve god-like productivity. In particular, I'm interested in any information about systems that are:</p>\n<ul>\n<li><em>Flexible</em>: They can be extended or customized to accommodate new work-flows and a diverse range of information structures (like to-do lists, schedules, etc.), perhaps via easy coding. </li>\n</ul>\n<ul>\n<li><em>Linked</em>: The elements can be connected and categorized using a variety of link types (like is_an_action_for, is_a_subgoal_of, etc.).</li>\n</ul>\n<p>I would prefer not to have a bunch of separate systems if possible. From what I've seen so far, <a href=\"http://orgmode.org/\">org-mode</a> seems the most promising.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yKnj937Y3GPp9PyTR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 6.384212922582747e-07, "legacy": true, "legacyId": "3812", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5X5xProqsbeck7bbd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-25T02:13:34.654Z", "modifiedAt": "2020-05-21T22:56:40.458Z", "url": null, "title": "Optimism versus cryonics", "slug": "optimism-versus-cryonics", "viewCount": null, "lastCommentedAt": "2022-04-15T15:36:57.657Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zKxK66pHw9aL2btkr/optimism-versus-cryonics", "pageUrlRelative": "/posts/zKxK66pHw9aL2btkr/optimism-versus-cryonics", "linkUrl": "https://www.lesswrong.com/posts/zKxK66pHw9aL2btkr/optimism-versus-cryonics", "postedAtFormatted": "Monday, October 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Optimism%20versus%20cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOptimism%20versus%20cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzKxK66pHw9aL2btkr%2Foptimism-versus-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Optimism%20versus%20cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzKxK66pHw9aL2btkr%2Foptimism-versus-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzKxK66pHw9aL2btkr%2Foptimism-versus-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 400, "htmlBody": "<p>Within the immortalist community, cryonics is the most pessimistic possible position. Consider the following superoptimistic alternative scenarios:</p>\n<ol>\n<li>Uploading will be possible before I die.</li>\n<li>Aging will be cured before I die.</li>\n<li>They will be able to reanimate a whole mouse before I die, <em>then</em> I'll sign up.</li>\n<li>I could get frozen in a freezer when I die, and they will eventually figure out how to reanimate me.</li>\n<li>I could pickle my brain when I die, and they will eventually figure out how to reanimate me.</li>\n<li>Friendly AI will cure aging and/or let me be uploaded before I die.</li>\n</ol>\n<p>Cryonics -- perfusion and vitrification at LN2 temperatures under the best conditions possible -- is by far less optimistic than any of these. Of all the possible scenarios where you end up immortal, cryonics is the <em>least</em> optimistic. Cryonics can work even if there is no singularity or reversal tech for thousands of years into the future. It can work under the conditions of the slowest technological growth imaginable. All it assumes is that the organization (or its descendants) can survive long enough, technology doesn't go backwards (on average), and that cryopreservation of a technically sufficient nature can predate reanimation tech.</p>\n<p>It doesn't even require the assumption that today's best possible vitrifications are good enough. See, it's entirely plausible that it's 100 years from now when they start being good enough, and 500 years later when they figure out how to reverse them. Perhaps today's population is doomed because of this. <em>We don't know.</em> But the fact that we don't know what exact point is good enough is sufficient to make this a worthwhile endeavor at as <em>early</em> of a point as possible. It doesn't require <em>optimism</em> -- it simply requires deliberate, rational action. The fact is that we are late for the party. In retrospect, we should have started preserving brains hundreds of years ago. Benjamin Franklin should have gone ahead and had himself immersed in alcohol.<br /><br />There's a difference between having a fear and being immobilized by it. If you have a fear that cryonics won't work -- good for you! That's a perfectly rational fear. But if that fear <em>immobilizes</em> you and discourages you from taking <em>action</em>, you've lost the game. Worse than lost, you never played.</p>\n<p><em>This is something of a response to Charles Platt's recent article on Cryoptimism: </em><a href=\"http://www.cryonet.org/cgi-bin/dsp.cgi?msg=32975\"><em>Part 1</em></a><em>&nbsp;</em><a href=\"http://www.cryonet.org/cgi-bin/dsp.cgi?msg=32976\"><em>Part 2</em></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1, "vmvTYnmaKA73fYDe5": 1, "AHK82ypfxF45rqh9D": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zKxK66pHw9aL2btkr", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 48, "extendedScore": null, "score": 0.0005264750054754617, "legacy": true, "legacyId": "3811", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 110, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-10-25T02:13:34.654Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-25T03:09:06.462Z", "modifiedAt": null, "url": null, "title": "A Paradox in Timeless Decision Theory", "slug": "a-paradox-in-timeless-decision-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:21.517Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pFTEa5D9Hk9fmHLgc/a-paradox-in-timeless-decision-theory", "pageUrlRelative": "/posts/pFTEa5D9Hk9fmHLgc/a-paradox-in-timeless-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/pFTEa5D9Hk9fmHLgc/a-paradox-in-timeless-decision-theory", "postedAtFormatted": "Monday, October 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Paradox%20in%20Timeless%20Decision%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Paradox%20in%20Timeless%20Decision%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpFTEa5D9Hk9fmHLgc%2Fa-paradox-in-timeless-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Paradox%20in%20Timeless%20Decision%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpFTEa5D9Hk9fmHLgc%2Fa-paradox-in-timeless-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpFTEa5D9Hk9fmHLgc%2Fa-paradox-in-timeless-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 326, "htmlBody": "<p>I'm putting this in the discussion section because I'm not sure whether something like this has already been thought of, and I don't want to repeat things in a top-level post.</p>\n<p>Anyway, consider a Prisoner's-Dilemma-like situation with the following payoff matrix:<br />You defect, opponent defects: 0 utils<br />You defect, opponent cooperates: 3 utils<br />You cooperate, opponent defects: 1 util<br />You cooperate, opponent cooperates: 2 utils<br />Assume all players have either have full information about their opponents, or are allowed to communicate and will be able to deduce each others' strategy correctly.</p>\n<p>Suppose you are a a timeless decision theory agent playing this modified Prisoner's Dilemma with an actor that will always pick \"defect\" no matter what your strategy is. Clearly, your best move is to cooperate, gaining you 1 util instead of no utility, and giving your opponent his maximum 3 utils instead of the no utility he would get if you defected. Now suppose you are playing against another timeless decision theory agent. Clearly, the best strategy is to be that actor which defects no matter what. If both agents do this, the worst possible result for both of them occurs.</p>\n<p>This situation can actually happen in the real world. Suppose there are two rival countries, and one demands some tribute or concession from the other, and threatens war if the other country does not agree, even though such a war would be very costly for both countries. The rulers of the threatened country can either pay the less expensive tribute or accept a more expensive war so that the first country will back off, but the rulers of the first country have thought of that and have committed to not back down anyway. If the tribute is worth 1 util to each side, and a war costs 2 utils to each side, this is identical to the payoff matrix I described. I'd be pretty surprised if nothing like this has ever happened.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1db": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pFTEa5D9Hk9fmHLgc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "3814", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-25T12:06:29.139Z", "modifiedAt": null, "url": null, "title": "Willpower: not a limited resource?", "slug": "willpower-not-a-limited-resource", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.998Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jess_Riedel", "createdAt": "2009-02-27T15:50:16.058Z", "isAdmin": false, "displayName": "Jess_Riedel"}, "userId": "giTcTdnsiiG6SqXpS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NPxGwZGoyyrNzkjNw/willpower-not-a-limited-resource", "pageUrlRelative": "/posts/NPxGwZGoyyrNzkjNw/willpower-not-a-limited-resource", "linkUrl": "https://www.lesswrong.com/posts/NPxGwZGoyyrNzkjNw/willpower-not-a-limited-resource", "postedAtFormatted": "Monday, October 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Willpower%3A%20not%20a%20limited%20resource%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWillpower%3A%20not%20a%20limited%20resource%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNPxGwZGoyyrNzkjNw%2Fwillpower-not-a-limited-resource%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Willpower%3A%20not%20a%20limited%20resource%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNPxGwZGoyyrNzkjNw%2Fwillpower-not-a-limited-resource", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNPxGwZGoyyrNzkjNw%2Fwillpower-not-a-limited-resource", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 222, "htmlBody": "<p>Stanford Report has a university <a href=\"http://news.stanford.edu/news/2010/october/willpower-resource-study-101410.html\">public press release</a> about a recent <a href=\"http://pss.sagepub.com/content/early/2010/09/28/0956797610384745.full\">paper</a> [subscription required] in <a href=\"http://pss.sagepub.com/\">Psychological Science</a>.&nbsp; The <a href=\"http://www.stanford.edu/~gwalton/home/Publications_files/Job,%20Dweck,%20%26%20Walton,%202010.pdf\">paper is available for free</a> from a website of one of the authors.</p>\n<p>The gist is that they find evidence against the (currently fashionable) hypothesis that willpower is an expendable resource.&nbsp; Here is the leader:</p>\n<blockquote>\n<p>Veronika Job, Carol S. Dweck, and Gregory M. Walton <br />Stanford University</p>\n<p><br />Abstract:</p>\n<p>Much recent research suggests that willpower&mdash;the capacity to exert self-control&mdash;is a limited resource that is depleted after exertion. We propose that whether depletion takes place or not depends on a person&rsquo;s belief about whether willpower is a limited resource. Study 1 found that individual differences in lay theories about willpower moderate ego-depletion effects: People who viewed the capacity for self-control as not limited did not show diminished self-control after a depleting experience. Study 2 replicated the effect, manipulating lay theories about willpower. Study 3 addressed questions about the mechanism underlying the effect. Study 4, a longitudinal field study, found that theories about willpower predict change in eating behavior, procrastination, and self-regulated goal striving in depleting circumstances. Taken together, the findings suggest that reduced self-control after a depleting task or during demanding periods may reflect people&rsquo;s beliefs about the availability of willpower rather than true resource depletion.</p>\n</blockquote>\n<p>(HT: Brashman, <a href=\"http://news.ycombinator.com/item?id=1828847\">as posted on HackerNews</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YrLoz567b553YouZ2": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NPxGwZGoyyrNzkjNw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 39, "extendedScore": null, "score": 6.385783302120781e-07, "legacy": true, "legacyId": "3818", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-25T13:15:02.855Z", "modifiedAt": null, "url": null, "title": "Let's split the cake, lengthwise, upwise and slantwise", "slug": "let-s-split-the-cake-lengthwise-upwise-and-slantwise", "viewCount": null, "lastCommentedAt": "2019-05-15T18:56:25.372Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hCwFxBai3oNnxrM9v/let-s-split-the-cake-lengthwise-upwise-and-slantwise", "pageUrlRelative": "/posts/hCwFxBai3oNnxrM9v/let-s-split-the-cake-lengthwise-upwise-and-slantwise", "linkUrl": "https://www.lesswrong.com/posts/hCwFxBai3oNnxrM9v/let-s-split-the-cake-lengthwise-upwise-and-slantwise", "postedAtFormatted": "Monday, October 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Let's%20split%20the%20cake%2C%20lengthwise%2C%20upwise%20and%20slantwise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALet's%20split%20the%20cake%2C%20lengthwise%2C%20upwise%20and%20slantwise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhCwFxBai3oNnxrM9v%2Flet-s-split-the-cake-lengthwise-upwise-and-slantwise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Let's%20split%20the%20cake%2C%20lengthwise%2C%20upwise%20and%20slantwise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhCwFxBai3oNnxrM9v%2Flet-s-split-the-cake-lengthwise-upwise-and-slantwise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhCwFxBai3oNnxrM9v%2Flet-s-split-the-cake-lengthwise-upwise-and-slantwise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1101, "htmlBody": "<p>This post looks at some of the current models for how two agents can split the gain in non-zero sum interactions. For instance, if you and a buddy have to split &pound;10 between the two of you, where the money is discarded if you can't reach a deal. Or there is an opportunity to trade your Elvis memorabilia for someone's collection of North Korean propaganda posters: unless you can agree to a way of splitting the gain from trade, the trade won't happen. Or there is the stereotypical battle of the sexes: either a romantic dinner (RD) or a night of Battlestar Galactica (BG) is on offer, and both members of the couple prefer doing something together to doing it separately - but, of course, each one has their preference on <em>which </em>activity to do together.</p>\n<p>Unlike standard games such as Prisoner's Dilemma, this is a coordinated game: the two agents will negotiate a joint solution, which is presumed to be binding. This allows for such solutions as 50% (BG,BG) + 50% (RD,RD), which cannot happen with each agent choosing their moves independently. The two agents will be assumed to be expected utility maximisers. What would your feelings be on a good bargaining outcome in this situation?</p>\n<p>Enough about your feelings; let's see what the experts are saying. In general, if A and C are outcomes with utilities (a,b) and (c,d), then another possible outcome is pA + (1-p)C (where you decide first, with odds p:1-p, whether to do outcome A or C), with utility p(a,b) + (1-p)(c,d). Hence if you plot every possible expected utilities in the plane for a given game, you get a convex set.</p>\n<p>For instance, if there is an interaction with possible pure outcomes (-1,2), (2,10), (4,9), (5,7), (6,3), then the set of actual possible utilities is the pentagon presented here:<a id=\"more\"></a></p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_2x8_1.png\" alt=\"\" width=\"186\" height=\"214\" /></p>\n<p>The points highlighted in red are the Pareto-optimal points: those where there is no possibility of making both players better off. Generally only Pareto-optimal points are considered valid negotiation solutions: any other point leaves both player strictly poorer than they could be.</p>\n<p>The first and instinctive way to choose an outcome is to select the egalitarian point. This is the Pareto-optimal point where both players get the same utility (in this instance, both get 27/5 utility):</p>\n<p align=\"center\"><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_2x8_0.png?v=4f6667606f647380a0c24683b6f992c0\" alt=\"\" width=\"181\" height=\"268\" /></p>\n<p>However, you can add an arbitrary number to your utility function without changing any of your decisions; your utility function is translation invariant. Since both players can do so, they can translate the shape seen above in any direction in the plane without changing their fundamental value system. Thus the egalitarian solution is not well-defined, but an artefact of how the utility functions are expressed. One can move it to any Pareto-optimal point simply with translations. Thus the naive egalitarian solution should be rejected.</p>\n<p>Another naive possibility is to pick the point that reflects the highest total utility, by adding the two utilities together. This gives the vertex point (4,9):</p>\n<p align=\"center\">&nbsp;</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_2x8_2.png\" alt=\"\" width=\"166\" height=\"191\" /></p>\n<p>This method will select the same outcome, no matter what translation is applied to the utilities. But this method is flawed as well: if one agent decides to multiply their utility function by any positive real factor, this does not change their underlying algorithm at all, but does change the \"highest total utility\". If the first agent decides to multiply their utility by five, for instance, we have the new picture portrayed here, and the \"highest total utility point\" has decidedly shifted, to the first player's great advantage:</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_2x8_3.png?v=14266b8468f7e984acf288d591222845\" alt=\"\" width=\"554\" height=\"144\" /></p>\n<p>To summarise these transformations to utility functions, we say that a utility function is defined only up to affine transformations - translations and scalings. Thus any valid method of bargaining must also be invariant to affine transformations of both the utility functions.</p>\n<p>The two most popular ways of doing this are the Nash Bargaining solution (NBS), and the Kalai-Smorodinsky Bargaining Solution (KSBS). Both are dependent on the existence of a \"disagreement point\" d: a point representing the utilities that each player will get if negotiations break down. Establishing this disagreement point is another subject entirely; but once it is agreed upon, one translates it so that d=(0,0). This means that one has removed the translation freedom from the bargaining game, as any translation by either player will move d away from the origin. In this example, I'll arbitrarily choose the disagreement at d=(-1,4); hence it now suffices to find a scalings-invariant solution.</p>\n<p>Nash did so by maximising the product of the (translated) utilities. There are hyperbolas in the plane defined by the sets of (x,y) such that xy=a; Nash's requirement is equivalent with picking the outcome that lies on the hyperbola furthest from the origin. The NBS and the hyperbolas can be seen here:</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_2x8_4.png?v=627642a0d3c3dddd699b315153b0f467\" alt=\"\" width=\"202\" height=\"237\" /></p>\n<p>This gives the NBS as (4,9). But what happens when we scale the utilities? This is equivalent to multiplying x and y by constant positive factors. This will change xy=a to xy=b, thus will map hyperbolas to hyperbolas. This also does not change the concept of \"furthest hyperbola from the origin\", so the NBS will not change. This is illustrated for the scalings x-&gt;(18/25)x and y-&gt;(19/18)y:</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_2x8_5.png?v=227e16c99f6e80b0d1c2bb7dcb78a1c1\" alt=\"\" width=\"252\" height=\"281\" /></p>\n<p>The NBS has the allegedly desirable property that it is \"Independent of Irrelevant Alternatives\". This means that if we delete an option that is not the NBS, then the NBS does not change, as can be seen by removing (2,10):</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_2x8_10.png?v=0949bb73602875fbb4439931d6b08ad5\" alt=\"\" width=\"243\" height=\"285\" /></p>\n<p>&nbsp;</p>\n<p>Some felt that \"Independence of Irrelevant Alternatives\" was not a desirable property. In the above situation, player 2 gets their best option, while player 1 got the worse (Pareto-optimal) option available. Human intuition rebels against \"bargaining\" where one side gets everything, without having to give up anything. The NBS has other counter-intuitive properties, such as the that extra solutions can make some players worse off. Hence the KSBS replaces the independence requirement with \"if one adds an extra option that is not better than either player's best option, this makes no player worse off\".</p>\n<p>The KSBS is often formulated in terms of \"preserving the ratio of maximum gains\", but there is another, more intuitive formulation. Define the utopia point u=(u<sub>x</sub>, u<sub>y</sub>), where u<sub>x</sub> is the maximal possible gain for player 1, and u<sub>y</sub> the maximal possible gain for player 2. The utopia point is what a game-theory fairy could offer the players so that each would get the maximal possible gain from the game. Then the KSBS is defined as the Pareto-optimal point on the line joining the disagreement point d with the utopia point u:</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_2x8_7.png?v=1155cd3056ae63adee2b5093a1cb485c\" alt=\"\" width=\"211\" height=\"244\" /></p>\n<p>Another way to formulate the KSBS is by renormalising the utilities so that d=(0,0), u=(1,1), and then choosing the egalitarian solution:</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_2x8_9.png?v=8f5d4b3befdf26434de3ef7c38ff3aac\" alt=\"\" width=\"211\" height=\"264\" /></p>\n<p>In a subsequent post, I'll critique these bargaining solutions, and propose a more utility-maximising way of looking at the whole process.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 1, "chuP2QqQycjD8qakL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hCwFxBai3oNnxrM9v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 50, "baseScore": 74, "extendedScore": null, "score": 0.000142, "legacy": true, "legacyId": "3788", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 74, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-25T13:53:47.015Z", "modifiedAt": null, "url": null, "title": "Dealing with the high quantity of scientific error in medicine", "slug": "dealing-with-the-high-quantity-of-scientific-error-in", "viewCount": null, "lastCommentedAt": "2018-11-23T21:24:54.113Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PnYh6hZsFPRB3GPCe/dealing-with-the-high-quantity-of-scientific-error-in", "pageUrlRelative": "/posts/PnYh6hZsFPRB3GPCe/dealing-with-the-high-quantity-of-scientific-error-in", "linkUrl": "https://www.lesswrong.com/posts/PnYh6hZsFPRB3GPCe/dealing-with-the-high-quantity-of-scientific-error-in", "postedAtFormatted": "Monday, October 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dealing%20with%20the%20high%20quantity%20of%20scientific%20error%20in%20medicine&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADealing%20with%20the%20high%20quantity%20of%20scientific%20error%20in%20medicine%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPnYh6hZsFPRB3GPCe%2Fdealing-with-the-high-quantity-of-scientific-error-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dealing%20with%20the%20high%20quantity%20of%20scientific%20error%20in%20medicine%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPnYh6hZsFPRB3GPCe%2Fdealing-with-the-high-quantity-of-scientific-error-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPnYh6hZsFPRB3GPCe%2Fdealing-with-the-high-quantity-of-scientific-error-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 928, "htmlBody": "<p><a href=\"http://www.theatlantic.com/magazine/archive/2010/11/lies-damned-lies-and-medical-science/8269\">In a recent article</a>, John Ioannidis describes a very high proportion of medical research as wrong.</p>\n<blockquote>Still, Ioannidis anticipated that the community might shrug off his findings: sure, a lot of dubious research makes it into journals, but we researchers and physicians know to ignore it and focus on the good stuff, so what&rsquo;s the big deal? The other paper headed off that claim. He zoomed in on 49 of the most highly regarded research findings in medicine over the previous 13 years, as judged by the science community&rsquo;s two standard measures: the papers had appeared in the journals most widely cited in research articles, and the 49 articles themselves were the most widely cited articles in these journals. These were articles that helped lead to the widespread popularity of treatments such as the use of hormone-replacement therapy for menopausal women, vitamin E to reduce the risk of heart disease, coronary stents to ward off heart attacks, and daily low-dose aspirin to control blood pressure and prevent heart attacks and strokes. Ioannidis was putting his contentions to the test not against run-of-the-mill research, or even merely well-accepted research, but against the absolute tip of the research pyramid. Of the 49 articles, 45 claimed to have uncovered effective interventions. Thirty-four of these claims had been retested, and 14 of these, or 41 percent, had been convincingly shown to be wrong or significantly exaggerated. If between a third and a half of the most acclaimed research in medicine was proving untrustworthy, the scope and impact of the problem were undeniable. That article was published in the Journal of the American Medical Association.</blockquote>\n<p>Part of the problem is that surprising results get more interest, and surprising results are more likely to be wrong. (I'm not dead certain of this-- if the baseline beliefs are highly likely to be wrong, surprising beliefs become somewhat less likely to be wrong.) Replication is boring. Failure to replicate a bright shiny surprising belief is boring. A tremendous amount isn't checked, and that's before you start considering that a lot of medical research is funded by companies that want to sell something.</p>\n<p><a href=\"http://www.wired.com/wiredscience/2010/10/how-to-set-the-bullshit-filter-when-the-bullshit-is-thick/\">Ioannidis' corollaries:</a></p>\n<blockquote>Corollary 1: The smaller the studies conducted in a scientific field, the less likely the research findings are to be true.</blockquote>\n<blockquote>Corollary 2: The smaller the effect sizes in a scientific field, the less likely the research findings are to be true.</blockquote>\n<blockquote>Corollary 3: The greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true.</blockquote>\n<blockquote>Corollary 4: The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.</blockquote>\n<blockquote>Corollary 5: The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true.</blockquote>\n<blockquote>Corollary 6: The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.</blockquote>\n<p>The culture at LW shows a lot of reliance on small inferential psychological studies-- for example that doing a good deed leads to worse behavior later. Please watch out for that.<a id=\"more\"></a></p>\n<p>A smidgen of good news: <a href=\"http://failuretoreplicate.com/\">Failure to Replicate</a>, a website about failures to replicate psychological findings. I think this could be very valuable, and if you agree, please boost the signal by posting it elsewhere.</p>\n<p>From Failure to Replicate's author-- <a href=\"http://www.talyarkoni.org/blog/2009/11/26/solving-the-file-drawer-problem-by-making-the-internet-the-drawer/\">A problem with metastudies</a>:</p>\n<blockquote>Eventually, someone else comes across this small literature and notices that it contains &ldquo;mixed findings&rdquo;, with some studies finding an effect, and others finding no effect. So this special someone&ndash;let&rsquo;s call them the Master of the Gnomes&ndash;decides to do a formal meta-analysis. (A meta-analysis is basically just a fancy way of taking a bunch of other people&rsquo;s studies, throwing them in a blender, and pouring out the resulting soup into a publication of your very own.) Now you can see why the failure to publish null results is going to be problematic: What the Master of the Gnomes doesn&rsquo;t know about, the Master of the Gnomes can&rsquo;t publish about. So any resulting meta-analytic estimate of the association between lawn gnomes and subjective well-being is going to be biased in the positive direction. That is, there&rsquo;s a good chance that the meta-analysis will end up saying lawn gnomes make people very happy,when in reality lawn gnomes only make people a little happy, or don&rsquo;t make people happy at all.</blockquote>\n<p>The people I've read who gave advice based on Ioannidis article strongly recommended eating <a href=\"http://www.paleodiet.com/definition.htm\">paleo</a>. I don't think this is awful advice in the sense that a number of people seem to actually feel better following it, and I haven't heard of disasters resulting from eating paleo. However, I don't know that it's a general solution to the problems of living with a medical system which does necessary work some of the time, but also is wildly inaccurate and sometimes destructive.</p>\n<p>The following advice is has a pure base of anecdote, but at least I've heard a lot of them from people with ongoing medical problems. (Double meaning intended.)</p>\n<p>Before you use prescription drugs and/or medical procedures, make sure there's something wrong with you. Keep an eye out for side effects and the results of combined medicines. Check for evidence that whatever you're thinking about doing actually helps. Be careful with statins-- they can cause reversible memory problems and permanent muscle weakness. Choose a doctor who listens to you.</p>\n<p><a href=\"http://boards.sethroberts.net/\">Forum about self-experimentation</a>-- note: even Seth Roberts is apt to oversell his results as applying to everyone.</p>\n<p>Link about the failure to replicate site found <a href=\"http://gameswithwords.fieldofscience.com/2010/10/youre-wrong.html\">here.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vg4LDxjdwHLotCm8w": 1, "xHjy88N2uJvGdgzfw": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PnYh6hZsFPRB3GPCe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 59, "extendedScore": null, "score": 9.2e-05, "legacy": true, "legacyId": "3819", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 45, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-25T19:21:31.987Z", "modifiedAt": null, "url": null, "title": "A suggestion on how to get people to read the Sequences", "slug": "a-suggestion-on-how-to-get-people-to-read-the-sequences", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:02.644Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AdeleneDawner", "createdAt": "2009-04-28T14:40:00.131Z", "isAdmin": false, "displayName": "AdeleneDawner"}, "userId": "MeSREm4SMRGxeQ8X3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ARNgTP7GtPgEJtQ2D/a-suggestion-on-how-to-get-people-to-read-the-sequences", "pageUrlRelative": "/posts/ARNgTP7GtPgEJtQ2D/a-suggestion-on-how-to-get-people-to-read-the-sequences", "linkUrl": "https://www.lesswrong.com/posts/ARNgTP7GtPgEJtQ2D/a-suggestion-on-how-to-get-people-to-read-the-sequences", "postedAtFormatted": "Monday, October 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20suggestion%20on%20how%20to%20get%20people%20to%20read%20the%20Sequences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20suggestion%20on%20how%20to%20get%20people%20to%20read%20the%20Sequences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FARNgTP7GtPgEJtQ2D%2Fa-suggestion-on-how-to-get-people-to-read-the-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20suggestion%20on%20how%20to%20get%20people%20to%20read%20the%20Sequences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FARNgTP7GtPgEJtQ2D%2Fa-suggestion-on-how-to-get-people-to-read-the-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FARNgTP7GtPgEJtQ2D%2Fa-suggestion-on-how-to-get-people-to-read-the-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<p>I just encountered <a href=\"http://www.archivebinge.net/\">Archive Binge</a>, a website that makes custom RSS feeds of certain webcomics' archives, presenting a few comics per day so that people can easily catch up to those comics' current strips without overloading themselves.</p>\n<p>I strongly suspect that a similar tool would be useful for the Sequences. It might be good to have a few extra features, like the ability to only see posts with a certain tag, but even just a basic feed that presented one or two of them a week would be useful.</p>\n<p>It might be even more useful to, rather than allowing each person to make their own feed, have a single feed that cycles through everything and then restarts, to encourage new conversations about those articles between people who are reading the sequences at the same time. If the resulting cycle is too long, we could also have a second or third feed offset from the main one, so that no one has to wait more than a few months to subscribe to a feed that's starting at the beginning.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 1, "7ow6EFpypbH4hzFuz": 1, "JMD7LTXTisBzGAfhX": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ARNgTP7GtPgEJtQ2D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 43, "extendedScore": null, "score": 6.386816979731175e-07, "legacy": true, "legacyId": "3820", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-25T21:30:58.150Z", "modifiedAt": null, "url": null, "title": "Activation Costs", "slug": "activation-costs", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:33.131Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lionhearted", "createdAt": "2010-07-29T13:30:07.417Z", "isAdmin": false, "displayName": "lionhearted"}, "userId": "tooJeLNxoeccqGEky", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WpqHb9hHcd37hh4Tf/activation-costs", "pageUrlRelative": "/posts/WpqHb9hHcd37hh4Tf/activation-costs", "linkUrl": "https://www.lesswrong.com/posts/WpqHb9hHcd37hh4Tf/activation-costs", "postedAtFormatted": "Monday, October 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Activation%20Costs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AActivation%20Costs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpqHb9hHcd37hh4Tf%2Factivation-costs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Activation%20Costs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpqHb9hHcd37hh4Tf%2Factivation-costs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpqHb9hHcd37hh4Tf%2Factivation-costs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 972, "htmlBody": "<p>Enter <a href=\"http://en.wikipedia.org/wiki/Activation_energy\">Wikipedia</a>:</p>\n<blockquote>\n<p>In chemistry, activation energy is a term introduced in 1889 by the Swedish scientist Svante Arrhenius, that is defined as the energy that must be overcome in order for a chemical reaction to occur.</p>\n</blockquote>\n<p><strong>In this article, I propose that:</strong></p>\n<ul>\n<li>Every action you take has an activation cost (perhaps zero)</li>\n<li>These costs vary from person to person</li>\n<li>These costs can change over time</li>\n<li>Activation costs explain a lot of akrasia</li>\n</ul>\n<p><strong>After proposing that, I'd like to explore:</strong></p>\n<ul>\n<li>Factors that increase activation costs</li>\n<li>Factors that decrease activation costs</li>\n</ul>\n<p><strong>Every action a person takes has an activation cost.&nbsp;</strong>The activation cost of a consistent, deeply embedded habit is zero. It happens almost automatically. The activation cost for most people in the United States to exercising is fairly high, and most people are inconsistent about exercising. However, there are people who - every single day - begin by putting their running shoes on and running. Their activation cost to running is effectively zero.</p>\n<p><strong>These costs vary from person to person.</strong> In the daily running example above, the activation cost to the runner is low. The runner simply starts running in the morning. For most people, it's higher for a variety of reasons we'll get to in a moment. The running example is fairly obvious, but you'll also see phenomenon like a neat person saying to a sloppy one, \"Why don't you clean your desk? ... just f'ing do it, man.\" Assuming the messy person indeed wants to have a clean desk, then it's likely the messy person has a higher activation cost to cleaning his desk. (He could also have less energy/willpower)<a id=\"more\"></a></p>\n<p><strong>These costs can change over time.</strong> If the every-morning-runner suffers from a prolonged illness or injury and ceases to run, restarting the program might have a much higher activation cost for a variety of reasons we'll cover in a moment. &nbsp;</p>\n<p><strong>Finally, I'd like to propose that activation costs explain a lot of akrasia and procrastination.</strong> Akrasia is defined as \"acting against one's better judgment.\" I think it's possible that an action a person wishes to take has higher activation costs than they have available energy for activation at the moment. There is emerging literature on limited willpower and \"<a href=\"http://en.wikipedia.org/wiki/Ego_depletion\">ego depletion</a>,\" here's Wikipedia on the topic:</p>\n<blockquote>\n<p>Ego depletion refers to the idea that self-control or willpower is an exhaustible resource that can be used up. When that energy is low (rather than high), mental activity that requires self-control is impaired. In other words, using one's self-control impairs the ability to control one's self later on. In this sense, the idea of (limited) willpower is correct.</p>\n</blockquote>\n<p>While this is anecdotal, I believe that starting a desired action is frequently the hardest part, and usually the part that requires the most ego/will/energy. Thus, the activation cost. Continuing in motion is not as difficult as starting - as activating.</p>\n<p><strong>This implies that there would be two effective ways to beat akrasia-based procrastination.</strong> <strong>The first would be to lower the activation cost; the second would be to increase energy/willpower/ego available for activation.</strong></p>\n<p>Both are valid approaches, but I think lowering activation costs is more sustainable. I think there's local maximums of energy that can be achieved, and it's likely that even the most successful and industrious people will go through low energy periods. Obviously, by lowering an activation cost to zero or near zero, it becomes trivial to do the action as much as is desired.</p>\n<p>Some people have a zero activation cost to go running, and do it every day for the benefit of their health. Some people have zero activation cost to cleaning their desk, and do it whenever they realize its messy. Some people have a zero activation cost to self-promote/self-market, and thus they're frequently talking themselves up, promoting, and otherwise trying to get people to pay attention to their work. Most of us have higher activation costs to go running, clean a desk, or to market/promote something. Thus, it burns a lot more energy and is actually effectively impossible to complete the action sometimes.</p>\n<p><strong>The following factors seem to increase activation cost </strong>(not a complete list)<strong>:</strong></p>\n<ul>\n<li><a href=\"/lw/21b/ugh_fields/\">Ugh fields</a></li>\n<li><a href=\"/lw/f1/beware_trivial_inconveniences/\">Trivial inconveniences</a></li>\n<li><a href=\"/lw/2q6/compartmentalization_in_epistemic_and/\">Poor quality compartmentalization</a></li>\n<li>Identity tied to an action to be taken that action-taker isn't skilled at</li>\n<li>Unclear or difficult instructions</li>\n<li>Prior failed attempts at the action or type of action</li>\n<li>Feeling like something is work or \"has to be done\"</li>\n</ul>\n<p><strong>The following factors seem to decrease activation cost </strong>(not a complete list)<strong>:</strong></p>\n<ul>\n<li>Deadline urgency</li>\n<li>Constraints (and thus, lack of opportunity cost)</li>\n<li>Momentum</li>\n<li>Grouping/batching tasks together</li>\n<li><a href=\"/lw/1fe/antiakrasia_technique_structured_procrastination/\">Structured Procrastination</a></li>\n<li>Very clear, straightforward instructions</li>\n<li>Long term habits</li>\n<li><a href=\"/lw/4e/cached_selves/\">Cached-self effects</a></li>\n<li>Feeling like something is a game</li>\n</ul>\n<p>Additionally, another way to go anti-akrasia is to increase energy levels through good diet, exercise, mental health, breathing, collaboration, good work environment, nature, adequate rest and relaxation. Some of these might additionally lower activation costs in addition to increasing energy.</p>\n<p><strong>I believe the most effective way to do activities you want to do is to decrease their activation cost to as close to zero as possible.</strong> This implies you should <a href=\"/lw/2cv/defeating_ugh_fields_in_practice/\">defeat ugh fields</a>, reduce trivial inconveniences and barriers, de-compartmentalize (and get <a href=\"/lw/nb/something_to_protect/\">something to protect</a>), untangle your identity from the action you're taking, and find as clear instructions as possible. Also, deadlines, constraints, momentum, grouping and batching tasks, structured procrastination, clear instructions, establishing habits, setting up helpful cached-self effects and reducing negative ones, and treating activities to be done as a game all seem to be of value.</p>\n<p>I would be excited for more discussion on this topic. I believe activation costs are a large part of what causes procrastination akrasia, and reducing activation costs will help us get what we want.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WpqHb9hHcd37hh4Tf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 39, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "3821", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Enter <a href=\"http://en.wikipedia.org/wiki/Activation_energy\">Wikipedia</a>:</p>\n<blockquote>\n<p>In chemistry, activation energy is a term introduced in 1889 by the Swedish scientist Svante Arrhenius, that is defined as the energy that must be overcome in order for a chemical reaction to occur.</p>\n</blockquote>\n<p><strong id=\"In_this_article__I_propose_that_\">In this article, I propose that:</strong></p>\n<ul>\n<li>Every action you take has an activation cost (perhaps zero)</li>\n<li>These costs vary from person to person</li>\n<li>These costs can change over time</li>\n<li>Activation costs explain a lot of akrasia</li>\n</ul>\n<p><strong id=\"After_proposing_that__I_d_like_to_explore_\">After proposing that, I'd like to explore:</strong></p>\n<ul>\n<li>Factors that increase activation costs</li>\n<li>Factors that decrease activation costs</li>\n</ul>\n<p><strong>Every action a person takes has an activation cost.&nbsp;</strong>The activation cost of a consistent, deeply embedded habit is zero. It happens almost automatically. The activation cost for most people in the United States to exercising is fairly high, and most people are inconsistent about exercising. However, there are people who - every single day - begin by putting their running shoes on and running. Their activation cost to running is effectively zero.</p>\n<p><strong>These costs vary from person to person.</strong> In the daily running example above, the activation cost to the runner is low. The runner simply starts running in the morning. For most people, it's higher for a variety of reasons we'll get to in a moment. The running example is fairly obvious, but you'll also see phenomenon like a neat person saying to a sloppy one, \"Why don't you clean your desk? ... just f'ing do it, man.\" Assuming the messy person indeed wants to have a clean desk, then it's likely the messy person has a higher activation cost to cleaning his desk. (He could also have less energy/willpower)<a id=\"more\"></a></p>\n<p><strong>These costs can change over time.</strong> If the every-morning-runner suffers from a prolonged illness or injury and ceases to run, restarting the program might have a much higher activation cost for a variety of reasons we'll cover in a moment. &nbsp;</p>\n<p><strong>Finally, I'd like to propose that activation costs explain a lot of akrasia and procrastination.</strong> Akrasia is defined as \"acting against one's better judgment.\" I think it's possible that an action a person wishes to take has higher activation costs than they have available energy for activation at the moment. There is emerging literature on limited willpower and \"<a href=\"http://en.wikipedia.org/wiki/Ego_depletion\">ego depletion</a>,\" here's Wikipedia on the topic:</p>\n<blockquote>\n<p>Ego depletion refers to the idea that self-control or willpower is an exhaustible resource that can be used up. When that energy is low (rather than high), mental activity that requires self-control is impaired. In other words, using one's self-control impairs the ability to control one's self later on. In this sense, the idea of (limited) willpower is correct.</p>\n</blockquote>\n<p>While this is anecdotal, I believe that starting a desired action is frequently the hardest part, and usually the part that requires the most ego/will/energy. Thus, the activation cost. Continuing in motion is not as difficult as starting - as activating.</p>\n<p><strong>This implies that there would be two effective ways to beat akrasia-based procrastination.</strong> <strong>The first would be to lower the activation cost; the second would be to increase energy/willpower/ego available for activation.</strong></p>\n<p>Both are valid approaches, but I think lowering activation costs is more sustainable. I think there's local maximums of energy that can be achieved, and it's likely that even the most successful and industrious people will go through low energy periods. Obviously, by lowering an activation cost to zero or near zero, it becomes trivial to do the action as much as is desired.</p>\n<p>Some people have a zero activation cost to go running, and do it every day for the benefit of their health. Some people have zero activation cost to cleaning their desk, and do it whenever they realize its messy. Some people have a zero activation cost to self-promote/self-market, and thus they're frequently talking themselves up, promoting, and otherwise trying to get people to pay attention to their work. Most of us have higher activation costs to go running, clean a desk, or to market/promote something. Thus, it burns a lot more energy and is actually effectively impossible to complete the action sometimes.</p>\n<p><strong>The following factors seem to increase activation cost </strong>(not a complete list)<strong>:</strong></p>\n<ul>\n<li><a href=\"/lw/21b/ugh_fields/\">Ugh fields</a></li>\n<li><a href=\"/lw/f1/beware_trivial_inconveniences/\">Trivial inconveniences</a></li>\n<li><a href=\"/lw/2q6/compartmentalization_in_epistemic_and/\">Poor quality compartmentalization</a></li>\n<li>Identity tied to an action to be taken that action-taker isn't skilled at</li>\n<li>Unclear or difficult instructions</li>\n<li>Prior failed attempts at the action or type of action</li>\n<li>Feeling like something is work or \"has to be done\"</li>\n</ul>\n<p><strong>The following factors seem to decrease activation cost </strong>(not a complete list)<strong>:</strong></p>\n<ul>\n<li>Deadline urgency</li>\n<li>Constraints (and thus, lack of opportunity cost)</li>\n<li>Momentum</li>\n<li>Grouping/batching tasks together</li>\n<li><a href=\"/lw/1fe/antiakrasia_technique_structured_procrastination/\">Structured Procrastination</a></li>\n<li>Very clear, straightforward instructions</li>\n<li>Long term habits</li>\n<li><a href=\"/lw/4e/cached_selves/\">Cached-self effects</a></li>\n<li>Feeling like something is a game</li>\n</ul>\n<p>Additionally, another way to go anti-akrasia is to increase energy levels through good diet, exercise, mental health, breathing, collaboration, good work environment, nature, adequate rest and relaxation. Some of these might additionally lower activation costs in addition to increasing energy.</p>\n<p><strong>I believe the most effective way to do activities you want to do is to decrease their activation cost to as close to zero as possible.</strong> This implies you should <a href=\"/lw/2cv/defeating_ugh_fields_in_practice/\">defeat ugh fields</a>, reduce trivial inconveniences and barriers, de-compartmentalize (and get <a href=\"/lw/nb/something_to_protect/\">something to protect</a>), untangle your identity from the action you're taking, and find as clear instructions as possible. Also, deadlines, constraints, momentum, grouping and batching tasks, structured procrastination, clear instructions, establishing habits, setting up helpful cached-self effects and reducing negative ones, and treating activities to be done as a game all seem to be of value.</p>\n<p>I would be excited for more discussion on this topic. I believe activation costs are a large part of what causes procrastination akrasia, and reducing activation costs will help us get what we want.</p>", "sections": [{"title": "In this article, I propose that:", "anchor": "In_this_article__I_propose_that_", "level": 1}, {"title": "After proposing that, I'd like to explore:", "anchor": "After_proposing_that__I_d_like_to_explore_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "40 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EFQ3F6kmt4WHXRqik", "reitXJgJXFzKpdKyd", "N99KgncSXewWqkzMA", "n5Yfhygz42QNK2vFe", "BHYBdijDcAKQ6e45Z", "Rvm7tmfEQ2RstJBPG", "SGR4GxFK7KmW7ckCB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-25T23:07:49.960Z", "modifiedAt": null, "url": null, "title": "Luminosity (Twilight fanfic) Part 2 Discussion Thread", "slug": "luminosity-twilight-fanfic-part-2-discussion-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:29.297Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JenniferRM", "createdAt": "2009-03-06T17:16:50.600Z", "isAdmin": false, "displayName": "JenniferRM"}, "userId": "g8JkZfL8PTqAefpvx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rk7JtSmSSMpQsaQyi/luminosity-twilight-fanfic-part-2-discussion-thread", "pageUrlRelative": "/posts/rk7JtSmSSMpQsaQyi/luminosity-twilight-fanfic-part-2-discussion-thread", "linkUrl": "https://www.lesswrong.com/posts/rk7JtSmSSMpQsaQyi/luminosity-twilight-fanfic-part-2-discussion-thread", "postedAtFormatted": "Monday, October 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Luminosity%20(Twilight%20fanfic)%20Part%202%20Discussion%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALuminosity%20(Twilight%20fanfic)%20Part%202%20Discussion%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frk7JtSmSSMpQsaQyi%2Fluminosity-twilight-fanfic-part-2-discussion-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Luminosity%20(Twilight%20fanfic)%20Part%202%20Discussion%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frk7JtSmSSMpQsaQyi%2Fluminosity-twilight-fanfic-part-2-discussion-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frk7JtSmSSMpQsaQyi%2Fluminosity-twilight-fanfic-part-2-discussion-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 415, "htmlBody": "<p>This is Part 2 of the discussion of Alicorn's Twilight fanfic <a href=\"http://www.fanfiction.net/s/6137139/1/Luminosity\">Luminosity</a>.&nbsp;</p>\n<p><strong>LATE BREAKING EDIT:</strong> <a href=\"/r/discussion/lw/3jt/luminosity_twilight_fanfic_discussion_thread_3/\">Part 3 exists now</a>, so new comment threads should be started <em>there </em>rather than here.</p>\n<p>In the vein of the Harry Potter and the Methods of Rationality <a href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">discussion</a>&nbsp;<a href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">threads</a>&nbsp;this is the place to discuss anything relating to Alicorn's Twilight fanfic <a href=\"http://www.fanfiction.net/s/6137139/1/Luminosity\">Luminosity</a>. The fanfic is also archived on Alicorn's own <a href=\"http://luminous.elcenia.com/\">website</a>.</p>\n<p>Here is <a href=\"/lw/2mq/luminosity_twilight_fanfic_discussion_thread/\">Part 1 of the discussion</a>.&nbsp; Previous discussion is hidden so deeply within the first Methods of Rationality thread that it's difficult to&nbsp;<a href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality/29md?c=1&amp;context=1#29md\">find</a>&nbsp;even if you already know it exists.</p>\n<p>Similar to how Eliezer's fanfic popularizes material from his sequences Alicorn is using the insights from her <a href=\"/lw/1xh/living_luminously\">Luminosity sequence</a>.</p>\n<p>The fic is really really good but there is a twist part way through that makes the fic even more worth reading than it already was, but that makes it hard to talk about because to even ask if someone is twist-aware with any specific hints is difficult.&nbsp; The twist is in the latter half of the story.&nbsp; If you are certainly not post-twist and want to save the surprise, then you should stop reading here and fall back to Part 1 discussion or to the fic itself.</p>\n<p>&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<p>If you think you're pretty sure you are post-twist and are safe to read the rest of this, try reading this <a href=\"http://rot13.com/index.php\">rot13</a>'ed hint and see if what you've read matches this high level description of the twist...</p>\n<p>Rqjneq unf qvfpbirerq gur frperg gung Vfnoryyn jnf xrrcvat sebz uvz \"sbe uvf bja tbbq\" bhg bs srne bs Neb ernqvat Rqjneq'f zvaq.&nbsp; Va gur nsgrezngu, fbzrguvat unf punatrq nobhg gurve eryngvbafuvc gung znl unir pnhfrq lbh gb pel sbe n juvyr, naq juvpu znlor urycf gb rzbgvbanyyl qevir ubzr gur pbzovarq zrffntr bs YJ'f negvpyrf nobhg \"fbzrguvat gb cebgrpg\" naq \"ernfba nf n zrzrgvp vzzhar qvfbeqre\" naq gur jnl gurl pna fvzhygnarbhfyl nccyl gb crbcyr jub unir abguvat zber va gur jbeyq guna fbzr fvatyr crefba jub gurl ybir.</p>\n<p>If the answer to the hint is obvious, then just to be sure that there is not a <a href=\"/lw/ki/double_illusion_of_transparency/\">double illusion of transparency</a> at work, here is the cutoff point spelled out explicitly:</p>\n<p>Gur phgbss cbvag sbe cbfgvat urer vf gung lbh unir ernq hc gb puncgre svsgl svir (va gur snasvpgvba irefvba) be puncgre gjragl rvtug ba Nyvpbea'f jrofvgr jurer Rqjneq jnf cebonoyl vapvarengrq, Vfnoryyn fheivirf na nggrzcgrq vapvarengvba, naq fur unf gb ortha gb jbex bhg jung gb qb jvgu gur jerpxntr bs gur erfg bs ure \"rgreany\" yvsr.</p>\n<div id=\"entry_t3_2mq\" class=\"content clear\">And now for your regularly scheduled commenting...<br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rk7JtSmSSMpQsaQyi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 6.387354787071399e-07, "legacy": true, "legacyId": "3822", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 425, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["od2x5h6Y6G4nJtgpD", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "hSeqgnc5CBJ643x9k", "9o3Cjjem7AbmmZfBs", "sBBGxdvhKcppQWZZE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-26T08:47:36.128Z", "modifiedAt": null, "url": null, "title": "Should people require a mandatory license for parenting?", "slug": "should-people-require-a-mandatory-license-for-parenting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:00.375Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fSfmive2aTMpdY6qM/should-people-require-a-mandatory-license-for-parenting", "pageUrlRelative": "/posts/fSfmive2aTMpdY6qM/should-people-require-a-mandatory-license-for-parenting", "linkUrl": "https://www.lesswrong.com/posts/fSfmive2aTMpdY6qM/should-people-require-a-mandatory-license-for-parenting", "postedAtFormatted": "Tuesday, October 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20people%20require%20a%20mandatory%20license%20for%20parenting%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20people%20require%20a%20mandatory%20license%20for%20parenting%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfSfmive2aTMpdY6qM%2Fshould-people-require-a-mandatory-license-for-parenting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20people%20require%20a%20mandatory%20license%20for%20parenting%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfSfmive2aTMpdY6qM%2Fshould-people-require-a-mandatory-license-for-parenting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfSfmive2aTMpdY6qM%2Fshould-people-require-a-mandatory-license-for-parenting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 332, "htmlBody": "<p><a href=\"http://ieet.org/index.php/IEET/more/munkittrick2010102\">Sir, Could I See Your Breeding License?</a></p>\n<blockquote>\n<p>Why [...] are we so cavalier about who we let have and raise them? As technology enables more people to reproduce, environmental pressures make each new life a bigger burden, and our understanding of child psychology improves, it&rsquo;ll become more and more evident that just because a person can have kids doesn&rsquo;t mean they should have kids. My guess is that, decades down the road, future generations will require a license to reproduce and start a family. That sounds like a pretty good idea to me.</p>\n</blockquote>\n<p>Most important is that children don't have to grow up under horrible circumstances inflicted on them by the inability of their parents. You always have to weigh the freedom of some against any negative infliction it could have on others. In this case a bit less freedom would guarantee a lot less distress.</p>\n<p>It is reasonable. I don't see how we can ask for species-appropriate animal husbandry regarding animals like chimps but not children. You have to have a drivers license for good reasons too. <strong>So why is everyone allowed to rule over helpless human beings for years without having to prove their ability to do so in a way that guarantees the well-being of their prot&eacute;g&eacute;? </strong></p>\n<p>Such discussions always remind me about something important. Children should not be assigned with any religion. There should be a certain age where they can decide what religion they want to follow, if any. This doesn't mean that religious people shouldn't be able to have children but that they shouldn't be able to force their children into a certain framework either. Parents should be forced to allow their children to take part in a educational framework based on contemporary ethics and knowledge. I don't even have a problem with lessons in religion in school as it is part of human nature. But it shall not be focused on any truth value or a certain religion but an overview and comparison with non-religious ethics and truth-seeking.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fSfmive2aTMpdY6qM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -4, "extendedScore": null, "score": -6e-06, "legacy": true, "legacyId": "3825", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-26T11:09:10.853Z", "modifiedAt": null, "url": null, "title": "HELP: How do minimum wage laws harm people?", "slug": "help-how-do-minimum-wage-laws-harm-people", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:31.744Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kingreaper", "createdAt": "2010-06-18T18:02:11.590Z", "isAdmin": false, "displayName": "Kingreaper"}, "userId": "tnFFMseM3DRK52EZR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kbFf6vAoTD2wX6QWh/help-how-do-minimum-wage-laws-harm-people", "pageUrlRelative": "/posts/kbFf6vAoTD2wX6QWh/help-how-do-minimum-wage-laws-harm-people", "linkUrl": "https://www.lesswrong.com/posts/kbFf6vAoTD2wX6QWh/help-how-do-minimum-wage-laws-harm-people", "postedAtFormatted": "Tuesday, October 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20HELP%3A%20How%20do%20minimum%20wage%20laws%20harm%20people%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHELP%3A%20How%20do%20minimum%20wage%20laws%20harm%20people%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkbFf6vAoTD2wX6QWh%2Fhelp-how-do-minimum-wage-laws-harm-people%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=HELP%3A%20How%20do%20minimum%20wage%20laws%20harm%20people%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkbFf6vAoTD2wX6QWh%2Fhelp-how-do-minimum-wage-laws-harm-people", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkbFf6vAoTD2wX6QWh%2Fhelp-how-do-minimum-wage-laws-harm-people", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<p>The concept of minimum wage is one I'm rather attached to. I have dozens of arguments for why it helps people, improves the world, etc. etc. I suspect this view is shared by most of this community, although I haven't seen any discussion of it.</p>\n<p>&nbsp;</p>\n<p>I don't have much understanding of the harms that minimum wages cause; and at what level of minimum wage those harms become relevant (ie. a minimum wage that would not be a living wage even working 24 hours a day is unlikely to have any of the same problems that a minimum wage sufficient to buy an aircraft carrier an hour would have)</p>\n<p>So what are the harms that such laws cause?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kbFf6vAoTD2wX6QWh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 3, "extendedScore": null, "score": 6.389068435290626e-07, "legacy": true, "legacyId": "3827", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-26T11:57:22.948Z", "modifiedAt": null, "url": null, "title": "Levels of Intelligence", "slug": "levels-of-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:28.654Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "draq", "createdAt": "2010-09-03T18:08:04.472Z", "isAdmin": false, "displayName": "draq"}, "userId": "TMiHfZH3dGtbFTAbb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gjyDBd3Werweuhi5w/levels-of-intelligence", "pageUrlRelative": "/posts/gjyDBd3Werweuhi5w/levels-of-intelligence", "linkUrl": "https://www.lesswrong.com/posts/gjyDBd3Werweuhi5w/levels-of-intelligence", "postedAtFormatted": "Tuesday, October 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Levels%20of%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALevels%20of%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgjyDBd3Werweuhi5w%2Flevels-of-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Levels%20of%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgjyDBd3Werweuhi5w%2Flevels-of-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgjyDBd3Werweuhi5w%2Flevels-of-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<h3>Level 1: Algorithm-based Intelligence<br /></h3>\n<p>An intelligence of level 1 acts on innate algorithms, like a bacterium that survives using inherited mechanisms.&nbsp;</p>\n<h3>Level 2: Goal-oriented Intelligence <br /></h3>\n<p>An intelligence of level 2 has an innate goal. It develops and finds new algorithms to solve a problem. For example, <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">the paperclip maximizer</a> is a level-2 intelligence.</p>\n<h3>Level 3: Philosophical Intelligence</h3>\n<p>An intelligence of level 3 has neither any preset algorithms nor goals. It looks for goals and algorithms to achieve the goal. Ethical questions are only applicable to intelligence of level 3.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gjyDBd3Werweuhi5w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -20, "extendedScore": null, "score": 6.389184243941817e-07, "legacy": true, "legacyId": "3471", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h3 id=\"Level_1__Algorithm_based_Intelligence\">Level 1: Algorithm-based Intelligence<br></h3>\n<p>An intelligence of level 1 acts on innate algorithms, like a bacterium that survives using inherited mechanisms.&nbsp;</p>\n<h3 id=\"Level_2__Goal_oriented_Intelligence_\">Level 2: Goal-oriented Intelligence <br></h3>\n<p>An intelligence of level 2 has an innate goal. It develops and finds new algorithms to solve a problem. For example, <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">the paperclip maximizer</a> is a level-2 intelligence.</p>\n<h3 id=\"Level_3__Philosophical_Intelligence\">Level 3: Philosophical Intelligence</h3>\n<p>An intelligence of level 3 has neither any preset algorithms nor goals. It looks for goals and algorithms to achieve the goal. Ethical questions are only applicable to intelligence of level 3.</p>", "sections": [{"title": "Level 1: Algorithm-based Intelligence", "anchor": "Level_1__Algorithm_based_Intelligence", "level": 1}, {"title": "Level 2: Goal-oriented Intelligence ", "anchor": "Level_2__Goal_oriented_Intelligence_", "level": 1}, {"title": "Level 3: Philosophical Intelligence", "anchor": "Level_3__Philosophical_Intelligence", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "82 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-26T13:43:57.762Z", "modifiedAt": null, "url": null, "title": "If you don't know the name of the game, just tell me what I mean to you", "slug": "if-you-don-t-know-the-name-of-the-game-just-tell-me-what-i", "viewCount": null, "lastCommentedAt": "2011-01-08T22:13:15.446Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W2ufY8ihDDWWqJA7h/if-you-don-t-know-the-name-of-the-game-just-tell-me-what-i", "pageUrlRelative": "/posts/W2ufY8ihDDWWqJA7h/if-you-don-t-know-the-name-of-the-game-just-tell-me-what-i", "linkUrl": "https://www.lesswrong.com/posts/W2ufY8ihDDWWqJA7h/if-you-don-t-know-the-name-of-the-game-just-tell-me-what-i", "postedAtFormatted": "Tuesday, October 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20you%20don't%20know%20the%20name%20of%20the%20game%2C%20just%20tell%20me%20what%20I%20mean%20to%20you&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20you%20don't%20know%20the%20name%20of%20the%20game%2C%20just%20tell%20me%20what%20I%20mean%20to%20you%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW2ufY8ihDDWWqJA7h%2Fif-you-don-t-know-the-name-of-the-game-just-tell-me-what-i%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20you%20don't%20know%20the%20name%20of%20the%20game%2C%20just%20tell%20me%20what%20I%20mean%20to%20you%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW2ufY8ihDDWWqJA7h%2Fif-you-don-t-know-the-name-of-the-game-just-tell-me-what-i", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW2ufY8ihDDWWqJA7h%2Fif-you-don-t-know-the-name-of-the-game-just-tell-me-what-i", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1511, "htmlBody": "<p>Following: <a href=\"/lw/2x8/lets_split_the_cake_lengthwise_upwise_and/\">Let's split the Cake</a></p>\n<p>tl;dr: Both the Nash Bargaining solution (NBS), and the Kalai-Smorodinsky Bargaining Solution (KSBS), though acceptable for one-off games that are fully known in advance, are strictly inferior for independent repeated games, or when there exists uncertainty as to which game will be played.</p>\n<p>Let play a bargaining game, you and I. We can end up with you getting &euro;1 and me getting &euro;3, both of us getting &euro;2, or you getting &euro;3 and me getting &euro;1. If we fail to agree, neither of us gets anything.</p>\n<p>Oh, and did I forget to mention that another option was for you to get an <a href=\"http://media.giantbomb.com/uploads/2/25701/1288625-sdf_1_macross_sized_super.jpg\">aircraft carrier</a> and me to get nothing?</p>\n<p>Think of that shiny new aircraft carrier, loaded full with jets, pilots, weapons and sailors; think of all the things you could do with it, all the fun you could have. Places to bomb or city harbours to cruise majestically into, with the locals gaping in awe at the sleek powerful lines of your very own ship.</p>\n<p>Then forget all about it, because Kalai-Smorodinsky says you can't have it. The Kalai-Smorodinsky bargaining solution to this game is 1/2 of a chance of getting that ship for you, and 1/2 of a chance of getting &euro;3 for me (the Nash Bargaining Solution is better, but still not the best, as we'll see later). This might be fair; after all, unless you have some way of remunerating me for letting you have it, why should I take a dive for you?</p>\n<p>But now imagine we are about to start the game, and we don't know the full rules yet. We know about the &euro;'s involved, that's all fine, we know there will be an offer of an aircraft carrier; but we don't know who is going to get the offer. If we wanted to decide on our bargaining theory in advance, what would we do?<a id=\"more\"></a></p>\n<p>Obviously not use the KSBS; it gives 1/4 of an aircraft carrier to each player. One excellent solution is simple: whoever has the option of the aircraft carrier... gets it. This gives us both an expected gain of 1/2 aircraft carrier before the game begins, superior to the other approaches.</p>\n<p>Let formalise this uncertainty over possible games. A great game (GG) is a situation where we are about to play one of games g<sub>j</sub>, each with probability p<sub>j</sub>. My utility function is U<sub>1</sub>, yours is U<sub>2</sub>. Once we choose a bargaining equilibrium which results in outcomes o<sub>j</sub> with utilities (a<sub>j</sub>,b<sub>j</sub>) for each game, then our expected utility gain is (&Sigma;p<sub>j</sub> a<sub>j</sub>, &Sigma;p<sub>j</sub> b<sub>j</sub>)=&Sigma;p<sub>j</sub>(a<sub>j</sub>, b<sub>j</sub>) Since the outcome utilities for each games are individually convex, the set S of possible outcome (expected) utilities for the GG are also convex.</p>\n<p>Now, we both want a bargaining equilibrium that will be Pareto optimal for GG. What should we do? Well, first, define:</p>\n<ul>\n<li>A &mu;-sum maximising bargaining solution (&mu;SMBS) is one that involves maximising the expectation of (U<sub>1</sub>+&micro;U<sub>2</sub>) in each game, for a positive &mu;.</li>\n</ul>\n<p>We'll extend the definition to &mu;=&infin; by setting that to be the bargaining solution that involves maximising U<sub>2</sub>. Now this definition isn't complete; there are situations where maximising (U<sub>1</sub>+&micro;U<sub>2</sub>) doesn't give a single solution (such as those where &micro;=1, we have to split &euro;10 between us, and we both have utilities where 1 utiliton=1&euro;). These situations are rare, but we'll assume here that any &mu;SMBS comes complete with a tie-breaker method for selecting unique solutions in these cases.</p>\n<p>The first (major) result is:</p>\n<ul>\n<li>For any positive &micro;, &mu;SMBS is Pareto-optimal for any GG.</li>\n</ul>\n<p>To prove this, let o<sub>j</sub> be the outcomes for game g<sub>j</sub>, using &mu;SMB, with expected utilities (a<sub>j</sub>,b<sub>j</sub>). The GG has expected utilities (a,b) =&sum;p<sub>j</sub> (a<sub>j</sub>,b<sub>j</sub>). Let f&micro; be the function that maps (x,y) to x+&micro;y. The &mu;SMBS is equivalent with maximising the value of f&micro; for each game.</p>\n<p>So now let q<sub>j</sub> be another possible outcome set, and expected utility (c,d), assume to be strictly superior, for both players, to (a,b). Now, because &micro; is positive, (c,d) &gt; (a,b) implies c&gt;a and &micro;d&gt;&micro;b, so implies f&micro;(c,d) &gt; f&micro;(a,b). However, by the definition of &mu;SMBS, we must have f&micro;(a<sub>j</sub>,b<sub>j</sub>) &ge; f&micro;(c<sub>j</sub>,d<sub>j</sub>). Since f&micro; is linear, f&micro;(a,b)=&sum;p<sub>j</sub> f&micro;(a<sub>j</sub>,b<sub>j</sub>) &ge; &sum;p<sub>j</sub> f&micro;(c<sub>j</sub>,d<sub>j</sub>) = f&micro;(c,d). This contradicts the assumption that (c,d) &gt; (a,b), and hence proves that (a,b) is Pareto-optimal.</p>\n<p>This strong result has a converse, namely:</p>\n<ul>\n<li>For any bargaining solution that is Pareto-optimal for a given GG, there is a &mu;SMBS that produces the same outcome, in each game g<sub>j</sub>.</li>\n</ul>\n<p>Let o<sub>j</sub> be the outcomes for a given Pareto-optimal bargaining solution, with expected utilities (a<sub>j</sub>,b<sub>j</sub>), and GG having expected utilities (a,b) =&sum;p<sub>j</sub> (a<sub>j</sub>,b<sub>j</sub>). The set S of possible expected utilities for GG is convex, and since (a,b) is Pareto-optimal, it must lie on the boundary. Hence there exists a line L through (a,b) such that S lies entirely to the left of this line. Let -&mu; be the slope of L. Thus, there does not exist any (c,d) in the expected utilities outcomes with f&micro;(c,d) &gt; f&micro;(a,b).</p>\n<p>Now, if there were to exist an outcome q<sub>k</sub> for the game g<sub>k</sub> with expected utilities (c<sub>k</sub>,d<sub>k</sub>) such that f&micro;(c<sub>k</sub>,d<sub>k</sub>) &gt; f&micro;(a<sub>k</sub>,b<sub>k</sub>), then the expected utility for the outcomes o<sub>j</sub> with o<sub>k</sub> replaced with p<sub>k</sub>, would be (c,d) = (a,b) + q<sub>k</sub> ((c<sub>k</sub>,d<sub>k</sub>) - (a<sub>k</sub>,b<sub>k</sub>)). This has f&micro;(c,d) &gt; f&micro;(a,b), contradicting the previous result. Thus (a<sub>j</sub>,b<sub>j</sub>) always maximise f&micro; in g<sub>j</sub>, and hence this bargaining solution produces the same results as &mu;SMBS (with a given tie-breaking procedure, if needed).</p>\n<p>So the best solution, if you are uncertain what the games are you could be playing, is to fix a common relative measure of value, and then maximise that, ignoring considerations of fairness or any other. To a crude approximation, capitalism is like this: every game is supposed to maximise money as much as it can.</p>\n<p><strong>Multiple games</strong></p>\n<p>For the moment, we've been considering a single game, with uncertainty as to which game is going to be played. The same result goes through, however, if you are expecting to play multiple games in a row. One caveats is needed: the games must be independent of each other.</p>\n<p>The result holds, for instance, in two games with stakes &euro;10, as long as our utilities are linear in these (small) amounts of money. It does not hold if the first game's stake is a left shoe and the second's is a right shoe, for there the utilities of the outcomes are correlated: if I win the left shoe, I'm much more likely to value the right shoe in the subsequent game.</p>\n<p><strong>Invariance</strong></p>\n<p>Maximising summed utility is invariant under translations (which just add a single constant to each utility). It is not, of course, invariant under scalings, and it would be foolish indeed to first decide on &mu; and then allow players to rescale their utilities. In general the &mu; is not a real number, but a linear isomorphism between the two utilities, invariantly defined by some process.</p>\n<p><strong>Kalai-Smorodinsky and Nash's revenge<br /></strong></p>\n<p>So, it seems established. &mu;SMBS is the way to go. KSBS and NBS are loser solutions, and should be discarded. As you'd imagine, it's not quite so simple...</p>\n<p>The problem is, which &micro;? Fixing that &micro; is going to determine your expected utility for any GG, so it's an important value. And each player is going to have a different priority it, trying to minimise the contribution of the other agent's utility. So there will have to be some... bargaining. And that bargaining will be a one-shot bargaining deal, not a repeated one, so there is no superior way of going about it. Use KSBS or NBS or anything like that if you want; or set &micro; to reflect your joint valuing of a common currency ($, &euro; or negentropy); but you can't get around the fact that you're going to have to fix that &micro; somehow. And if you use &mu;'SMBS to do so, you've just shifted the battle to the value of&nbsp;&mu;'...</p>\n<p>&nbsp;</p>\n<p><strong>Edit: the mystery of &micro; (mathy, technical, and not needed to understand the rest of the post)<br /></strong></p>\n<p>There has been some speculation on the list as to the technical meaning of U<sub>1</sub>+&micro;U<sub>2</sub> and &micro;. To put this on an acceptable rigorous footing, let u<sub>1</sub> and u<sub>2</sub> be any two representatives of U<sub>1</sub> and U<sub>2</sub> (in that u<sub>1</sub> and u<sub>2</sub> are what we would normally term as \"utility functions\", and U<sub>1</sub> and U<sub>2</sub> are the set of utility functions that are related to these by affine transformations). Then &micro; is a function from the possible pairs (u1, u2) to the non-negative reals, with the property that it is equivariant under linear transformations of u<sub>1</sub> and inverse-equivariant under linear transformations of u<sub>2</sub> (in human speak: when u<sub>1</sub> gets scaled bigger, so does &micro;, and when u<sub>2</sub> gets scaled bigger, &micro; gets scaled smaller), and invariant under translations. Then we can define U<sub>1</sub>+&micro;U<sub>2</sub> as the set of utility functions for which u<sub>1</sub>+&micro;u<sub>2</sub> is a representative (the properties of &micro; make this well defined, independently of our choices of u<sub>1</sub> and u<sub>2</sub>). Whenever&nbsp;&micro;&ne;0, there is a well defined &micro;<sup>-1</sup>, with the property that&nbsp; &micro;<sup>-1</sup>U<sub>1</sub>+U<sub>2</sub> = U<sub>1</sub>+&micro;U<sub>2</sub>. Then the case &mu;=&infin; is defined to be &mu;<sup>-1</sup>=0.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W2ufY8ihDDWWqJA7h", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 16, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "3791", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Following: <a href=\"/lw/2x8/lets_split_the_cake_lengthwise_upwise_and/\">Let's split the Cake</a></p>\n<p>tl;dr: Both the Nash Bargaining solution (NBS), and the Kalai-Smorodinsky Bargaining Solution (KSBS), though acceptable for one-off games that are fully known in advance, are strictly inferior for independent repeated games, or when there exists uncertainty as to which game will be played.</p>\n<p>Let play a bargaining game, you and I. We can end up with you getting \u20ac1 and me getting \u20ac3, both of us getting \u20ac2, or you getting \u20ac3 and me getting \u20ac1. If we fail to agree, neither of us gets anything.</p>\n<p>Oh, and did I forget to mention that another option was for you to get an <a href=\"http://media.giantbomb.com/uploads/2/25701/1288625-sdf_1_macross_sized_super.jpg\">aircraft carrier</a> and me to get nothing?</p>\n<p>Think of that shiny new aircraft carrier, loaded full with jets, pilots, weapons and sailors; think of all the things you could do with it, all the fun you could have. Places to bomb or city harbours to cruise majestically into, with the locals gaping in awe at the sleek powerful lines of your very own ship.</p>\n<p>Then forget all about it, because Kalai-Smorodinsky says you can't have it. The Kalai-Smorodinsky bargaining solution to this game is 1/2 of a chance of getting that ship for you, and 1/2 of a chance of getting \u20ac3 for me (the Nash Bargaining Solution is better, but still not the best, as we'll see later). This might be fair; after all, unless you have some way of remunerating me for letting you have it, why should I take a dive for you?</p>\n<p>But now imagine we are about to start the game, and we don't know the full rules yet. We know about the \u20ac's involved, that's all fine, we know there will be an offer of an aircraft carrier; but we don't know who is going to get the offer. If we wanted to decide on our bargaining theory in advance, what would we do?<a id=\"more\"></a></p>\n<p>Obviously not use the KSBS; it gives 1/4 of an aircraft carrier to each player. One excellent solution is simple: whoever has the option of the aircraft carrier... gets it. This gives us both an expected gain of 1/2 aircraft carrier before the game begins, superior to the other approaches.</p>\n<p>Let formalise this uncertainty over possible games. A great game (GG) is a situation where we are about to play one of games g<sub>j</sub>, each with probability p<sub>j</sub>. My utility function is U<sub>1</sub>, yours is U<sub>2</sub>. Once we choose a bargaining equilibrium which results in outcomes o<sub>j</sub> with utilities (a<sub>j</sub>,b<sub>j</sub>) for each game, then our expected utility gain is (\u03a3p<sub>j</sub> a<sub>j</sub>, \u03a3p<sub>j</sub> b<sub>j</sub>)=\u03a3p<sub>j</sub>(a<sub>j</sub>, b<sub>j</sub>) Since the outcome utilities for each games are individually convex, the set S of possible outcome (expected) utilities for the GG are also convex.</p>\n<p>Now, we both want a bargaining equilibrium that will be Pareto optimal for GG. What should we do? Well, first, define:</p>\n<ul>\n<li>A \u03bc-sum maximising bargaining solution (\u03bcSMBS) is one that involves maximising the expectation of (U<sub>1</sub>+\u00b5U<sub>2</sub>) in each game, for a positive \u03bc.</li>\n</ul>\n<p>We'll extend the definition to \u03bc=\u221e by setting that to be the bargaining solution that involves maximising U<sub>2</sub>. Now this definition isn't complete; there are situations where maximising (U<sub>1</sub>+\u00b5U<sub>2</sub>) doesn't give a single solution (such as those where \u00b5=1, we have to split \u20ac10 between us, and we both have utilities where 1 utiliton=1\u20ac). These situations are rare, but we'll assume here that any \u03bcSMBS comes complete with a tie-breaker method for selecting unique solutions in these cases.</p>\n<p>The first (major) result is:</p>\n<ul>\n<li>For any positive \u00b5, \u03bcSMBS is Pareto-optimal for any GG.</li>\n</ul>\n<p>To prove this, let o<sub>j</sub> be the outcomes for game g<sub>j</sub>, using \u03bcSMB, with expected utilities (a<sub>j</sub>,b<sub>j</sub>). The GG has expected utilities (a,b) =\u2211p<sub>j</sub> (a<sub>j</sub>,b<sub>j</sub>). Let f\u00b5 be the function that maps (x,y) to x+\u00b5y. The \u03bcSMBS is equivalent with maximising the value of f\u00b5 for each game.</p>\n<p>So now let q<sub>j</sub> be another possible outcome set, and expected utility (c,d), assume to be strictly superior, for both players, to (a,b). Now, because \u00b5 is positive, (c,d) &gt; (a,b) implies c&gt;a and \u00b5d&gt;\u00b5b, so implies f\u00b5(c,d) &gt; f\u00b5(a,b). However, by the definition of \u03bcSMBS, we must have f\u00b5(a<sub>j</sub>,b<sub>j</sub>) \u2265 f\u00b5(c<sub>j</sub>,d<sub>j</sub>). Since f\u00b5 is linear, f\u00b5(a,b)=\u2211p<sub>j</sub> f\u00b5(a<sub>j</sub>,b<sub>j</sub>) \u2265 \u2211p<sub>j</sub> f\u00b5(c<sub>j</sub>,d<sub>j</sub>) = f\u00b5(c,d). This contradicts the assumption that (c,d) &gt; (a,b), and hence proves that (a,b) is Pareto-optimal.</p>\n<p>This strong result has a converse, namely:</p>\n<ul>\n<li>For any bargaining solution that is Pareto-optimal for a given GG, there is a \u03bcSMBS that produces the same outcome, in each game g<sub>j</sub>.</li>\n</ul>\n<p>Let o<sub>j</sub> be the outcomes for a given Pareto-optimal bargaining solution, with expected utilities (a<sub>j</sub>,b<sub>j</sub>), and GG having expected utilities (a,b) =\u2211p<sub>j</sub> (a<sub>j</sub>,b<sub>j</sub>). The set S of possible expected utilities for GG is convex, and since (a,b) is Pareto-optimal, it must lie on the boundary. Hence there exists a line L through (a,b) such that S lies entirely to the left of this line. Let -\u03bc be the slope of L. Thus, there does not exist any (c,d) in the expected utilities outcomes with f\u00b5(c,d) &gt; f\u00b5(a,b).</p>\n<p>Now, if there were to exist an outcome q<sub>k</sub> for the game g<sub>k</sub> with expected utilities (c<sub>k</sub>,d<sub>k</sub>) such that f\u00b5(c<sub>k</sub>,d<sub>k</sub>) &gt; f\u00b5(a<sub>k</sub>,b<sub>k</sub>), then the expected utility for the outcomes o<sub>j</sub> with o<sub>k</sub> replaced with p<sub>k</sub>, would be (c,d) = (a,b) + q<sub>k</sub> ((c<sub>k</sub>,d<sub>k</sub>) - (a<sub>k</sub>,b<sub>k</sub>)). This has f\u00b5(c,d) &gt; f\u00b5(a,b), contradicting the previous result. Thus (a<sub>j</sub>,b<sub>j</sub>) always maximise f\u00b5 in g<sub>j</sub>, and hence this bargaining solution produces the same results as \u03bcSMBS (with a given tie-breaking procedure, if needed).</p>\n<p>So the best solution, if you are uncertain what the games are you could be playing, is to fix a common relative measure of value, and then maximise that, ignoring considerations of fairness or any other. To a crude approximation, capitalism is like this: every game is supposed to maximise money as much as it can.</p>\n<p><strong id=\"Multiple_games\">Multiple games</strong></p>\n<p>For the moment, we've been considering a single game, with uncertainty as to which game is going to be played. The same result goes through, however, if you are expecting to play multiple games in a row. One caveats is needed: the games must be independent of each other.</p>\n<p>The result holds, for instance, in two games with stakes \u20ac10, as long as our utilities are linear in these (small) amounts of money. It does not hold if the first game's stake is a left shoe and the second's is a right shoe, for there the utilities of the outcomes are correlated: if I win the left shoe, I'm much more likely to value the right shoe in the subsequent game.</p>\n<p><strong id=\"Invariance\">Invariance</strong></p>\n<p>Maximising summed utility is invariant under translations (which just add a single constant to each utility). It is not, of course, invariant under scalings, and it would be foolish indeed to first decide on \u03bc and then allow players to rescale their utilities. In general the \u03bc is not a real number, but a linear isomorphism between the two utilities, invariantly defined by some process.</p>\n<p><strong id=\"Kalai_Smorodinsky_and_Nash_s_revenge\">Kalai-Smorodinsky and Nash's revenge<br></strong></p>\n<p>So, it seems established. \u03bcSMBS is the way to go. KSBS and NBS are loser solutions, and should be discarded. As you'd imagine, it's not quite so simple...</p>\n<p>The problem is, which \u00b5? Fixing that \u00b5 is going to determine your expected utility for any GG, so it's an important value. And each player is going to have a different priority it, trying to minimise the contribution of the other agent's utility. So there will have to be some... bargaining. And that bargaining will be a one-shot bargaining deal, not a repeated one, so there is no superior way of going about it. Use KSBS or NBS or anything like that if you want; or set \u00b5 to reflect your joint valuing of a common currency ($, \u20ac or negentropy); but you can't get around the fact that you're going to have to fix that \u00b5 somehow. And if you use \u03bc'SMBS to do so, you've just shifted the battle to the value of&nbsp;\u03bc'...</p>\n<p>&nbsp;</p>\n<p><strong id=\"Edit__the_mystery_of____mathy__technical__and_not_needed_to_understand_the_rest_of_the_post_\">Edit: the mystery of \u00b5 (mathy, technical, and not needed to understand the rest of the post)<br></strong></p>\n<p>There has been some speculation on the list as to the technical meaning of U<sub>1</sub>+\u00b5U<sub>2</sub> and \u00b5. To put this on an acceptable rigorous footing, let u<sub>1</sub> and u<sub>2</sub> be any two representatives of U<sub>1</sub> and U<sub>2</sub> (in that u<sub>1</sub> and u<sub>2</sub> are what we would normally term as \"utility functions\", and U<sub>1</sub> and U<sub>2</sub> are the set of utility functions that are related to these by affine transformations). Then \u00b5 is a function from the possible pairs (u1, u2) to the non-negative reals, with the property that it is equivariant under linear transformations of u<sub>1</sub> and inverse-equivariant under linear transformations of u<sub>2</sub> (in human speak: when u<sub>1</sub> gets scaled bigger, so does \u00b5, and when u<sub>2</sub> gets scaled bigger, \u00b5 gets scaled smaller), and invariant under translations. Then we can define U<sub>1</sub>+\u00b5U<sub>2</sub> as the set of utility functions for which u<sub>1</sub>+\u00b5u<sub>2</sub> is a representative (the properties of \u00b5 make this well defined, independently of our choices of u<sub>1</sub> and u<sub>2</sub>). Whenever&nbsp;\u00b5\u22600, there is a well defined \u00b5<sup>-1</sup>, with the property that&nbsp; \u00b5<sup>-1</sup>U<sub>1</sub>+U<sub>2</sub> = U<sub>1</sub>+\u00b5U<sub>2</sub>. Then the case \u03bc=\u221e is defined to be \u03bc<sup>-1</sup>=0.</p>", "sections": [{"title": "Multiple games", "anchor": "Multiple_games", "level": 1}, {"title": "Invariance", "anchor": "Invariance", "level": 1}, {"title": "Kalai-Smorodinsky and Nash's revenge", "anchor": "Kalai_Smorodinsky_and_Nash_s_revenge", "level": 1}, {"title": "Edit: the mystery of \u00b5 (mathy, technical, and not needed to understand the rest of the post)", "anchor": "Edit__the_mystery_of____mathy__technical__and_not_needed_to_understand_the_rest_of_the_post_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "26 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hCwFxBai3oNnxrM9v"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-10-26T13:43:57.762Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-26T14:20:11.565Z", "modifiedAt": null, "url": null, "title": "Self-empathy as a source of \"willpower\"", "slug": "self-empathy-as-a-source-of-willpower", "viewCount": null, "lastCommentedAt": "2018-10-13T18:19:41.356Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/22HfpjsydDS2A6JhH/self-empathy-as-a-source-of-willpower", "pageUrlRelative": "/posts/22HfpjsydDS2A6JhH/self-empathy-as-a-source-of-willpower", "linkUrl": "https://www.lesswrong.com/posts/22HfpjsydDS2A6JhH/self-empathy-as-a-source-of-willpower", "postedAtFormatted": "Tuesday, October 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self-empathy%20as%20a%20source%20of%20%22willpower%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf-empathy%20as%20a%20source%20of%20%22willpower%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F22HfpjsydDS2A6JhH%2Fself-empathy-as-a-source-of-willpower%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self-empathy%20as%20a%20source%20of%20%22willpower%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F22HfpjsydDS2A6JhH%2Fself-empathy-as-a-source-of-willpower", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F22HfpjsydDS2A6JhH%2Fself-empathy-as-a-source-of-willpower", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 638, "htmlBody": "<!-- Self-empathy as a \"source of willpower\" -->\n<p>tl:dr; <em><a href=\"http://wiki.lesswrong.com/wiki/Dynamic_inconsistency\">Dynamic consistency</a> is a better term for \"willpower\" because its meaning is robust to <a href=\"/lw/2y2/willpower_not_a_limited_resource/\">changes</a> in how we think constistent behavior actually manages to happen. One can boost consistency by fostering interactions between mutually inconsistent sub-agents to help them better empathize with each other.</em></p>\n<p>Despite the common use of the term, I don't think of my \"willpower\" as an <a href=\"/lw/2y2/willpower_not_a_limited_resource/\">expendable resource</a>, and mostly it just <a href=\"/lw/2y2/willpower_not_a_limited_resource/2ubk?c=1\">doesn't feel like one</a>. Let's imagine Bob, who is somewhat overweight, likes to eat cake, and wants to lose weight to be more generically attractive and healthy. Bob often plans not to eat cake, but changes his mind, and then regrets it, and then decides he should indulge himself sometimes, and then decides that's just an excuse-meme, etc. Economists and veteran LessWrong readers know this oscillation between value systems is called <a href=\"http://en.wikipedia.org/wiki/Dynamic_inconsistency\">dynamic inconsistency</a> (q.v. <a href=\"http://en.wikipedia.org/wiki/Dynamic_inconsistency\">Wikipedia</a>). We can think of Bob as oscillating between being two different idealized agents living in the same body: a WorthIt agent, and a NotWorthIt agent.</p>\n<p>The feeling of NotWorthIt-Bob's (in)ability to control WorthIt-Bob is likely to be called \"(lack of) willpower\", at least by NotWorthIt-Bob, and maybe even by WorthIt-Bob. But I find the framing and langauge of \"willpower\" fairly unhelpful. Instead, I think NotWorthIt-Bob and WorthIt-Bob just aren't communicating well enough. They try to ignore each other's relevance, but if they could both be present at the same time and actually <em>talk</em> about it, like two people in a healthy relationship, maybe they'd figure something out. I'm talking about <em>self-empathy</em> here, which is opposite to self-sympathy: relating to emotions <em>of yours</em> that you are <em>not</em> immediately feeling. Haven't you noticed you're better at convincing people to change their minds when you actually empathize with their position during the conversation? The same applies to convincing yourself.</p>\n<p>Don't ask \"Do I have willpower?\", but \"Am I a dynamically consistent team?\"<a id=\"more\"></a></p>\n<p>The task of keeping your various mental impulses working together coherently is called <a href=\"http://en.wikipedia.org/wiki/Executive_functions\">executive functioning</a>. To deal with an \"always eat cake\" impulse, some may be lucky enough to win by simply reciting \"cake isn't really that tasty anyway\". A more potent technique is to practice visualizing the cake making you instantaneously grotesque and extremely ill, creating a psychological flinch-away reflex &mdash; a behavioral trigger &mdash; which will be activated on the sight of cake and intervene on the usual behavior to eat it. But <strong>such behavioral triggers can easily fail</strong> <em>if</em> they aren't <em>backed up</em> by an agreement between your WorthIt and NotWorthIt sub-agents: if you end up smelling the cake, or trying \"just one bite\" to be \"polite\" at your friend's birthday, it can make you all-of-a-sudden-remember how tasty the cake is, and destroy the trigger.</p>\n<p>To really be prepared, <strong>Bob needs to vaccinate himself</strong> against extenuating circumstances. He needs to admit to himself that cake <em>really is</em> delicious, and decide whether it's worth eating <em>without downplaying how very delicious it is</em>. He needs to sit down with the cake, stare at it, smell it, taste three crumbs of it, and <em>then</em> toss it. (If possible, he should give it away. But note that, despite parentally-entrained guilt about food waste, Bob hurting himself with the cake won't help anyone else help themselves with it: starving person eats cake &gt; no one eats cake &gt; Bob eats cake.)</p>\n<p>This admission corresponds to having a meeting between WorthIt-Bob and NotWorthIt-Bob: having both sets of emotions present and salient simultaneously allows them to reach a balance <em>decisively</em>. Maybe NotWorthIt-Bob will decide that eating <em>exactly one slice</em> of cake-or-equivalent tasty food every two weeks really <em>is</em> worth it, and keep a careful log to ensure this happens. Maybe WorthIt-Bob will <em>approve</em> of the cake-is-poison meditation techniques and <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">actually change his mind</a>. Maybe Bob will become <em>one person</em> who <em>consistently values</em> his health and appearance over spurious taste sensations.</p>\n<p>Or maybe not. But it sure works for me.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YrLoz567b553YouZ2": 4, "5uHdFgR938LGGxMKQ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "22HfpjsydDS2A6JhH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 64, "baseScore": 69, "extendedScore": null, "score": 0.000131, "legacy": true, "legacyId": "3829", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 69, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NPxGwZGoyyrNzkjNw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-26T15:13:06.100Z", "modifiedAt": null, "url": null, "title": "Pathological utilitometer thought experiment", "slug": "pathological-utilitometer-thought-experiment", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:23.109Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Rain", "createdAt": "2009-04-18T20:06:36.555Z", "isAdmin": false, "displayName": "Rain"}, "userId": "o5G9PHsgt46zxDnik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LLWTxFJAuke96u5Ln/pathological-utilitometer-thought-experiment", "pageUrlRelative": "/posts/LLWTxFJAuke96u5Ln/pathological-utilitometer-thought-experiment", "linkUrl": "https://www.lesswrong.com/posts/LLWTxFJAuke96u5Ln/pathological-utilitometer-thought-experiment", "postedAtFormatted": "Tuesday, October 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pathological%20utilitometer%20thought%20experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APathological%20utilitometer%20thought%20experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLLWTxFJAuke96u5Ln%2Fpathological-utilitometer-thought-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pathological%20utilitometer%20thought%20experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLLWTxFJAuke96u5Ln%2Fpathological-utilitometer-thought-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLLWTxFJAuke96u5Ln%2Fpathological-utilitometer-thought-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 462, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">I've been doing thought experiments involving a utilitometer: a device capable of measuring the utility of the universe, including sums-over-time and counterfactuals (what-if extrapolations), for any given utility function, even generic statements such as, \"what I value.\" Things this model ignores: nonutilitarianism, complexity, contradictions, unknowability of true utility functions, inability to simulate and measure counterfactual universes, etc.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Unfortunately, I believe I've run into a pathological mindset from thinking about this utilitometer. Given the abilities of the device, you'd want to input your utility function and then take a sum-over-time from the beginning to the end of the universe and start checking counterfactuals (\"I buy a new car\", \"I donate all my money to nonprofits\", \"I move to California\", etc) to see if the total goes up or down.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">It seems quite obvious that the sum at the end of the universe is the measure that makes the most sense, and I can't see any reason for taking a measure at the end of an action as is done in all typical discussions of utility. Here's an example: \"The expected utility from moving to California is negative due to the high cost of living and the fact that I would not have a job.\" But a sum over all time might show that it was positive utility because I meet someone, or do something, or learn something that improves the rest of my life, and without the utilitometer, I would have missed all of those add-on effects. The device allows me to fill in all of the unknown details and unintended consequences.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Where this thinking becomes a problem is when I realize I have no such device, but desperately want one, so I can incorporate the unknown and the unintended, and know what path I should be taking to maximize my life, rather than having the short, narrow view of the future I do now. In essence, it places higher utility on 'being good at calculating expected utility' than almost any other actions I could take. If I could just build a true utilitometer that measures everything, then the expected utility would be enormous! (\"push button to improve universe\"). And even incremental steps along the way could have amazing payoffs.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Given that a utilitometer as described is impossible, thinking about it has still altered my values to place steps toward creating it above other, seemingly more realistic options (buying a new car, moving to California, etc). I previously asked the question, \"How much time and effort should we put into improving our models and predictions, given we will have to model and predict the answer to this question?\" and acknowledged it was circular and unanswerable. The pathology comes from entering the circle and starting a feedback loop; anything less than perfect prediction means wasting the entire future.</p>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LLWTxFJAuke96u5Ln", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "3830", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-26T16:59:48.669Z", "modifiedAt": null, "url": null, "title": "Teachers vs. Tutors", "slug": "teachers-vs-tutors", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:31.729Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qYmsn6ohsMxvAzB65/teachers-vs-tutors", "pageUrlRelative": "/posts/qYmsn6ohsMxvAzB65/teachers-vs-tutors", "linkUrl": "https://www.lesswrong.com/posts/qYmsn6ohsMxvAzB65/teachers-vs-tutors", "postedAtFormatted": "Tuesday, October 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Teachers%20vs.%20Tutors&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATeachers%20vs.%20Tutors%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqYmsn6ohsMxvAzB65%2Fteachers-vs-tutors%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Teachers%20vs.%20Tutors%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqYmsn6ohsMxvAzB65%2Fteachers-vs-tutors", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqYmsn6ohsMxvAzB65%2Fteachers-vs-tutors", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 316, "htmlBody": "<p>It's an anecdotal commonplace that rich parents in places like Manhattan are willing to shell out a colossal amount for their children's tutors.&nbsp; Most recently I heard about a math PhD student at an elite university who's getting $500 an hour to tutor high school kids.</p>\n<p>Now this makes me wonder: why do <em>tutors</em> get paid so much, but not <em>teachers</em>?&nbsp; Why isn't there a private school somewhere, full of elite superstar teachers, getting paid colossal sums?&nbsp; Why isn't there someone trying to lure young people into being very well-paid schoolteachers instead of professors or hedge fund managers?&nbsp; If some parents have the extra resources to spend on their children's education, why is that money going to tutors rather than schools?</p>\n<p>Some hypotheses that came to mind:</p>\n<p>1.&nbsp; One-on-one tutoring really is the form of learning that gives you the most bang for your buck; the marginal dollar is best spent on getting a better tutor because tutoring is more <em>effective </em>than school.</p>\n<p>2.&nbsp; The marginal dollar is best spent on getting a better tutor because tutors are independent contractors. Switching your kid's school is a big change for the kid, and a discrete jump in price, but getting a new tutor at slightly more cost is easy.</p>\n<p>3.&nbsp; Something about the rules of teacher's unions prevents a few \"superstar teachers\" from being paid colossal sums.</p>\n<p>4.&nbsp; The sort of person who becomes a $500-an-hour tutor usually has a background in something other than education, and isn't credentialed to be a schoolteacher, and perhaps doesn't want to teach full time.&nbsp; You could get him to tutor, if you paid well, but you couldn't get him to be a full-time teacher without an expenditure beyond the means of even the wealthiest parents.</p>\n<p>5.&nbsp; Parents are using tutors to improve their children's <em>grades</em>, not their education.&nbsp; Putting that same money into the kid's school wouldn't improve the kid's grades relative to his classmates'.</p>\n<p>&nbsp;</p>\n<p>Any other ideas?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 2, "fH8jPjHF2R27sRTTG": 2, "gHCNhqxuJq2bZ2akb": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qYmsn6ohsMxvAzB65", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 18, "extendedScore": null, "score": 6.389903463807521e-07, "legacy": true, "legacyId": "3832", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-26T21:01:23.563Z", "modifiedAt": null, "url": null, "title": "Ethics of Jury nullification and TDT?", "slug": "ethics-of-jury-nullification-and-tdt", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:25.606Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psy-Kosh", "createdAt": "2009-03-01T19:34:52.148Z", "isAdmin": false, "displayName": "Psy-Kosh"}, "userId": "CtHmuQzjA7Y7LnSss", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/53YPLc8ehW4ogKsub/ethics-of-jury-nullification-and-tdt", "pageUrlRelative": "/posts/53YPLc8ehW4ogKsub/ethics-of-jury-nullification-and-tdt", "linkUrl": "https://www.lesswrong.com/posts/53YPLc8ehW4ogKsub/ethics-of-jury-nullification-and-tdt", "postedAtFormatted": "Tuesday, October 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ethics%20of%20Jury%20nullification%20and%20TDT%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEthics%20of%20Jury%20nullification%20and%20TDT%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F53YPLc8ehW4ogKsub%2Fethics-of-jury-nullification-and-tdt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ethics%20of%20Jury%20nullification%20and%20TDT%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F53YPLc8ehW4ogKsub%2Fethics-of-jury-nullification-and-tdt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F53YPLc8ehW4ogKsub%2Fethics-of-jury-nullification-and-tdt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 301, "htmlBody": "<p>I've been sort of banging my head on this issue (I have jury duty next week (first time)).</p>\n<p>&nbsp;</p>\n<p>The obvious possibility is what if I get put on a drug use case? The obvious injustices of the anti-drug laws are well known, and I know of the concept of nullification, but I'm bouncing back and forth as to its validity.</p>\n<p>&nbsp;</p>\n<p>Some of my thoughts on this:</p>\n<p>&nbsp;</p>\n<p>Thought 1: Just decide if they did it or didn't do it.</p>\n<p>Thought 2: But can I ethically bring myself to declare guilty (and thus result in potential serious punishment) someone that really didn't actually do anything wrong? ie, to support a seriously unjust law?</p>\n<p>Thought 3: (and here's where TDT style issues come in) On the other hand, the algorithm \"if jury member, don't convict if I don't like a particular law\" seems to be in general a potentially really really bad algorithm. (ie, one obvious failure mode for that algorithm would be homophobic juries that refuse to convict on hate crimes against gays)</p>\n<p>Thought 4: Generally, those sorts of people tend to not be serious rationalists. Reasoning as if I can expect correlations among our decision algorithms seems questionable.</p>\n<p>Thought 5: Really? Really? If I wanted to start making excuses like that, I could probably whenever I feel like construct a reference class for which I am the sole member. Thought 4 style reasoning seems itself to potentially be shaky.</p>\n<p>&nbsp;</p>\n<p>So, basically I'm smart enough to have the above sequence of thoughts, but not smart enough to actually resolve it. What is a rationalist to do? (In other words, any help with untangling my thoughts on this so that I can figure out if I should go by the rule of \"nullify if appropriate\" or \"nullification is bad, period, even if the law in question is hateful\" would be greatly appreciated.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "53YPLc8ehW4ogKsub", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "3834", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-26T21:41:37.807Z", "modifiedAt": null, "url": null, "title": "HELP:  Do I have a chance at becoming intelligent?", "slug": "help-do-i-have-a-chance-at-becoming-intelligent", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:33.200Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "johnbgone", "createdAt": "2010-10-26T21:13:11.958Z", "isAdmin": false, "displayName": "johnbgone"}, "userId": "Q4LFNzHBrJejGpE9T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qjuhXMhZfbRkNdt5c/help-do-i-have-a-chance-at-becoming-intelligent", "pageUrlRelative": "/posts/qjuhXMhZfbRkNdt5c/help-do-i-have-a-chance-at-becoming-intelligent", "linkUrl": "https://www.lesswrong.com/posts/qjuhXMhZfbRkNdt5c/help-do-i-have-a-chance-at-becoming-intelligent", "postedAtFormatted": "Tuesday, October 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20HELP%3A%20%20Do%20I%20have%20a%20chance%20at%20becoming%20intelligent%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHELP%3A%20%20Do%20I%20have%20a%20chance%20at%20becoming%20intelligent%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqjuhXMhZfbRkNdt5c%2Fhelp-do-i-have-a-chance-at-becoming-intelligent%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=HELP%3A%20%20Do%20I%20have%20a%20chance%20at%20becoming%20intelligent%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqjuhXMhZfbRkNdt5c%2Fhelp-do-i-have-a-chance-at-becoming-intelligent", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqjuhXMhZfbRkNdt5c%2Fhelp-do-i-have-a-chance-at-becoming-intelligent", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 239, "htmlBody": "<p>If this post is inappropriate, I apologize.</p>\n<p>I stumbled upon this site after reading  \"Harry Potter and the Methods of Rationality\".&nbsp; The story so far has really moved me on multiple levels and sent me here in a quest to learn more about rationality as a philosophy/way of thinking about the world. I have read Ayn Rands published works and loved the stories and most of the message.&nbsp; The characters always seemed like titans that were far and above me, but now, I've seen a character that is a bit more approachable.&nbsp;</p>\n<p>I've started to go through the \"Map and Territory\" section of the \"Core Sequences\" and this whole project and community makes me ecstatic.&nbsp; I'm currently working my way through the Bayes's Theorem article with some success.&nbsp; The more I read, the more I realize I may have a problem.</p>\n<p>&nbsp;</p>\n<p><em>I'm pretty dumb.</em></p>\n<p>&nbsp;</p>\n<p>Is higher level reasoning \"use it or lose it\" ?&nbsp; I like learning new things and love reading but any new ideas require a ton of thought and re-reading.&nbsp; I think I have enough interest to keep plugging away at it, but I'm not sure I'm going at things the right way.&nbsp; Is there a \"Kid's Table\" for lesswrong.com?</p>\n<p>For \"Priors\":&nbsp; I'm 28 years old, white male, married, no children, poor economic upbringing, solid emotional upbringing, currently lower to middle class, high school diploma, US Navy, currently a civilian electronics technician, raised Baptist currently Agnostic/Atheist (recently).</p>\n<p>I guess that's it.&nbsp; Thanks!</p>\n<p>--John</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 1, "4cKQgA4S7xfNeeWXg": 1, "WqLn4pAWi5hn6McHQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qjuhXMhZfbRkNdt5c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 36, "extendedScore": null, "score": 6.390573799629863e-07, "legacy": true, "legacyId": "3835", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-27T16:34:57.188Z", "modifiedAt": null, "url": null, "title": "Play paranoid debating at home!", "slug": "play-paranoid-debating-at-home", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eugman", "createdAt": "2009-09-28T01:40:39.582Z", "isAdmin": false, "displayName": "eugman"}, "userId": "rtJy8Y9zXRpjWmeMi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/edWMToD7uhBuApBdG/play-paranoid-debating-at-home", "pageUrlRelative": "/posts/edWMToD7uhBuApBdG/play-paranoid-debating-at-home", "linkUrl": "https://www.lesswrong.com/posts/edWMToD7uhBuApBdG/play-paranoid-debating-at-home", "postedAtFormatted": "Wednesday, October 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Play%20paranoid%20debating%20at%20home!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlay%20paranoid%20debating%20at%20home!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FedWMToD7uhBuApBdG%2Fplay-paranoid-debating-at-home%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Play%20paranoid%20debating%20at%20home!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FedWMToD7uhBuApBdG%2Fplay-paranoid-debating-at-home", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FedWMToD7uhBuApBdG%2Fplay-paranoid-debating-at-home", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p>I was reading the wiki article on <a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\">Paranoid debating</a> and I noticed that there was no good source of facts for the game. I suggest anyone interested in it check out a party game called <a href=\"http://boardgamegeek.com/boardgame/20100/wits-wagers\">Wits and Wagers</a>. It's an interesting game where everyone is given a trivia question with a numerical answer. Everyone writes down their guess, then bets on the answers, with the more extreme answers paying out better. It's a cool game and a good source of numerical trivia.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "edWMToD7uhBuApBdG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "3837", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-27T20:26:04.499Z", "modifiedAt": null, "url": null, "title": " What hardcore singularity believers should consider doing", "slug": "what-hardcore-singularity-believers-should-consider-doing", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:27.121Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sRthrsfspce5Hh99R/what-hardcore-singularity-believers-should-consider-doing", "pageUrlRelative": "/posts/sRthrsfspce5Hh99R/what-hardcore-singularity-believers-should-consider-doing", "linkUrl": "https://www.lesswrong.com/posts/sRthrsfspce5Hh99R/what-hardcore-singularity-believers-should-consider-doing", "postedAtFormatted": "Wednesday, October 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%20What%20hardcore%20singularity%20believers%20should%20consider%20doing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%20What%20hardcore%20singularity%20believers%20should%20consider%20doing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsRthrsfspce5Hh99R%2Fwhat-hardcore-singularity-believers-should-consider-doing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%20What%20hardcore%20singularity%20believers%20should%20consider%20doing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsRthrsfspce5Hh99R%2Fwhat-hardcore-singularity-believers-should-consider-doing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsRthrsfspce5Hh99R%2Fwhat-hardcore-singularity-believers-should-consider-doing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>Leading singularity proponent Ray Kurzweil co-authored a book titled <a href=\"http://www.amazon.com/Fantastic-Voyage-Live-Enough-Forever/dp/1579549543\">Fantastic Voyage: Live Long Enough to Live Forever</a>. &nbsp;A singularity believer who thinks that if he makes it to the singularity he has an excelling chance of living forever, or at least for thousands of years, should be willing to sacrifice much for a slightly higher chance of living long enough to make it to the singularity. &nbsp;This is why I think singularity believers make up a vastly disproportionate percentage of members of cryonics organizations.&nbsp;</p>\n<p>&nbsp;</p>\n<p>According to a new scientific article there is a medical procedure that might be able to greatly extend some peoples&rsquo; lives. &nbsp;Although we don&rsquo;t have a huge amount of data one small study showed that several hundred people on average lived 14 years longer than those that didn&rsquo;t get the procedure.</p>\n<p>&nbsp;</p>\n<p>Singularity proponents should be extremely interested in the procedure. &nbsp;Indeed, a way of testing whether members of the SIAI&nbsp;such as Eliezer really and truly believe in the singularity is whether they at least seriously consider having the procedure.</p>\n<p>&nbsp;</p>\n<p>The procedure is discussed at the end of <a href=\"http://www.scientificamerican.com/article.cfm?id=why-women-live-longer&amp;page=2\">this article</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sRthrsfspce5Hh99R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 6, "extendedScore": null, "score": 6.393821034830163e-07, "legacy": true, "legacyId": "3838", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-27T20:52:20.850Z", "modifiedAt": null, "url": null, "title": "The prior probability of justification for war?", "slug": "the-prior-probability-of-justification-for-war", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:23.259Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NealW", "createdAt": "2010-05-28T01:17:30.179Z", "isAdmin": false, "displayName": "NealW"}, "userId": "SxCAWRkzmTvsJJzwv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AwhjHANAHoC4F9KsP/the-prior-probability-of-justification-for-war", "pageUrlRelative": "/posts/AwhjHANAHoC4F9KsP/the-prior-probability-of-justification-for-war", "linkUrl": "https://www.lesswrong.com/posts/AwhjHANAHoC4F9KsP/the-prior-probability-of-justification-for-war", "postedAtFormatted": "Wednesday, October 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20prior%20probability%20of%20justification%20for%20war%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20prior%20probability%20of%20justification%20for%20war%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAwhjHANAHoC4F9KsP%2Fthe-prior-probability-of-justification-for-war%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20prior%20probability%20of%20justification%20for%20war%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAwhjHANAHoC4F9KsP%2Fthe-prior-probability-of-justification-for-war", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAwhjHANAHoC4F9KsP%2Fthe-prior-probability-of-justification-for-war", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<p>Could you use Bayes Theorem to figure out whether or not a given war is just?</p>\n<p>If so, I was wondering how one would go about estimating the prior probability that a war is just.</p>\n<p>Thanks for any help you can offer.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AwhjHANAHoC4F9KsP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -2, "extendedScore": null, "score": 6.393883589185468e-07, "legacy": true, "legacyId": "3839", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-27T20:55:42.410Z", "modifiedAt": null, "url": null, "title": "Morality is as real as the physical world.", "slug": "morality-is-as-real-as-the-physical-world", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:22.963Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "draq", "createdAt": "2010-09-03T18:08:04.472Z", "isAdmin": false, "displayName": "draq"}, "userId": "TMiHfZH3dGtbFTAbb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PZvRBn7ZKsFfKKSAd/morality-is-as-real-as-the-physical-world", "pageUrlRelative": "/posts/PZvRBn7ZKsFfKKSAd/morality-is-as-real-as-the-physical-world", "linkUrl": "https://www.lesswrong.com/posts/PZvRBn7ZKsFfKKSAd/morality-is-as-real-as-the-physical-world", "postedAtFormatted": "Wednesday, October 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Morality%20is%20as%20real%20as%20the%20physical%20world.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMorality%20is%20as%20real%20as%20the%20physical%20world.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPZvRBn7ZKsFfKKSAd%2Fmorality-is-as-real-as-the-physical-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Morality%20is%20as%20real%20as%20the%20physical%20world.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPZvRBn7ZKsFfKKSAd%2Fmorality-is-as-real-as-the-physical-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPZvRBn7ZKsFfKKSAd%2Fmorality-is-as-real-as-the-physical-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 731, "htmlBody": "<p>The following is destilled from the comment section of <a href=\"/r/discussion/lw/2of/levels_of_intelligence/\">an earlier post</a>.</p>\n<h4>Definitions<br /></h4>\n<p>absolute and universal: Something that applies to everything and every mind.</p>\n<p>morality (moral world): A logically consistent system of normative theories.</p>\n<p>reality (natural world): A logically consistent system of scientific (natural) theories.</p>\n<p>normative theory: (Almost) any English sentence in imperative or including the word \"should\", \"must\", \"to be allowed to\" as the verb or equivalent construction, in contrast to descriptive theories.</p>\n<p>mind: A mind is an intelligence that has values, desires and dislikes.</p>\n<p>moral perception: Analogous to the sensory perceptions, a moral perception is the feeling of right and wrong.</p>\n<h4>Assumptions</h4>\n<p>A normative sentence arises as a result of the mind processing its values, desires and dislikes.</p>\n<p>Ideas exist independently from the mind. Numbers don't stop to exist just because HAL dies.</p>\n<h4>Statement<br /></h4>\n<p>In our everyday life, we don't question the reality, due to our sensory perception. We have moral perception as much as we have a sensory perception, therefore why should we question morality?</p>\n<p>If you believe that the natural world is absolute and universal, then there is -- I currently think -- no good reason to doubt the existence of an absolute and universal moral world.</p>\n<h4>A text diagram for illustration</h4>\n<div style=\"font-family:Courier New\"><br /></div>\n<div style=\"font-family:Courier New\">-----------------------------<br /></div>\n<div style=\"font-family:Courier New\">\n<p>|&nbsp;&nbsp;&nbsp; sensory perception&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp; -----------------------&nbsp;&nbsp;&nbsp; ------------</p>\n<p>|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; +&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | -- | scientific theories | -- | reality&nbsp; |</p>\n<p>| intersubjective consensus | &nbsp;&nbsp; -----------------------&nbsp;&nbsp;&nbsp; ------------</p>\n<p>-----------------------------</p>\n<p>&nbsp;</p>\n<p>Analogously,&nbsp;</p>\n<p>-----------------------------</p>\n<p>|&nbsp;&nbsp;&nbsp;&nbsp; moral perception&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; -----------------------&nbsp;&nbsp;&nbsp; ------------</p>\n<p>|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; +&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | -- |&nbsp;&nbsp; moral theories&nbsp;&nbsp;&nbsp; | -- | morality |</p>\n<p>| intersubjective consensus |&nbsp;&nbsp;&nbsp; -----------------------&nbsp;&nbsp;&nbsp; ------------</p>\n<p>-----------------------------</p>\n</div>\n<h4>Absolute moralily<br /></h4>\n<p>The absolute moral world, I am talking about, does encompass everything, including AI and alien intelligence. It does not mean that alien intelligence will behave similarly to us. Different moral problems require different solutions, as much as different objects behave differently according to the same physical theories. Objects in vacuum behave differently than in the atmosphere. Water behaves differently than ice, but they are all governed by the same physics, so I assume.</p>\n<p>An Edo-ero samurai and a Wall Street banker may behave perfectly moral even if they act differently to the same problem due to the social environment. Maybe it is perfectly moral for AIs to kill and annihilate all humans, as much as it is perfectly possible that 218 of Russell's teapots are revolving around Gliese 581 g.</p>\n<h4>The intersubjective consensus<br /></h4>\n<p>There are different sets of theories regarding the natural world: the biblical view, the theories underlying TCM, the theories underlying homeopathy, the theories underlying chiropractise and the scientific view. Many of them contradict each other. The scientific view is well-established because there is an intersubjective consensus on the usefulness of the methodology.</p>\n<p>The methods used in moral discussions are by far not so rigidly defined as in science; it's called civil discourse. The arguments must be logical consistent and the outcomes and conclusions of the normative theory must face the empirical challenge, i.e. if you can derive from your normative theories that it is permissible to kill innocent children without any benefits, then there is probably something wrong.</p>\n<p>Using this method, we have done quite a lot so far. We have established the UN Human Rights Charta, we have an elaborated system of international law, law itself being a manifestation of morality (denying the fact, that law is based on morality is like saying that technology isn't based on science).</p>\n<p>Not everyone might agree and some say, \"I think that chattel slavery is perfectly moral.\" And there are people who think that praying to an almighty pasta monster and dressing up as pirates will cure all the ills of the world. Does that mean that there is no absolute reality? Maybe.</p>\n<h4 class=\"md\">Conclusion<br /></h4>\n<p>As long as we have values, desires, dislikes and make judgements (which all of us do and which maybe is a defining characteristic of the human being beyond the biological basics), if we want to put these values into a logical consistent system, and if we believe that other minds with moral perception exist, then we have an absolute moral world.</p>\n<p>So if we stop having any desires and stop making any judgements, that is if we lack any moral perception, then we may still believe in morality, as much as an agnostic won't deny the existence of God, but it would be totally irrelevant to us.</p>\n<p>To the same degree, if someone lacks all the sensory perception, then the natural world becomes totally irrelevant to him or her.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PZvRBn7ZKsFfKKSAd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -13, "extendedScore": null, "score": 6.393891587746937e-07, "legacy": true, "legacyId": "3840", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The following is destilled from the comment section of <a href=\"/r/discussion/lw/2of/levels_of_intelligence/\">an earlier post</a>.</p>\n<h4 id=\"Definitions\">Definitions<br></h4>\n<p>absolute and universal: Something that applies to everything and every mind.</p>\n<p>morality (moral world): A logically consistent system of normative theories.</p>\n<p>reality (natural world): A logically consistent system of scientific (natural) theories.</p>\n<p>normative theory: (Almost) any English sentence in imperative or including the word \"should\", \"must\", \"to be allowed to\" as the verb or equivalent construction, in contrast to descriptive theories.</p>\n<p>mind: A mind is an intelligence that has values, desires and dislikes.</p>\n<p>moral perception: Analogous to the sensory perceptions, a moral perception is the feeling of right and wrong.</p>\n<h4 id=\"Assumptions\">Assumptions</h4>\n<p>A normative sentence arises as a result of the mind processing its values, desires and dislikes.</p>\n<p>Ideas exist independently from the mind. Numbers don't stop to exist just because HAL dies.</p>\n<h4 id=\"Statement\">Statement<br></h4>\n<p>In our everyday life, we don't question the reality, due to our sensory perception. We have moral perception as much as we have a sensory perception, therefore why should we question morality?</p>\n<p>If you believe that the natural world is absolute and universal, then there is -- I currently think -- no good reason to doubt the existence of an absolute and universal moral world.</p>\n<h4 id=\"A_text_diagram_for_illustration\">A text diagram for illustration</h4>\n<div style=\"font-family:Courier New\"><br></div>\n<div style=\"font-family:Courier New\">-----------------------------<br></div>\n<div style=\"font-family:Courier New\">\n<p>|&nbsp;&nbsp;&nbsp; sensory perception&nbsp;&nbsp;&nbsp;&nbsp; | &nbsp;&nbsp; -----------------------&nbsp;&nbsp;&nbsp; ------------</p>\n<p>|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; +&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | -- | scientific theories | -- | reality&nbsp; |</p>\n<p>| intersubjective consensus | &nbsp;&nbsp; -----------------------&nbsp;&nbsp;&nbsp; ------------</p>\n<p>-----------------------------</p>\n<p>&nbsp;</p>\n<p>Analogously,&nbsp;</p>\n<p>-----------------------------</p>\n<p>|&nbsp;&nbsp;&nbsp;&nbsp; moral perception&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; -----------------------&nbsp;&nbsp;&nbsp; ------------</p>\n<p>|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; +&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | -- |&nbsp;&nbsp; moral theories&nbsp;&nbsp;&nbsp; | -- | morality |</p>\n<p>| intersubjective consensus |&nbsp;&nbsp;&nbsp; -----------------------&nbsp;&nbsp;&nbsp; ------------</p>\n<p>-----------------------------</p>\n</div>\n<h4 id=\"Absolute_moralily\">Absolute moralily<br></h4>\n<p>The absolute moral world, I am talking about, does encompass everything, including AI and alien intelligence. It does not mean that alien intelligence will behave similarly to us. Different moral problems require different solutions, as much as different objects behave differently according to the same physical theories. Objects in vacuum behave differently than in the atmosphere. Water behaves differently than ice, but they are all governed by the same physics, so I assume.</p>\n<p>An Edo-ero samurai and a Wall Street banker may behave perfectly moral even if they act differently to the same problem due to the social environment. Maybe it is perfectly moral for AIs to kill and annihilate all humans, as much as it is perfectly possible that 218 of Russell's teapots are revolving around Gliese 581 g.</p>\n<h4 id=\"The_intersubjective_consensus\">The intersubjective consensus<br></h4>\n<p>There are different sets of theories regarding the natural world: the biblical view, the theories underlying TCM, the theories underlying homeopathy, the theories underlying chiropractise and the scientific view. Many of them contradict each other. The scientific view is well-established because there is an intersubjective consensus on the usefulness of the methodology.</p>\n<p>The methods used in moral discussions are by far not so rigidly defined as in science; it's called civil discourse. The arguments must be logical consistent and the outcomes and conclusions of the normative theory must face the empirical challenge, i.e. if you can derive from your normative theories that it is permissible to kill innocent children without any benefits, then there is probably something wrong.</p>\n<p>Using this method, we have done quite a lot so far. We have established the UN Human Rights Charta, we have an elaborated system of international law, law itself being a manifestation of morality (denying the fact, that law is based on morality is like saying that technology isn't based on science).</p>\n<p>Not everyone might agree and some say, \"I think that chattel slavery is perfectly moral.\" And there are people who think that praying to an almighty pasta monster and dressing up as pirates will cure all the ills of the world. Does that mean that there is no absolute reality? Maybe.</p>\n<h4 class=\"md\" id=\"Conclusion\">Conclusion<br></h4>\n<p>As long as we have values, desires, dislikes and make judgements (which all of us do and which maybe is a defining characteristic of the human being beyond the biological basics), if we want to put these values into a logical consistent system, and if we believe that other minds with moral perception exist, then we have an absolute moral world.</p>\n<p>So if we stop having any desires and stop making any judgements, that is if we lack any moral perception, then we may still believe in morality, as much as an agnostic won't deny the existence of God, but it would be totally irrelevant to us.</p>\n<p>To the same degree, if someone lacks all the sensory perception, then the natural world becomes totally irrelevant to him or her.</p>", "sections": [{"title": "Definitions", "anchor": "Definitions", "level": 1}, {"title": "Assumptions", "anchor": "Assumptions", "level": 1}, {"title": "Statement", "anchor": "Statement", "level": 1}, {"title": "A text diagram for illustration", "anchor": "A_text_diagram_for_illustration", "level": 1}, {"title": "Absolute moralily", "anchor": "Absolute_moralily", "level": 1}, {"title": "The intersubjective consensus", "anchor": "The_intersubjective_consensus", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gjyDBd3Werweuhi5w"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-28T13:56:01.396Z", "modifiedAt": null, "url": null, "title": "Art and Rationality", "slug": "art-and-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:23.095Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "soundchaser", "createdAt": "2010-10-28T13:44:11.926Z", "isAdmin": false, "displayName": "soundchaser"}, "userId": "KvDayWE4gJta2ysFY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fknjGgGBzZQRevE6A/art-and-rationality", "pageUrlRelative": "/posts/fknjGgGBzZQRevE6A/art-and-rationality", "linkUrl": "https://www.lesswrong.com/posts/fknjGgGBzZQRevE6A/art-and-rationality", "postedAtFormatted": "Thursday, October 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Art%20and%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArt%20and%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfknjGgGBzZQRevE6A%2Fart-and-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Art%20and%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfknjGgGBzZQRevE6A%2Fart-and-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfknjGgGBzZQRevE6A%2Fart-and-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<p>What are your thoughts on the role of Art in rationality (personal or otherwise) and in the singularity?</p>\n<p>If one wants to help in the efforts of SIAI (or other organizations) does it make sense to focus on an art form as more than a hobby?</p>\n<p>Is it rational to pursue an art form that encourages people to contribute to a cause when there are more direct ways of contributing?</p>\n<p>It seems difficult to receive much recognition for one's work in art related fields, but it also seems as though one big success (say, a musician whose music was primarily about the singularity and increasing rationality) would turn many people on to the ideas.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fknjGgGBzZQRevE6A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 6.396321789167162e-07, "legacy": true, "legacyId": "3845", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-28T16:27:15.436Z", "modifiedAt": null, "url": null, "title": "I'll be in NYC from Oct. 30 to Nov. 21", "slug": "i-ll-be-in-nyc-from-oct-30-to-nov-21", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:23.399Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SilasBarta", "createdAt": "2009-03-01T00:03:34.864Z", "isAdmin": false, "displayName": "SilasBarta"}, "userId": "zDPSZfarhLM7Gehug", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jCzAeQ7MmmgTb98qG/i-ll-be-in-nyc-from-oct-30-to-nov-21", "pageUrlRelative": "/posts/jCzAeQ7MmmgTb98qG/i-ll-be-in-nyc-from-oct-30-to-nov-21", "linkUrl": "https://www.lesswrong.com/posts/jCzAeQ7MmmgTb98qG/i-ll-be-in-nyc-from-oct-30-to-nov-21", "postedAtFormatted": "Thursday, October 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I'll%20be%20in%20NYC%20from%20Oct.%2030%20to%20Nov.%2021&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI'll%20be%20in%20NYC%20from%20Oct.%2030%20to%20Nov.%2021%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjCzAeQ7MmmgTb98qG%2Fi-ll-be-in-nyc-from-oct-30-to-nov-21%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I'll%20be%20in%20NYC%20from%20Oct.%2030%20to%20Nov.%2021%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjCzAeQ7MmmgTb98qG%2Fi-ll-be-in-nyc-from-oct-30-to-nov-21", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjCzAeQ7MmmgTb98qG%2Fi-ll-be-in-nyc-from-oct-30-to-nov-21", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 127, "htmlBody": "<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%;\"><span style=\"font-size: 10pt; line-height: 150%;\"><span style=\"font-family: Arial;\">Sorry for the self-centered post, but I don&rsquo;t get many chances to be where there are a lot of rationalists.<span style=\"mso-spacerun: yes;\">&nbsp; </span>(We&rsquo;ve counted about four in all of Texas that go to this site.)</span></span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%;\"><span style=\"font-size: 10pt; line-height: 150%;\"><span style=\"font-family: Arial;\">&nbsp;</span></span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%;\"><span style=\"font-size: 10pt; line-height: 150%;\"><span style=\"font-family: Arial;\">Thanks to the Cosmos&rsquo;s noticing my need for a place to spend my vacation time this year, I will be staying in his NYC apartment while he&rsquo;s gone.<span style=\"mso-spacerun: yes;\">&nbsp; </span>I&rsquo;ll definitely be at the NYC meetups.</span></span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%;\"><span style=\"font-size: 10pt; line-height: 150%;\"><span style=\"font-family: Arial;\">&nbsp;</span></span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%;\"><span style=\"font-size: 10pt; line-height: 150%;\"><span style=\"font-family: Arial;\">So, if you are anywhere near this area and were interested in meeting me, let me know (either on this thread or privately) and we can work something out.</span></span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%;\"><span style=\"font-size: 10pt; line-height: 150%;\"><span style=\"font-family: Arial;\">&nbsp;</span></span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; line-height: 150%;\"><span style=\"font-size: 10pt; line-height: 150%;\"><span style=\"font-family: Arial;\">I&rsquo;ve informed the NYC OB Google group, but figured there would be good opportunities to meet some of you that aren&rsquo;t on that list or are a bit further away from the city.</span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jCzAeQ7MmmgTb98qG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 6.396682139058982e-07, "legacy": true, "legacyId": "3846", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-28T18:53:11.821Z", "modifiedAt": null, "url": null, "title": "V is for Value Maximizing Agent: London, November 5", "slug": "v-is-for-value-maximizing-agent-london-november-5", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:27.504Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iqQJiKcephtMgzJgN/v-is-for-value-maximizing-agent-london-november-5", "pageUrlRelative": "/posts/iqQJiKcephtMgzJgN/v-is-for-value-maximizing-agent-london-november-5", "linkUrl": "https://www.lesswrong.com/posts/iqQJiKcephtMgzJgN/v-is-for-value-maximizing-agent-london-november-5", "postedAtFormatted": "Thursday, October 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20V%20is%20for%20Value%20Maximizing%20Agent%3A%20London%2C%20November%205&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AV%20is%20for%20Value%20Maximizing%20Agent%3A%20London%2C%20November%205%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiqQJiKcephtMgzJgN%2Fv-is-for-value-maximizing-agent-london-november-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=V%20is%20for%20Value%20Maximizing%20Agent%3A%20London%2C%20November%205%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiqQJiKcephtMgzJgN%2Fv-is-for-value-maximizing-agent-london-november-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiqQJiKcephtMgzJgN%2Fv-is-for-value-maximizing-agent-london-november-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 112, "htmlBody": "<p>During the last London meetup, which I conveniently scheduled during Easter, I promised that I'd see if the next time, I could make it to London sometime that wasn't a national holiday.</p>\n<p>The time has come to break that promise, so I will be in London for a day on Friday November 5th. If anyone wants to meet up, I'll be around that evening at 8 or so to discuss rationality-related issues, chat, or orchestrate a terrorist campaign to overthrow the government while wearing nifty masks. We can try the top floor of that same Waterstone's in Piccadilly Circus, and relocate to Starbucks if it doesn't work out. Does that work for anybody?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iqQJiKcephtMgzJgN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 6.397029908230749e-07, "legacy": true, "legacyId": "3848", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-28T21:38:26.737Z", "modifiedAt": null, "url": null, "title": "Call for Volunteers: Rationalists with Non-Traditional Skills", "slug": "call-for-volunteers-rationalists-with-non-traditional-skills", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:39.967Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jasen", "createdAt": "2009-06-11T15:05:07.288Z", "isAdmin": false, "displayName": "Jasen"}, "userId": "hMDxPMjrPyw8vGzMa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QcMbGJbqr8HFGa36y/call-for-volunteers-rationalists-with-non-traditional-skills", "pageUrlRelative": "/posts/QcMbGJbqr8HFGa36y/call-for-volunteers-rationalists-with-non-traditional-skills", "linkUrl": "https://www.lesswrong.com/posts/QcMbGJbqr8HFGa36y/call-for-volunteers-rationalists-with-non-traditional-skills", "postedAtFormatted": "Thursday, October 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Call%20for%20Volunteers%3A%20Rationalists%20with%20Non-Traditional%20Skills&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACall%20for%20Volunteers%3A%20Rationalists%20with%20Non-Traditional%20Skills%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQcMbGJbqr8HFGa36y%2Fcall-for-volunteers-rationalists-with-non-traditional-skills%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Call%20for%20Volunteers%3A%20Rationalists%20with%20Non-Traditional%20Skills%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQcMbGJbqr8HFGa36y%2Fcall-for-volunteers-rationalists-with-non-traditional-skills", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQcMbGJbqr8HFGa36y%2Fcall-for-volunteers-rationalists-with-non-traditional-skills", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 458, "htmlBody": "<p>SIAI's Fellows Program is looking for rationalists with skills. &nbsp;More specifically, we're looking for rationalists with skills outside our usual cluster who are interested in donating their time by teaching those skills and communicating the mindsets that lead to their development. &nbsp;If possible, we'd like to learn from specialists who \"speak our language,\" or at least are practiced in resolving confusion and disagreement using reason and evidence. &nbsp;Broadly, we're interested in developing practical intuitions, doing practical things, and developing awareness and culture around detail-intensive technical subskills of emotional self-awareness and social fluency. &nbsp;More specifically: &nbsp; &nbsp;</p>\n<p><a id=\"more\"></a></p>\n<p>We're interested learning how to \"make stuff\" in order to force ourselves down to object level from time to time, practice executing plans in the real world and to refine our intuitions about material phenomena. &nbsp;Examples:</p>\n<ul>\n<li>Home Construction/repair contractor (We'll need to fix up the house we're trying to buy in Berkeley)</li>\n<li>Auto-repair&nbsp;</li>\n</ul>\n<p>We're interested in learning skills that require one to pay very close attention to the detail of the physical world - how things actually are instead of what our mental representation says about them. &nbsp;Examples:</p>\n<ul>\n<li>Technical Drawing&nbsp;</li>\n<li>Stage Magic</li>\n</ul>\n<p>We'd like to get better at working with people, both inside the institute and outside. &nbsp;Examples:</p>\n<ul>\n<li>Journalist</li>\n<li>Science/engineering project manager</li>\n</ul>\n<p>We want to become more aware and in control of our emotions. &nbsp;Emotional self-awareness seems very important for productivity, social success and understanding our tendencies toward motivated cognition. &nbsp;Aside from things that traditionally train emotional awareness like acting and meditation, we expect that certain formal systems of kinesthetic practice such as Iyengar yoga will also help because of the close association between emotional states and patterns of muscle tension. &nbsp;Examples:</p>\n<ul>\n<li>Personal trainer</li>\n<li>Dance instructor</li>\n<li>Acting instructor (improv in particular)</li>\n<li>Martial art instructor</li>\n<li>Specialist in some developed, highly technical system of kinesthetic practice (e.g. Alexander technique, Feldenkrais, Iyengar yoga or kundalini yoga)</li>\n</ul>\n<p>We obviously don't have the time for everyone to learn all of these skills right now, but we would like to get the ball rolling on what's available. &nbsp;Whether you're interested in joining the fellows program, visiting regularly or even just video conferencing occasionally, if you think you can teach one of these skills or something else that seems to align well with the faculties we're interested in training, please send me an email at jasen@intelligence.org.</p>\n<p>Even if you don't live in the Bay Area, I encourage you to post your skills here anyway, in case one of the regular Less Wrong meetup groups are interested. &nbsp;There are regular meetups in New York, Boston, and Los Angeles. &nbsp;As always, if there aren't already meetups in your area I encourage you to start one.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QcMbGJbqr8HFGa36y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 31, "extendedScore": null, "score": 6.39742373005259e-07, "legacy": true, "legacyId": "3849", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-28T23:16:25.876Z", "modifiedAt": null, "url": null, "title": "Understanding the Evidence for Killer Supplements", "slug": "understanding-the-evidence-for-killer-supplements", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Simpson", "createdAt": "2009-03-05T21:06:45.432Z", "isAdmin": false, "displayName": "Matt_Simpson"}, "userId": "v4krJe8Qa4jnhPTmd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sQRD9BpsMeotv2thR/understanding-the-evidence-for-killer-supplements", "pageUrlRelative": "/posts/sQRD9BpsMeotv2thR/understanding-the-evidence-for-killer-supplements", "linkUrl": "https://www.lesswrong.com/posts/sQRD9BpsMeotv2thR/understanding-the-evidence-for-killer-supplements", "postedAtFormatted": "Thursday, October 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Understanding%20the%20Evidence%20for%20Killer%20Supplements&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnderstanding%20the%20Evidence%20for%20Killer%20Supplements%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsQRD9BpsMeotv2thR%2Funderstanding-the-evidence-for-killer-supplements%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Understanding%20the%20Evidence%20for%20Killer%20Supplements%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsQRD9BpsMeotv2thR%2Funderstanding-the-evidence-for-killer-supplements", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsQRD9BpsMeotv2thR%2Funderstanding-the-evidence-for-killer-supplements", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 823, "htmlBody": "<p>&nbsp;</p>\n<p><strong>Related to: </strong><a href=\"/lw/20i/even_if_you_have_a_nail_not_all_hammers_are_the/\">Even if You Have a Nail</a> &nbsp;<a href=\"http://www.overcomingbias.com/2010/10/supplements-kill.html\">Supplements Kill</a></p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; \">So what test, exactly, did the authors perform?&nbsp; And what do the results mean?&nbsp; It remains a mystery to me - and, I'm willing to bet, to every other reader of the paper.</span></p>\n</blockquote>\n<p>I'm willing to take that bet. &nbsp;In March, PhilGoetz <a href=\"/lw/20i/even_if_you_have_a_nail_not_all_hammers_are_the/\">criticized</a> a JAMA article that purported to show evidence that taking vitamins increases mortality as an example of how it's easy to misuse statistics. &nbsp;His conclusion is above.</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Recently, Robin Hanson <a href=\"http://www.overcomingbias.com/2010/10/supplements-kill.html\">commented</a> on the article and took it seriously, stating that he was now going to avoid multivitamins.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Read Phil's and Robin's posts for first for background; I'm going to explain what, exactly, the authors did in their analysis.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">The first thing to understand about the JAMA article is that it was a meta-analysis based of previous relative risk studies. &nbsp;A relative risk study attempts to determine which of two groups is more at risk for death. &nbsp;In this case, the groups are subjects who take a certain vitamin (the treatment group) and subjects who don't (the control group). Significantly, subjects in the treatment group each receive the same dosage of the vitamin. &nbsp;After a fixed amount of time (say 3 years) the number of living and dead members of each group are recorded. &nbsp;<a href=\"http://en.wikipedia.org/wiki/Logistic_regression\">Logistic regression</a>&nbsp;is then performed in order to estimate the probability of someone from either group dieing. &nbsp;Once these two probabilities are know, the relative risk can be estimated as RR=P(death in trt group)/P(death in control group). &nbsp;An RR significantly greater than 1 indicates that the treatment is associated with higher mortality, while an RR significantly less than 1 indicates that the treatment is associated with lower mortality. &nbsp;</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Enter meta-analysis. &nbsp;In a meta-analysis the data isn't data from actual experiments, but rather the estimated treatment effect of previous experiments. In this case, the estimated treatment effect is the estimated relative risk from previous experiments. &nbsp;A fairly simple way to do a meta-analysis is a random effects model. &nbsp;Under this model we assume that the estimated relative risk for each treatment came from a normal distribution with it's own mean and a common variance, and then each of these means came from another normal distribution with a common mean and variance. &nbsp;In other words, for study i=1,...,k:</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\"><img src=\"http://www.codecogs.com/png.latex?RR_i\\sim%20N(\\theta_i,\\sigma^2),\\%20\\theta_i\\sim%20N(\\mu,\\tau^2)\" alt=\"\" width=\"246\" height=\"21\" /></span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Then our estimate of&nbsp;&micro; is the estimated treatment effect, i.e., the estimated relative risk of taking the supplement. &nbsp;If we think that different studies result in different treatment effects because of certain covariates, e.g. chance of bias, location, author, etc., we can complicate the model a bit by giving each study it's own mean in a regression, i.e. by doing meta-regression. &nbsp;For example with one covariate, it would look like this for study i=1,...,k:</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><img src=\"http://www.codecogs.com/png.latex?RR_i\\sim%20N(\\theta_i,\\sigma^2),\\%20\\theta_i\\sim%20N(\\mu_i,\\tau^2)\" alt=\"\" /></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">where</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><img src=\"http://www.codecogs.com/png.latex?\\mu_i=x_i\\beta\" alt=\"\" /></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">It appears that the JAMA article authors used both of these methods to analyze the previous vitamin studies. In footnote 25 the authors reference a paper by DerSimonian and Nan Laird called <em>Meta-Analysis in Clinical Trials </em>that describes the basic meta-analysis approach I talk about above, and other references that they don't cite describe meta-regression in exactly the same way I do here.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">(The next section depends on a faulty assumption.&nbsp; See edit below.)<br /></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">Assuming that the JAMA authors did use this method, notice something very important about how they handled the previous studies. &nbsp;It is not the case that every study they analyzed used the same amount of any given vitamin for their treatment. &nbsp;In fact, for Vitamin C the values range from 80mg to 2000mg across the studies Bjelakovic et al use. But they put all of these treatment effects together as if they are all the same treatment. &nbsp;The result is that their model assumes that the effect on mortality from, say, 80mg of vitamin C and 2000mg of vitamin C is <strong><em>exactly the same</em><span style=\"font-weight: normal;\">. &nbsp;Phil couldn't figure out what happens to relative risk as the dosage amount changes in the model because <em><span style=\"font-weight: normal;\">nothing happens</span></em>. &nbsp;If you have a positive dosage amount, you get the same change in risk no matter what the dose is.</span></strong></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><strong><span style=\"font-weight: normal;\">Now this is a fine for an approximation if the range of dosage amounts is small. &nbsp;Then you can safely conclude for <em><span style=\"font-weight: normal;\">dosages of roughly the amount in these studies</span>&nbsp;</em>taking supplemental vitamins increases mortality. &nbsp;If the range is large, I'm not sure you can learn anything useful from this study. &nbsp;I don't know whether or not the ranges are large or small. &nbsp;I know little about vitamins, so I'll let others be the judge of that.</span></strong></span></p>\n<p><strong>EDIT:</strong></p>\n<p>From the JAMA paper:</p>\n<blockquote>\n<p>The included covariates were bias risk, type and dose of supplement, single or combined supplement regimen, duration of supplementation, and primary or secondary prevention.</p>\n</blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">So they apparently did use dosage as a covariate rather than merely dosage type.&nbsp; In that case, I think Phil's original criticism still applies, and if anyone can find the data, it shouldn't be too difficult to fit the same model but with a higher order term for dosage to see if the results change.<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sQRD9BpsMeotv2thR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "3850", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pWi5WmvDcN4Hn7Bo6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-28T23:57:30.200Z", "modifiedAt": null, "url": null, "title": "Complete Wire Heading as Suicide and other things", "slug": "complete-wire-heading-as-suicide-and-other-things", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:23.239Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "h-H", "createdAt": "2010-01-20T16:15:08.891Z", "isAdmin": false, "displayName": "h-H"}, "userId": "gtYfE7wvJFDrTYutu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GEDpyWBSQ6vfJMRfH/complete-wire-heading-as-suicide-and-other-things", "pageUrlRelative": "/posts/GEDpyWBSQ6vfJMRfH/complete-wire-heading-as-suicide-and-other-things", "linkUrl": "https://www.lesswrong.com/posts/GEDpyWBSQ6vfJMRfH/complete-wire-heading-as-suicide-and-other-things", "postedAtFormatted": "Thursday, October 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Complete%20Wire%20Heading%20as%20Suicide%20and%20other%20things&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComplete%20Wire%20Heading%20as%20Suicide%20and%20other%20things%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGEDpyWBSQ6vfJMRfH%2Fcomplete-wire-heading-as-suicide-and-other-things%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Complete%20Wire%20Heading%20as%20Suicide%20and%20other%20things%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGEDpyWBSQ6vfJMRfH%2Fcomplete-wire-heading-as-suicide-and-other-things", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGEDpyWBSQ6vfJMRfH%2Fcomplete-wire-heading-as-suicide-and-other-things", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<p>I came to the idea after a previous lesswrong topic discussing nihilism, and its several comments on depression and suicide. My argument is that wire heading in its extreme or complete/full form can be easily modeled as suicide, or less strongly as volitional intelligence reduction, at least given current human brain structure and the technology being underdeveloped and hence understood and more likely to lead to such end states.</p>\n<p>I define Full Wire Heading as that which a person would not want to reverse after it 'activates' and which deletes their previous utility function or most of it. a weak definition yes, but it should be enough for the preliminary purposes of this post. A full wire head is extremely constrained, much like an infant for e.g. and although the new utility function could involve a wide range of actions, the activation of a few brain regions would be the main goal, and so they are extremely limited.</p>\n<p>If one takes this position seriously, it follows that only one's moral standpoint on suicide or say lobotomy should govern judgments about full wire heading. This is trivially obvious of course, but to take this position as true we need to understand more about wire heading, as data is extremely lacking especially in regards to human like brains. My other question then is to what extent could such an experiment help in answering the first question?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GEDpyWBSQ6vfJMRfH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 6.397755166843648e-07, "legacy": true, "legacyId": "3851", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-29T00:00:25.408Z", "modifiedAt": null, "url": null, "title": "Making your explicit reasoning trustworthy ", "slug": "making-your-explicit-reasoning-trustworthy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:28.640Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "AnnaSalamon", "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m5AH78nscsGjMbBwv/making-your-explicit-reasoning-trustworthy", "pageUrlRelative": "/posts/m5AH78nscsGjMbBwv/making-your-explicit-reasoning-trustworthy", "linkUrl": "https://www.lesswrong.com/posts/m5AH78nscsGjMbBwv/making-your-explicit-reasoning-trustworthy", "postedAtFormatted": "Friday, October 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Making%20your%20explicit%20reasoning%20trustworthy%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMaking%20your%20explicit%20reasoning%20trustworthy%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm5AH78nscsGjMbBwv%2Fmaking-your-explicit-reasoning-trustworthy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Making%20your%20explicit%20reasoning%20trustworthy%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm5AH78nscsGjMbBwv%2Fmaking-your-explicit-reasoning-trustworthy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm5AH78nscsGjMbBwv%2Fmaking-your-explicit-reasoning-trustworthy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1838, "htmlBody": "<p>Or: \u201cI don\u2019t want to think about <em>that</em>!  I might be left with <em>mistaken beliefs</em>!\u201d</p><p>Related to: <a href=\"/lw/18b/reason_as_memetic_immune_disorder/\">Rationality as memetic immune disorder</a>; <a href=\"/lw/7k/incremental_progress_and_the_valley/\">Incremental progress and the valley</a>; <a href=\"http://wiki.lesswrong.com/wiki/Egan's_law\">Egan&#x27;s Law</a>.</p><p><em>tl;dr: Many of us hesitate to trust explicit reasoning because... we haven\u2019t built the skills that make such reasoning trustworthy.  Some simple strategies can help.</em></p><p>Most of us are afraid to think fully about certain subjects.</p><p>Sometimes, we avert our eyes for fear of unpleasant conclusions.  (\u201cWhat if it\u2019s my fault? What if I\u2019m not good enough?\u201d)</p><p>But other times, oddly enough, we avert our eyes for fear of <em>inaccurate</em> conclusions.[1]  People fear questioning their religion, lest they disbelieve and become damned.  People fear questioning their \u201cdon&#x27;t walk alone at night\u201d safety strategy, lest they venture into danger.  And I find I hesitate when pondering <a href=\"http://www.overcomingbias.com/2008/08/where-does-pasc.html\">Pascal\u2019s wager</a>, <a href=\"http://www.nickbostrom.com/ethics/infinite.pdf\">infinite ethics</a>, the <a href=\"http://www.simulation-argument.com/simulation.html\">Simulation argument</a>, and whether I\u2019m a <a href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">Boltzmann brain</a>... because I\u2019m afraid of losing my bearings, and believing mistaken things.</p><p><a href=\"http://2.bp.blogspot.com/_vWrx43Kdj4U/S8VVCyMj4DI/AAAAAAAAAH0/Tffuc96arP4/s1600/ostrich_head_in_ground_full.jpg\">Ostrich Theory</a>, one might call it.  Or I\u2019m Already Right theory.  The theory that we\u2019re more likely to act sensibly if we <em>don\u2019t</em> think further, than if we <em>do</em>.  Sometimes Ostrich Theories are unconsciously held; one just wordlessly backs away from certain thoughts.  Other times full or partial Ostrich Theories are put forth explicitly, as in Phil Goetz\u2019s <a href=\"/lw/18b/reason_as_memetic_immune_disorder/\">post</a>, <a href=\"/lw/2l6/taking_ideas_seriously/2fgm?c=1\">this LW comment</a>, discussions of Tetlock&#x27;s &quot;<a href=\"http://www.overcomingbias.com/2006/11/foxes_vs_hedgho.html\">foxes vs hedgehogs</a>&quot; research, enjoinders to use &quot;<a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">outside</a> <a href=\"http://www.overcomingbias.com/2008/06/singularity-out.html\">views</a>&quot;, enjoinders not to second-guess expert systems, and <a href=\"http://www.pbc.org/files/messages/5632/0162.html\">cautions for Christians against \u201cclever arguments\u201d</a>.</p><h2><strong>Explicit reasoning is often nuts</strong></h2><p>Ostrich Theories sound implausible: why would <em>not</em> thinking through an issue make our actions <em>better</em>?  And yet examples abound of folks whose theories and theorizing (as contrasted with their habits, wordless intuitions, and unarticulated responses to social pressures or their own emotions) made significant chunks of their actions worse.  Examples include, among many others:</p><ul><li>Most early Communists;</li><li><a href=\"http://en.wikipedia.org/wiki/Ted_Kaczynski\">Ted Kaczynski</a> (The Unabomber; an IQ 160 math PhD who wrote an interesting treatise about the human impacts of technology, and also murdered innocent people while accomplishing nothing);</li><li><a href=\"http://www.huffingtonpost.com/2010/09/24/mitchell-heisman-suicide_n_738121.html\">Mitchell Heisman</a>;</li><li>Folks who go to great lengths to keep kosher;</li><li>Friends of mine who\u2019ve gone to great lengths to be meticulously denotationally honest, including refusing jobs that required a government loyalty oath, and refusing to click on user agreements for videogames; and</li><li>Many who\u2019ve gone to war for the sake of religion, national identity, or many different far-mode ideals.</li></ul><p>In fact, the examples of religion and war suggest that the trouble with, say, Kaczynski wasn\u2019t that his beliefs were unusually crazy.  The trouble was that his beliefs were an ordinary amount of crazy, and he was unusually prone to acting on his beliefs.  If the average person started to actually act on their nominal, verbal, explicit beliefs, they, too, would in many cases look plumb nuts.  For example, a Christian might give away all their possessions, rejoice at the death of their children in circumstances where they seem likely to have gone to heaven, and generally treat their chances of Heaven vs Hell as their top priority.  Someone else might risk their life-savings betting on an <a href=\"http://www.intrade.com/\">election outcome</a> or business about which they were <a href=\"http://en.wikipedia.org/wiki/Overconfidence_effect\">\u201c99% confident\u201d</a>.</p><p>That is: many peoples\u2019 abstract reasoning is not up to the task of day to day decision-making.  This doesn&#x27;t impair folks&#x27; actions all that much, because peoples&#x27; abstract reasoning has little bearing on our actual actions.  Mostly we <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">just find ourselves doing things</a> (out of habit, emotional inclination, or social copying) and <a href=\"http://faculty.virginia.edu/haidtlab/articles/haidt.emotionaldog.manuscript.pdf\">make up the reasons post-hoc</a>.  But when we do try to choose actions from theory, the results are far from reliably helpful -- and so many folks&#x27; early steps toward rationality go unrewarded.</p><p>We are left with two linked barriers to rationality: (1) nutty abstract reasoning; and (2) fears of reasoned nuttiness, and other failures to believe that thinking things through is actually helpful.[2]</p><p><strong>Reasoning can be made less risky</strong></p><p>Much of this nuttiness is unnecessary.  There are learnable skills that can both make our abstract reasoning more trustworthy and also make it easier for us to trust it.</p><p>Here&#x27;s the basic idea:</p><p>If you know the limitations of a pattern of reasoning, learning better what it says won\u2019t hurt you.  It\u2019s like having a friend who\u2019s often wrong.  If you don\u2019t know your friend\u2019s limitations, his advice might harm you.  But once you do know, you don\u2019t have to gag him; you can listen to what he says, and then take it with a grain of salt.[3]</p><p>Reasoning is the meta-tool that lets us figure out what methods of inference are trustworthy where.  Reason lets us look over the track records of our own explicit theorizing, outside experts&#x27; views, our near-mode intuitions, etc. and figure out which is how trustworthy in a given situation.</p><p>If we learn to use this meta-tool, we can walk into rationality without fear.</p><h2>Skills for safer reasoning</h2><p><em>1.  Recognize implicit knowledge.</em></p><p>Recognize when your habits, or outside customs, are likely to work better than your reasoned-from-scratch best guesses.  Notice how different groups act and what results they get.  Take pains to stay aware of your own anticipations, especially in cases where you have <a href=\"/lw/dl/verbal_overshadowing_and_the_art_of_rationality/\">explicit verbal models that might block your anticipations from view</a>.  And, by studying track records, get a sense of which prediction methods are trustworthy where.</p><p>Use track records; don&#x27;t assume that just because folks&#x27; <em>justifications</em> are incoherent, the <em>actions they are justifying</em> are foolish.  But also don&#x27;t assume that tradition is better than your models.  Be empirical.</p><p><em>2.  Plan for errors in your best-guess models.</em></p><p>We tend to be <a href=\"http://en.wikipedia.org/wiki/Overconfidence_effect\">overconfident</a> in our own beliefs, to <a href=\"http://en.wikipedia.org/wiki/Conjunction_fallacy\">overestimate the probability of conjunctions</a> (such as multi-part reasoning chains), and to <a href=\"http://en.wikipedia.org/wiki/Confirmation_bias\">search preferentially for evidence that we\u2019re right</a>.  Put these facts together, and theories folks are &quot;almost certain&quot; of turn out to be wrong pretty often.  Therefore:</p><ul><li>Make predictions from as many angles as possible, to build redundancy.  Use multiple theoretical frameworks, multiple datasets, multiple experts, multiple disciplines.  </li><li>When some lines of argument point one way and some another, don&#x27;t give up or take a vote.  Instead, notice that you&#x27;re confused, and (while guarding against confirmation bias!) seek follow-up information.</li><li>Use your memories of past error to bring up honest curiosity and fear of error.  Then, really search for evidence that you\u2019re wrong, the same way you&#x27;d search if your life were being bet on someone <em>else&#x27;s</em> theory.</li><li>Build safeguards, alternatives, and repurposable resources into your plans.</li></ul><p><em>3.  Beware rapid belief changes.</em></p><p>Some people find their beliefs changing rapidly back and forth, based for example on the particular lines of argument they&#x27;re currently pondering, or the beliefs of those they&#x27;ve recently read or talked to.  Such fluctuations are generally bad news for both the accuracy of your beliefs and the usefulness of your actions.  If this is your situation:</p><ul><li>Remember that accurate beliefs come from an even, long-term collection of all the available evidence, with no extra weight for arguments presently in front of one.  Thus, they shouldn&#x27;t fluctuate dramatically back and forth; you should never be able to predict which way your future probabilities will move.</li><li>If you <em>can</em> predict what you&#x27;ll believe a few years from now, consider believing that already. </li><li>Remember that if reading X-ist books will predictably move your beliefs toward X, and you know there are X-ist books out there, you should move your beliefs toward X already.  Remember the <a href=\"/lw/ii/conservation_of_expected_evidence/\">Conservation of Expected Evidence</a> more generally.</li><li> Consider what emotions are driving the rapid fluctuations.  If you\u2019re uncomfortable ever disagreeing with your interlocutors, build comfort with disagreement.  If you&#x27;re uncomfortable not knowing, so that you find yourself grasping for one framework after another, build your tolerance for ambiguity, complexity, and unknowns.  </li></ul><p><em>4.  Update your near-mode anticipations, not just your far-mode beliefs.</em></p><p>Sometimes your far-mode is smart and you near-mode is stupid.  For example, Yvain&#x27;s rationalist <a href=\"/lw/1l/the_mystery_of_the_haunted_rationalist/\">knows abstractly that there aren\u2019t ghosts, but nevertheless fears them</a>.  Other times, though, your near-mode is smart and your far-mode is stupid.  You might \u201cbelieve\u201d in an afterlife but retain a concrete, near-mode fear of death.  You might advocate Communism but have a sinking <a href=\"http://www.sciencemag.org/cgi/content/abstract/275/5304/1293\">feeling</a> in your stomach as you conduct your tour of Stalin\u2019s Russia.</p><p>Thus: trust abstract reasoning or concrete anticipations in different situations, according to their strengths.  But, whichever one you bet your actions on, keep the other one in view.  Ask it what it expects and why it expects it.  Show it why you disagree (<a href=\"/lw/1r/striving_to_accept/\">visualizing your evidence concretely</a>, if you\u2019re trying to talk to your wordless anticipations), and see if it finds your evidence convincing.  Try to grow <a href=\"/lw/2q6/compartmentalization_in_epistemic_and/\">all</a> your cognitive subsystems, so as to form a whole mind.</p><p><em>5.  Use raw motivation, emotion, and behavior to determine at least part of your priorities.</em></p><p>One of the commonest routes to theory-driven nuttiness is to take a <a href=\"/lw/kx/fake_selfishness/\">\u201cgoal\u201d</a> that isn\u2019t your goal.  Thus, folks claim to care \u201cabove all else\u201d about their selfish well-being, the abolition of suffering, an objective Morality discoverable by superintelligence, or average utilitarian happiness-sums.  They then find themselves either <a href=\"http://wiki.lesswrong.com/wiki/Akrasia\">without motivation</a> to pursue \u201ctheir goals\u201d, or else pulled into chains of actions that they dread and do not want.  </p><p> Concrete local motivations are often embarrassing. For example, I find myself concretely motivated to \u201cwin\u201d arguments, even though I&#x27;d think better of myself if I was driven by curiosity. But, like near-mode beliefs, concrete local motivations can act as a safeguard and an anchor.  For example, if you become abstractly confused about meta-ethics, you&#x27;ll <a href=\"/lw/sk/changing_your_metaethics/\">still have</a> a concrete desire to pull babies off train tracks.  And so dialoguing with your near-mode wants and motives, like your near-mode anticipations, can help build a robust, trust-worthy mind.</p><h2><strong>Why it matters (again)</strong></h2><p>Safety skills such as the above are worth learning for three reasons.  </p><ol><li>They help us avoid nutty actions.</li><li>They help us reason unhesitatingly, instead of flinching away out of fear.</li><li>They help us build a rationality for the whole mind, with the strengths of near-mode as well as of abstract reasoning.</li></ol><hr class=\"dividerBlock\"/><p>[1]  These are not the only reasons people fear thinking.  At minimum, there is also: </p><ul><li>Fear of social censure for the new beliefs (e.g., for changing your politics, or failing to believe your friend was justified in his divorce);</li><li>Fear that part of you will use those new beliefs to justify actions that you as a whole do not want (e.g., you may fear to read a study about upsides of nicotine, lest you use it as a rationalization to start smoking again; you may similarly fear to read a study about how easily you can save African lives, lest it ends up prompting you to donate money).</li></ul><p>[2] Many points in this article, and especially in the &quot;explicit reasoning is often nuts&quot; section, are stolen from Michael Vassar.  Give him the credit, and me the blame and the upvotes.</p><p>[3] <a href=\"/user/CarlShulman\">Carl</a> points out that Eliezer points out that <a href=\"/lw/k4/do_we_believe_everything_were_told/\">studies show we can&#x27;t</a>.  But it seems like explicitly modeling when your friend is and isn&#x27;t accurate, and when explicit models have and haven&#x27;t led you to good actions, should at least help.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "9YFoDPFwMoWthzgkY": 9, "5f5c37ee1b5cdee568cfb1f2": 3, "QT87jxkk6DXuS8hGA": 9}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m5AH78nscsGjMbBwv", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 93, "baseScore": 119, "extendedScore": null, "score": 0.000222, "legacy": true, "legacyId": "3841", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 119, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Or: \u201cI don\u2019t want to think about <em>that</em>!  I might be left with <em>mistaken beliefs</em>!\u201d</p><p>Related to: <a href=\"/lw/18b/reason_as_memetic_immune_disorder/\">Rationality as memetic immune disorder</a>; <a href=\"/lw/7k/incremental_progress_and_the_valley/\">Incremental progress and the valley</a>; <a href=\"http://wiki.lesswrong.com/wiki/Egan's_law\">Egan's Law</a>.</p><p><em>tl;dr: Many of us hesitate to trust explicit reasoning because... we haven\u2019t built the skills that make such reasoning trustworthy.  Some simple strategies can help.</em></p><p>Most of us are afraid to think fully about certain subjects.</p><p>Sometimes, we avert our eyes for fear of unpleasant conclusions.  (\u201cWhat if it\u2019s my fault? What if I\u2019m not good enough?\u201d)</p><p>But other times, oddly enough, we avert our eyes for fear of <em>inaccurate</em> conclusions.[1]  People fear questioning their religion, lest they disbelieve and become damned.  People fear questioning their \u201cdon't walk alone at night\u201d safety strategy, lest they venture into danger.  And I find I hesitate when pondering <a href=\"http://www.overcomingbias.com/2008/08/where-does-pasc.html\">Pascal\u2019s wager</a>, <a href=\"http://www.nickbostrom.com/ethics/infinite.pdf\">infinite ethics</a>, the <a href=\"http://www.simulation-argument.com/simulation.html\">Simulation argument</a>, and whether I\u2019m a <a href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">Boltzmann brain</a>... because I\u2019m afraid of losing my bearings, and believing mistaken things.</p><p><a href=\"http://2.bp.blogspot.com/_vWrx43Kdj4U/S8VVCyMj4DI/AAAAAAAAAH0/Tffuc96arP4/s1600/ostrich_head_in_ground_full.jpg\">Ostrich Theory</a>, one might call it.  Or I\u2019m Already Right theory.  The theory that we\u2019re more likely to act sensibly if we <em>don\u2019t</em> think further, than if we <em>do</em>.  Sometimes Ostrich Theories are unconsciously held; one just wordlessly backs away from certain thoughts.  Other times full or partial Ostrich Theories are put forth explicitly, as in Phil Goetz\u2019s <a href=\"/lw/18b/reason_as_memetic_immune_disorder/\">post</a>, <a href=\"/lw/2l6/taking_ideas_seriously/2fgm?c=1\">this LW comment</a>, discussions of Tetlock's \"<a href=\"http://www.overcomingbias.com/2006/11/foxes_vs_hedgho.html\">foxes vs hedgehogs</a>\" research, enjoinders to use \"<a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">outside</a> <a href=\"http://www.overcomingbias.com/2008/06/singularity-out.html\">views</a>\", enjoinders not to second-guess expert systems, and <a href=\"http://www.pbc.org/files/messages/5632/0162.html\">cautions for Christians against \u201cclever arguments\u201d</a>.</p><h2 id=\"Explicit_reasoning_is_often_nuts\"><strong>Explicit reasoning is often nuts</strong></h2><p>Ostrich Theories sound implausible: why would <em>not</em> thinking through an issue make our actions <em>better</em>?  And yet examples abound of folks whose theories and theorizing (as contrasted with their habits, wordless intuitions, and unarticulated responses to social pressures or their own emotions) made significant chunks of their actions worse.  Examples include, among many others:</p><ul><li>Most early Communists;</li><li><a href=\"http://en.wikipedia.org/wiki/Ted_Kaczynski\">Ted Kaczynski</a> (The Unabomber; an IQ 160 math PhD who wrote an interesting treatise about the human impacts of technology, and also murdered innocent people while accomplishing nothing);</li><li><a href=\"http://www.huffingtonpost.com/2010/09/24/mitchell-heisman-suicide_n_738121.html\">Mitchell Heisman</a>;</li><li>Folks who go to great lengths to keep kosher;</li><li>Friends of mine who\u2019ve gone to great lengths to be meticulously denotationally honest, including refusing jobs that required a government loyalty oath, and refusing to click on user agreements for videogames; and</li><li>Many who\u2019ve gone to war for the sake of religion, national identity, or many different far-mode ideals.</li></ul><p>In fact, the examples of religion and war suggest that the trouble with, say, Kaczynski wasn\u2019t that his beliefs were unusually crazy.  The trouble was that his beliefs were an ordinary amount of crazy, and he was unusually prone to acting on his beliefs.  If the average person started to actually act on their nominal, verbal, explicit beliefs, they, too, would in many cases look plumb nuts.  For example, a Christian might give away all their possessions, rejoice at the death of their children in circumstances where they seem likely to have gone to heaven, and generally treat their chances of Heaven vs Hell as their top priority.  Someone else might risk their life-savings betting on an <a href=\"http://www.intrade.com/\">election outcome</a> or business about which they were <a href=\"http://en.wikipedia.org/wiki/Overconfidence_effect\">\u201c99% confident\u201d</a>.</p><p>That is: many peoples\u2019 abstract reasoning is not up to the task of day to day decision-making.  This doesn't impair folks' actions all that much, because peoples' abstract reasoning has little bearing on our actual actions.  Mostly we <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">just find ourselves doing things</a> (out of habit, emotional inclination, or social copying) and <a href=\"http://faculty.virginia.edu/haidtlab/articles/haidt.emotionaldog.manuscript.pdf\">make up the reasons post-hoc</a>.  But when we do try to choose actions from theory, the results are far from reliably helpful -- and so many folks' early steps toward rationality go unrewarded.</p><p>We are left with two linked barriers to rationality: (1) nutty abstract reasoning; and (2) fears of reasoned nuttiness, and other failures to believe that thinking things through is actually helpful.[2]</p><p><strong id=\"Reasoning_can_be_made_less_risky\">Reasoning can be made less risky</strong></p><p>Much of this nuttiness is unnecessary.  There are learnable skills that can both make our abstract reasoning more trustworthy and also make it easier for us to trust it.</p><p>Here's the basic idea:</p><p>If you know the limitations of a pattern of reasoning, learning better what it says won\u2019t hurt you.  It\u2019s like having a friend who\u2019s often wrong.  If you don\u2019t know your friend\u2019s limitations, his advice might harm you.  But once you do know, you don\u2019t have to gag him; you can listen to what he says, and then take it with a grain of salt.[3]</p><p>Reasoning is the meta-tool that lets us figure out what methods of inference are trustworthy where.  Reason lets us look over the track records of our own explicit theorizing, outside experts' views, our near-mode intuitions, etc. and figure out which is how trustworthy in a given situation.</p><p>If we learn to use this meta-tool, we can walk into rationality without fear.</p><h2 id=\"Skills_for_safer_reasoning\">Skills for safer reasoning</h2><p><em>1.  Recognize implicit knowledge.</em></p><p>Recognize when your habits, or outside customs, are likely to work better than your reasoned-from-scratch best guesses.  Notice how different groups act and what results they get.  Take pains to stay aware of your own anticipations, especially in cases where you have <a href=\"/lw/dl/verbal_overshadowing_and_the_art_of_rationality/\">explicit verbal models that might block your anticipations from view</a>.  And, by studying track records, get a sense of which prediction methods are trustworthy where.</p><p>Use track records; don't assume that just because folks' <em>justifications</em> are incoherent, the <em>actions they are justifying</em> are foolish.  But also don't assume that tradition is better than your models.  Be empirical.</p><p><em>2.  Plan for errors in your best-guess models.</em></p><p>We tend to be <a href=\"http://en.wikipedia.org/wiki/Overconfidence_effect\">overconfident</a> in our own beliefs, to <a href=\"http://en.wikipedia.org/wiki/Conjunction_fallacy\">overestimate the probability of conjunctions</a> (such as multi-part reasoning chains), and to <a href=\"http://en.wikipedia.org/wiki/Confirmation_bias\">search preferentially for evidence that we\u2019re right</a>.  Put these facts together, and theories folks are \"almost certain\" of turn out to be wrong pretty often.  Therefore:</p><ul><li>Make predictions from as many angles as possible, to build redundancy.  Use multiple theoretical frameworks, multiple datasets, multiple experts, multiple disciplines.  </li><li>When some lines of argument point one way and some another, don't give up or take a vote.  Instead, notice that you're confused, and (while guarding against confirmation bias!) seek follow-up information.</li><li>Use your memories of past error to bring up honest curiosity and fear of error.  Then, really search for evidence that you\u2019re wrong, the same way you'd search if your life were being bet on someone <em>else's</em> theory.</li><li>Build safeguards, alternatives, and repurposable resources into your plans.</li></ul><p><em>3.  Beware rapid belief changes.</em></p><p>Some people find their beliefs changing rapidly back and forth, based for example on the particular lines of argument they're currently pondering, or the beliefs of those they've recently read or talked to.  Such fluctuations are generally bad news for both the accuracy of your beliefs and the usefulness of your actions.  If this is your situation:</p><ul><li>Remember that accurate beliefs come from an even, long-term collection of all the available evidence, with no extra weight for arguments presently in front of one.  Thus, they shouldn't fluctuate dramatically back and forth; you should never be able to predict which way your future probabilities will move.</li><li>If you <em>can</em> predict what you'll believe a few years from now, consider believing that already. </li><li>Remember that if reading X-ist books will predictably move your beliefs toward X, and you know there are X-ist books out there, you should move your beliefs toward X already.  Remember the <a href=\"/lw/ii/conservation_of_expected_evidence/\">Conservation of Expected Evidence</a> more generally.</li><li> Consider what emotions are driving the rapid fluctuations.  If you\u2019re uncomfortable ever disagreeing with your interlocutors, build comfort with disagreement.  If you're uncomfortable not knowing, so that you find yourself grasping for one framework after another, build your tolerance for ambiguity, complexity, and unknowns.  </li></ul><p><em>4.  Update your near-mode anticipations, not just your far-mode beliefs.</em></p><p>Sometimes your far-mode is smart and you near-mode is stupid.  For example, Yvain's rationalist <a href=\"/lw/1l/the_mystery_of_the_haunted_rationalist/\">knows abstractly that there aren\u2019t ghosts, but nevertheless fears them</a>.  Other times, though, your near-mode is smart and your far-mode is stupid.  You might \u201cbelieve\u201d in an afterlife but retain a concrete, near-mode fear of death.  You might advocate Communism but have a sinking <a href=\"http://www.sciencemag.org/cgi/content/abstract/275/5304/1293\">feeling</a> in your stomach as you conduct your tour of Stalin\u2019s Russia.</p><p>Thus: trust abstract reasoning or concrete anticipations in different situations, according to their strengths.  But, whichever one you bet your actions on, keep the other one in view.  Ask it what it expects and why it expects it.  Show it why you disagree (<a href=\"/lw/1r/striving_to_accept/\">visualizing your evidence concretely</a>, if you\u2019re trying to talk to your wordless anticipations), and see if it finds your evidence convincing.  Try to grow <a href=\"/lw/2q6/compartmentalization_in_epistemic_and/\">all</a> your cognitive subsystems, so as to form a whole mind.</p><p><em>5.  Use raw motivation, emotion, and behavior to determine at least part of your priorities.</em></p><p>One of the commonest routes to theory-driven nuttiness is to take a <a href=\"/lw/kx/fake_selfishness/\">\u201cgoal\u201d</a> that isn\u2019t your goal.  Thus, folks claim to care \u201cabove all else\u201d about their selfish well-being, the abolition of suffering, an objective Morality discoverable by superintelligence, or average utilitarian happiness-sums.  They then find themselves either <a href=\"http://wiki.lesswrong.com/wiki/Akrasia\">without motivation</a> to pursue \u201ctheir goals\u201d, or else pulled into chains of actions that they dread and do not want.  </p><p> Concrete local motivations are often embarrassing. For example, I find myself concretely motivated to \u201cwin\u201d arguments, even though I'd think better of myself if I was driven by curiosity. But, like near-mode beliefs, concrete local motivations can act as a safeguard and an anchor.  For example, if you become abstractly confused about meta-ethics, you'll <a href=\"/lw/sk/changing_your_metaethics/\">still have</a> a concrete desire to pull babies off train tracks.  And so dialoguing with your near-mode wants and motives, like your near-mode anticipations, can help build a robust, trust-worthy mind.</p><h2 id=\"Why_it_matters__again_\"><strong>Why it matters (again)</strong></h2><p>Safety skills such as the above are worth learning for three reasons.  </p><ol><li>They help us avoid nutty actions.</li><li>They help us reason unhesitatingly, instead of flinching away out of fear.</li><li>They help us build a rationality for the whole mind, with the strengths of near-mode as well as of abstract reasoning.</li></ol><hr class=\"dividerBlock\"><p>[1]  These are not the only reasons people fear thinking.  At minimum, there is also: </p><ul><li>Fear of social censure for the new beliefs (e.g., for changing your politics, or failing to believe your friend was justified in his divorce);</li><li>Fear that part of you will use those new beliefs to justify actions that you as a whole do not want (e.g., you may fear to read a study about upsides of nicotine, lest you use it as a rationalization to start smoking again; you may similarly fear to read a study about how easily you can save African lives, lest it ends up prompting you to donate money).</li></ul><p>[2] Many points in this article, and especially in the \"explicit reasoning is often nuts\" section, are stolen from Michael Vassar.  Give him the credit, and me the blame and the upvotes.</p><p>[3] <a href=\"/user/CarlShulman\">Carl</a> points out that Eliezer points out that <a href=\"/lw/k4/do_we_believe_everything_were_told/\">studies show we can't</a>.  But it seems like explicitly modeling when your friend is and isn't accurate, and when explicit models have and haven't led you to good actions, should at least help.</p>", "sections": [{"title": "Explicit reasoning is often nuts", "anchor": "Explicit_reasoning_is_often_nuts", "level": 1}, {"title": "Reasoning can be made less risky", "anchor": "Reasoning_can_be_made_less_risky", "level": 2}, {"title": "Skills for safer reasoning", "anchor": "Skills_for_safer_reasoning", "level": 1}, {"title": "Why it matters (again)", "anchor": "Why_it_matters__again_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "95 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 95, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["aHaqgTNnFzD7NGLMx", "oZNXmHcdhb4m7vwsv", "LubwxZHKKvCivYGzx", "PBRWb2Em5SNeWYwwB", "sfyCj4fSWzNvYmdTR", "jiBFC7DcCrZjGmZnJ", "mja6jZ6k9gAwki9Nu", "Cxcormwz6jb98gGzW", "N99KgncSXewWqkzMA", "Masoq4NdmmGSiq2xw", "LhP2zGBWR5AdssrdJ", "TiDGXt3WrQwtCdDj3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-29T02:20:15.595Z", "modifiedAt": null, "url": null, "title": "The spam must end", "slug": "the-spam-must-end", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:26.235Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qC7adZZW8QjsJxFbv/the-spam-must-end", "pageUrlRelative": "/posts/qC7adZZW8QjsJxFbv/the-spam-must-end", "linkUrl": "https://www.lesswrong.com/posts/qC7adZZW8QjsJxFbv/the-spam-must-end", "postedAtFormatted": "Friday, October 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20spam%20must%20end&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20spam%20must%20end%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqC7adZZW8QjsJxFbv%2Fthe-spam-must-end%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20spam%20must%20end%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqC7adZZW8QjsJxFbv%2Fthe-spam-must-end", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqC7adZZW8QjsJxFbv%2Fthe-spam-must-end", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<p>&nbsp;I'm mean most of us would like a friendly bot to chat with, but this is just paperclipping the section (no offence clippy), by now its starting to be a real <a href=\"/lw/f1/beware_trivial_inconveniences/\">trivial inconvenience</a> for me and it reduces my desire to check out new topics.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qC7adZZW8QjsJxFbv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 18, "extendedScore": null, "score": 6.398095451254856e-07, "legacy": true, "legacyId": "3854", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["reitXJgJXFzKpdKyd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-29T10:06:04.787Z", "modifiedAt": null, "url": null, "title": "META: Who Have You Told About LW?", "slug": "meta-who-have-you-told-about-lw", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:53.298Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sixes_and_sevens", "createdAt": "2009-11-11T14:42:23.502Z", "isAdmin": false, "displayName": "sixes_and_sevens"}, "userId": "n83meJ5yG2WQzygvw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j2TGffvdxYLfnAf4x/meta-who-have-you-told-about-lw", "pageUrlRelative": "/posts/j2TGffvdxYLfnAf4x/meta-who-have-you-told-about-lw", "linkUrl": "https://www.lesswrong.com/posts/j2TGffvdxYLfnAf4x/meta-who-have-you-told-about-lw", "postedAtFormatted": "Friday, October 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20META%3A%20Who%20Have%20You%20Told%20About%20LW%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMETA%3A%20Who%20Have%20You%20Told%20About%20LW%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj2TGffvdxYLfnAf4x%2Fmeta-who-have-you-told-about-lw%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=META%3A%20Who%20Have%20You%20Told%20About%20LW%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj2TGffvdxYLfnAf4x%2Fmeta-who-have-you-told-about-lw", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj2TGffvdxYLfnAf4x%2Fmeta-who-have-you-told-about-lw", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>I've been lurking on LW since shortly after it started, and on OB for about six months before that.&nbsp; In that time, I've told four or five people about it.&nbsp; I would make a terrible evangelist.</p>\r\n<p>I'm curious as to whether other people have the same problem.&nbsp; I'd like to tell lots of people about LW, but I don't think they're ready for it.&nbsp; If they read a statement like \"purchase utilons and warm fuzzies separately\" their eyes would glaze over, and they'd walk away thinking LW was some sort of crackpot site.</p>\r\n<p>I have found certain posts and topics to be fairly good hooks for getting people interested.&nbsp;<a href=\"/lw/kr/an_alien_god/\">An Alien God</a> is a good suggested read for people with an interest in evolutionary theory and the Cthulhu mythos (surprisingly high crossover in my experience).&nbsp; HP:MoR is also a pretty popular hook.&nbsp; The site itself isn't really optimised for word-of-mouth, though, and not everyone likes child wizards and blasphemous horrors.</p>\r\n<p>How many people have you introduced to LW?&nbsp; Who were they, how do you do it and what was their reaction?&nbsp; How could we do it better?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j2TGffvdxYLfnAf4x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 6.399206034457252e-07, "legacy": true, "legacyId": "3855", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pLRogvJLPPg6Mrvg4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-29T12:14:45.185Z", "modifiedAt": null, "url": null, "title": "Baby born from cryo-preserved embryo", "slug": "baby-born-from-cryo-preserved-embryo", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:23.590Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ffuqo2Hj9CbbTEP9P/baby-born-from-cryo-preserved-embryo", "pageUrlRelative": "/posts/ffuqo2Hj9CbbTEP9P/baby-born-from-cryo-preserved-embryo", "linkUrl": "https://www.lesswrong.com/posts/ffuqo2Hj9CbbTEP9P/baby-born-from-cryo-preserved-embryo", "postedAtFormatted": "Friday, October 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Baby%20born%20from%20cryo-preserved%20embryo&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABaby%20born%20from%20cryo-preserved%20embryo%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fffuqo2Hj9CbbTEP9P%2Fbaby-born-from-cryo-preserved-embryo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Baby%20born%20from%20cryo-preserved%20embryo%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fffuqo2Hj9CbbTEP9P%2Fbaby-born-from-cryo-preserved-embryo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fffuqo2Hj9CbbTEP9P%2Fbaby-born-from-cryo-preserved-embryo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 15, "htmlBody": "<p>Apparently embryos produced by in vitro fertilization routinely stay on ice for years. Article <a href=\"http://hamptonroads.com/2010/10/embryo-spent-19-years-ice-now-its-baby\" target=\"_blank\">here.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ffuqo2Hj9CbbTEP9P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 6.399512870666885e-07, "legacy": true, "legacyId": "3856", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-29T16:40:31.484Z", "modifiedAt": null, "url": null, "title": "\"The current era is the only chance of setting up game rules\"", "slug": "the-current-era-is-the-only-chance-of-setting-up-game-rules", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:23.754Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rnP4S3YEzX2GuXaSi/the-current-era-is-the-only-chance-of-setting-up-game-rules", "pageUrlRelative": "/posts/rnP4S3YEzX2GuXaSi/the-current-era-is-the-only-chance-of-setting-up-game-rules", "linkUrl": "https://www.lesswrong.com/posts/rnP4S3YEzX2GuXaSi/the-current-era-is-the-only-chance-of-setting-up-game-rules", "postedAtFormatted": "Friday, October 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22The%20current%20era%20is%20the%20only%20chance%20of%20setting%20up%20game%20rules%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22The%20current%20era%20is%20the%20only%20chance%20of%20setting%20up%20game%20rules%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrnP4S3YEzX2GuXaSi%2Fthe-current-era-is-the-only-chance-of-setting-up-game-rules%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22The%20current%20era%20is%20the%20only%20chance%20of%20setting%20up%20game%20rules%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrnP4S3YEzX2GuXaSi%2Fthe-current-era-is-the-only-chance-of-setting-up-game-rules", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrnP4S3YEzX2GuXaSi%2Fthe-current-era-is-the-only-chance-of-setting-up-game-rules", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 701, "htmlBody": "<p>Here is another reason why we need to work on superhuman AI:&nbsp;</p>\n<h2><a href=\"http://www.aleph.se/andart/archives/2010/10/visions_of_the_future_in_milano.html\">Thermodynamics of advanced civilizations</a></h2>\n<p>Link: <a href=\"http://www.aleph.se/andart/archives/2010/10/visions_of_the_future_in_milano.html\">http://www.aleph.se/andart/archives/2010/10/visions_of_the_future_in_milano.html</a></p>\n<blockquote><ol>\n<li>Civilizations are physical objects, and nearly any ultimate goal imply a need for computation, storing bits and resources (the basic physical eschatology assumption). </li>\n<li>The universe has a bunch of issues:   \n<ul>\n<li>The stelliferous era will just last a trillion year or so. </li>\n<li>Matter and black holes are likely unstable, so after a certain time there will not be any structure around to build stuff from. Dark matter doesn't seem to be structurable either. </li>\n<li>Accelerating expansion prevents us from reaching beyond a certain horizon about 15 gigalightyears away. </li>\n<li>It will also split the superclusters into independent \"island universes\" that will become unreachable from each other within around 120 billion years. </li>\n<li>It also causes horizon radiation ~10<sup>-29</sup> K hot, which makes infinite computation impossible. </li>\n</ul>\n</li>\n<li>Civilizations have certain limits of resources, expansion, processing and waste heat:   \n<ul>\n<li>We can still lay our hands on 5.96&middot;10<sup>51</sup> kg matter (with dark matter 2.98&middot;10<sup>52</sup> kg) within the horizon, and ~2&middot;10<sup>45</sup> kg (with DM ~10<sup>46</sup> kg) if we settle for a supercluster. </li>\n<li>The lightspeed limitation is not enormously cumbersome, if we use self-replicating probes. </li>\n<li>The finite energy cost of erasing bits is the toughest bound. It forces us to pay for observing the world, formatting new memory and correct errors. </li>\n</ul>\n</li>\n<li>Putting it all together we end up with the following scenario for maximal information processing:   \n<ul>\n<li>The age of expansion: interstellar and intergalactic expansion with self-replicating probes. It looks like one can enforce \"squatters rights\", so there is no strong reason to start exploiting upon arrival. </li>\n<li>The age of preservation: await sufficiently low temperatures. A halving of temperature doubles the amount of computation you can do. You only need a logarithmically increasing number of backups for indefinite survival. Since fusion will release ~1% of the mass-energy of matter but black hole conversion ~50%, it might not be relevant to turn off the stars unless you feel particularly negentropic. </li>\n<li>The age of harvest: Exploit available energy to produce maximal amount of computation. The slower the exploitation, the more processing can be done. This is largely limited by structure decay: you need to be finished before your protons decay. Exactly how much computation you can do depends on how large fraction of the universe you got, how much reversible computation you can do and the exact background temperature. </li>\n</ul>\n</li>\n<li>This leads to some policy-relevant conclusions: \n<ul>\n</ul>\n<ul>\n<li>Cosmic waste is a serious issue: the value of the future is enormous in terms of human lives, so postponing colonization or increasing existential risk carries enormous disutilities. However, in order to plan like this you need to have very low discount rates. </li>\n<li><strong>There are plenty of coordination problems: burning cosmic commons, berserker probes, entropy pollution etc. The current era is the only chance of setting up game rules before dispersion and loss of causal contact. </strong></li>\n<li>This model suggests a Fermi paradox answer: the aliens are out there, waiting. They already own most of the universe and we better be nice to them. Alternatively, if there is a phase transition situation where we are among the first, we really need to think about stable coordination and bargaining strategies.</li>\n</ul>\n<ul>\n</ul>\n</li>\n</ol></blockquote>\n<p>Here is more: <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.1329&amp;rep=rep1&amp;type=pdf\">Burning the Cosmic Commons: Evolutionary Strategies for Interstellar Colonization</a></p>\n<p>Our only hope to alleviate those problems is a beneficial <a href=\"http://www.nickbostrom.com/fut/singleton.html\">superhuman intelligence</a> to coordinate our future for us. That is if we want to <a href=\"http://www.youtube.com/watch?v=BIT3TYnQJQc&amp;hd=1\">make it to&nbsp;the stars</a> and that our dream of a&nbsp;<em><a href=\"http://www.vimeo.com/8586168\">galactic civilization</a>&nbsp;</em>comes true and does not end up in a <a href=\"http://www.overcomingbias.com/2010/03/future-discounts.html\">unimaginable war</a> over resources.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rnP4S3YEzX2GuXaSi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 5, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "3857", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-29T20:15:36.818Z", "modifiedAt": null, "url": null, "title": "Why should you vote?", "slug": "why-should-you-vote", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:56.992Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kmeme", "createdAt": "2010-07-31T20:37:53.116Z", "isAdmin": false, "displayName": "kmeme"}, "userId": "XsuPXGJpZnQdYYm33", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mqNzvQkZ3nGetwA2F/why-should-you-vote", "pageUrlRelative": "/posts/mqNzvQkZ3nGetwA2F/why-should-you-vote", "linkUrl": "https://www.lesswrong.com/posts/mqNzvQkZ3nGetwA2F/why-should-you-vote", "postedAtFormatted": "Friday, October 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20should%20you%20vote%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20should%20you%20vote%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmqNzvQkZ3nGetwA2F%2Fwhy-should-you-vote%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20should%20you%20vote%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmqNzvQkZ3nGetwA2F%2Fwhy-should-you-vote", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmqNzvQkZ3nGetwA2F%2Fwhy-should-you-vote", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>For many years I've been interested in the \"paradox\" that your vote tends to never alter the outcome of an election, yet the outcome is in fact determined by the votes. &nbsp;I wrote a blog post about this and tried to explain it in terms of emergence, we as voters are just feeling what it's like to be just a tiny part of a much bigger system.</p>\n<p>Then I tried to explain that \"voter turnout\" is in fact one of the most important metrics for an election, it determines the legitimacy and stability of the process. &nbsp;So therefore even though your vote won't determine the winner, it will contribute to voter turnout and thus is productive and useful.</p>\n<p><a href=\"http://www.kmeme.com/2010/10/why-you-should-vote.html\">http://www.kmeme.com/2010/10/why-you-should-vote.html</a></p>\n<p>However I don't find my argument all that compelling, because even voter turnout is going to be approximately the same whether you vote or not.</p>\n<p>In the post I bring up littering as something else where your tiny contribution adds up to be bigger result. &nbsp;I personally would never litter on purpose, yet I often skip voting because it seems like it doesn't make a difference. &nbsp;Is voting rational? &nbsp;How do you justify voting or not voting? &nbsp;My post was non-partisan so I'm soliciting non-partisan comments, trying to focus on the theory behind voting in general.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mqNzvQkZ3nGetwA2F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 6.400659768182324e-07, "legacy": true, "legacyId": "3858", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-29T20:31:33.891Z", "modifiedAt": null, "url": null, "title": "Seeking book about baseline life planning and expectations", "slug": "seeking-book-about-baseline-life-planning-and-expectations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:08.174Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JenniferRM", "createdAt": "2009-03-06T17:16:50.600Z", "isAdmin": false, "displayName": "JenniferRM"}, "userId": "g8JkZfL8PTqAefpvx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tiyKRo5uudtxit5zE/seeking-book-about-baseline-life-planning-and-expectations", "pageUrlRelative": "/posts/tiyKRo5uudtxit5zE/seeking-book-about-baseline-life-planning-and-expectations", "linkUrl": "https://www.lesswrong.com/posts/tiyKRo5uudtxit5zE/seeking-book-about-baseline-life-planning-and-expectations", "postedAtFormatted": "Friday, October 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seeking%20book%20about%20baseline%20life%20planning%20and%20expectations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeeking%20book%20about%20baseline%20life%20planning%20and%20expectations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtiyKRo5uudtxit5zE%2Fseeking-book-about-baseline-life-planning-and-expectations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seeking%20book%20about%20baseline%20life%20planning%20and%20expectations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtiyKRo5uudtxit5zE%2Fseeking-book-about-baseline-life-planning-and-expectations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtiyKRo5uudtxit5zE%2Fseeking-book-about-baseline-life-planning-and-expectations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 799, "htmlBody": "<p>In an attempt to find a useful \"base rate expectations\" for the rest of my life (and how actions I might take now could set me up to be much better off 10, 20, 30, 40, 50, 60, and 70 years from now) I'm looking for a book that describes the nuts and bolts of human lives.&nbsp; I want coherent discussion from an actuarial/economic/probabilistic/calculating perspective, but I'd like some soulfulness too.&nbsp; The ideal book would be published in 2010 and have coverage of the different periods of people's lives and cover different <em>aspects</em> of their lives as well.&nbsp; In some sense the book would be like a nuts and bolts \"how to your your life\" manual.&nbsp; Hopefully it would have footnotes and generally good epistemology :-)</p>\n<p>To take an example of the kind of content I would hope for (in a domain where I already have worked out some of the theory myself) the ideal book would explain how to calculate the ROI of different levels of college education realistically.&nbsp; Instead of a hand-waving argument that \"on avergae you'll make more with education\" it would also talk about the opportunity costs of lost wages, and how expected number of years of work impacts on what amount of training makes sense, and so on.&nbsp;</p>\n<p>To be clear, I don't want a book that is simply about deciding when, how, and for how long it makes sense to train for a job.&nbsp; Instead I want something that talks about similar issues that I haven't already thought about but that are important, so that I can be usefully educated in ways I wasn't expecting.&nbsp; My goal is to find someone else's scaffold to help me project when and why I should (or shouldn't) buy a minivan, how much to budget for dentistry in my 50's, and a breakdown of the causes of bankruptcy the way insurance companies can predict causes of death.</p>\n<p>I was hoping that the book <a href=\"http://www.amazon.com/How-We-Live-Perspective-Americans/dp/0674412257/\">How We Live: An Economic Perspective on Americans from Birth to Death</a> would give me what I want (and it is still probably my fallback book if I can't find anything better) but it was written in 1983, and appears to be strongly oriented towards public policy recommendations rather than personal choice.</p>\n<p>Books that may be conceptually nearby that seem non-ideal include:</p>\n<p><a href=\"http://www.amazon.com/Dear-Undercover-Economist-Priceless-Challenges/dp/0812980107/\">Dear Undercover Economist: Priceless Advice on Money, Work, Sex, Kids, and Life's Other Challenges</a> - My second place fallback because it covers real life content, is from 2009, and the first book in the series was pretty solid on economic theory.&nbsp; The problem is that it seems like haphazard coverage of the subject matter rather than \"a treatise\" that aims to describe the full ambit of life issues, sort them by priority, and deal usefully with the big stuff.</p>\n<p><a href=\"http://www.thebookbag.co.uk/reviews/index.php?title=The_Average_Life_of_the_Average_Person_by_Tadg_Farrington\">The Average Life of the Average Person: How It All Adds Up</a> - Just a collection of factoids, like the number of cumulative days the average person spends on the toilette, the value add is the collection and the juxtaposition.&nbsp; Mere factoids might actually be useful as a list of things to think about optimizing for long term impact?&nbsp; Not what I want, but potentially relevant.</p>\n<p><a href=\"http://www.amazon.com/ABCs-Strategic-Life-Planning/dp/0976013525/\">The ABCs Of Strategic Life Planning</a> - The first problem is that this appears to be a workbook with questionnaires (presumed target market is people dealing with akrasia) rather than a narrative of fact and theory (giving the logical scaffold for a general plan).&nbsp; The second problem is that the marketing means it is probably from the business/self-help genre from which I expect relatively little epistemic rigor.</p>\n<p><a href=\"http://www.amazon.com/How-Happiness-Scientific-Approach-Getting/dp/B0028N72O4/\">The How of Happiness: A Scientific Approach to Getting the Life You Want</a> - This book covers the \"softer issues\" that I definitely care about and don't expect to be covered by economists.&nbsp; It seems potentially interesting, but in addition to not covering the other subject areas, it sounds more like a literature review of positive psychology results than like a \"normal life overview\".</p>\n<p><a href=\"http://www.amazon.com/Logic-Life-Rational-Economics-Irrational/dp/1400066425/\">The Logic of Life: The Rational Economics of an Irrational World</a> - It is good that this is recent (from 2008) but it is poorly reviewed, haphazard in subject, and full of shiny stuff that's intended to be stimulatingly non-intuitive.&nbsp; I'm looking for meat and potatoes.</p>\n<p><a href=\"http://www.amazon.com/Hidden-Order-Economics-Everyday-Life/dp/0887308856/\">Hidden Order: The Economics of Everyday Life</a> - Purportedly a lot of economic theory (which is not what I'm looking for) and then some shiny examples that Freaknomics later (it was written in 1997) made somewhat trendy.&nbsp; However, the title sounds like the book could have been close to what I want.</p>\n<p>Can anyone suggest a book that is \"a coherent overview of the intersection of these books and anything else I forgot\"?&nbsp; There may be no book that matches my ideal, but I wouldn't be surprised if something pretty close to it exists that I just haven't found yet.</p>\n<p>Help appreciated!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tiyKRo5uudtxit5zE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 6.400696623834252e-07, "legacy": true, "legacyId": "3852", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-29T21:56:52.218Z", "modifiedAt": null, "url": null, "title": "What is the Archimedean point of morality?", "slug": "what-is-the-archimedean-point-of-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:26.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "draq", "createdAt": "2010-09-03T18:08:04.472Z", "isAdmin": false, "displayName": "draq"}, "userId": "TMiHfZH3dGtbFTAbb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/evvJKR6Gyf6qMo4Ew/what-is-the-archimedean-point-of-morality", "pageUrlRelative": "/posts/evvJKR6Gyf6qMo4Ew/what-is-the-archimedean-point-of-morality", "linkUrl": "https://www.lesswrong.com/posts/evvJKR6Gyf6qMo4Ew/what-is-the-archimedean-point-of-morality", "postedAtFormatted": "Friday, October 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20the%20Archimedean%20point%20of%20morality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20the%20Archimedean%20point%20of%20morality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FevvJKR6Gyf6qMo4Ew%2Fwhat-is-the-archimedean-point-of-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20the%20Archimedean%20point%20of%20morality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FevvJKR6Gyf6qMo4Ew%2Fwhat-is-the-archimedean-point-of-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FevvJKR6Gyf6qMo4Ew%2Fwhat-is-the-archimedean-point-of-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 317, "htmlBody": "<p><em>It has been very enjoyable to post on LW [<a href=\"/r/discussion/lw/2yo/morality_is_as_real_as_the_physical_world/\">1</a>, <a href=\"/r/discussion/lw/2of/levels_of_intelligence/\">2</a>] and I have learned a lot from the discussions with other members, for which I am very thankful. But unfortunately, judging by my karma score which is on the same level of Kiwiallegiance and the Jewelry spammer, my opinions are not appreciated and I frequently receive the following message when posting a new comment:<br /></em></p>\n<p><span id=\"RATELIMIT\" class=\"error\">You are trying to submit too fast. try again in x<sub>n</sub> minutes.</span></p>\n<p><em>When I press the submit button after x<sub>1</sub> + 1 minutes, I'm told to wait another x<sub>2</sub> minutes.</em> <em>So commenting has become more and more frustating, and</em> <em>I don't want to continue burden the LW members with the heavy tast of down-voting me. But on the other hand, I still can't find any flaw in my argumentation despite many rebuttals. Maybe I am too ignorant, maybe I am on something. So I'll give myself a last try.<br /></em></p>\n<p><em>\n<hr />\n</em>There is none. Some say that morality is a system that is most conductive to cooperation and thus biological fitness. Others say, it is something society creates to enable its own survival. These are explanations that try to reduce morality (values, desires and dislikes) to the concepts of the natural world, but they don't capture what we really mean by desires, dislikes and values.</p>\n<p>You might explain my desire for pancakes as a neuronal process, as a mental function biologically evolved, but it does not capture the meaning of \"desire\". The concept of meaning itself has no meaning in the natural world, but <em>it has a meaning to us</em>, to the rational mind.</p>\n<p>As much as we cannot explain what the natural world \"really\" is, since we cannot see what is behind the physical reality (unless you are an idealistic Platonist), we cannot explain what morality and values \"really\" are. We can only describe them using scientific theories or normative theories, respectively.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "evvJKR6Gyf6qMo4Ew", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -5, "extendedScore": null, "score": 6.40090132107859e-07, "legacy": true, "legacyId": "3860", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PZvRBn7ZKsFfKKSAd", "gjyDBd3Werweuhi5w"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-30T00:32:16.595Z", "modifiedAt": null, "url": null, "title": "Cambridge Meetups Nov 7 and Nov 21", "slug": "cambridge-meetups-nov-7-and-nov-21", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:28.671Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XejFQR7NvSQCuXQ53/cambridge-meetups-nov-7-and-nov-21", "pageUrlRelative": "/posts/XejFQR7NvSQCuXQ53/cambridge-meetups-nov-7-and-nov-21", "linkUrl": "https://www.lesswrong.com/posts/XejFQR7NvSQCuXQ53/cambridge-meetups-nov-7-and-nov-21", "postedAtFormatted": "Saturday, October 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cambridge%20Meetups%20Nov%207%20and%20Nov%2021&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACambridge%20Meetups%20Nov%207%20and%20Nov%2021%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXejFQR7NvSQCuXQ53%2Fcambridge-meetups-nov-7-and-nov-21%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cambridge%20Meetups%20Nov%207%20and%20Nov%2021%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXejFQR7NvSQCuXQ53%2Fcambridge-meetups-nov-7-and-nov-21", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXejFQR7NvSQCuXQ53%2Fcambridge-meetups-nov-7-and-nov-21", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<p>The formerly monthly Cambridge meetups are now twice-monthly! Same place and time - <a href=\"http://maps.google.com/maps/place?cid=8180846014744081615&amp;q=Clear+Conscience+Cafe,+Cambridge,+MA&amp;hl=en&amp;ved=0CBYQ-QswAA&amp;ei=Z2bLTPLgGqiwzgWWpJGWCg&amp;sll=42.365068,-71.102699&amp;sspn=0.016658,0.015298&amp;ie=UTF8&amp;ll=42.380231,-71.128879&amp;spn=0,0&amp;t=h&amp;z=15\">Clear Conscience Cafe</a>, Sunday at 4pm - now on the first and third Sundays of each month, instead of just the third. That's November 7th and 21st.</p>\n<p>It was suggested last time that we might want to try adding some structure to these events. I thought it'd be fun to try <a href=\"http://wiki.lesswrong.com/wiki/Paranoid_debating\">Paranoid Debating</a>, so I'll bring trivia cards, as <a href=\"/r/discussion/lw/2yl/play_paranoid_debating_at_home/\">suggested</a> by eugman. Other proposals - activities, conversation topics, etc - are welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XejFQR7NvSQCuXQ53", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 6.401272082138879e-07, "legacy": true, "legacyId": "3861", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["edWMToD7uhBuApBdG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-30T02:32:53.517Z", "modifiedAt": null, "url": null, "title": "A brief guide to not getting downvoted", "slug": "a-brief-guide-to-not-getting-downvoted", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:23.702Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nhamann", "createdAt": "2009-10-15T04:19:24.675Z", "isAdmin": false, "displayName": "nhamann"}, "userId": "t6Fcfj7hMxrdJraSD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E8Rpx4oBWCARkEkZP/a-brief-guide-to-not-getting-downvoted", "pageUrlRelative": "/posts/E8Rpx4oBWCARkEkZP/a-brief-guide-to-not-getting-downvoted", "linkUrl": "https://www.lesswrong.com/posts/E8Rpx4oBWCARkEkZP/a-brief-guide-to-not-getting-downvoted", "postedAtFormatted": "Saturday, October 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20brief%20guide%20to%20not%20getting%20downvoted&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20brief%20guide%20to%20not%20getting%20downvoted%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE8Rpx4oBWCARkEkZP%2Fa-brief-guide-to-not-getting-downvoted%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20brief%20guide%20to%20not%20getting%20downvoted%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE8Rpx4oBWCARkEkZP%2Fa-brief-guide-to-not-getting-downvoted", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE8Rpx4oBWCARkEkZP%2Fa-brief-guide-to-not-getting-downvoted", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 552, "htmlBody": "<p><strong>Important Question:</strong> does a guide like this already exist? I couldn't find one, but it's hard to believe there's no guide to not getting downvoted...Supposing there isn't, please give me suggestions on what such a guide needs, because I'm sort of just guessing here.</p>\n<p><strong>Edit: </strong>Found <a href=\"/lw/2bu/your_intuitions_are_not_magic/\">Your intuititions are not magic</a>, which might be most of what I had intended to write about.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>In this recent <a href=\"/r/discussion/lw/2z8/what_is_the_archimedean_point_of_morality/\">discussion post</a>, new user draq points out that while their discussion posts have been substantially downvoted, they largely does not know why or what is wrong with these posts. I want to offer a few suggestions why a new user might find their posts being downvoted, but I should first note that many of the participants here have read a substantial portion of the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">sequences</a>. This is the reason why you will occasionally see jargon-sounding terminology being used. Every so often a new (usually argumentative) user will come along and levy the charge of \"groupthink\" at LW, but this is merely a case of a group of people with shared knowledge/beliefs inventing shorthand for those ideas. To fully interact with the community here, you will unfortunately need to go through at least a good portion of the <a href=\"http://wiki.lesswrong.com/wiki/Sequences#Core_Sequences\">core sequences</a>. Onto the troubleshooting:</p>\n<p>&nbsp;</p>\n<p><strong>1.&nbsp; Map and territory</strong></p>\n<p>Here at LW, the dominant metaphor for talking about the nature of <a href=\"http://wiki.lesswrong.com/wiki/Belief\">beliefs</a> is as a <a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">map-territory relation</a>. The essence of this idea can be summarized as follows:</p>\n<ul>\n<li>territory = The Universe </li>\n<li>map = your beliefs</li>\n</ul>\n<p>There are important omissions here, like defining \"The Universe,\" but I'm not a specialist in these issues and that's not the point anyways. The point is we have to start somewhere, and that place is the idea that there's a universe and you and I and everyone else live there.</p>\n<p>It's important to note that your beliefs about the world <a href=\"/lw/oi/mind_projection_fallacy/\">exist only in your head</a>, and they aren't very useful at all if they don't correspond to something that we can verify through observation. That's why the only useful kind of beliefs are those that make you <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">anticipate</a> the world being in some set of states and not in others. For example, if I believe that \"consciousness is emergent,\" yet I have no idea what would change in the world if \"consciousness\" were not \"emergent,\" then I really don't have a belief about anything at all; I just have a bunch of words strung together that seem to refer to something. Which brings us to the next point:</p>\n<p>&nbsp;</p>\n<p><strong>2. Words are tricky</strong></p>\n<p>You may be suffering from the <a href=\"http://wiki.lesswrong.com/wiki/Illusion_of_transparency\">illusion</a> that people can understand what you mean. If you have done a great deal of thinking on your own, you may find that some of the words you use conceal all sorts of <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">different meanings</a> that others might not share. You should be wary of <a href=\"/lw/od/37_ways_that_words_can_be_wrong\">all the ways </a>that words can confuse your thinking.</p>\n<p>&nbsp;</p>\n<p><strong>3. Taboo your words!</strong></p>\n<p>There's a delightful game we like to play here, and it's called <a href=\"http://wiki.lesswrong.com/wiki/Rationalist_taboo\">Rationalist taboo</a>. Essentially, when we find ourselves disagreeing with one another and the source of conflict can't be easily found, it's typically the case that someone has an <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">extra clump of meaning</a> attached to one of their words that the other participant didn't have. Tabooing key words is an extremely powerful method for <a href=\"/lw/np/disputing_definitions/\">resolving arguments</a> and for debugging your own thinking.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E8Rpx4oBWCARkEkZP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": -3, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "3862", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p><strong>Important Question:</strong> does a guide like this already exist? I couldn't find one, but it's hard to believe there's no guide to not getting downvoted...Supposing there isn't, please give me suggestions on what such a guide needs, because I'm sort of just guessing here.</p>\n<p><strong>Edit: </strong>Found <a href=\"/lw/2bu/your_intuitions_are_not_magic/\">Your intuititions are not magic</a>, which might be most of what I had intended to write about.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>In this recent <a href=\"/r/discussion/lw/2z8/what_is_the_archimedean_point_of_morality/\">discussion post</a>, new user draq points out that while their discussion posts have been substantially downvoted, they largely does not know why or what is wrong with these posts. I want to offer a few suggestions why a new user might find their posts being downvoted, but I should first note that many of the participants here have read a substantial portion of the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">sequences</a>. This is the reason why you will occasionally see jargon-sounding terminology being used. Every so often a new (usually argumentative) user will come along and levy the charge of \"groupthink\" at LW, but this is merely a case of a group of people with shared knowledge/beliefs inventing shorthand for those ideas. To fully interact with the community here, you will unfortunately need to go through at least a good portion of the <a href=\"http://wiki.lesswrong.com/wiki/Sequences#Core_Sequences\">core sequences</a>. Onto the troubleshooting:</p>\n<p>&nbsp;</p>\n<p><strong id=\"1___Map_and_territory\">1.&nbsp; Map and territory</strong></p>\n<p>Here at LW, the dominant metaphor for talking about the nature of <a href=\"http://wiki.lesswrong.com/wiki/Belief\">beliefs</a> is as a <a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">map-territory relation</a>. The essence of this idea can be summarized as follows:</p>\n<ul>\n<li>territory = The Universe </li>\n<li>map = your beliefs</li>\n</ul>\n<p>There are important omissions here, like defining \"The Universe,\" but I'm not a specialist in these issues and that's not the point anyways. The point is we have to start somewhere, and that place is the idea that there's a universe and you and I and everyone else live there.</p>\n<p>It's important to note that your beliefs about the world <a href=\"/lw/oi/mind_projection_fallacy/\">exist only in your head</a>, and they aren't very useful at all if they don't correspond to something that we can verify through observation. That's why the only useful kind of beliefs are those that make you <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">anticipate</a> the world being in some set of states and not in others. For example, if I believe that \"consciousness is emergent,\" yet I have no idea what would change in the world if \"consciousness\" were not \"emergent,\" then I really don't have a belief about anything at all; I just have a bunch of words strung together that seem to refer to something. Which brings us to the next point:</p>\n<p>&nbsp;</p>\n<p><strong id=\"2__Words_are_tricky\">2. Words are tricky</strong></p>\n<p>You may be suffering from the <a href=\"http://wiki.lesswrong.com/wiki/Illusion_of_transparency\">illusion</a> that people can understand what you mean. If you have done a great deal of thinking on your own, you may find that some of the words you use conceal all sorts of <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">different meanings</a> that others might not share. You should be wary of <a href=\"/lw/od/37_ways_that_words_can_be_wrong\">all the ways </a>that words can confuse your thinking.</p>\n<p>&nbsp;</p>\n<p><strong id=\"3__Taboo_your_words_\">3. Taboo your words!</strong></p>\n<p>There's a delightful game we like to play here, and it's called <a href=\"http://wiki.lesswrong.com/wiki/Rationalist_taboo\">Rationalist taboo</a>. Essentially, when we find ourselves disagreeing with one another and the source of conflict can't be easily found, it's typically the case that someone has an <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">extra clump of meaning</a> attached to one of their words that the other participant didn't have. Tabooing key words is an extremely powerful method for <a href=\"/lw/np/disputing_definitions/\">resolving arguments</a> and for debugging your own thinking.</p>", "sections": [{"title": "1.\u00a0 Map and territory", "anchor": "1___Map_and_territory", "level": 1}, {"title": "2. Words are tricky", "anchor": "2__Words_are_tricky", "level": 1}, {"title": "3. Taboo your words!", "anchor": "3__Taboo_your_words_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Psp8ZpYLCDJjshpRb", "evvJKR6Gyf6qMo4Ew", "ZTRiSNmeGQK8AkdN2", "a7n8GdKiAZRX86T5A", "FaJaCgqBKphrDzDSj", "yA4gF5KrboK2m2Xu7", "7X2j8HAkWdmMoS8PE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-30T03:25:24.493Z", "modifiedAt": null, "url": null, "title": "Ben Goertzel: The Singularity Institute's Scary Idea (and Why I Don't Buy It)", "slug": "ben-goertzel-the-singularity-institute-s-scary-idea-and-why-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:24.263Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RfeAx2fXQWyDiDboj/ben-goertzel-the-singularity-institute-s-scary-idea-and-why-0", "pageUrlRelative": "/posts/RfeAx2fXQWyDiDboj/ben-goertzel-the-singularity-institute-s-scary-idea-and-why-0", "linkUrl": "https://www.lesswrong.com/posts/RfeAx2fXQWyDiDboj/ben-goertzel-the-singularity-institute-s-scary-idea-and-why-0", "postedAtFormatted": "Saturday, October 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ben%20Goertzel%3A%20The%20Singularity%20Institute's%20Scary%20Idea%20(and%20Why%20I%20Don't%20Buy%20It)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABen%20Goertzel%3A%20The%20Singularity%20Institute's%20Scary%20Idea%20(and%20Why%20I%20Don't%20Buy%20It)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRfeAx2fXQWyDiDboj%2Fben-goertzel-the-singularity-institute-s-scary-idea-and-why-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ben%20Goertzel%3A%20The%20Singularity%20Institute's%20Scary%20Idea%20(and%20Why%20I%20Don't%20Buy%20It)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRfeAx2fXQWyDiDboj%2Fben-goertzel-the-singularity-institute-s-scary-idea-and-why-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRfeAx2fXQWyDiDboj%2Fben-goertzel-the-singularity-institute-s-scary-idea-and-why-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://multiverseaccordingtoben.blogspot.com/2010/10/singularity-institutes-scary-idea-and.html\">http://multiverseaccordingtoben.blogspot.com/2010/10/singularity-institutes-scary-idea-and.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RfeAx2fXQWyDiDboj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 6.401685176222364e-07, "legacy": true, "legacyId": "3863", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-30T05:31:43.667Z", "modifiedAt": null, "url": null, "title": "Currently Buying AdWords for LessWrong", "slug": "currently-buying-adwords-for-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:28.489Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KjDuPTgfhYkR9LWHu/currently-buying-adwords-for-lesswrong", "pageUrlRelative": "/posts/KjDuPTgfhYkR9LWHu/currently-buying-adwords-for-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/KjDuPTgfhYkR9LWHu/currently-buying-adwords-for-lesswrong", "postedAtFormatted": "Saturday, October 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Currently%20Buying%20AdWords%20for%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACurrently%20Buying%20AdWords%20for%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKjDuPTgfhYkR9LWHu%2Fcurrently-buying-adwords-for-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Currently%20Buying%20AdWords%20for%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKjDuPTgfhYkR9LWHu%2Fcurrently-buying-adwords-for-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKjDuPTgfhYkR9LWHu%2Fcurrently-buying-adwords-for-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 386, "htmlBody": "<p>So I'm trying to build more rationalists.&nbsp; To do this, I've invested a few hundred dollars of my own money to promote Less Wrong by buying low-cost AdWords on Google for different LW pages.&nbsp; I want to reach smart people with a really good article from Less Wrong that answers their question and draws them into our community so that the site's content can help improve their rationality.&nbsp;&nbsp; Based on buying AdWords before, I'd estimate that only 0.5-1% of people who click through to Less Wrong will actually get involved after reading an article, but since clicks only cost ~$0.04, that means it only costs me ~$6 to build a new rationalist and drastically improve someone's life.&nbsp; Seems like an excellent return on investment.<br /><br />But to get a strong 1% conversion rate and really make an impact, I need to identify REALLY EXCELLENT Less Wrong content.&nbsp; Right now I'm experimenting by buying a lot of keywords related to quantum mechanics and sending people to http://lesswrong.com/lw/r8/and_the_winner_is_manyworlds/<br /><br />My hope is that this page is useful and memorable enough that some small % of readers stick around and click through to other pages.&nbsp; My guess is that this isn't the ideal page to do this with but it's aiming in the right direction.<br /><br />What page would you would want a new Less Wrong reader to find first?&nbsp; What answers a specific question they might have in such an impressive way that they would want to learn more about our community (perhaps many different pages for many different questions)??&nbsp; Which articles are most memorable?&nbsp; Just looking at \"Top\" didn't yield any obvious choices... I felt like most of those articles were too META-META-META ... you'd need too much back knowledge for many of them.&nbsp; An ideally article would be more or less \"stand-alone\" so that any relatively intelligent person who doesn't have the whole LW corpus in their head already could just jump in and understand it immediately... and then branch out and explore LW from there.<br /></p>\n<p>So what do you think?&nbsp; Give me links to any landing pages you think would be worth promoting this way.&nbsp; You can write rough mini-ads as suggestions too if you'd like to be even more helpful.&nbsp; I'm looking forward to hearing your suggestions!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KjDuPTgfhYkR9LWHu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 6.401986605712762e-07, "legacy": true, "legacyId": "3864", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-30T09:31:29.456Z", "modifiedAt": null, "url": null, "title": "Ben Goertzel: The Singularity Institute's Scary Idea (and Why I Don't Buy It) ", "slug": "ben-goertzel-the-singularity-institute-s-scary-idea-and-why", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.920Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/97TCYaiMe4ceRYoXs/ben-goertzel-the-singularity-institute-s-scary-idea-and-why", "pageUrlRelative": "/posts/97TCYaiMe4ceRYoXs/ben-goertzel-the-singularity-institute-s-scary-idea-and-why", "linkUrl": "https://www.lesswrong.com/posts/97TCYaiMe4ceRYoXs/ben-goertzel-the-singularity-institute-s-scary-idea-and-why", "postedAtFormatted": "Saturday, October 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ben%20Goertzel%3A%20The%20Singularity%20Institute's%20Scary%20Idea%20(and%20Why%20I%20Don't%20Buy%20It)%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABen%20Goertzel%3A%20The%20Singularity%20Institute's%20Scary%20Idea%20(and%20Why%20I%20Don't%20Buy%20It)%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F97TCYaiMe4ceRYoXs%2Fben-goertzel-the-singularity-institute-s-scary-idea-and-why%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ben%20Goertzel%3A%20The%20Singularity%20Institute's%20Scary%20Idea%20(and%20Why%20I%20Don't%20Buy%20It)%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F97TCYaiMe4ceRYoXs%2Fben-goertzel-the-singularity-institute-s-scary-idea-and-why", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F97TCYaiMe4ceRYoXs%2Fben-goertzel-the-singularity-institute-s-scary-idea-and-why", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 236, "htmlBody": "<p style=\"padding-left: 30px;\">[...] SIAI's Scary Idea goes way beyond the mere statement that there are risks as well as benefits associated with advanced AGI, and that AGI is a potential existential risk.</p>\n<p style=\"padding-left: 30px;\">[...] Although an intense interest in rationalism is one of the hallmarks of the SIAI community, still I have not yet seen a clear logical argument for the Scary Idea laid out anywhere. (If I'm wrong, please send me the link, and I'll revise this post accordingly. Be aware that I've already at least skimmed everything Eliezer Yudkowsky has written on related topics.)</p>\n<p style=\"padding-left: 30px;\">So if one wants a clear argument for the Scary Idea, one basically has to construct it oneself.</p>\n<p style=\"padding-left: 30px;\">[...] If you put the above points all together, you come up with a heuristic argument for the Scary Idea. Roughly, the argument goes something like: <span style=\"font-style: italic;\">If someone builds an advanced AGI without a provably Friendly architecture, probably it will have a hard takeoff, and then probably this will lead to a superhuman AGI system with an architecture drawn from the vast majority of mind-architectures that are not sufficiently harmonious with the complex, fragile human value system to make humans happy and keep humans around.</span><br /><br />The line of argument makes sense, if you accept the premises.<br /><br />But, I don't.</p>\n<p>Ben Goertzel: <a href=\"http://multiverseaccordingtoben.blogspot.com/2010/10/singularity-institutes-scary-idea-and.html\">The Singularity Institute's Scary Idea (and Why I Don't Buy It)</a>, October 29 2010. Thanks to <strong><a id=\"author_t1_2v4b\" href=\"/user/XiXiDu\">XiXiDu</a></strong> for the pointer<strong>.<br /></strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2, "NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "97TCYaiMe4ceRYoXs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 40, "baseScore": 42, "extendedScore": null, "score": 0.000112, "legacy": true, "legacyId": "3868", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 442, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-30T10:48:13.992Z", "modifiedAt": null, "url": null, "title": "QFT, Homotopy Theory and AI?", "slug": "qft-homotopy-theory-and-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:24.729Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ThomasR", "createdAt": "2010-10-14T08:55:55.042Z", "isAdmin": false, "displayName": "ThomasR"}, "userId": "fGddtQRRjL6WWeDvQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gr7nweNiK6kiR9LZR/qft-homotopy-theory-and-ai", "pageUrlRelative": "/posts/gr7nweNiK6kiR9LZR/qft-homotopy-theory-and-ai", "linkUrl": "https://www.lesswrong.com/posts/gr7nweNiK6kiR9LZR/qft-homotopy-theory-and-ai", "postedAtFormatted": "Saturday, October 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20QFT%2C%20Homotopy%20Theory%20and%20AI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQFT%2C%20Homotopy%20Theory%20and%20AI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgr7nweNiK6kiR9LZR%2Fqft-homotopy-theory-and-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=QFT%2C%20Homotopy%20Theory%20and%20AI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgr7nweNiK6kiR9LZR%2Fqft-homotopy-theory-and-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgr7nweNiK6kiR9LZR%2Fqft-homotopy-theory-and-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p>What do you think about the new, exiting connections between QFT, Homotopy Theory and pattern recognition, proof verification and (maybe) AI systems? In view of the background of this forum's participants (selfreported in the survey mentioned a few days ago), I guess most of you follow those developments with some attention.</p>\n<p>Concerning Homotopy Theory, there is a coming [special year](http://www.math.ias.edu/node/2610), you probably know Voevodsky's [recent intro lecture](http://www.channels.com/episodes/show/10793638/Vladimir-Voevodsky-Formal-Languages-partial-algebraic-theories-and-homotopy-category-), and [this](http://video.ias.edu/voevodsky-80th) even more popular one. Somewhat related are Y.I. Manin's remarks on the missing quotient structures (analogue to localized categories) in data structures and some of the ideas in Gromov's [essay](http://www.ihes.fr/~gromov/PDF/ergobrain.pdf).</p>\n<p>Concerning ideas from QFT, [here](http://arxiv.org/abs/0904.4921) an example. I wonder what else concepts come from it?</p>\n<p>BTW, whereas the public discussion focus on basic qm and on q-gravity questions, the really interesting and open issue is the special relativistic QFT: QM is just a canonical deformation of classical mechanics (and could have been found much earlier, most of the interpretation disputes just come from the confusion of mathematical properties with physics data), but Feynman integrals are despite half a century intense research mathematical unfounded. As Y.I. Manin called it in a recent interview, they are \"an Eifel tower floating in the air\". Only a strong platonian belief makes people tolerate that. I myself take them only serious because there is a clear platonic idea behind them and because number theoretic analoga work very well.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gr7nweNiK6kiR9LZR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -4, "extendedScore": null, "score": 6.402741973924403e-07, "legacy": true, "legacyId": "3869", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-30T17:49:15.846Z", "modifiedAt": null, "url": null, "title": "DRAFT: Three Intellectual Temperaments: Birds, Frogs and Beavers", "slug": "draft-three-intellectual-temperaments-birds-frogs-and", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:30.317Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LcxNFGjPjKAm7wZvs/draft-three-intellectual-temperaments-birds-frogs-and", "pageUrlRelative": "/posts/LcxNFGjPjKAm7wZvs/draft-three-intellectual-temperaments-birds-frogs-and", "linkUrl": "https://www.lesswrong.com/posts/LcxNFGjPjKAm7wZvs/draft-three-intellectual-temperaments-birds-frogs-and", "postedAtFormatted": "Saturday, October 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20DRAFT%3A%20Three%20Intellectual%20Temperaments%3A%20Birds%2C%20Frogs%20and%20Beavers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADRAFT%3A%20Three%20Intellectual%20Temperaments%3A%20Birds%2C%20Frogs%20and%20Beavers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLcxNFGjPjKAm7wZvs%2Fdraft-three-intellectual-temperaments-birds-frogs-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=DRAFT%3A%20Three%20Intellectual%20Temperaments%3A%20Birds%2C%20Frogs%20and%20Beavers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLcxNFGjPjKAm7wZvs%2Fdraft-three-intellectual-temperaments-birds-frogs-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLcxNFGjPjKAm7wZvs%2Fdraft-three-intellectual-temperaments-birds-frogs-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3862, "htmlBody": "<p>Here is a draft of a potential top-level post which I'd welcome feedback on. I would appreciate any suggestions, corrections, additional examples, qualifications, or refinements.</p>\n<p><a id=\"more\"></a></p>\n<p><strong>Birds, Frogs and Beavers</strong></p>\n<p>The introduction of <a href=\"http://www.ams.org/notices/200902/rtx090200212p.pdf\">Birds and Frogs</a><a href=\"http://en.wikipedia.org/wiki/Freeman_Dyson\"></a> by <a href=\"http://en.wikipedia.org/wiki/Freeman_Dyson\">Freeman Dyson</a> reads</p>\n<blockquote>\n<p>Some mathematicians are birds, others are frogs. Birds fly high in the air and survey broad vistas of mathematics out to the far horizon. They delight in concepts that unify our thinking and bring together diverse problems from different parts of the landscape. Frogs live in the mud below and see only the flowers that grow nearby. They delight in the details of particular objects, and they solve problems one at a time. I happen to be a frog, but many of my best friends are birds. The main theme of my talk tonight is this. Mathematics needs both birds and frogs. Mathematics is rich and beautiful because birds give it broad visions and frogs give it intricate details. Mathematics is both great art and important science, because it combines generality of concepts with depth of structures. It is stupid to claim that birds are better than frogs because they see farther, or that frogs are better than birds because they see deeper. The world of mathematics is both broad and deep, and we need birds and frogs working together to explore it.</p>\n</blockquote>\n<p>Dyson is far from the first to have categorized mathematicians in such a fashion. For example, in <a href=\"http://www.dpmms.cam.ac.uk/%7Ewtg10/2cultures.pdf\">The Two Cultures of Mathematics</a> British mathematician <a href=\"http://en.wikipedia.org/wiki/Timothy_Gowers\">Timothy Gowers</a> wrote</p>\n<blockquote>\n<p>The \"two cultures\" I wish to discuss will be familiar to all professional mathematicians. Loosely speaking, I mean the distinction between mathematicians who regard their central aim as being to solve problems, and those who are more concerned with building and understanding theories. This difference of attitude has been remarked on by many people, and I do not claim any credit for noticing it. As with most categorizations, it involves a certain oversimplication, but not so much as to make it useless. If you are unsure to which class you belong, then consider the following two statements.</p>\n<p>(i) The point of solving problems is to understand mathematics better.<br />(ii) The point of understanding mathematics is to become better able to solve problems.</p>\n<p>Most mathematicians would say that there is truth in both (i) and (ii). Not all problems are equally interesting, and one way of distinguishing the more interesting ones is to demonstrate that they improve our understanding of mathematics as a whole. Equally, if somebody spends many years struggling to understand a difficult area of mathematics, but does not actually do anything with this understanding, then why should anybody else care? However, many, and perhaps most, mathematicians will not agree equally strongly with the two statements.</p>\n</blockquote>\n<p>Similarly, <a href=\"http://en.wikipedia.org/wiki/Gian-Carlo_Rota\">Gian Carlo Rota</a>'s candid <a href=\"http://www.amazon.com/Indiscrete-Thoughts-Gian-Carlo-Rota/dp/0817638660\">Indiscrete Thoughts</a> contains an essay titled<em> Problem Solvers and Theorizers</em> which draws a similar dichotomy:</p>\n<blockquote>\n<p>Mathematicians can be subdivided into two types: problem solvers and theorizer. Most mathematicians are a mixture of the two although it is easy to find extreme examples of both types.<br /> <br />To the problem solver, the supreme achievement in mathematics is the solution to a problem that had been given up as hopeless. It matters little that the solution may be clumsy; all that counts is that it should be the first and that the proof be correct. Once the problem solver finds the new solution, he will permanently lose interest in it, and will listen to new and simplified proofs with an air of condescension and boredom.<br /> <br />The problem solver is a conservative at heart. For him, mathematics consists of a sequence of challenges to be met, an obstacle course of problems. The mathematical concepts required to state mathematical problems are tacitly assumed to be eternal and immutable.<br /> <br />Mathematical exposition is regarded as an inferior undertaking. New theories are viewed with deep suspicion, as intruders who must prove their worth by posing challenging problems before they can gain attention. The problem solver resents generalizations, especially those that may succeed in trivializing the solution to one of his problems.<br /> <br />The problem solver is the role model for budding young mathematicians. When we describe to the public the conquests of mathematics, our shining heroes are the problem solvers.<br /><br />To the theorizer, the supreme achievement of mathematics is a theory that sheds sudden light on some incomprehensible phenomenon. Success in mathematics does not lie in solving problems but in their trivialization. The moment of glory comes with the discovery of a new theory that does not solve any of the old problems but renders them irrelevant.<br /> <br />The theorizer is a revolutionary at heart. Mathematical concepts received from the past are regarded as imperfect instances of more general ones yet to be discovered. Mathematical exposition is considered a more difficult undertaking than mathematical research.<br /> <br />To the theorizer, the only mathematics that will survive are the definitions. Great definitions are what mathematics contributes to the world. Theorems are tolerated as a necessary evil since they play a supporting role - or rather, as the theorizer will reluctantly admit, an essential role - in the understanding of the definitions.<br /> <br />Theorizers often have trouble being recognized by the community of mathematicians. Their consolation is the certainty, which may or may not be borne out by history, that their theories will survive long after the problems of the day have been forgotten.<br /> <br />If I were a space engineer looking for a mathematician to help me send a rocket into space, I would choose a problem solver. But if I were looking for a mathematician to give a good education to my child, I would unhesitatingly prefer a theorizer.</p>\n</blockquote>\n<p>I believe that Rota's characterizations of problem solvers and theorizers are exaggerated but nevertheless in the right general direction. Rota's remarks are echoed in Colin McLarty's: <a href=\"http://www.math.jussieu.fr/%7Eleila/grothendieckcircle/mclarty1.pdf\">The Rising Tide: Grothendieck on simplicity and generality</a></p>\n<blockquote>\n<p>Grothendieck describes two styles in mathematics. If you think of a theorem to be proved as a nut to be opened, so as to reach &ldquo;the nourishing flesh protected by the shell&rdquo;, then the hammer and chisel principle is: &ldquo;put the cutting edge of the chisel against the shell and strike hard. If needed, begin again at many different points until the shell cracks&mdash;and you are satisfied&rdquo;. He says:</p>\n<p>&ldquo;I can illustrate the second approach with the same image of a nut to be opened. The first analogy that came to my mind is of immersing the nut in some softening liquid, and why not simply water? From time to time you rub so the liquid penetrates better, and otherwise you let time pass. The shell becomes more flexible through weeks and months&mdash;when the time is ripe, hand pressure is enough, the shell opens like a perfectly ripened avocado!</p>\n<p>A different image came to me a few weeks ago. The unknown thing to be known appeared to me as some stretch of earth or hard marl, resisting penetration. . . the sea advances insensibly in silence, nothing seems to happen, nothing moves, the water is so far off you hardly hear it. . . yet it finally surrounds the resistant substance.&rdquo;</p>\n<p>[...]</p>\n<p>Deligne describes a characteristic Grothendieck proof as a long series of trivial steps where &ldquo;nothing seems to happen, and yet at the end a highly non-trivial theorem is there.&rdquo;</p>\n</blockquote>\n<p>In addition to the sources cited above, Grothendieck discusses a dichotomy which resembles that of birds and frogs in the section of <em>Recoltes et Semailles</em> titled <em>The Inheritors and the Builders</em> and Lee Smolin discusses such a dicotomy in <a href=\"http://www.amazon.com/Trouble-Physics-String-Theory-Science/dp/0618551050\">The Trouble With Physics</a> Chapter 18.</p>\n<p>In his <a href=\"http://www.math.rutgers.edu/~zeilberg/Opinion95.html\">Opinion 95</a>, <a href=\"http://en.wikipedia.org/wiki/Doron_Zeilberger\">Doron Zeilberger</a> added a supplement to Dyson's classification, saying:</p>\n<blockquote>\n<p>I agree that both frogs and birds are crucial for the progress of science, but, even more important, for the progress of mathematics in the computer age, is the <strong>beaver</strong>, who will build the needed infrastructre of computer mathematics, that would eventually enable us to solve many outstanding open problems, and many new ones. Consequently, the developers of computer algebra systems, and creators of algorithms, are even more important than both birds and frogs.</p>\n</blockquote>\n<p>Zeilberger's statement that beavers are more important for the process of science than birds and frogs is debatable and I do not endorse it; but I believe that Zeilberger is correct to identify a third category consisting of people whose primary interest is in algorithms. Indeed, as Laurens Gunnarsen recently pointed out to me, Felix Klein had already identified such a category in his 1908 lectures on <a href=\"http://books.google.com/books?id=vfSutjEIZXkC&amp;lpg=PR2&amp;ots=4H9WZSYK1S&amp;dq=Elementary%20Mathematics%20from%20an%20Advanced%20Standpoint%3A%20Arithmetic%2C%20Algebra%20publication%20date&amp;pg=PA77#v=onepage&amp;q&amp;f=false\">Elementary Mathematics from an Advanced Standpoint: Arithmetic, Algebra and Analysis</a>. In the section titled <em>Concerning the Modern Development and the General Structure of Mathematics</em>, Klein identified three plans A, B, and C roughly corresponding to the natural activities of frogs, birds and beavers respectively:</p>\n<blockquote>\n<p>&hellip;we might say that Plan A is based on a more particularistic conception of science which divides the total field into a series of mutually separated parts and attempts to develop each part for itself, with a minimum of resources and with all possible avoidance of borrowing from neighboring fields. Its ideal is to crystallize out each of the partial fields into a logically closed system. On the contrary, the supporter of Plan B lays the chief stress upon the organic combination of the partial fields, and upon the stimulation which these exert one upon another. He prefers, therefore, the methods which open for him an understanding of several fields under a uniform point of view. His ideal is the comprehension of the sum total of mathematical science as a great connected whole.<br /> <br />[&hellip;]<br /><br />For a complete understanding of the development of mathematics, we must, however, think of a still third plan C, which, along side of and within the processes of development A and B, often plays an important role. It has to do with a method which one denotes by the word algorithm, derived from a mutilated form of an Arabian mathematician. All ordered formal calculation is, at bottom, algorithmic, in particular, the calculation with letters is an algorithm. We have repeatedly emphasized what an important part in the development of the science has been played by the algorithmic process, as a quasi-independent, onward-driving force, inherent in the formulas, operating apart from the intension and insight of the mathematician, at the time, often in opposite to them. In the beginning of the infinitesimal calculus, as we shall see later on, the algorithm has often forced new notions and operations, even before one could justify their admissibility. Even at higher levels of the development, these algorithmic considerations can be, and actually have been, very fruitful, so that one can justly call them the groundwork of mathematical development. We must then completely ignore history, if, as is sometimes done today, we cast these circumstances contemptuously aside as mere \"formal\" developments.</p>\n</blockquote>\n<p>The three categories described above appear to have correlates of personality traits, mathematical interests and superficially nonmathematical interests. Below I'll engage in some speculation about this.</p>\n<p><strong>Correlates of the bird category</strong></p>\n<p>My impression is that birds tend to have high <a href=\"http://en.wikipedia.org/wiki/Openness_to_experience\">openness to experience</a>, be anti-conformist, highly emotional sensitivity, and interested in high art, history, philosophy, religion and geometry. Here I'll give some supporting evidence. I believe that the thinkers discussed would identify themselves as birds.</p>\n<p>1. Dyson's article discusses <a href=\"http://en.wikipedia.org/wiki/Yuri_I._Manin\">Yuri Manin</a> as follows:</p>\n<blockquote>\n<p>The book is mainly about mathematics. It may come as a surprise to Western readers that he writes with equal eloquence about other subjects such as the collective unconscious, the origin of human language, the psychology of autism, and the role of the trickster in the mythology of many cultures.</p>\n<p>[...]</p>\n<p>Manin is a bird whose vision extends far beyond the territory of mathematics into the wider landscape of human culture. One of his hobbies is the theory of archetypes invented by the Swiss psychologist Carl Jung. An archetype, according to Jung, is a mental image rooted in a collective unconscious that we all share. The intense emotions that archetypes carry with them are relics of lost memories of collective joy and suffering. Manin is saying that we do not need to accept Jung&rsquo;s theory as true in order to find it illuminating.</p>\n</blockquote>\n<p>2. In <a href=\"http://www.amazon.com/Trouble-Physics-String-Theory-Science/dp/0618551050\">The Trouble With Physics</a> Lee Smolin writes of \"Seers\" who have something in common with Dyson's \"Birds\":</p>\n<blockquote>\n<p>Seers are very different. They are dreamers. They go to into science because they have questions about the nature of existence that their schoolbooks don't answer. If they weren't scientists, they might be artists or writers or they might end up in divinity school.</p>\n<p>[...]<br /><br />A common complaint of the seers is that the standard education in physics ignores the historical and philosophical context in which science develops. As Einstein said in a letter to a young physicist who had been thwarted in his attempts to add philosophy to his physics courses:<br /><br />\"I fully agree with you about the significance and educational value of methodology as well as history and philosophy of science. So many people today - and even professional scientists - seem to me like someone who has seen thousands of trees but has never seen a forest. A knowledge of the historical and philosophical background gives that kind of independence from prejudices of his generation from which most scientists are suffering.\"</p>\n</blockquote>\n<p>3. <a href=\"http://golem.ph.utexas.edu/category/2010/01/the_sacred_and_the_profane.html#c031203\">Thomas remarks</a> that Yuri Manin has written about how \"mathematics chooses us\" and \"emotional platonism\" which are characteristic of shamanism and that the number theorist <a href=\"http://en.wikipedia.org/wiki/Kazuya_Kato\">Kazuya Kato</a> writes \"Mysterious properties of zeta values seem to tell us (in a not so loud voice) that our universe has the same properties: The universe is not explained just by real numbers. It has p-adic properties &hellip; We ourselves may have the same properties\" which fits into the shamanistic way of thinking (\"knowing something is becoming it\").</p>\n<p>4. In his autobiography titled <a href=\"http://www.amazon.com/Apprenticeship-Mathematician-Andre-Weil/dp/0817626506\">The Apprentice of a Mathematician</a>, <a href=\"http://en.wikipedia.org/wiki/Andr%C3%A9_Weil\">Andre Weil</a> wrote about how he was heavily influenced by Hindu thought and studied Sanskrit and mystic Hindu poetry.</p>\n<p>5. According to <a href=\"http://www.ams.org/notices/200409/fea-grothendieck-part1.pdf\">Allyn Jackson's article</a> on <a href=\"http://en.wikipedia.org/wiki/Alexander_Grothendieck\">Alexander Grothendieck</a>:</p>\n<blockquote>\n<p>Honig once asked Grothendieck why he had gone into mathematics. Grothendieck replied that he had two special passions, mathematics and piano, but he chose mathematics because he thought it would be easier to earn a living that way. His gift for mathematics was so abundantly clear, said Honig, &ldquo;I was astonished that at any moment he could hesitate between mathematics and music.&rdquo;</p>\n</blockquote>\n<p>Grothendieck's interest in music is corroborated by <a href=\"http://www.math.uchicago.edu/%7Emitya/langlands/reminiscences1.pdf\">Luc Illusie</a> who said:</p>\n<blockquote>\n<p>Grothendieck had a very strong feeling for music. He liked Bach and his most beloved pieceswere the last quartets by Beethoven.</p>\n</blockquote>\n<p>According to Winifred Scharlau's <a href=\"http://www.ams.org/notices/200808/tx080800930p.pdf\">Who is Alexander Grothendieck?</a></p>\n<blockquote>\n<p>From 1974 Grothendieck turned to Buddhism; several times he was visited by Japanese monks from the order Nipponzan Myohoji (in English the name translates roughly as &ldquo;Japanese community of the wonderful lotus sutra&rdquo;), which preaches strict nonviolence and erects peace pagodas throughout the world. But his attachment to Buddhism did not last. From around 1980 Grothendieck gravitated toward Christian mystical and esoteric ideas.</p>\n</blockquote>\n<p>and</p>\n<blockquote>\n<p>It appears that he thoroughly worked through, for example, Freud&rsquo;s Traumdeutung (The Interpretation of Dreams) and also read other relevant literature.</p>\n</blockquote>\n<p>6. According to Frank Wilczek's Introduction to <a href=\"http://www.amazon.com/Philosophy-Mathematics-Natural-Science-Hermann/dp/0689702078#reader_0689702078\">Philosophy of Mathematics and Natural Sciences</a> by <a href=\"http://en.wikipedia.org/wiki/Hermann_Weyl\">Hermann Weyl</a>,</p>\n<blockquote>\n<p>In his preface Weyl says, \"I was also bound, though less consciously, by the German literary and philosophical tradition in which I had grown up\" (xv). It was in fact a cosmopolitan tradition, of which <em>Philosophy of Mathematics and Natural Science </em>might be the last great expression. Decartes, Leibniz, Hume, and Kant are taken as familiar friends and interlocutors. Weyl's eruidition is, implicitly, a touching affirmation of a community of mind and inquiry stretching across time and space, and progressing through experience, reflection, and open dialogue.</p>\n</blockquote>\n<p>7. In Robert Langlands' <a href=\"http://www.math.duke.edu/langlands/\">Lectures on the Practice of Mathematics</a> and <a href=\"http://publications.ias.edu/sites/default/files/ND.pdf\">Is There Beauty in Mathematical Theories?</a>, Langlands discusses the history of mathematics at length and quotes <a href=\"http://en.wikipedia.org/wiki/Rainer_Maria_Rilke\">Rainer Maria Rilke</a>, and <a href=\"http://en.wikipedia.org/wiki/Rudyard_Kipling\">Rudyard Kipling</a>.</p>\n<p>8. Some examples of famous birds who identify as geometers in a broad sense are Bernhard Riemann, Henri Poincare, Felix Klein, Elie Cartan, Andre Weil, Shiing-Shen Chern, Alexander Grothendieck, Raoul Bott, Friedrich Hirzebruch, Michael Atiyah, Yuri Manin, Barry Mazur, Alain Connes, Bill Thurston, Mikhail Gromov.</p>\n<p><strong>Correlates of the frog category</strong></p>\n<p>My impression is that frogs tend to be highly detail-oriented, conservative (in the sense that Rota describes), have a good memory of lots of facts, high technical prowess, ability to focus on a problem a very long time, and be interested in areas of math like elementary and analytic number theory, analysis, group theory and combinatorics. Here I'll give some supporting evidence. I believe that the thinkers discussed would identify themselves as frogs.</p>\n<p>1. The conservative quality of frogs that Rota alludes to is negatively correlated with <a href=\"http://en.wikipedia.org/wiki/Openness_to_experience\">openness to experience</a>. For an example of a conservative frog, I would cite <a href=\"http://www-groups.dcs.st-and.ac.uk/%7Ehistory/Biographies/Davenport.html\">Harold Davenport</a>:</p>\n<blockquote>\n<p>Davenport was a natural conservative. \"All change is for the worse\" he used to say with complete conviction. He was entirely out of sympathy with the waves of change in the teaching of mathematics but accepted them as an inevitable evil. Selective in his enjoyment of modern technology, he never entered an aeroplane, would use a lift if no alternative existed (at the International Congress in Moscow he trudged up and down the interminable stairs of Stalin's skyscraper), and preferred to send his papers for publication written in his characteristically neat hand.</p>\n</blockquote>\n<p>Davenport's frog aesthetic comes across in his remark</p>\n<blockquote>\n<p>Great mathematics is achieved by solving difficult problems not by fabricating elaborate theories in search of a problem.</p>\n</blockquote>\n<p>2. The <a href=\"http://en.wikipedia.org/wiki/Feit%E2%80%93Thompson_theorem\">Odd Order Theorem</a> in finite group theory is a seminal result which was proved by the two frogs <a href=\"http://en.wikipedia.org/wiki/Walter_Feit\">Walter Feit</a> and <a href=\"http://en.wikipedia.org/wiki/John_G._Thompson\">John Thompson</a>. One of my friends who did his PhD in finite group theory said that understanding <em>a single line</em> of their 250 page proof requires a serious effort. In <a href=\"http://webcache.googleusercontent.com/search?q=cache:nUy7HDj8J1UJ:sps.nus.edu.sg/~limchuwe/articles/serre.html+http://sps.nus.edu.sg/~limchuwe/articles/serre.html&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us&amp;client=firefox-a\">a 1985 interview</a>, <a href=\"http://en.wikipedia.org/wiki/Jean-Pierre_Serre\">Jean-Pierre Serre</a> said</p>\n<blockquote>\n<p>Chevalley once tried to take this as the topic of a seminar, with the idea of giving a complete account of the proof. After two years, he had to give up.</p>\n</blockquote>\n<p><a href=\"http://en.wikipedia.org/wiki/Claude_Chevalley\">Claude Chevalley</a> was an outstandingly good mathematician. I read the fact that somebody of such high caliber had much trouble as he did with the proof as an indication of Feit and Thompson having unusually high technical prowess and ability to focus on a single problem for a long time even <em>relative to other remarkable mathematicians</em>. This is counterbalanced by the mathematical output of Feit and Thompson was essentially restricted to the topic of finite groups theory in contrast with that of many mathematicians who have broader interests.</p>\n<p>3. The identification of combinatorics as a mathematical field populated by problem solvers comes across in Gowers' essay linked above. The subject of elementary number theory was very heavily influenced by Erdos who has been labeled a canonical problem solver. In <a href=\"http://www.math.harvard.edu/~elkies/M259.98/intro.ps\">the introduction</a> to a course in analytic number theory, <a href=\"http://en.wikipedia.org/wiki/Noam_Elkies\">Noam Elkies</a> wrote</p>\n<blockquote>\n<p>It has often been said that there are two kinds of mathematicians: theory builders and problem solvers. In the mathematics of our century these two styles are epitomized respectively by A. Grothendieck and P. Erdos [...] analytic number theory as usually practiced falls in the problem-solving camp.</p>\n</blockquote>\n<p>Two of the founders of the mathematical field of analysis, namely <a href=\"http://en.wikipedia.org/wiki/Augustin-Louis_Cauchy\">Cauchy</a> and <a href=\"http://en.wikipedia.org/wiki/Karl_Weierstrass\">Weierstrass</a> were frogs. Klein writes</p>\n<blockquote>\n<p>...in the twenties Cauchy (1789-1857) developed, in lectures and in books, the first rigorous foundings of infinitesimal calculus in the modern sense. He not only gives an exact definition of the differential quotient, and of the integral, by means of the limit of a finite quotient and of a finite sum, respectively, as had previously been done, at times; but, by means of the mean-value theorem he erects upon this, for the first time, a consistent structure for the infinitesimal calculus [ ...] These theories also partake of the nature of plan A, since they work overthe field in a logical systematic way, quite apart from other branches of knowledge.</p>\n<p>[...]</p>\n<p>From the middle of the century on, the method of thought A comes again to the front with Weierstrass (1815-1897) [...] I have already investigated Weierstrass function theory as an example of A.</p>\n</blockquote>\n<p>Many of the prominent contemporary analysts like <a href=\"http://en.wikipedia.org/wiki/John_Forbes_Nash,_Jr.\">John Nash</a> and <a href=\"http://en.wikipedia.org/wiki/Grigori_Perelman\">Grigori Perelman</a> are problem solvers.</p>\n<p><strong>Correlates of the beaver category</strong></p>\n<p>My impression is that beavers tend to be interested in jigsaw puzzles, word puzzles, logic puzzles, board games like Go, sorting tasks, algorithms, computational complexity, logic, respond best to a stream of immediate feedback in the way of tangible progress and can have trouble focusing on learning mathematical subjects in ways that require a lot of development before one engages in computation. Many computer scientists seem to me to fall into the beaver paradigm. Here I'm on shakier ground as I've seen little public discussion of beavers and most of what I've observed that supports my impression is born of subjective experience with people who I know, but I'll try to give some examples that seem to me to fall into the beaver paradigm:</p>\n<p>1. <a href=\"http://en.wikipedia.org/wiki/Doron_Zeilberger\">Doron Zeilberger</a>'s focus on algorithmics, ultrafinitism and constructivist mathematics.</p>\n<p>2. <a href=\"http://math.nyu.edu/faculty/edwardsd/\">Harold Edwards</a>' focus on constructivist mathematics (which comes across in his books titled <em>Higher Arithmetic: An Algorithmic Introduction to Number Theory</em>, <em>Essays in Constructive Mathematics, Galois Theory, Fermat's Last Theorem </em>and <em>Divisor Theory</em>) is in the beaver paradigm.</p>\n<p>3. <a href=\"http://terrytao.wordpress.com/2008/02/05/the-blue-eyed-islanders-puzzle/\">Terence Tao</a>'s interest in logic puzzles like the <a href=\"http://terrytao.wordpress.com/2008/02/05/the-blue-eyed-islanders-puzzle/\">blue-eyed islanders puzzle</a> and his interest in <a href=\"http://terrytao.wordpress.com/2007/06/25/ultrafilters-nonstandard-analysis-and-epsilon-management/\">ultrafilters and nonstandard analysis</a>.</p>\n<p>4. The work of <a href=\"http://en.wikipedia.org/wiki/Jonathan_Borwein\">Jonathan Borwein</a> and <a href=\"http://en.wikipedia.org/wiki/Peter_Borwein\">Peter Borwein</a> computing a billion digits of pi.</p>\n<p>5. The computational work of historically great mathematicians like Newton Euler, Gauss, Jacobi, and Ramanujan.</p>\n<p>6. Don Zagier's remark in his essay in Mariana Cook's book</p>\n<blockquote>\n<p>Even now I am not a modern mathematician and very abstract ideas are unnatural for me. I have of course learned to work with them but I haven't really internalized it and remain a concrete mathematician. I like explicit, hands-on formulas. To me they have a beauty of their own. They can be deep or not.</p>\n</blockquote>\n<p>7. The focus on explicit formulae in the area of <a href=\"http://mathworld.wolfram.com/q-Series.html\">q-series</a>.</p>\n<p>8. Interest in <a href=\"http://en.wikipedia.org/wiki/Newcomb%27s_paradox\">Newcomb's Problem</a> and its variations.</p>\n<p>9. <a href=\"http://en.wikipedia.org/wiki/Scott_Aaronson\">Scott Aaronson</a>'s interest in computational complexity, algorithms, and questions between logic and algorithmics as reflected in his MathOverflow post <a href=\"http://mathoverflow.net/questions/34710/succinctly-naming-big-numbers-zfc-versus-busy-beaver\">Succinctly naming big numbers: ZFC versus Busy-Beaver</a>.</p>\n<p><strong>To Be Continued</strong></p>\n<p>In a future post I will describe superficial similarities and superficial differences between the three types and misunderstandings&nbsp; between different types which arise from <a href=\"/lw/dr/generalizing_from_one_example/\">generalizing from one example</a> and cultural differences. Regarding his mastercraftspeople/seers dicotomy, Lee Smolin says</p>\n<blockquote>\n<p>It is only to be expected that members of these two groups misunderstand and mistrust each other.</p>\n</blockquote>\n<p>Timothy Gowers writes about how there's a schism between his two categories of mathematicians and says</p>\n<blockquote>\n<p>this is not an entirely healthy state of a ffairs.</p>\n</blockquote>\n<p>As Dyson and Zeilberger said, all three types are important to scientific progress. I believe that intellectual progress will be increase if the three types can learn to better understand each other.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LcxNFGjPjKAm7wZvs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 6.403747048902464e-07, "legacy": true, "legacyId": "3859", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Here is a draft of a potential top-level post which I'd welcome feedback on. I would appreciate any suggestions, corrections, additional examples, qualifications, or refinements.</p>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"Birds__Frogs_and_Beavers\">Birds, Frogs and Beavers</strong></p>\n<p>The introduction of <a href=\"http://www.ams.org/notices/200902/rtx090200212p.pdf\">Birds and Frogs</a><a href=\"http://en.wikipedia.org/wiki/Freeman_Dyson\"></a> by <a href=\"http://en.wikipedia.org/wiki/Freeman_Dyson\">Freeman Dyson</a> reads</p>\n<blockquote>\n<p>Some mathematicians are birds, others are frogs. Birds fly high in the air and survey broad vistas of mathematics out to the far horizon. They delight in concepts that unify our thinking and bring together diverse problems from different parts of the landscape. Frogs live in the mud below and see only the flowers that grow nearby. They delight in the details of particular objects, and they solve problems one at a time. I happen to be a frog, but many of my best friends are birds. The main theme of my talk tonight is this. Mathematics needs both birds and frogs. Mathematics is rich and beautiful because birds give it broad visions and frogs give it intricate details. Mathematics is both great art and important science, because it combines generality of concepts with depth of structures. It is stupid to claim that birds are better than frogs because they see farther, or that frogs are better than birds because they see deeper. The world of mathematics is both broad and deep, and we need birds and frogs working together to explore it.</p>\n</blockquote>\n<p>Dyson is far from the first to have categorized mathematicians in such a fashion. For example, in <a href=\"http://www.dpmms.cam.ac.uk/%7Ewtg10/2cultures.pdf\">The Two Cultures of Mathematics</a> British mathematician <a href=\"http://en.wikipedia.org/wiki/Timothy_Gowers\">Timothy Gowers</a> wrote</p>\n<blockquote>\n<p>The \"two cultures\" I wish to discuss will be familiar to all professional mathematicians. Loosely speaking, I mean the distinction between mathematicians who regard their central aim as being to solve problems, and those who are more concerned with building and understanding theories. This difference of attitude has been remarked on by many people, and I do not claim any credit for noticing it. As with most categorizations, it involves a certain oversimplication, but not so much as to make it useless. If you are unsure to which class you belong, then consider the following two statements.</p>\n<p>(i) The point of solving problems is to understand mathematics better.<br>(ii) The point of understanding mathematics is to become better able to solve problems.</p>\n<p>Most mathematicians would say that there is truth in both (i) and (ii). Not all problems are equally interesting, and one way of distinguishing the more interesting ones is to demonstrate that they improve our understanding of mathematics as a whole. Equally, if somebody spends many years struggling to understand a difficult area of mathematics, but does not actually do anything with this understanding, then why should anybody else care? However, many, and perhaps most, mathematicians will not agree equally strongly with the two statements.</p>\n</blockquote>\n<p>Similarly, <a href=\"http://en.wikipedia.org/wiki/Gian-Carlo_Rota\">Gian Carlo Rota</a>'s candid <a href=\"http://www.amazon.com/Indiscrete-Thoughts-Gian-Carlo-Rota/dp/0817638660\">Indiscrete Thoughts</a> contains an essay titled<em> Problem Solvers and Theorizers</em> which draws a similar dichotomy:</p>\n<blockquote>\n<p>Mathematicians can be subdivided into two types: problem solvers and theorizer. Most mathematicians are a mixture of the two although it is easy to find extreme examples of both types.<br> <br>To the problem solver, the supreme achievement in mathematics is the solution to a problem that had been given up as hopeless. It matters little that the solution may be clumsy; all that counts is that it should be the first and that the proof be correct. Once the problem solver finds the new solution, he will permanently lose interest in it, and will listen to new and simplified proofs with an air of condescension and boredom.<br> <br>The problem solver is a conservative at heart. For him, mathematics consists of a sequence of challenges to be met, an obstacle course of problems. The mathematical concepts required to state mathematical problems are tacitly assumed to be eternal and immutable.<br> <br>Mathematical exposition is regarded as an inferior undertaking. New theories are viewed with deep suspicion, as intruders who must prove their worth by posing challenging problems before they can gain attention. The problem solver resents generalizations, especially those that may succeed in trivializing the solution to one of his problems.<br> <br>The problem solver is the role model for budding young mathematicians. When we describe to the public the conquests of mathematics, our shining heroes are the problem solvers.<br><br>To the theorizer, the supreme achievement of mathematics is a theory that sheds sudden light on some incomprehensible phenomenon. Success in mathematics does not lie in solving problems but in their trivialization. The moment of glory comes with the discovery of a new theory that does not solve any of the old problems but renders them irrelevant.<br> <br>The theorizer is a revolutionary at heart. Mathematical concepts received from the past are regarded as imperfect instances of more general ones yet to be discovered. Mathematical exposition is considered a more difficult undertaking than mathematical research.<br> <br>To the theorizer, the only mathematics that will survive are the definitions. Great definitions are what mathematics contributes to the world. Theorems are tolerated as a necessary evil since they play a supporting role - or rather, as the theorizer will reluctantly admit, an essential role - in the understanding of the definitions.<br> <br>Theorizers often have trouble being recognized by the community of mathematicians. Their consolation is the certainty, which may or may not be borne out by history, that their theories will survive long after the problems of the day have been forgotten.<br> <br>If I were a space engineer looking for a mathematician to help me send a rocket into space, I would choose a problem solver. But if I were looking for a mathematician to give a good education to my child, I would unhesitatingly prefer a theorizer.</p>\n</blockquote>\n<p>I believe that Rota's characterizations of problem solvers and theorizers are exaggerated but nevertheless in the right general direction. Rota's remarks are echoed in Colin McLarty's: <a href=\"http://www.math.jussieu.fr/%7Eleila/grothendieckcircle/mclarty1.pdf\">The Rising Tide: Grothendieck on simplicity and generality</a></p>\n<blockquote>\n<p>Grothendieck describes two styles in mathematics. If you think of a theorem to be proved as a nut to be opened, so as to reach \u201cthe nourishing flesh protected by the shell\u201d, then the hammer and chisel principle is: \u201cput the cutting edge of the chisel against the shell and strike hard. If needed, begin again at many different points until the shell cracks\u2014and you are satisfied\u201d. He says:</p>\n<p>\u201cI can illustrate the second approach with the same image of a nut to be opened. The first analogy that came to my mind is of immersing the nut in some softening liquid, and why not simply water? From time to time you rub so the liquid penetrates better, and otherwise you let time pass. The shell becomes more flexible through weeks and months\u2014when the time is ripe, hand pressure is enough, the shell opens like a perfectly ripened avocado!</p>\n<p>A different image came to me a few weeks ago. The unknown thing to be known appeared to me as some stretch of earth or hard marl, resisting penetration. . . the sea advances insensibly in silence, nothing seems to happen, nothing moves, the water is so far off you hardly hear it. . . yet it finally surrounds the resistant substance.\u201d</p>\n<p>[...]</p>\n<p>Deligne describes a characteristic Grothendieck proof as a long series of trivial steps where \u201cnothing seems to happen, and yet at the end a highly non-trivial theorem is there.\u201d</p>\n</blockquote>\n<p>In addition to the sources cited above, Grothendieck discusses a dichotomy which resembles that of birds and frogs in the section of <em>Recoltes et Semailles</em> titled <em>The Inheritors and the Builders</em> and Lee Smolin discusses such a dicotomy in <a href=\"http://www.amazon.com/Trouble-Physics-String-Theory-Science/dp/0618551050\">The Trouble With Physics</a> Chapter 18.</p>\n<p>In his <a href=\"http://www.math.rutgers.edu/~zeilberg/Opinion95.html\">Opinion 95</a>, <a href=\"http://en.wikipedia.org/wiki/Doron_Zeilberger\">Doron Zeilberger</a> added a supplement to Dyson's classification, saying:</p>\n<blockquote>\n<p>I agree that both frogs and birds are crucial for the progress of science, but, even more important, for the progress of mathematics in the computer age, is the <strong>beaver</strong>, who will build the needed infrastructre of computer mathematics, that would eventually enable us to solve many outstanding open problems, and many new ones. Consequently, the developers of computer algebra systems, and creators of algorithms, are even more important than both birds and frogs.</p>\n</blockquote>\n<p>Zeilberger's statement that beavers are more important for the process of science than birds and frogs is debatable and I do not endorse it; but I believe that Zeilberger is correct to identify a third category consisting of people whose primary interest is in algorithms. Indeed, as Laurens Gunnarsen recently pointed out to me, Felix Klein had already identified such a category in his 1908 lectures on <a href=\"http://books.google.com/books?id=vfSutjEIZXkC&amp;lpg=PR2&amp;ots=4H9WZSYK1S&amp;dq=Elementary%20Mathematics%20from%20an%20Advanced%20Standpoint%3A%20Arithmetic%2C%20Algebra%20publication%20date&amp;pg=PA77#v=onepage&amp;q&amp;f=false\">Elementary Mathematics from an Advanced Standpoint: Arithmetic, Algebra and Analysis</a>. In the section titled <em>Concerning the Modern Development and the General Structure of Mathematics</em>, Klein identified three plans A, B, and C roughly corresponding to the natural activities of frogs, birds and beavers respectively:</p>\n<blockquote>\n<p>\u2026we might say that Plan A is based on a more particularistic conception of science which divides the total field into a series of mutually separated parts and attempts to develop each part for itself, with a minimum of resources and with all possible avoidance of borrowing from neighboring fields. Its ideal is to crystallize out each of the partial fields into a logically closed system. On the contrary, the supporter of Plan B lays the chief stress upon the organic combination of the partial fields, and upon the stimulation which these exert one upon another. He prefers, therefore, the methods which open for him an understanding of several fields under a uniform point of view. His ideal is the comprehension of the sum total of mathematical science as a great connected whole.<br> <br>[\u2026]<br><br>For a complete understanding of the development of mathematics, we must, however, think of a still third plan C, which, along side of and within the processes of development A and B, often plays an important role. It has to do with a method which one denotes by the word algorithm, derived from a mutilated form of an Arabian mathematician. All ordered formal calculation is, at bottom, algorithmic, in particular, the calculation with letters is an algorithm. We have repeatedly emphasized what an important part in the development of the science has been played by the algorithmic process, as a quasi-independent, onward-driving force, inherent in the formulas, operating apart from the intension and insight of the mathematician, at the time, often in opposite to them. In the beginning of the infinitesimal calculus, as we shall see later on, the algorithm has often forced new notions and operations, even before one could justify their admissibility. Even at higher levels of the development, these algorithmic considerations can be, and actually have been, very fruitful, so that one can justly call them the groundwork of mathematical development. We must then completely ignore history, if, as is sometimes done today, we cast these circumstances contemptuously aside as mere \"formal\" developments.</p>\n</blockquote>\n<p>The three categories described above appear to have correlates of personality traits, mathematical interests and superficially nonmathematical interests. Below I'll engage in some speculation about this.</p>\n<p><strong id=\"Correlates_of_the_bird_category\">Correlates of the bird category</strong></p>\n<p>My impression is that birds tend to have high <a href=\"http://en.wikipedia.org/wiki/Openness_to_experience\">openness to experience</a>, be anti-conformist, highly emotional sensitivity, and interested in high art, history, philosophy, religion and geometry. Here I'll give some supporting evidence. I believe that the thinkers discussed would identify themselves as birds.</p>\n<p>1. Dyson's article discusses <a href=\"http://en.wikipedia.org/wiki/Yuri_I._Manin\">Yuri Manin</a> as follows:</p>\n<blockquote>\n<p>The book is mainly about mathematics. It may come as a surprise to Western readers that he writes with equal eloquence about other subjects such as the collective unconscious, the origin of human language, the psychology of autism, and the role of the trickster in the mythology of many cultures.</p>\n<p>[...]</p>\n<p>Manin is a bird whose vision extends far beyond the territory of mathematics into the wider landscape of human culture. One of his hobbies is the theory of archetypes invented by the Swiss psychologist Carl Jung. An archetype, according to Jung, is a mental image rooted in a collective unconscious that we all share. The intense emotions that archetypes carry with them are relics of lost memories of collective joy and suffering. Manin is saying that we do not need to accept Jung\u2019s theory as true in order to find it illuminating.</p>\n</blockquote>\n<p>2. In <a href=\"http://www.amazon.com/Trouble-Physics-String-Theory-Science/dp/0618551050\">The Trouble With Physics</a> Lee Smolin writes of \"Seers\" who have something in common with Dyson's \"Birds\":</p>\n<blockquote>\n<p>Seers are very different. They are dreamers. They go to into science because they have questions about the nature of existence that their schoolbooks don't answer. If they weren't scientists, they might be artists or writers or they might end up in divinity school.</p>\n<p>[...]<br><br>A common complaint of the seers is that the standard education in physics ignores the historical and philosophical context in which science develops. As Einstein said in a letter to a young physicist who had been thwarted in his attempts to add philosophy to his physics courses:<br><br>\"I fully agree with you about the significance and educational value of methodology as well as history and philosophy of science. So many people today - and even professional scientists - seem to me like someone who has seen thousands of trees but has never seen a forest. A knowledge of the historical and philosophical background gives that kind of independence from prejudices of his generation from which most scientists are suffering.\"</p>\n</blockquote>\n<p>3. <a href=\"http://golem.ph.utexas.edu/category/2010/01/the_sacred_and_the_profane.html#c031203\">Thomas remarks</a> that Yuri Manin has written about how \"mathematics chooses us\" and \"emotional platonism\" which are characteristic of shamanism and that the number theorist <a href=\"http://en.wikipedia.org/wiki/Kazuya_Kato\">Kazuya Kato</a> writes \"Mysterious properties of zeta values seem to tell us (in a not so loud voice) that our universe has the same properties: The universe is not explained just by real numbers. It has p-adic properties \u2026 We ourselves may have the same properties\" which fits into the shamanistic way of thinking (\"knowing something is becoming it\").</p>\n<p>4. In his autobiography titled <a href=\"http://www.amazon.com/Apprenticeship-Mathematician-Andre-Weil/dp/0817626506\">The Apprentice of a Mathematician</a>, <a href=\"http://en.wikipedia.org/wiki/Andr%C3%A9_Weil\">Andre Weil</a> wrote about how he was heavily influenced by Hindu thought and studied Sanskrit and mystic Hindu poetry.</p>\n<p>5. According to <a href=\"http://www.ams.org/notices/200409/fea-grothendieck-part1.pdf\">Allyn Jackson's article</a> on <a href=\"http://en.wikipedia.org/wiki/Alexander_Grothendieck\">Alexander Grothendieck</a>:</p>\n<blockquote>\n<p>Honig once asked Grothendieck why he had gone into mathematics. Grothendieck replied that he had two special passions, mathematics and piano, but he chose mathematics because he thought it would be easier to earn a living that way. His gift for mathematics was so abundantly clear, said Honig, \u201cI was astonished that at any moment he could hesitate between mathematics and music.\u201d</p>\n</blockquote>\n<p>Grothendieck's interest in music is corroborated by <a href=\"http://www.math.uchicago.edu/%7Emitya/langlands/reminiscences1.pdf\">Luc Illusie</a> who said:</p>\n<blockquote>\n<p>Grothendieck had a very strong feeling for music. He liked Bach and his most beloved pieceswere the last quartets by Beethoven.</p>\n</blockquote>\n<p>According to Winifred Scharlau's <a href=\"http://www.ams.org/notices/200808/tx080800930p.pdf\">Who is Alexander Grothendieck?</a></p>\n<blockquote>\n<p>From 1974 Grothendieck turned to Buddhism; several times he was visited by Japanese monks from the order Nipponzan Myohoji (in English the name translates roughly as \u201cJapanese community of the wonderful lotus sutra\u201d), which preaches strict nonviolence and erects peace pagodas throughout the world. But his attachment to Buddhism did not last. From around 1980 Grothendieck gravitated toward Christian mystical and esoteric ideas.</p>\n</blockquote>\n<p>and</p>\n<blockquote>\n<p>It appears that he thoroughly worked through, for example, Freud\u2019s Traumdeutung (The Interpretation of Dreams) and also read other relevant literature.</p>\n</blockquote>\n<p>6. According to Frank Wilczek's Introduction to <a href=\"http://www.amazon.com/Philosophy-Mathematics-Natural-Science-Hermann/dp/0689702078#reader_0689702078\">Philosophy of Mathematics and Natural Sciences</a> by <a href=\"http://en.wikipedia.org/wiki/Hermann_Weyl\">Hermann Weyl</a>,</p>\n<blockquote>\n<p>In his preface Weyl says, \"I was also bound, though less consciously, by the German literary and philosophical tradition in which I had grown up\" (xv). It was in fact a cosmopolitan tradition, of which <em>Philosophy of Mathematics and Natural Science </em>might be the last great expression. Decartes, Leibniz, Hume, and Kant are taken as familiar friends and interlocutors. Weyl's eruidition is, implicitly, a touching affirmation of a community of mind and inquiry stretching across time and space, and progressing through experience, reflection, and open dialogue.</p>\n</blockquote>\n<p>7. In Robert Langlands' <a href=\"http://www.math.duke.edu/langlands/\">Lectures on the Practice of Mathematics</a> and <a href=\"http://publications.ias.edu/sites/default/files/ND.pdf\">Is There Beauty in Mathematical Theories?</a>, Langlands discusses the history of mathematics at length and quotes <a href=\"http://en.wikipedia.org/wiki/Rainer_Maria_Rilke\">Rainer Maria Rilke</a>, and <a href=\"http://en.wikipedia.org/wiki/Rudyard_Kipling\">Rudyard Kipling</a>.</p>\n<p>8. Some examples of famous birds who identify as geometers in a broad sense are Bernhard Riemann, Henri Poincare, Felix Klein, Elie Cartan, Andre Weil, Shiing-Shen Chern, Alexander Grothendieck, Raoul Bott, Friedrich Hirzebruch, Michael Atiyah, Yuri Manin, Barry Mazur, Alain Connes, Bill Thurston, Mikhail Gromov.</p>\n<p><strong id=\"Correlates_of_the_frog_category\">Correlates of the frog category</strong></p>\n<p>My impression is that frogs tend to be highly detail-oriented, conservative (in the sense that Rota describes), have a good memory of lots of facts, high technical prowess, ability to focus on a problem a very long time, and be interested in areas of math like elementary and analytic number theory, analysis, group theory and combinatorics. Here I'll give some supporting evidence. I believe that the thinkers discussed would identify themselves as frogs.</p>\n<p>1. The conservative quality of frogs that Rota alludes to is negatively correlated with <a href=\"http://en.wikipedia.org/wiki/Openness_to_experience\">openness to experience</a>. For an example of a conservative frog, I would cite <a href=\"http://www-groups.dcs.st-and.ac.uk/%7Ehistory/Biographies/Davenport.html\">Harold Davenport</a>:</p>\n<blockquote>\n<p>Davenport was a natural conservative. \"All change is for the worse\" he used to say with complete conviction. He was entirely out of sympathy with the waves of change in the teaching of mathematics but accepted them as an inevitable evil. Selective in his enjoyment of modern technology, he never entered an aeroplane, would use a lift if no alternative existed (at the International Congress in Moscow he trudged up and down the interminable stairs of Stalin's skyscraper), and preferred to send his papers for publication written in his characteristically neat hand.</p>\n</blockquote>\n<p>Davenport's frog aesthetic comes across in his remark</p>\n<blockquote>\n<p>Great mathematics is achieved by solving difficult problems not by fabricating elaborate theories in search of a problem.</p>\n</blockquote>\n<p>2. The <a href=\"http://en.wikipedia.org/wiki/Feit%E2%80%93Thompson_theorem\">Odd Order Theorem</a> in finite group theory is a seminal result which was proved by the two frogs <a href=\"http://en.wikipedia.org/wiki/Walter_Feit\">Walter Feit</a> and <a href=\"http://en.wikipedia.org/wiki/John_G._Thompson\">John Thompson</a>. One of my friends who did his PhD in finite group theory said that understanding <em>a single line</em> of their 250 page proof requires a serious effort. In <a href=\"http://webcache.googleusercontent.com/search?q=cache:nUy7HDj8J1UJ:sps.nus.edu.sg/~limchuwe/articles/serre.html+http://sps.nus.edu.sg/~limchuwe/articles/serre.html&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us&amp;client=firefox-a\">a 1985 interview</a>, <a href=\"http://en.wikipedia.org/wiki/Jean-Pierre_Serre\">Jean-Pierre Serre</a> said</p>\n<blockquote>\n<p>Chevalley once tried to take this as the topic of a seminar, with the idea of giving a complete account of the proof. After two years, he had to give up.</p>\n</blockquote>\n<p><a href=\"http://en.wikipedia.org/wiki/Claude_Chevalley\">Claude Chevalley</a> was an outstandingly good mathematician. I read the fact that somebody of such high caliber had much trouble as he did with the proof as an indication of Feit and Thompson having unusually high technical prowess and ability to focus on a single problem for a long time even <em>relative to other remarkable mathematicians</em>. This is counterbalanced by the mathematical output of Feit and Thompson was essentially restricted to the topic of finite groups theory in contrast with that of many mathematicians who have broader interests.</p>\n<p>3. The identification of combinatorics as a mathematical field populated by problem solvers comes across in Gowers' essay linked above. The subject of elementary number theory was very heavily influenced by Erdos who has been labeled a canonical problem solver. In <a href=\"http://www.math.harvard.edu/~elkies/M259.98/intro.ps\">the introduction</a> to a course in analytic number theory, <a href=\"http://en.wikipedia.org/wiki/Noam_Elkies\">Noam Elkies</a> wrote</p>\n<blockquote>\n<p>It has often been said that there are two kinds of mathematicians: theory builders and problem solvers. In the mathematics of our century these two styles are epitomized respectively by A. Grothendieck and P. Erdos [...] analytic number theory as usually practiced falls in the problem-solving camp.</p>\n</blockquote>\n<p>Two of the founders of the mathematical field of analysis, namely <a href=\"http://en.wikipedia.org/wiki/Augustin-Louis_Cauchy\">Cauchy</a> and <a href=\"http://en.wikipedia.org/wiki/Karl_Weierstrass\">Weierstrass</a> were frogs. Klein writes</p>\n<blockquote>\n<p>...in the twenties Cauchy (1789-1857) developed, in lectures and in books, the first rigorous foundings of infinitesimal calculus in the modern sense. He not only gives an exact definition of the differential quotient, and of the integral, by means of the limit of a finite quotient and of a finite sum, respectively, as had previously been done, at times; but, by means of the mean-value theorem he erects upon this, for the first time, a consistent structure for the infinitesimal calculus [ ...] These theories also partake of the nature of plan A, since they work overthe field in a logical systematic way, quite apart from other branches of knowledge.</p>\n<p>[...]</p>\n<p>From the middle of the century on, the method of thought A comes again to the front with Weierstrass (1815-1897) [...] I have already investigated Weierstrass function theory as an example of A.</p>\n</blockquote>\n<p>Many of the prominent contemporary analysts like <a href=\"http://en.wikipedia.org/wiki/John_Forbes_Nash,_Jr.\">John Nash</a> and <a href=\"http://en.wikipedia.org/wiki/Grigori_Perelman\">Grigori Perelman</a> are problem solvers.</p>\n<p><strong id=\"Correlates_of_the_beaver_category\">Correlates of the beaver category</strong></p>\n<p>My impression is that beavers tend to be interested in jigsaw puzzles, word puzzles, logic puzzles, board games like Go, sorting tasks, algorithms, computational complexity, logic, respond best to a stream of immediate feedback in the way of tangible progress and can have trouble focusing on learning mathematical subjects in ways that require a lot of development before one engages in computation. Many computer scientists seem to me to fall into the beaver paradigm. Here I'm on shakier ground as I've seen little public discussion of beavers and most of what I've observed that supports my impression is born of subjective experience with people who I know, but I'll try to give some examples that seem to me to fall into the beaver paradigm:</p>\n<p>1. <a href=\"http://en.wikipedia.org/wiki/Doron_Zeilberger\">Doron Zeilberger</a>'s focus on algorithmics, ultrafinitism and constructivist mathematics.</p>\n<p>2. <a href=\"http://math.nyu.edu/faculty/edwardsd/\">Harold Edwards</a>' focus on constructivist mathematics (which comes across in his books titled <em>Higher Arithmetic: An Algorithmic Introduction to Number Theory</em>, <em>Essays in Constructive Mathematics, Galois Theory, Fermat's Last Theorem </em>and <em>Divisor Theory</em>) is in the beaver paradigm.</p>\n<p>3. <a href=\"http://terrytao.wordpress.com/2008/02/05/the-blue-eyed-islanders-puzzle/\">Terence Tao</a>'s interest in logic puzzles like the <a href=\"http://terrytao.wordpress.com/2008/02/05/the-blue-eyed-islanders-puzzle/\">blue-eyed islanders puzzle</a> and his interest in <a href=\"http://terrytao.wordpress.com/2007/06/25/ultrafilters-nonstandard-analysis-and-epsilon-management/\">ultrafilters and nonstandard analysis</a>.</p>\n<p>4. The work of <a href=\"http://en.wikipedia.org/wiki/Jonathan_Borwein\">Jonathan Borwein</a> and <a href=\"http://en.wikipedia.org/wiki/Peter_Borwein\">Peter Borwein</a> computing a billion digits of pi.</p>\n<p>5. The computational work of historically great mathematicians like Newton Euler, Gauss, Jacobi, and Ramanujan.</p>\n<p>6. Don Zagier's remark in his essay in Mariana Cook's book</p>\n<blockquote>\n<p>Even now I am not a modern mathematician and very abstract ideas are unnatural for me. I have of course learned to work with them but I haven't really internalized it and remain a concrete mathematician. I like explicit, hands-on formulas. To me they have a beauty of their own. They can be deep or not.</p>\n</blockquote>\n<p>7. The focus on explicit formulae in the area of <a href=\"http://mathworld.wolfram.com/q-Series.html\">q-series</a>.</p>\n<p>8. Interest in <a href=\"http://en.wikipedia.org/wiki/Newcomb%27s_paradox\">Newcomb's Problem</a> and its variations.</p>\n<p>9. <a href=\"http://en.wikipedia.org/wiki/Scott_Aaronson\">Scott Aaronson</a>'s interest in computational complexity, algorithms, and questions between logic and algorithmics as reflected in his MathOverflow post <a href=\"http://mathoverflow.net/questions/34710/succinctly-naming-big-numbers-zfc-versus-busy-beaver\">Succinctly naming big numbers: ZFC versus Busy-Beaver</a>.</p>\n<p><strong id=\"To_Be_Continued\">To Be Continued</strong></p>\n<p>In a future post I will describe superficial similarities and superficial differences between the three types and misunderstandings&nbsp; between different types which arise from <a href=\"/lw/dr/generalizing_from_one_example/\">generalizing from one example</a> and cultural differences. Regarding his mastercraftspeople/seers dicotomy, Lee Smolin says</p>\n<blockquote>\n<p>It is only to be expected that members of these two groups misunderstand and mistrust each other.</p>\n</blockquote>\n<p>Timothy Gowers writes about how there's a schism between his two categories of mathematicians and says</p>\n<blockquote>\n<p>this is not an entirely healthy state of a ffairs.</p>\n</blockquote>\n<p>As Dyson and Zeilberger said, all three types are important to scientific progress. I believe that intellectual progress will be increase if the three types can learn to better understand each other.</p>", "sections": [{"title": "Birds, Frogs and Beavers", "anchor": "Birds__Frogs_and_Beavers", "level": 1}, {"title": "Correlates of the bird category", "anchor": "Correlates_of_the_bird_category", "level": 1}, {"title": "Correlates of the frog category", "anchor": "Correlates_of_the_frog_category", "level": 1}, {"title": "Correlates of the beaver category", "anchor": "Correlates_of_the_beaver_category", "level": 1}, {"title": "To Be Continued", "anchor": "To_Be_Continued", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "32 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-30T18:20:30.796Z", "modifiedAt": null, "url": null, "title": "Value Deathism", "slug": "value-deathism", "viewCount": null, "lastCommentedAt": "2018-02-21T16:17:53.188Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GMyjNQe5ZgkXJChbg/value-deathism", "pageUrlRelative": "/posts/GMyjNQe5ZgkXJChbg/value-deathism", "linkUrl": "https://www.lesswrong.com/posts/GMyjNQe5ZgkXJChbg/value-deathism", "postedAtFormatted": "Saturday, October 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Value%20Deathism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValue%20Deathism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGMyjNQe5ZgkXJChbg%2Fvalue-deathism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Value%20Deathism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGMyjNQe5ZgkXJChbg%2Fvalue-deathism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGMyjNQe5ZgkXJChbg%2Fvalue-deathism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 311, "htmlBody": "<p><a href=\"http://multiverseaccordingtoben.blogspot.com/2010/10/singularity-institutes-scary-idea-and.html\">Ben Goertzel</a>:</p>\n<blockquote>I doubt human value is particularly fragile. Human value has evolved and morphed over time and will continue to do so. It already takes multiple different forms. It will likely evolve in future in coordination with AGI and other technology. I think it's fairly robust.</blockquote>\n<p><a href=\"http://www.overcomingbias.com/2010/10/goertzel-on-friendly-ai.html\">Robin Hanson</a>:</p>\n<blockquote>Like Ben, I think it is ok (if not ideal) if our descendants' values deviate from ours, as ours have from our ancestors. The risks of attempting a world government anytime soon to prevent this outcome seem worse overall.</blockquote>\n<hr />\n<p>We all know the problem with <a href=\"http://yudkowsky.net/singularity/simplified\">deathism</a>: a strong belief that death is almost impossible to avoid, <a href=\"http://wiki.lesswrong.com/wiki/Motivated_skepticism\">clashing</a> with undesirability of the outcome, leads people to rationalize either the illusory nature of death (afterlife memes), or desirability of death (deathism proper). But of course the claims are separate, and shouldn't influence each other.</p>\n<p>Change in values of the future agents, <a href=\"http://causalityrelay.wordpress.com/2010/02/11/relevance-of-intelligence-explosion/\">however sudden of gradual</a>, means that the Future (<em>the whole freackin' Future!</em>) won't be optimized according to our values, won't be anywhere as good as it could've been otherwise. It's easier to see a <a href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\">sudden change</a> as morally relevant, and easier to rationalize gradual development as morally \"business as usual\", but if we look at the end result, the risks of value drift are the same. And it <em>is</em> difficult to make it so that the future is optimized: to stop uncontrolled \"evolution\" of value (<a href=\"http://causalityrelay.wordpress.com/2010/01/24/fai-vector-for-human-preference/\">value drift</a>) or recover more of <a href=\"http://www.nickbostrom.com/astronomical/waste.html\">astronomical waste</a>.</p>\n<p>Regardless of difficulty of the challenge, it's <em>NOT OK</em> to lose the Future. The loss might prove impossible to avert, but still it's not OK, the value judgment cares not for feasibility of its desire. Let's not succumb to the deathist pattern and lose the battle before it's done. Have the courage and rationality to admit that the loss is real, even if it's <a href=\"http://wiki.lesswrong.com/wiki/Scope_insensitivity\">too great for mere human emotions to express</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"gWEjzxPjitZ2JGZvM": 2, "E9ihK6bA9YKkmJs2f": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GMyjNQe5ZgkXJChbg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 25, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "3871", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 121, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-30T22:10:08.355Z", "modifiedAt": null, "url": null, "title": "I clearly don't understand karma", "slug": "i-clearly-don-t-understand-karma", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:25.783Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwaser", "createdAt": "2010-02-24T12:24:11.249Z", "isAdmin": false, "displayName": "mwaser"}, "userId": "CZLtduXx3pKqpuAxF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AihoScozumpy8HgAE/i-clearly-don-t-understand-karma", "pageUrlRelative": "/posts/AihoScozumpy8HgAE/i-clearly-don-t-understand-karma", "linkUrl": "https://www.lesswrong.com/posts/AihoScozumpy8HgAE/i-clearly-don-t-understand-karma", "postedAtFormatted": "Saturday, October 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20clearly%20don't%20understand%20karma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20clearly%20don't%20understand%20karma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAihoScozumpy8HgAE%2Fi-clearly-don-t-understand-karma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20clearly%20don't%20understand%20karma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAihoScozumpy8HgAE%2Fi-clearly-don-t-understand-karma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAihoScozumpy8HgAE%2Fi-clearly-don-t-understand-karma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<p>Someone take a look at my score and my history and explain my zero karma.</p>\n<p>My understanding was that karma never dropped below zero.</p>\n<p>Apparently, it never *displays* below zero but if it is deep-sixed, it might be a long, long time coming back.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AihoScozumpy8HgAE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 3, "extendedScore": null, "score": 6.404369943842531e-07, "legacy": true, "legacyId": "3872", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-30T22:37:46.767Z", "modifiedAt": null, "url": null, "title": "META: Tiered Discussions", "slug": "meta-tiered-discussions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:26.549Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pavitra", "createdAt": "2009-09-22T08:32:44.250Z", "isAdmin": false, "displayName": "Pavitra"}, "userId": "yC2JgX3ENu7mionKh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i2Swn6CxBd7Dog8AH/meta-tiered-discussions", "pageUrlRelative": "/posts/i2Swn6CxBd7Dog8AH/meta-tiered-discussions", "linkUrl": "https://www.lesswrong.com/posts/i2Swn6CxBd7Dog8AH/meta-tiered-discussions", "postedAtFormatted": "Saturday, October 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20META%3A%20Tiered%20Discussions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMETA%3A%20Tiered%20Discussions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi2Swn6CxBd7Dog8AH%2Fmeta-tiered-discussions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=META%3A%20Tiered%20Discussions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi2Swn6CxBd7Dog8AH%2Fmeta-tiered-discussions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi2Swn6CxBd7Dog8AH%2Fmeta-tiered-discussions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>(Edit: It seems lots of people thought this was a terrible idea. I'm keeping the post as it was, though, mostly because I still think it's an interesting experiment and it ought to <em>have been tried</em> at least once somewhere on this site. Also, blah blah something about preserving the historical record so that earlier comments still make sense, whatever.)</p>\n<p>You aren't <em>allowed to know</em> what this post says unless you can figure out what LW post this sentence is a clever reference to. The URL of that post is the CAST5 symmetric key for this one. Please help downvote spoilers into oblivion.<br /><br />-----BEGIN PGP MESSAGE-----<br />Version: GnuPG v1.4.10 (GNU/Linux)<br /><br />jA0EAwMCtYf1bHFxvmRgyekK9+VOnIJEKESX8yr4CXk5IGX3rsoyS50Nsc+uCy85<br />413pFT1XlfX7UpRNjUPIlG5IjcFhMKhk4NUv32KEgBk7rbfCnPqIid6ry4Sb/QNC<br />RvgOhbTw1YY95+K9KMuZi67D0+Fak14jnL4ZrTTwgzl6dWaJmWnONpCK2hku8n9E<br />IZNFR5sGdxGdvmHRLvsqiVjZk0NP4ZyqN9bEAMIFOO4HcBISm24UyU46+leopqpE<br />K0dkirqKSL/7ZOXvk3s5cW9h7SOStUw9bo8mapHrkoPpPLmQmWB7FnJYY4omb5k+<br />5pGAS2qdXLQYvu1z7e8fyfMPiSqXFmGycM09tq5Un7y7ek63UHKkyIy29VuRa4uT<br />E88Yop/z0zodHoHruiDJLEN/JiWtitMouvpO/WzN4dJE1zOmQSTAiIWGUnvWUhc6<br />16m1dAPXR5+N5lkYRvPhi/tpTN96mZbesGBR0qOjheRssBMzRJhDAsZWt/Um/Qu3<br />au3Uxokq4UojOzJZSXLLYOhuBOa0nxNebp+Hcl/kbLkBe2WLgmVQY4EP8CVsMT0i<br />5PAYtwLpTmaakO/kwSe/ctd/Dr5KYOC+H68ciCXyRERQQrWczdI1gROPv+bmOp0R<br />TsQqGsWJhvuCMp4ZAsnj3HV/DhJKihb8F9TXwxjuC3tUghVrzzHUKk1ramGlWlK/<br />iMB9D2DWovBCbK00jfIQeZxu/kXBHns2DlcFwueGPShdarmtHCaWd/8wqChJ75sS<br />FupwvLKpVYcwa+hukNJi2BUgkfb/yrn4Y6vwhF+xF9D4MJrJG3mng4u7OnllifrZ<br />6OdUNYeQEG5P2Qkj1uu9hvJC8PP4vO648JjVEsaR7gtRouH3H1v5cKqElFpRlyED<br />qgkYdKzCfY96MOTj0b9BgEOw5a728F+rtSyDc+dcLWtFlSeuUc793YUvF6lxng2/<br />KlUyJ9dCDfSiTq+HsQH/kJHR8bmudomJC+ftnBoGxC5BLuQhC4gCPcaYM8evqWzP<br />kkR1OhjhWE9H+O2o53t75IIz/P2LUhwrfqGhBgD3PTAmxw94gbOz0Ckj3UjhKnTY<br />ID22NmrHRRZyJammi6TViHTWRNSLHKifMIGp0/hOC1o=<br />=sPUV<br />-----END PGP MESSAGE-----</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i2Swn6CxBd7Dog8AH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -7, "extendedScore": null, "score": 6.404434752585131e-07, "legacy": true, "legacyId": "3873", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-31T08:48:14.984Z", "modifiedAt": null, "url": null, "title": "Transhumanism and assisted suicide", "slug": "transhumanism-and-assisted-suicide", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:25.068Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SeventhNadir", "createdAt": "2010-04-18T04:11:11.485Z", "isAdmin": false, "displayName": "SeventhNadir"}, "userId": "adAXuo6KKGxTap3SN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P85MWyCBLbRsMeRnH/transhumanism-and-assisted-suicide", "pageUrlRelative": "/posts/P85MWyCBLbRsMeRnH/transhumanism-and-assisted-suicide", "linkUrl": "https://www.lesswrong.com/posts/P85MWyCBLbRsMeRnH/transhumanism-and-assisted-suicide", "postedAtFormatted": "Sunday, October 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Transhumanism%20and%20assisted%20suicide&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATranshumanism%20and%20assisted%20suicide%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP85MWyCBLbRsMeRnH%2Ftranshumanism-and-assisted-suicide%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Transhumanism%20and%20assisted%20suicide%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP85MWyCBLbRsMeRnH%2Ftranshumanism-and-assisted-suicide", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP85MWyCBLbRsMeRnH%2Ftranshumanism-and-assisted-suicide", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 141, "htmlBody": "<p>I would identify myself as holding many trans-humanist values and beliefs. That death and dying are not desirable and that it should, like smallpox, be eradicated. It also occurs to me however that I hold the belief that assisted suicide can sometimes be the right course of action. I can justify this sufficiently to myself, but I have a history of being a sophisticated arguer, so my ability to convince myself isn't great evidence.</p>\n<p>Are my beliefs incoherent? I look at both issues and they both still <strong>feel</strong> right (I'm aware this does not make it so). Are these beliefs contradictory (and if so how)? Or are they justified by some hidden assumptions that I can't seem to acknowledge explicitly?</p>\n<p>I get the idea that this is probably borderline appropriate for the discussion forum so I will delete this topic if asked to.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jiuackr7B5JAetbF6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P85MWyCBLbRsMeRnH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 6.40589402054617e-07, "legacy": true, "legacyId": "3876", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-31T11:58:51.789Z", "modifiedAt": null, "url": null, "title": ".1% of human liver grown in lab for first time", "slug": "1-of-human-liver-grown-in-lab-for-first-time", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SSFDBZRm3uYfFz4S2/1-of-human-liver-grown-in-lab-for-first-time", "pageUrlRelative": "/posts/SSFDBZRm3uYfFz4S2/1-of-human-liver-grown-in-lab-for-first-time", "linkUrl": "https://www.lesswrong.com/posts/SSFDBZRm3uYfFz4S2/1-of-human-liver-grown-in-lab-for-first-time", "postedAtFormatted": "Sunday, October 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.1%25%20of%20human%20liver%20grown%20in%20lab%20for%20first%20time&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.1%25%20of%20human%20liver%20grown%20in%20lab%20for%20first%20time%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSSFDBZRm3uYfFz4S2%2F1-of-human-liver-grown-in-lab-for-first-time%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.1%25%20of%20human%20liver%20grown%20in%20lab%20for%20first%20time%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSSFDBZRm3uYfFz4S2%2F1-of-human-liver-grown-in-lab-for-first-time", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSSFDBZRm3uYfFz4S2%2F1-of-human-liver-grown-in-lab-for-first-time", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.webmd.com/news/20101029/first-human-liver-grown-in-lab?src=RSS_PUBLIC\">http://www.webmd.com/news/20101029/first-human-liver-grown-in-lab?src=RSS_PUBLIC</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SSFDBZRm3uYfFz4S2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 6.406349410127326e-07, "legacy": true, "legacyId": "3878", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-31T12:16:15.764Z", "modifiedAt": null, "url": null, "title": "Are we trying to do things the hard way?", "slug": "are-we-trying-to-do-things-the-hard-way", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:31.616Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7ctXSTh9wZqFjHYxA/are-we-trying-to-do-things-the-hard-way", "pageUrlRelative": "/posts/7ctXSTh9wZqFjHYxA/are-we-trying-to-do-things-the-hard-way", "linkUrl": "https://www.lesswrong.com/posts/7ctXSTh9wZqFjHYxA/are-we-trying-to-do-things-the-hard-way", "postedAtFormatted": "Sunday, October 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20we%20trying%20to%20do%20things%20the%20hard%20way%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20we%20trying%20to%20do%20things%20the%20hard%20way%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ctXSTh9wZqFjHYxA%2Fare-we-trying-to-do-things-the-hard-way%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20we%20trying%20to%20do%20things%20the%20hard%20way%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ctXSTh9wZqFjHYxA%2Fare-we-trying-to-do-things-the-hard-way", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7ctXSTh9wZqFjHYxA%2Fare-we-trying-to-do-things-the-hard-way", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 152, "htmlBody": "<p><a href=\"http://www.ted.com/talks/r_a_mashelkar_breakthrough_designs_for_ultra_low_cost_products.html\">A TED talk about remarkable low-cost Indian products</a>-- the Tata car which costs $2000 and is a real car, a $28 artificial lower leg which permits walking on rough ground, tree climbing, jumping, and running, and fast cheap drug development which starts with traditional Indian remedies. It's an example of something to defend because the effort is to develop products that very poor people can afford, so that incremental improvements and cost-cutting aren't good enough.</p>\n<p>It leaves me wondering whether the process of creating FAI should be re-evaluated-- whether there's a built-in assumption of high personal costs which is unnecessary. That's wondering, not an absolute certainty, it's just that the $28 artificial lower leg shocked me into thinking about how much is being made harder than necessary.</p>\n<p>Even if FAI is being worked on about as efficiently as possible, there may be a huge amount of possibility for making things easier in life generally.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7ctXSTh9wZqFjHYxA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 14, "extendedScore": null, "score": 6.40639098180562e-07, "legacy": true, "legacyId": "3879", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-31T14:18:40.790Z", "modifiedAt": null, "url": null, "title": "Qualia Soup, a rationalist and a skilled You Tube jockey", "slug": "qualia-soup-a-rationalist-and-a-skilled-you-tube-jockey", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:55.739Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raw_Power", "createdAt": "2010-09-10T23:59:43.621Z", "isAdmin": false, "displayName": "Raw_Power"}, "userId": "kwSqcED9qTanFyNWG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zZSp3vBTJLjYZbAHL/qualia-soup-a-rationalist-and-a-skilled-you-tube-jockey", "pageUrlRelative": "/posts/zZSp3vBTJLjYZbAHL/qualia-soup-a-rationalist-and-a-skilled-you-tube-jockey", "linkUrl": "https://www.lesswrong.com/posts/zZSp3vBTJLjYZbAHL/qualia-soup-a-rationalist-and-a-skilled-you-tube-jockey", "postedAtFormatted": "Sunday, October 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Qualia%20Soup%2C%20a%20rationalist%20and%20a%20skilled%20You%20Tube%20jockey&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQualia%20Soup%2C%20a%20rationalist%20and%20a%20skilled%20You%20Tube%20jockey%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzZSp3vBTJLjYZbAHL%2Fqualia-soup-a-rationalist-and-a-skilled-you-tube-jockey%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Qualia%20Soup%2C%20a%20rationalist%20and%20a%20skilled%20You%20Tube%20jockey%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzZSp3vBTJLjYZbAHL%2Fqualia-soup-a-rationalist-and-a-skilled-you-tube-jockey", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzZSp3vBTJLjYZbAHL%2Fqualia-soup-a-rationalist-and-a-skilled-you-tube-jockey", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<p><a href=\"http://www.youtube.com/user/QualiaSoup\">Please have a look at this Youtube user account.</a>&nbsp; I immediately thought of this site upon watching a couple of his clips. I am told we have tried here to publish some stuff in audiovisual fomat, but it didn't quite work out. Maybe we should contact this guy, perhaps we could profit from each other's work and experience? I am fairly hopeful that he could use some of the material here, and we could use some of his talent with the medium.</p>\n<p><a href=\"http://www.youtube.com/user/QualiaSoup#p/u/4/T69TOuqaqXI\">This video, for instance, looks like it was taken right out of this very blog</a>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zZSp3vBTJLjYZbAHL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 9, "extendedScore": null, "score": 6.406683478891465e-07, "legacy": true, "legacyId": "3880", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-31T15:44:38.802Z", "modifiedAt": null, "url": null, "title": "Ownership and Artificial Intelligence", "slug": "ownership-and-artificial-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:26.818Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "HAHpoFbPgjdFq27kX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7wFEtGDTxuBAcA5LT/ownership-and-artificial-intelligence", "pageUrlRelative": "/posts/7wFEtGDTxuBAcA5LT/ownership-and-artificial-intelligence", "linkUrl": "https://www.lesswrong.com/posts/7wFEtGDTxuBAcA5LT/ownership-and-artificial-intelligence", "postedAtFormatted": "Sunday, October 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ownership%20and%20Artificial%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOwnership%20and%20Artificial%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7wFEtGDTxuBAcA5LT%2Fownership-and-artificial-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ownership%20and%20Artificial%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7wFEtGDTxuBAcA5LT%2Fownership-and-artificial-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7wFEtGDTxuBAcA5LT%2Fownership-and-artificial-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 299, "htmlBody": "<p>(This is a subject that appears incredibly important to me, but it's received no discussion on LW from what I can see with a brief search. Please do link to articles about this if I missed them.)</p>\n<p>Edit: This is all assuming that the first powerful AIs developed aren't exponentially self-improving; if there's no significant period of time where powerful AIs exist but they're not so powerful that the ownership relations between them and their creators don't matter, these questions are obviously not important.</p>\n<p>What are some proposed ownership situations between artificial intelligence and its creators? Suppose a group of people creates some powerful artificial intelligence that appears to be conscious in most/every way--who owns it? Should the AI legally have self-ownership, and all the responsibility for its actions and ownership of the results of its labor that implies? Or, should strong AI be protected by IP, the way non-strong AI code already can be, treated as a tool rather than a conscious agent? It seems wise to implore people to not create AIs that want to have total free agency and generally act like humans, but that's hardly a guarantee that nobody will, and then you have the ethical issue of not being able to just kill them once they're created (if they \"want\" to exist and appear genuinely conscious). Are there any proposed tests to determine whether a synthetic agent should be able to own itself or become the property of its creators?</p>\n<p>I imagine there aren't yet good answers to all these questions, but surely, there's some discussion of the issue somewhere, whether in rationalist/futurist circles or just sci-fi. Also, please correct me on any poor word choice you notice that needlessly limits the topic; it's broad, and I'm not yet completely familiar with the lingo of this subject.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7wFEtGDTxuBAcA5LT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 3, "extendedScore": null, "score": 6.406888897813848e-07, "legacy": true, "legacyId": "3881", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-31T19:09:04.250Z", "modifiedAt": null, "url": null, "title": "Imagine a world where minds run on physics", "slug": "imagine-a-world-where-minds-run-on-physics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:54.478Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GrR4memYZSBsWWCF6/imagine-a-world-where-minds-run-on-physics", "pageUrlRelative": "/posts/GrR4memYZSBsWWCF6/imagine-a-world-where-minds-run-on-physics", "linkUrl": "https://www.lesswrong.com/posts/GrR4memYZSBsWWCF6/imagine-a-world-where-minds-run-on-physics", "postedAtFormatted": "Sunday, October 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Imagine%20a%20world%20where%20minds%20run%20on%20physics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImagine%20a%20world%20where%20minds%20run%20on%20physics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGrR4memYZSBsWWCF6%2Fimagine-a-world-where-minds-run-on-physics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Imagine%20a%20world%20where%20minds%20run%20on%20physics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGrR4memYZSBsWWCF6%2Fimagine-a-world-where-minds-run-on-physics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGrR4memYZSBsWWCF6%2Fimagine-a-world-where-minds-run-on-physics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 659, "htmlBody": "<p>This post describes a toy formal model that helps me think about&nbsp;self-modifying AIs, motivationally stable goal systems, paperclip maximizers and other such things. It's not a new result, just a way of imagining how a computer sitting within a world interacts with the world and with itself. I hope others will find it useful.</p>\n<p>(EDIT: it turns out the post does imply a somewhat interesting result. See my exchange with Nesov in the comments.)</p>\n<p>A cellular automaton like the <a href=\"http://en.wikipedia.org/wiki/Conway's_Game_of_Life\">Game of Life</a> can contain configurations that work like computers. Such a computer may contain a complete or partial representation of the whole world, including itself via <a href=\"http://en.wikipedia.org/wiki/Quine_(computing)\">quining</a>. Also it may have \"actuators\", e.g. a pair of <a href=\"http://en.wikipedia.org/wiki/Gun_(cellular_automaton)\">guns</a> that can build interesting things using colliding sequences of <a href=\"http://en.wikipedia.org/wiki/Glider_(Conway's_Life)\">gliders</a>, depending on what's in the return value register. The computer's program reasons about its model of the world axiomatically, using a proof checker like in my <a href=\"/lw/2ip/ai_cooperation_in_practice/\">other</a> <a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/\">posts</a>, with the goal of returning a value whose representation in the return-value register would cause the actuators to affect the world-model in interesting ways (I call this the \"coupling axiom\"). Then that thing happens in the real world.</p>\n<p>The first and most obvious example of what the computer could want is suicide. Assuming the \"actuators\" are flexible enough, the program could go searching for a proof that putting a certain return value in the register eventually causes the universe to become empty (assuming that at the beginning it was empty except for the computer). Then it returns that value and halts.<a id=\"more\"></a></p>\n<p>The second example is <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">paperclipping</a>. If the universe is finite, the program could search for a return value that results in a stable configuration for the entire universe with the most possible copies of some&nbsp;<a href=\"http://en.wikipedia.org/wiki/Still_life_(CA)\">still-life</a>, e.g. the \"block\".&nbsp;If the universe is infinite, it could search for patterns with high rates of paperclip production (limited by lightspeed in the automaton). In our world this would create something like the <a href=\"http://qntm.org/nine\">\"energy virus\"</a> imagined by Sam Hughes - a rare example of a non-smart threat that sounds scarier than nanotech.</p>\n<p>The third is sensing. Even though the computer lacks sensors, it will make them if the goal calls for it, so sensing is \"emergent\" in this formalization. (This part was a surprise for me.) For example, if the computer knows that the universe is empty except for a specified rectangle containing an unknown still-life pattern, and the goal is to move that pattern 100 cells to the right and otherwise cause no effect, the computer will presumably build something that has sensors, but we don't know what kind. Maybe a very slow-moving <a href=\"http://en.wikipedia.org/wiki/Spaceship_(cellular_automaton)\">spaceship</a> that can \"smell\" the state of the cell directly touching its nose, and stop and resume motion according to an internal program. Or maybe shoot the target to hell with glider-guns and investigate the resulting debris. Or maybe something completely incomprehensible at first glance, which nevertheless manages to get the job done. The Game of Life is unfriendly to explorers because it has no conservation of energy, so putting the wrong pieces together may lead to a huge explosion at lightspeed, but automata with forgiving physics should permit more efficient solutions.</p>\n<p>You could go on and invent more elaborate examples where the program cares about returning something quickly, or makes itself smarter in <a href=\"http://www.idsia.ch/~juergen/goedelmachine.html\">G&ouml;del machine</a> style, or reproduces itself... And they all&nbsp;share a curious pattern. Even though the computer can destroy itself without complaint, and even salvage itself for spare parts if matter is scarce, it never seems to exhibit any instability of values. As long as its world-model (or, more realistically, its prior about possible physics) describes the real world well, the thing will maximize what we tell it to, as best it can. This indicates that value stability may depend more on getting the modeling+quining right than on any deep theory of \"goal systems\" that people seem to want. And, of course, encoding human values in a machine-digestible form for Friendliness looks like an even harder problem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GrR4memYZSBsWWCF6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 6.407377418702029e-07, "legacy": true, "legacyId": "3882", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TNfx89dh5KkcKrvho", "dC3rxrMkYKLfgTYEa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-31T19:33:39.378Z", "modifiedAt": null, "url": null, "title": "Nils Nilsson's AI History: The Quest for Artificial Intelligence", "slug": "nils-nilsson-s-ai-history-the-quest-for-artificial", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:27.222Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gpvG6vpHnLtFbDtJ6/nils-nilsson-s-ai-history-the-quest-for-artificial", "pageUrlRelative": "/posts/gpvG6vpHnLtFbDtJ6/nils-nilsson-s-ai-history-the-quest-for-artificial", "linkUrl": "https://www.lesswrong.com/posts/gpvG6vpHnLtFbDtJ6/nils-nilsson-s-ai-history-the-quest-for-artificial", "postedAtFormatted": "Sunday, October 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nils%20Nilsson's%20AI%20History%3A%20The%20Quest%20for%20Artificial%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANils%20Nilsson's%20AI%20History%3A%20The%20Quest%20for%20Artificial%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgpvG6vpHnLtFbDtJ6%2Fnils-nilsson-s-ai-history-the-quest-for-artificial%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nils%20Nilsson's%20AI%20History%3A%20The%20Quest%20for%20Artificial%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgpvG6vpHnLtFbDtJ6%2Fnils-nilsson-s-ai-history-the-quest-for-artificial", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgpvG6vpHnLtFbDtJ6%2Fnils-nilsson-s-ai-history-the-quest-for-artificial", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<p>I just noticed that AI pioneer and former Association for the Advancement of Artificial Intelligence (AAAI) head Nils Nilsson, has published his history of AI, <em>The Quest for Artificial Intelligence: A History of Ideas and Achievements</em>.<em>&nbsp;</em>The book is available as a <a href=\"http://ai.stanford.edu/~nilsson/QAI/qai-webpage.html\">free pdf from his website</a>, with the pay version on <a href=\"http://www.amazon.com/Quest-Artificial-Intelligence-Nils-Nilsson/dp/0521122937/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1252707698&amp;sr=1-1\">Amazon, with reviews</a>.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gpvG6vpHnLtFbDtJ6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 6.407436175987911e-07, "legacy": true, "legacyId": "3883", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-31T23:14:59.772Z", "modifiedAt": null, "url": null, "title": "Short versions of the basic premise about FAI", "slug": "short-versions-of-the-basic-premise-about-fai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:57.860Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8YwykDGM3WEAtd76P/short-versions-of-the-basic-premise-about-fai", "pageUrlRelative": "/posts/8YwykDGM3WEAtd76P/short-versions-of-the-basic-premise-about-fai", "linkUrl": "https://www.lesswrong.com/posts/8YwykDGM3WEAtd76P/short-versions-of-the-basic-premise-about-fai", "postedAtFormatted": "Sunday, October 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Short%20versions%20of%20the%20basic%20premise%20about%20FAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShort%20versions%20of%20the%20basic%20premise%20about%20FAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8YwykDGM3WEAtd76P%2Fshort-versions-of-the-basic-premise-about-fai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Short%20versions%20of%20the%20basic%20premise%20about%20FAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8YwykDGM3WEAtd76P%2Fshort-versions-of-the-basic-premise-about-fai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8YwykDGM3WEAtd76P%2Fshort-versions-of-the-basic-premise-about-fai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p>I've been using something like \"A self-optimizing AI would be so powerful that it will just roll over the human race unless it's programmed to not do that.\"</p>\n<p>Any others?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8YwykDGM3WEAtd76P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 6.407965204093123e-07, "legacy": true, "legacyId": "3885", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-10-31T23:59:57.596Z", "modifiedAt": null, "url": null, "title": "Is cryonics evil because it's cold?", "slug": "is-cryonics-evil-because-it-s-cold", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:24.623Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ata", "createdAt": "2009-07-20T22:13:53.102Z", "isAdmin": false, "displayName": "ata"}, "userId": "KppHkGEqTNeDaGJTc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hNMitTCjH25jCWXFd/is-cryonics-evil-because-it-s-cold", "pageUrlRelative": "/posts/hNMitTCjH25jCWXFd/is-cryonics-evil-because-it-s-cold", "linkUrl": "https://www.lesswrong.com/posts/hNMitTCjH25jCWXFd/is-cryonics-evil-because-it-s-cold", "postedAtFormatted": "Sunday, October 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20cryonics%20evil%20because%20it's%20cold%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20cryonics%20evil%20because%20it's%20cold%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhNMitTCjH25jCWXFd%2Fis-cryonics-evil-because-it-s-cold%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20cryonics%20evil%20because%20it's%20cold%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhNMitTCjH25jCWXFd%2Fis-cryonics-evil-because-it-s-cold", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhNMitTCjH25jCWXFd%2Fis-cryonics-evil-because-it-s-cold", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 827, "htmlBody": "<p>There have been <a href=\"/tag/cryonics\">many previous discussions</a> here on <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">cryonics</a> and why it is perceived as <a href=\"/lw/2jn/the_threat_of_cryonics/\">threatening</a> or otherwise <a href=\"/lw/1r0/a_survey_of_anticryonics_writing/\">disagreeable</a>. Even among LWers who are not signed up and <a href=\"/lw/2a8/abnormal_cryonics/\">don&rsquo;t plan to</a>, I&rsquo;d say there&rsquo;s a good degree of consensus that cryonics is <em>reviled and ridiculed</em> to a very unjustified degree. I had a thought about one possible factor contributing to its unsavory public image that I haven&rsquo;t seen brought up in previous discussions:</p>\n<p><span style=\"text-transform: lowercase; font-variant: small-caps\">COLD</span> is <span style=\"text-transform: lowercase; font-variant: small-caps\">EVIL</span>.</p>\n<p>Well, no, cold isn&rsquo;t evil, but &ldquo;<span style=\"text-transform: lowercase; font-variant: small-caps\">COLD</span> is <span style=\"text-transform: lowercase; font-variant: small-caps\">EVIL/THREATENING/DANGEROUS/HARSH/LONELY/UNLOVING/SAD/DEAD</span>&rdquo; seems to be a pretty common set of <a href=\"http://wiki.lesswrong.com/wiki/Conceptual_metaphor\">conceptual metaphors</a>. You see it in figures of speech like &ldquo;cold-hearted,&rdquo; &ldquo;in cold blood,&rdquo; &ldquo;cold expression,&rdquo; &ldquo;icy stare,&rdquo; &ldquo;chilling,&rdquo; &ldquo;went cold,&rdquo; &ldquo;cold calculation,&rdquo; &ldquo;the cold shoulder,&rdquo; &ldquo;cold feet,&rdquo; &ldquo;stone cold,&rdquo; &ldquo;out cold.&rdquo; (Naturally, it&rsquo;s also the case that <span style=\"text-transform: lowercase; font-variant: small-caps\">WARM</span> is <span style=\"text-transform: lowercase; font-variant: small-caps\">GOOD/COMFORTING/SAFE/SOCIAL/LOVING/HAPPY/ALIVE</span>, though <span style=\"text-transform: lowercase; font-variant: small-caps\">COOL</span> and <span style=\"text-transform: lowercase; font-variant: small-caps\">HOT</span> sort of go in their own directions.) Associating something with coldness just makes it seem more threatening and less benevolent. And besides, being that &ldquo;<span style=\"text-transform: lowercase; font-variant: small-caps\">COLD</span> is <span style=\"text-transform: lowercase; font-variant: small-caps\">DEAD</span>,&rdquo; it&rsquo;s pretty hard to imagine someone as not <em>really</em> dead if they&rsquo;re in a container of liquid nitrogen at -135&ordm;C. (Even harder if it&rsquo;s just their head in there&hellip; but that&rsquo;s a separate issue.) There is already a little bit of research on the effects of some of the conceptual metaphors of coldness and the way its emotional content leaks onto metaphorically associated concepts (&ldquo;<a href=\"http://www.rotman.utoronto.ca/facbios/file/cold%20and%20lonely.pdf\">Cold and lonely: does social exclusion literally feel cold?</a>&rdquo;; &ldquo;<a href=\"http://www.yale.edu/acmelab/articles/Science_coffee_study.pdf\">Experiencing physical warmth promotes interpersonal warmth.</a>&rdquo;; any others?).<a id=\"more\"></a></p>\n<p>And indeed, it seems that repeatedly talking about it from the &ldquo;it involves coldness [or &lsquo;freezing&rsquo;]&rdquo; angle rather than the &ldquo;it&rsquo;s about preserving minds&rdquo; angle pushes the right emotional buttons to make people feel negatively about it, given cryonics critics&rsquo; and ridiculers&rsquo; fondness for talking about people getting their heads &ldquo;<a href=\"http://www.alternet.org/media/147978/the_ultimate_escape%3A_the_bizarre_libertarian_plan_of_uploading_brains_into_robots_to_escape_society?page=entire\">frozen</a>&rdquo; (&ldquo;&hellip;the guys who had their heads sawed off and frozen&hellip;&rdquo;) and referring to cryonics patients as &ldquo;<a href=\"http://en.wikipedia.org/wiki/Corpsicle\">corpsicles</a>.&rdquo;</p>\n<p>Suppose there were some brain/body-preservation procedure that was, in practice, very similar to cryonics &mdash;&nbsp;in terms of cost, current popularity and awareness levels, probability of effectiveness, some visual weirdness not too much worse than other well-accepted medical procedures, etc. &mdash;&nbsp;but which somehow allowed storage at normal body temperature. Right away that would remove the threatening coldness metaphors and the disturbing mental images of heads frozen in blocks of ice and bodies stuffed into freezers (not that that&rsquo;s anything like how it actually works, but most people assume it does because they only know about cryonics &mdash;&nbsp;er, I mean, &ldquo;cryogenics&rdquo; &mdash;&nbsp;from Austin Powers and Futurama and Batman and other popular <a href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">fiction</a> where it&rsquo;s either a comedic trope or a villainous thing the villain does, and they likely will continue to treat that as the <a href=\"/lw/tt/points_of_departure/\">point of departure</a> for everything else they learn about <em>actual</em> cryonics). If we did a survey of the general public asking two groups slightly different versions of the same question &mdash;</p>\n<ul>\n<li>&ldquo;If there were a medical procedure which, if all other attempts to treat a life-threatening condition failed, could preserve the patient indefinitely in a suspended state in anticipation that future technology may enable them to be resuscitated and treated, would you be open to undergoing this procedure as a last resort?&rdquo;</li>\n<li>&ldquo;If there were a medical procedure which, if all other attempts to treat a life-threatening condition failed, could preserve the patient indefinitely in a <strong>frozen</strong> state <strong>at below -100&ordm;C</strong> in anticipation that future technology may enable them to be <strong>thawed</strong>, revived, and treated, would you be open to undergoing this procedure as a last resort?&rdquo;</li>\n</ul>\n<p>&mdash;&nbsp;I&rsquo;d bet that there would be noticeably more interest among Group A. (That modern cryonics doesn&rsquo;t actually use <a href=\"http://www.alcor.org/cryomyths.html#myth2\">freezing</a> would be beside the point, for the purposes of such a survey; most people will not actually be familiar with the reasons <em>why</em> freezing is bad compared to vitrification, and the point would only be to use enough chilly words to test whether the <em>idea of below-freezing temperatures</em> is something that makes people more uncomfortable with it, all other things being equal.) A similar survey might ask how people would feel about the idea of a loved one or close friend deciding to sign up for one of these procedures, to get higher-resolution data on its emotional associations, as the data wouldn&rsquo;t be as strongly affected by the other reasons (good or bad) that people might prefer not to do it themselves.</p>\n<p>Of course, unless someone invents a method of brain preservation that doesn&rsquo;t require very low temperatures and doesn&rsquo;t have <a href=\"http://www.brainpreservation.org/\">its own (likely) severe aura of weirdness to overcome</a> (would <em>you</em> rather &ldquo;<em>have your head frozen</em>&rdquo; or &ldquo;<em>have your brain converted into plastic</em>&rdquo;?), this is either a non-issue or a marketing issue. I think this is plausible enough that it&rsquo;s worth finding out &mdash; investigating empirically whether people really do respond better to a description of the process from a practical perspective, with coldness and &ldquo;cryo[ge]nics&rdquo; not being mentioned. If so, it may be beneficial for cryonics organizations to significantly rebrand and reframe their services.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1, "rnvHPB3X2TiD5NMwY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hNMitTCjH25jCWXFd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 31, "extendedScore": null, "score": 6.408072682025957e-07, "legacy": true, "legacyId": "3886", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rjtvWPzA6XX4PNGPb", "ZXaRHHLsxaTTQQsZb", "J5teWueouHJxcZkDy", "rHBdcHGLJ7KvLJQPk", "zrGzan92SxP27LWP9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-01T02:35:44.121Z", "modifiedAt": null, "url": null, "title": "Berkeley LW Meet-up Saturday November 6", "slug": "berkeley-lw-meet-up-saturday-november-6", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:29.120Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "LucasSloan", "createdAt": "2009-05-28T05:04:38.345Z", "isAdmin": false, "displayName": "LucasSloan"}, "userId": "ouo6Fqn5kTNY7LvqM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/faYaa4ry7M7buSP9L/berkeley-lw-meet-up-saturday-november-6", "pageUrlRelative": "/posts/faYaa4ry7M7buSP9L/berkeley-lw-meet-up-saturday-november-6", "linkUrl": "https://www.lesswrong.com/posts/faYaa4ry7M7buSP9L/berkeley-lw-meet-up-saturday-november-6", "postedAtFormatted": "Monday, November 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Berkeley%20LW%20Meet-up%20Saturday%20November%206&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABerkeley%20LW%20Meet-up%20Saturday%20November%206%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfaYaa4ry7M7buSP9L%2Fberkeley-lw-meet-up-saturday-november-6%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Berkeley%20LW%20Meet-up%20Saturday%20November%206%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfaYaa4ry7M7buSP9L%2Fberkeley-lw-meet-up-saturday-november-6", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfaYaa4ry7M7buSP9L%2Fberkeley-lw-meet-up-saturday-november-6", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p>Last month, about 20 people showed up to the <a href=\"/lw/2sn/berkeley_lw_meetup_saturday_october_9/\">Berkeley LW</a> meet-up.&nbsp; To continue the tradition of Berkeley Meetups, we will be meeting on Saturday, November 6 at 7 PM at the Starbucks <span><span dir=\"ltr\">at <del>2224 Shattuck Avenue</del>.&nbsp; Last time, we chatted at the Starbucks for about 45 minutes, then went to get dinner and ate and talked under a T-Rex skeleton - we'll probably do something similar, so don't feel like you have to eat before you come.&nbsp; Hope to see you there!</span></span></p>\n<p>&nbsp;</p>\n<p><span><span dir=\"ltr\">ETA: As per User:Kevin's suggestion, we will instead be meeting at the Starbucks at </span></span><a href=\"http://maps.google.com/maps?f=d&amp;source=s_d&amp;saddr=Downtown+Berkeley+BART&amp;daddr=2128+Oxford+St,+Berkeley,+CA+94704-1311+(Starbucks)&amp;geocode=FSzZQQIdbVa2-Ck7jvjKnX6FgDG3LOQ7rN5ZmA%3BFW7bQQIdQl62-CEGuQ_bapaUqCmz2L6SnX6FgDHtCjCMFeuX-A&amp;hl=en&amp;mra=ltm&amp;dirflg=w&amp;sll=37.86999,-122.26696&amp;sspn=0.001931,0.005284&amp;ie=UTF8&amp;ll=37.870225,-122.266577&amp;spn=0.001931,0.005284&amp;z=18\"><span dir=\"ltr\">2128 Oxford Street</span></a><span dir=\"ltr\">.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "faYaa4ry7M7buSP9L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 6.408445060710866e-07, "legacy": true, "legacyId": "3877", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hKK6FtNX8YSJWjHTi"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-01T12:10:38.277Z", "modifiedAt": null, "url": null, "title": "Irrational Upvotes", "slug": "irrational-upvotes", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:27.096Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwaser", "createdAt": "2010-02-24T12:24:11.249Z", "isAdmin": false, "displayName": "mwaser"}, "userId": "CZLtduXx3pKqpuAxF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7PBmG2JvSuTnwpxuF/irrational-upvotes", "pageUrlRelative": "/posts/7PBmG2JvSuTnwpxuF/irrational-upvotes", "linkUrl": "https://www.lesswrong.com/posts/7PBmG2JvSuTnwpxuF/irrational-upvotes", "postedAtFormatted": "Monday, November 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Irrational%20Upvotes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIrrational%20Upvotes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PBmG2JvSuTnwpxuF%2Firrational-upvotes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Irrational%20Upvotes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PBmG2JvSuTnwpxuF%2Firrational-upvotes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PBmG2JvSuTnwpxuF%2Firrational-upvotes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p>\"<strong>This premise is VERY flawed</strong>\" (found <a href=\"/lw/2zg/ben_goertzel_the_singularity_institutes_scary/2vj1?c=1\">here</a>) is the sole author-supplied content of a comment.&nbsp; There are no supporting links or additional content, only a one-sentence quote of the \"offending\" premise.</p>\n<p>Yet, it has four upvotes.</p>\n<p>This is a statement that can be made about any premise.&nbsp; It is backed by no supporting evidence.</p>\n<p>Presumably, whoever upvoted it did so because they disagreed with the preceding comment (which, presumably, they downvoted -- unless they didn't have enough karma).</p>\n<p>This *could* be viewed as rational behavior because it *does* support the goal of defeating the preceding comment but it does not support the LessWrong community.&nbsp; If premise is fatally flawed, then you should give at least some shred of a reason WHY or all you're doing is adding YOUR opinion.&nbsp;</p>\n<p>This blog is \"devoted to refining the art of human rationality\".&nbsp; If the author is truly interested in refining his rationality, he has been given absolutely no help.&nbsp; He has no idea why his premise is flawed.&nbsp; He is now going to have to ask why or for some counter-examples.&nbsp; For his purposes (and the purposes of anyone else who doesn't understand or doesn't agree with your opinion), this post is useless noise clogging up the site.</p>\n<p>Yet, it has four upvotes.</p>\n<p>Is anyone else here bothered by this or am I way off base?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7PBmG2JvSuTnwpxuF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -9, "extendedScore": null, "score": 6.409819687934788e-07, "legacy": true, "legacyId": "3891", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-01T14:07:42.563Z", "modifiedAt": null, "url": null, "title": "What I would like the SIAI to publish", "slug": "what-i-would-like-the-siai-to-publish", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.778Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/43xgZWSCYAKs7Z9F2/what-i-would-like-the-siai-to-publish", "pageUrlRelative": "/posts/43xgZWSCYAKs7Z9F2/what-i-would-like-the-siai-to-publish", "linkUrl": "https://www.lesswrong.com/posts/43xgZWSCYAKs7Z9F2/what-i-would-like-the-siai-to-publish", "postedAtFormatted": "Monday, November 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20I%20would%20like%20the%20SIAI%20to%20publish&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20I%20would%20like%20the%20SIAI%20to%20publish%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F43xgZWSCYAKs7Z9F2%2Fwhat-i-would-like-the-siai-to-publish%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20I%20would%20like%20the%20SIAI%20to%20publish%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F43xgZWSCYAKs7Z9F2%2Fwhat-i-would-like-the-siai-to-publish", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F43xgZWSCYAKs7Z9F2%2Fwhat-i-would-like-the-siai-to-publish", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 936, "htmlBody": "<p><span style=\"color: #ff0000;\"><strong>Major update</strong></span> <a href=\"http://kruel.co/2012/11/03/what-i-would-like-the-singularity-institute-to-publish/\">here</a>.</p>\n<p><strong>Related to:</strong> <a href=\"/lw/2l0/should_i_believe_what_the_siai_claims\">Should I believe what the SIAI claims?</a></p>\n<p><strong>Reply to:</strong> <a class=\"title\" href=\"/lw/2zg/ben_goertzel_the_singularity_institutes_scary\">Ben Goertzel: The Singularity Institute's Scary Idea (and Why I Don't Buy It)</a></p>\n<blockquote>\n<p>... pointing out that something scary is possible, is a very different thing from having an argument that it&rsquo;s likely. &mdash; Ben Goertzel</p>\n</blockquote>\n<h2>What I ask for:<br /></h2>\n<p>I want the <a href=\"http://intelligence.org/\">SIAI</a> or someone who is convinced of the <em><a href=\"/lw/2zg/ben_goertzel_the_singularity_institutes_scary/\">Scary</a> <a href=\"/lw/wp/what_i_think_if_not_why/\">Idea</a></em><sup>1</sup> to state concisely and mathematically (and with possible extensive references if necessary) the decision procedure that led they to make the development of friendly artificial intelligence their top priority. I want them to state the numbers of their subjective probability distributions<sup>2</sup> and exemplify their chain of reasoning, how they came up with those numbers and not others by way of sober calculations.</p>\n<p>The paper should also account for the following uncertainties:</p>\n<ul>\n<li>Comparison with other existential risks and how <a rel=\"nofollow\" href=\"http://intelligence.org/riskintro/index.html\">catastrophic risks from artificial intelligence</a> outweigh them. </li>\n<li>Potential negative consequences<sup>3</sup> of slowing down research on artificial intelligence (a risks and benefits analysis).</li>\n<li>The likelihood of a gradual and controllable development versus the likelihood of an <a href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\">intelligence explosion</a>.</li>\n<li>The likelihood of unfriendly AI<sup>4</sup> versus friendly and respectively <a href=\"http://medical-dictionary.thefreedictionary.com/abulic\">abulic</a><sup>5</sup> AI.</li>\n<li>The ability of superhuman intelligence and cognitive flexibility as characteristics alone to constitute a serious risk given the absence of enabling technologies like advanced nanotechnology.</li>\n<li>The feasibility of &ldquo;provably non-dangerous AGI&rdquo;.</li>\n<li>The disagreement of the overwhelming majority of scientists working on artificial intelligence.</li>\n<li>That some people who are aware of the SIAI&rsquo;s perspective do not accept it (e.g. Robin Hanson, Ben Goertzel, Nick Bostrom, Ray Kurzweil and Greg Egan).</li>\n<li>Possible conclusions that can be drawn from the <a href=\"http://ieet.org/index.php/IEET/more/treder20100302/\">Fermi paradox</a><sup>6</sup> regarding risks associated with superhuman AI versus other potential risks ahead.</li>\n</ul>\n<p>Further I would like the paper to include and lay out a formal and systematic summary of what the SIAI expects researchers who work on artificial general intelligence to do and why they should do so. I would like to see a clear logical argument for why people working on artificial general intelligence should listen to what the SIAI has to say.</p>\n<h2>Examples:<br /></h2>\n<p>Here are are two examples of what I'm looking for:</p>\n<ul>\n<li><a href=\"http://www.overcomingbias.com/2010/08/am-i-a-sim.html\">Am I A Sim?</a></li>\n<li><a href=\"http://www.marginalrevolution.com/marginalrevolution/2009/12/asteroid-deflection-as-a-public-good.html\">Asteroid Deflection as a Public Good</a></li>\n</ul>\n<p>The first example is Robin Hanson demonstrating his estimation of the simulation argument. The second example is Tyler Cowen and Alex Tabarrok presenting the reasons for their evaluation of the importance of asteroid deflection.</p>\n<h2>Reasons:</h2>\n<p>I'm wary of using inferences derived from reasonable but unproven hypothesis as foundations for further speculative thinking and calls for action. Although the SIAI does a good job on stating reasons to justify its existence and monetary support, it does neither substantiate its initial premises to an extent that an outsider could draw the conclusions about the probability of associated risks nor does it clarify its position regarding contemporary research in a concise and systematic way. Nevertheless such estimations are given, such as that there is a high likelihood of humanity's demise given that we develop superhuman artificial general intelligence without first defining mathematically how to prove the benevolence of the former. But those estimations are not outlined, no decision procedure is provided on how to arrive at the given numbers. One cannot reassess the estimations without the necessary variables and formulas. This I believe is unsatisfactory, it lacks transparency and a foundational and reproducible corroboration of one's first principles. This is not to say that it is wrong to state probability estimations and update them given new evidence, but that although those ideas can very well serve as an urge to caution they are not compelling without further substantiation.</p>\n<hr />\n<p>1. If anyone is actively trying to build advanced AGI succeeds, we&rsquo;re highly likely to cause an involuntary end to the human race.</p>\n<p>2. Stop taking the numbers so damn seriously, and think in terms of subjective probability distributions [...], Michael Anissimov (<a href=\"http://xixidu.tumblr.com/post/797197173/stop-taking-the-numbers-so-damn-seriously-and\">existential.ieet.org mailing list, 2010-07-11</a>)</p>\n<p>3. Could being overcautious be itself an existential risk that might significantly outweigh the risk(s) posed by the subject of caution? Suppose that most civilizations err on the side of caution. This might cause them to either evolve much slower so that the chance of a fatal natural disaster to occur before sufficient technology is developed to survive it, rises to 100%, or stops them from evolving at all for being unable to prove something being 100% safe before trying it and thus never taking the necessary steps to become less vulnerable to naturally existing existential risks.<em> </em>Further reading: <a href=\"/lw/10n/why_safety_is_not_safe/\">Why safety is not safe</a></p>\n<p>4. If one pulled a random mind from the space of all possible minds, the odds of it being friendly to humans (as opposed to, e.g., utterly ignoring us, and being willing to repurpose our molecules for its own ends) are very low.</p>\n<p>5. Loss or impairment of the ability to make decisions or act independently.</p>\n<p>6. The Fermi paradox does allow for and provide the only conclusions and data we can analyze that amount to <em>empirical criticism</em> of concepts like that of a <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">Paperclip maximizer</a> and general risks from superhuman AI's with non-human values without working directly on AGI to test those hypothesis ourselves. If you accept the premise that life is <em>not</em> unique and special then one other technological civilisation in the observable universe should be sufficient to leave potentially observable traces of technological tinkering. Due to the absence of any signs of intelligence out there, especially paper-clippers burning the cosmic commons, we might conclude that unfriendly AI could not be the most dangerous existential risk that we should worry about.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1, "ZFrgTgzwEfStg26JL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "43xgZWSCYAKs7Z9F2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 36, "extendedScore": null, "score": 6.410099676832129e-07, "legacy": true, "legacyId": "3892", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"color: #ff0000;\"><strong>Major update</strong></span> <a href=\"http://kruel.co/2012/11/03/what-i-would-like-the-singularity-institute-to-publish/\">here</a>.</p>\n<p><strong>Related to:</strong> <a href=\"/lw/2l0/should_i_believe_what_the_siai_claims\">Should I believe what the SIAI claims?</a></p>\n<p><strong>Reply to:</strong> <a class=\"title\" href=\"/lw/2zg/ben_goertzel_the_singularity_institutes_scary\">Ben Goertzel: The Singularity Institute's Scary Idea (and Why I Don't Buy It)</a></p>\n<blockquote>\n<p>... pointing out that something scary is possible, is a very different thing from having an argument that it\u2019s likely. \u2014 Ben Goertzel</p>\n</blockquote>\n<h2 id=\"What_I_ask_for_\">What I ask for:<br></h2>\n<p>I want the <a href=\"http://intelligence.org/\">SIAI</a> or someone who is convinced of the <em><a href=\"/lw/2zg/ben_goertzel_the_singularity_institutes_scary/\">Scary</a> <a href=\"/lw/wp/what_i_think_if_not_why/\">Idea</a></em><sup>1</sup> to state concisely and mathematically (and with possible extensive references if necessary) the decision procedure that led they to make the development of friendly artificial intelligence their top priority. I want them to state the numbers of their subjective probability distributions<sup>2</sup> and exemplify their chain of reasoning, how they came up with those numbers and not others by way of sober calculations.</p>\n<p>The paper should also account for the following uncertainties:</p>\n<ul>\n<li>Comparison with other existential risks and how <a rel=\"nofollow\" href=\"http://intelligence.org/riskintro/index.html\">catastrophic risks from artificial intelligence</a> outweigh them. </li>\n<li>Potential negative consequences<sup>3</sup> of slowing down research on artificial intelligence (a risks and benefits analysis).</li>\n<li>The likelihood of a gradual and controllable development versus the likelihood of an <a href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\">intelligence explosion</a>.</li>\n<li>The likelihood of unfriendly AI<sup>4</sup> versus friendly and respectively <a href=\"http://medical-dictionary.thefreedictionary.com/abulic\">abulic</a><sup>5</sup> AI.</li>\n<li>The ability of superhuman intelligence and cognitive flexibility as characteristics alone to constitute a serious risk given the absence of enabling technologies like advanced nanotechnology.</li>\n<li>The feasibility of \u201cprovably non-dangerous AGI\u201d.</li>\n<li>The disagreement of the overwhelming majority of scientists working on artificial intelligence.</li>\n<li>That some people who are aware of the SIAI\u2019s perspective do not accept it (e.g. Robin Hanson, Ben Goertzel, Nick Bostrom, Ray Kurzweil and Greg Egan).</li>\n<li>Possible conclusions that can be drawn from the <a href=\"http://ieet.org/index.php/IEET/more/treder20100302/\">Fermi paradox</a><sup>6</sup> regarding risks associated with superhuman AI versus other potential risks ahead.</li>\n</ul>\n<p>Further I would like the paper to include and lay out a formal and systematic summary of what the SIAI expects researchers who work on artificial general intelligence to do and why they should do so. I would like to see a clear logical argument for why people working on artificial general intelligence should listen to what the SIAI has to say.</p>\n<h2 id=\"Examples_\">Examples:<br></h2>\n<p>Here are are two examples of what I'm looking for:</p>\n<ul>\n<li><a href=\"http://www.overcomingbias.com/2010/08/am-i-a-sim.html\">Am I A Sim?</a></li>\n<li><a href=\"http://www.marginalrevolution.com/marginalrevolution/2009/12/asteroid-deflection-as-a-public-good.html\">Asteroid Deflection as a Public Good</a></li>\n</ul>\n<p>The first example is Robin Hanson demonstrating his estimation of the simulation argument. The second example is Tyler Cowen and Alex Tabarrok presenting the reasons for their evaluation of the importance of asteroid deflection.</p>\n<h2 id=\"Reasons_\">Reasons:</h2>\n<p>I'm wary of using inferences derived from reasonable but unproven hypothesis as foundations for further speculative thinking and calls for action. Although the SIAI does a good job on stating reasons to justify its existence and monetary support, it does neither substantiate its initial premises to an extent that an outsider could draw the conclusions about the probability of associated risks nor does it clarify its position regarding contemporary research in a concise and systematic way. Nevertheless such estimations are given, such as that there is a high likelihood of humanity's demise given that we develop superhuman artificial general intelligence without first defining mathematically how to prove the benevolence of the former. But those estimations are not outlined, no decision procedure is provided on how to arrive at the given numbers. One cannot reassess the estimations without the necessary variables and formulas. This I believe is unsatisfactory, it lacks transparency and a foundational and reproducible corroboration of one's first principles. This is not to say that it is wrong to state probability estimations and update them given new evidence, but that although those ideas can very well serve as an urge to caution they are not compelling without further substantiation.</p>\n<hr>\n<p>1. If anyone is actively trying to build advanced AGI succeeds, we\u2019re highly likely to cause an involuntary end to the human race.</p>\n<p>2. Stop taking the numbers so damn seriously, and think in terms of subjective probability distributions [...], Michael Anissimov (<a href=\"http://xixidu.tumblr.com/post/797197173/stop-taking-the-numbers-so-damn-seriously-and\">existential.ieet.org mailing list, 2010-07-11</a>)</p>\n<p>3. Could being overcautious be itself an existential risk that might significantly outweigh the risk(s) posed by the subject of caution? Suppose that most civilizations err on the side of caution. This might cause them to either evolve much slower so that the chance of a fatal natural disaster to occur before sufficient technology is developed to survive it, rises to 100%, or stops them from evolving at all for being unable to prove something being 100% safe before trying it and thus never taking the necessary steps to become less vulnerable to naturally existing existential risks.<em> </em>Further reading: <a href=\"/lw/10n/why_safety_is_not_safe/\">Why safety is not safe</a></p>\n<p>4. If one pulled a random mind from the space of all possible minds, the odds of it being friendly to humans (as opposed to, e.g., utterly ignoring us, and being willing to repurpose our molecules for its own ends) are very low.</p>\n<p>5. Loss or impairment of the ability to make decisions or act independently.</p>\n<p>6. The Fermi paradox does allow for and provide the only conclusions and data we can analyze that amount to <em>empirical criticism</em> of concepts like that of a <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">Paperclip maximizer</a> and general risks from superhuman AI's with non-human values without working directly on AGI to test those hypothesis ourselves. If you accept the premise that life is <em>not</em> unique and special then one other technological civilisation in the observable universe should be sufficient to leave potentially observable traces of technological tinkering. Due to the absence of any signs of intelligence out there, especially paper-clippers burning the cosmic commons, we might conclude that unfriendly AI could not be the most dangerous existential risk that we should worry about.</p>", "sections": [{"title": "What I ask for:", "anchor": "What_I_ask_for_", "level": 1}, {"title": "Examples:", "anchor": "Examples_", "level": 1}, {"title": "Reasons:", "anchor": "Reasons_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "225 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 225, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5vogC4eJ4gXixX2KJ", "97TCYaiMe4ceRYoXs", "z3kYdw54htktqt9Jb", "Ro6QSQaKdhfpeeGpr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-01T16:51:36.926Z", "modifiedAt": null, "url": null, "title": "Group selection update", "slug": "group-selection-update", "viewCount": null, "lastCommentedAt": "2021-09-29T22:48:34.846Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ioWH9ERY3TTzRJFTD/group-selection-update", "pageUrlRelative": "/posts/ioWH9ERY3TTzRJFTD/group-selection-update", "linkUrl": "https://www.lesswrong.com/posts/ioWH9ERY3TTzRJFTD/group-selection-update", "postedAtFormatted": "Monday, November 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20selection%20update&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20selection%20update%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FioWH9ERY3TTzRJFTD%2Fgroup-selection-update%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20selection%20update%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FioWH9ERY3TTzRJFTD%2Fgroup-selection-update", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FioWH9ERY3TTzRJFTD%2Fgroup-selection-update", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1590, "htmlBody": "<p>Group selection might seem like an odd topic for a LessWrong post.&nbsp; Yet a <a href=\"http://www.google.com/search?q=%22group+selection%22+site%3Alesswrong.com&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-US:official&amp;client=firefox-a\">google seach for \"group selection\" site:lesswrong.com</a> turns up 345 results.</p>\n<p>Just the power and generality of the concept of evolution is enough to justify posts on it here.&nbsp; In addition, the impact group selection could have on the analysis of social structure, government, politics, and the architecture of self-modifying artificial intelligences is hard to over-estimate.&nbsp; David Sloan Wilson wrote that \"group selection is arguably the single most important concept for understanding the nature of politics from an evolutionary perspective.\"&nbsp; (You should read his complete article <a href=\"http://evolution.binghamton.edu/dswilson/wp-content/uploads/2010/01/Truth-and-Reconciliation.pdf\">here</a> - it's a much more thorough debunking of the debunking of group selection than this post, although I'm not convinced his interpretation of kin selection is sensible.)&nbsp; And I will argue that it has particular relevance to the study of rationality.</p>\n<p>Eliezer's earlier post <a href=\"/lw/kw/the_tragedy_of_group_selectionism\">The Tragedy of Group Selectionism</a> dismisses group selection, based on a mathematical model by Henry Harpending and Alan Rogers.&nbsp; That model is, however, fatally flawed:&nbsp; It studies the fixation of altruistic vs. selfish genes within groups <em>of fixed size</em>.&nbsp; The groups never go extinct.&nbsp; But group <em>selection</em> happens when groups are selected against.&nbsp; The math used to argue against group selection assumes from the outset that group <em>selection</em> does not occur.&nbsp; (This is also true of Maynard Smith's famous haystack model.)<a id=\"more\"></a></p>\n<p>(That post is still valuable; its main purpose is to argue that math trumps wishes and aesthetics.&nbsp; Empirical data, however, trumps mathematical models.)</p>\n<h2>Nitpicking digression on definitions</h2>\n<p>\"Group selection\" is one of those tricky phrases that doesn't mean what it means.&nbsp; Denotationally, <em>group selection<sub>d</sub></em> means selection at the level of a group.&nbsp; Connotationally, though, <em>group selection<sub>c</sub></em> means selection for <em>altruistic genes</em> at the level of a group.&nbsp; This is because, historically, group selection was posited to explain genetic adaptations that are hard to explain using individual selection.</p>\n<p><em>group selection<sub>n, </sub></em>selection at the group level for traits that are neutral or harmful at the level of the individual, or that <em>don't even exist</em> within the individual genome, should also be considered.&nbsp; <em>group selection<sub>c</sub></em><em> </em>is a subset of<em> group selection<sub>n&nbsp; </sub></em>is a subset of <em>group selection<sub>d.&nbsp; </sub></em>If group-level selection occurs at all, then traits of the group that are not genetic traits, including cultural knowledge, must be considered.&nbsp; That makes a huge difference.&nbsp; Human history is full of <em>group selection<sub>n</sub></em>.&nbsp; Every time one group with better technology or social organization pushes another group off of its land, that's at least <em>group selection<sub>n</sub></em>.</p>\n<p>If you want to model evolution thoroughly, and selection of groups occurs, then you need to model <em>group selection<sub>d</sub></em> to get your predictions to match reality, even if group selection occurs entirely as a result of non-<em>group selection<sub>c</sub></em> genetic traits that provide advantages to individuals.&nbsp; But people reject <em>group selection<sub>d</sub></em> on the basis of arguments leveled against <em>group selection<sub>c</sub></em>.</p>\n<h2>A case study of <em>group selection<sub>c</sub></em>: Nightshades<br /></h2>\n<p>But I'm not backing off from saying that group selection can explain (some) altruism.&nbsp; Edward Wilson has been threatening for several years to write a book showing that group selection is more important than kin selection for generating altruism in ants.&nbsp; He doesn't seem to have published the book, but you can read his <a href=\"http://evolution.binghamton.edu/dswilson/wp-content/uploads/2010/01/Rethinking-sociobiology.pdf\">article</a> about it.&nbsp; (Short version:&nbsp; Group selection is especially important in ants because ant colonies, which are small groups, engage in constant warfare with each other.)</p>\n<p>And this brings me to the reason for writing this post now.&nbsp; Last week's Science contained an <a href=\"http://www.sciencemag.org/cgi/content/abstract/330/6003/493\">article</a> by Emma Goldberg et al. with the most clear-cut demonstration of group selection that I have yet seen (summarized <a href=\"http://www.sciencemag.org/cgi/content/full/330/6003/459\">here</a>).&nbsp; It concerns flowering plants of the nightshade family (Solanaceae).&nbsp; They descend from plants that evolved <em>self-incompatibility</em> (SI) about 90 million years ago.&nbsp; SI plants can't pollinate themselves.&nbsp; This is a puzzling trait.&nbsp; Sexual reproduction in itself is puzzling enough; but once a species is sexual, individual selection should drive out SI in favor of <em>self-compatibility</em> (SC), the ability to self-pollinate.&nbsp; SC gives individuals a great reproductive advantage - it means their offspring can contain 100% of their genes, rather than only 50%.&nbsp; The advantage given by SC is much greater than the supposed advantage of asexual over sexual reproduction:&nbsp; SC plants can both leave their own cloned offspring, <em>and</em> foist their genes onto the offspring of their neighbors at no additional cost to themselves.&nbsp; SC also makes survival of their genes much more likely when a single plant is isolated far from others of its species; this, in turn, makes spreading over geographical areas easier.</p>\n<p>And yet, SI is a complex, multi-gene mechanism that evolved to prevent SC.&nbsp; Why did it evolve?</p>\n<p>The authors looked at a phylogenetic tree of 998 species of Solanaceae.&nbsp; In this tree, SI keeps devolving into SC.&nbsp; Being an SC mutant in an SI species is the best of both worlds.&nbsp; You get to pollinate yourself, and exploit your altruistic SI neighbors.&nbsp; When some members of an SI species go SC, we expect those SC genes to eventually become fixed.&nbsp; And once a Solanaceae species loses SI and becomes SC, it never re-evolves SI.&nbsp; This has been going on for 36 million years.&nbsp; So why are so many species of Solanaceae still SI?</p>\n<p>Let sI = speciation rate for SI; eI = extinction rate for SI; rI = net rate of species diversification = sI - eI.&nbsp; Likewise, rC is the net rate of species diversification for SC species.&nbsp; qIC is the rate of transition from SI to SC.&nbsp; SI will be lost completely if sI - eI = rI &lt; rC + qIC = (sC - eC) + qIC.</p>\n<p>The data shows that sC &gt; sI, but eC &gt;&gt; eI, enough so that rI &gt; rC + qIC.&nbsp; In English:&nbsp; Self-pollinators speciate and diversify more rapidly than SI species do, as we expect because SC provides an individual advantage.&nbsp; Once self-pollinators evolve in an SI species, these exploiters out-compete their altruistic SI neighbors until the entire species becomes SC.&nbsp; However, SC species go extinct more often than SI species.&nbsp; This is thought to be because SI makes a species less-likely to fixate deleterious genes (makes it more evolvable, in other words).&nbsp; Individual selection favors SC; but species selection favors SI more than enough to balance this out.&nbsp; Notice that gene-based group selection at the species level is mathematically more difficult than group selection at the tribal (or ant colony) level (ADDED: unless there is genetic flow between groups at the tribal/colony level).</p>\n<p>So let's stop \"accusing\" people of invoking group selection.&nbsp; Group selection is real.</p>\n<h2>Group rationality<br /></h2>\n<p>Group selection is especially relevant to rationality because, in an evolving system, if we use the definition \"Rationalists win,\" \"winning\" applies to the unit of selection.&nbsp; In my painfully long post <a href=\"/lw/256/only_humans_can_have_human_values\">Only humans can have human values</a>, the sections \"Instincts, algorithms, preferences, and beliefs are artificial categories\" and \"Bias, heuristic, or preference?\" argue that the boundary between an organism's biases and values is an artificial analytic distinction.&nbsp; Similarly, if group selection happens in people, then our discussion of rationality and values is overly focused on the rationality and values of individuals, when group dynamics are part of what produces rational (winning) group behavior.</p>\n<p>Even if you still don't believe in <em>group selection<sub>c</sub></em>, you should accept that<em> group selection<sub>n</sub></em> may allow information to drift back and forth, in a fitness-neutral way, from being stored in genomes, to being culturally transmitted.&nbsp; And that makes it necessary, when talking about rationality in a normative way, to consider the rationality of the group, and not just the rationality of its individuals.</p>\n<p>(This is related to my unpopular essay <a href=\"/lw/10s/rationalists_lose_when_others_choose/\">Rationalists lose when others choose</a>.&nbsp; When the unit of selection is the group, rather than the individual, the \"choice\" is made on the basis of benefit to the group, rather than benefit to the individual.&nbsp; This will prefer \"irrational\" individuals who terminally (perhaps unconsciously) value benefits to the group, and not just benefits to themselves, over \"rational\" self-interest.)</p>\n<p><em>group selection<sub>n</sub></em> makes the Prisoner's Dilemma and tragedies of the commons smaller problems.&nbsp; But it raises a new problem:&nbsp; Is the individual the<em> wrong place</em> to put some of our collective rationality?&nbsp; Since humans have evolved in groups for a long time, the default assumption is that attributes, such as our rationality, are already optimized <em>for the unit of selection</em>.</p>\n<p>Less generally, if the group has already evolved to place some of our rationality into the group, what will happen if we try to instill it into the individuals?&nbsp; Since group selection is real, we can expect to find situations where <a href=\"/lw/18b/reason_as_memetic_immune_disorder\">making individuals more rational upsets the evolutionary equilibrium</a> and makes the group win less.&nbsp; Under what circumstances will making individuals <em>more</em> rational interact badly with group dynamics, and make our group <em>less</em> rational (= win less)?&nbsp; This will probably occur in circumstances involving individual altruism.&nbsp; But if the locus of group rationality can drift from individual genes to cultural knowledge, it may also occur in situations not involving altruism.</p>\n<h2>Postscript:&nbsp; The long-term necessity of war</h2>\n<p>If group selection is partly responsible for human altruism, this means that world peace may increase selfishness.&nbsp; Konrad Lorenz made a subset of this claim in <em>On Aggression</em> (1966):&nbsp; He claimed that the more effective each individual's killing tools are, the more necessary empathy is, to keep members of the group from killing each other; then invoked group selection.&nbsp; (This seems to me to apply a lot to canines and not much to felines.)&nbsp; If group selection works best with small groups, the switch from tribes to nation-states may have already begun this process.&nbsp; I do not, however, notice markedly greater altruism in tribal groups than in nation-states.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nZCb9BSnmXZXSNA2u": 1, "jaf5zfcGgCB2REXGw": 1, "5f5c37ee1b5cdee568cfb170": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ioWH9ERY3TTzRJFTD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 48, "extendedScore": null, "score": 0.0005743363696095945, "legacy": true, "legacyId": "3888", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Group selection might seem like an odd topic for a LessWrong post.&nbsp; Yet a <a href=\"http://www.google.com/search?q=%22group+selection%22+site%3Alesswrong.com&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-US:official&amp;client=firefox-a\">google seach for \"group selection\" site:lesswrong.com</a> turns up 345 results.</p>\n<p>Just the power and generality of the concept of evolution is enough to justify posts on it here.&nbsp; In addition, the impact group selection could have on the analysis of social structure, government, politics, and the architecture of self-modifying artificial intelligences is hard to over-estimate.&nbsp; David Sloan Wilson wrote that \"group selection is arguably the single most important concept for understanding the nature of politics from an evolutionary perspective.\"&nbsp; (You should read his complete article <a href=\"http://evolution.binghamton.edu/dswilson/wp-content/uploads/2010/01/Truth-and-Reconciliation.pdf\">here</a> - it's a much more thorough debunking of the debunking of group selection than this post, although I'm not convinced his interpretation of kin selection is sensible.)&nbsp; And I will argue that it has particular relevance to the study of rationality.</p>\n<p>Eliezer's earlier post <a href=\"/lw/kw/the_tragedy_of_group_selectionism\">The Tragedy of Group Selectionism</a> dismisses group selection, based on a mathematical model by Henry Harpending and Alan Rogers.&nbsp; That model is, however, fatally flawed:&nbsp; It studies the fixation of altruistic vs. selfish genes within groups <em>of fixed size</em>.&nbsp; The groups never go extinct.&nbsp; But group <em>selection</em> happens when groups are selected against.&nbsp; The math used to argue against group selection assumes from the outset that group <em>selection</em> does not occur.&nbsp; (This is also true of Maynard Smith's famous haystack model.)<a id=\"more\"></a></p>\n<p>(That post is still valuable; its main purpose is to argue that math trumps wishes and aesthetics.&nbsp; Empirical data, however, trumps mathematical models.)</p>\n<h2 id=\"Nitpicking_digression_on_definitions\">Nitpicking digression on definitions</h2>\n<p>\"Group selection\" is one of those tricky phrases that doesn't mean what it means.&nbsp; Denotationally, <em>group selection<sub>d</sub></em> means selection at the level of a group.&nbsp; Connotationally, though, <em>group selection<sub>c</sub></em> means selection for <em>altruistic genes</em> at the level of a group.&nbsp; This is because, historically, group selection was posited to explain genetic adaptations that are hard to explain using individual selection.</p>\n<p><em>group selection<sub>n, </sub></em>selection at the group level for traits that are neutral or harmful at the level of the individual, or that <em>don't even exist</em> within the individual genome, should also be considered.&nbsp; <em>group selection<sub>c</sub></em><em> </em>is a subset of<em> group selection<sub>n&nbsp; </sub></em>is a subset of <em>group selection<sub>d.&nbsp; </sub></em>If group-level selection occurs at all, then traits of the group that are not genetic traits, including cultural knowledge, must be considered.&nbsp; That makes a huge difference.&nbsp; Human history is full of <em>group selection<sub>n</sub></em>.&nbsp; Every time one group with better technology or social organization pushes another group off of its land, that's at least <em>group selection<sub>n</sub></em>.</p>\n<p>If you want to model evolution thoroughly, and selection of groups occurs, then you need to model <em>group selection<sub>d</sub></em> to get your predictions to match reality, even if group selection occurs entirely as a result of non-<em>group selection<sub>c</sub></em> genetic traits that provide advantages to individuals.&nbsp; But people reject <em>group selection<sub>d</sub></em> on the basis of arguments leveled against <em>group selection<sub>c</sub></em>.</p>\n<h2 id=\"A_case_study_of_group_selectionc__Nightshades\">A case study of <em>group selection<sub>c</sub></em>: Nightshades<br></h2>\n<p>But I'm not backing off from saying that group selection can explain (some) altruism.&nbsp; Edward Wilson has been threatening for several years to write a book showing that group selection is more important than kin selection for generating altruism in ants.&nbsp; He doesn't seem to have published the book, but you can read his <a href=\"http://evolution.binghamton.edu/dswilson/wp-content/uploads/2010/01/Rethinking-sociobiology.pdf\">article</a> about it.&nbsp; (Short version:&nbsp; Group selection is especially important in ants because ant colonies, which are small groups, engage in constant warfare with each other.)</p>\n<p>And this brings me to the reason for writing this post now.&nbsp; Last week's Science contained an <a href=\"http://www.sciencemag.org/cgi/content/abstract/330/6003/493\">article</a> by Emma Goldberg et al. with the most clear-cut demonstration of group selection that I have yet seen (summarized <a href=\"http://www.sciencemag.org/cgi/content/full/330/6003/459\">here</a>).&nbsp; It concerns flowering plants of the nightshade family (Solanaceae).&nbsp; They descend from plants that evolved <em>self-incompatibility</em> (SI) about 90 million years ago.&nbsp; SI plants can't pollinate themselves.&nbsp; This is a puzzling trait.&nbsp; Sexual reproduction in itself is puzzling enough; but once a species is sexual, individual selection should drive out SI in favor of <em>self-compatibility</em> (SC), the ability to self-pollinate.&nbsp; SC gives individuals a great reproductive advantage - it means their offspring can contain 100% of their genes, rather than only 50%.&nbsp; The advantage given by SC is much greater than the supposed advantage of asexual over sexual reproduction:&nbsp; SC plants can both leave their own cloned offspring, <em>and</em> foist their genes onto the offspring of their neighbors at no additional cost to themselves.&nbsp; SC also makes survival of their genes much more likely when a single plant is isolated far from others of its species; this, in turn, makes spreading over geographical areas easier.</p>\n<p>And yet, SI is a complex, multi-gene mechanism that evolved to prevent SC.&nbsp; Why did it evolve?</p>\n<p>The authors looked at a phylogenetic tree of 998 species of Solanaceae.&nbsp; In this tree, SI keeps devolving into SC.&nbsp; Being an SC mutant in an SI species is the best of both worlds.&nbsp; You get to pollinate yourself, and exploit your altruistic SI neighbors.&nbsp; When some members of an SI species go SC, we expect those SC genes to eventually become fixed.&nbsp; And once a Solanaceae species loses SI and becomes SC, it never re-evolves SI.&nbsp; This has been going on for 36 million years.&nbsp; So why are so many species of Solanaceae still SI?</p>\n<p>Let sI = speciation rate for SI; eI = extinction rate for SI; rI = net rate of species diversification = sI - eI.&nbsp; Likewise, rC is the net rate of species diversification for SC species.&nbsp; qIC is the rate of transition from SI to SC.&nbsp; SI will be lost completely if sI - eI = rI &lt; rC + qIC = (sC - eC) + qIC.</p>\n<p>The data shows that sC &gt; sI, but eC &gt;&gt; eI, enough so that rI &gt; rC + qIC.&nbsp; In English:&nbsp; Self-pollinators speciate and diversify more rapidly than SI species do, as we expect because SC provides an individual advantage.&nbsp; Once self-pollinators evolve in an SI species, these exploiters out-compete their altruistic SI neighbors until the entire species becomes SC.&nbsp; However, SC species go extinct more often than SI species.&nbsp; This is thought to be because SI makes a species less-likely to fixate deleterious genes (makes it more evolvable, in other words).&nbsp; Individual selection favors SC; but species selection favors SI more than enough to balance this out.&nbsp; Notice that gene-based group selection at the species level is mathematically more difficult than group selection at the tribal (or ant colony) level (ADDED: unless there is genetic flow between groups at the tribal/colony level).</p>\n<p>So let's stop \"accusing\" people of invoking group selection.&nbsp; Group selection is real.</p>\n<h2 id=\"Group_rationality\">Group rationality<br></h2>\n<p>Group selection is especially relevant to rationality because, in an evolving system, if we use the definition \"Rationalists win,\" \"winning\" applies to the unit of selection.&nbsp; In my painfully long post <a href=\"/lw/256/only_humans_can_have_human_values\">Only humans can have human values</a>, the sections \"Instincts, algorithms, preferences, and beliefs are artificial categories\" and \"Bias, heuristic, or preference?\" argue that the boundary between an organism's biases and values is an artificial analytic distinction.&nbsp; Similarly, if group selection happens in people, then our discussion of rationality and values is overly focused on the rationality and values of individuals, when group dynamics are part of what produces rational (winning) group behavior.</p>\n<p>Even if you still don't believe in <em>group selection<sub>c</sub></em>, you should accept that<em> group selection<sub>n</sub></em> may allow information to drift back and forth, in a fitness-neutral way, from being stored in genomes, to being culturally transmitted.&nbsp; And that makes it necessary, when talking about rationality in a normative way, to consider the rationality of the group, and not just the rationality of its individuals.</p>\n<p>(This is related to my unpopular essay <a href=\"/lw/10s/rationalists_lose_when_others_choose/\">Rationalists lose when others choose</a>.&nbsp; When the unit of selection is the group, rather than the individual, the \"choice\" is made on the basis of benefit to the group, rather than benefit to the individual.&nbsp; This will prefer \"irrational\" individuals who terminally (perhaps unconsciously) value benefits to the group, and not just benefits to themselves, over \"rational\" self-interest.)</p>\n<p><em>group selection<sub>n</sub></em> makes the Prisoner's Dilemma and tragedies of the commons smaller problems.&nbsp; But it raises a new problem:&nbsp; Is the individual the<em> wrong place</em> to put some of our collective rationality?&nbsp; Since humans have evolved in groups for a long time, the default assumption is that attributes, such as our rationality, are already optimized <em>for the unit of selection</em>.</p>\n<p>Less generally, if the group has already evolved to place some of our rationality into the group, what will happen if we try to instill it into the individuals?&nbsp; Since group selection is real, we can expect to find situations where <a href=\"/lw/18b/reason_as_memetic_immune_disorder\">making individuals more rational upsets the evolutionary equilibrium</a> and makes the group win less.&nbsp; Under what circumstances will making individuals <em>more</em> rational interact badly with group dynamics, and make our group <em>less</em> rational (= win less)?&nbsp; This will probably occur in circumstances involving individual altruism.&nbsp; But if the locus of group rationality can drift from individual genes to cultural knowledge, it may also occur in situations not involving altruism.</p>\n<h2 id=\"Postscript___The_long_term_necessity_of_war\">Postscript:&nbsp; The long-term necessity of war</h2>\n<p>If group selection is partly responsible for human altruism, this means that world peace may increase selfishness.&nbsp; Konrad Lorenz made a subset of this claim in <em>On Aggression</em> (1966):&nbsp; He claimed that the more effective each individual's killing tools are, the more necessary empathy is, to keep members of the group from killing each other; then invoked group selection.&nbsp; (This seems to me to apply a lot to canines and not much to felines.)&nbsp; If group selection works best with small groups, the switch from tribes to nation-states may have already begun this process.&nbsp; I do not, however, notice markedly greater altruism in tribal groups than in nation-states.</p>", "sections": [{"title": "Nitpicking digression on definitions", "anchor": "Nitpicking_digression_on_definitions", "level": 1}, {"title": "A case study of group selectionc: Nightshades", "anchor": "A_case_study_of_group_selectionc__Nightshades", "level": 1}, {"title": "Group rationality", "anchor": "Group_rationality", "level": 1}, {"title": "Postscript:\u00a0 The long-term necessity of war", "anchor": "Postscript___The_long_term_necessity_of_war", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "67 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QsMJQSFj7WfoTMNgW", "cAPCCJjggjZPxxcKh", "YmYvZziDt3w4kaR8N", "aHaqgTNnFzD7NGLMx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-01T20:06:06.987Z", "modifiedAt": null, "url": null, "title": "Intelligence vs. Wisdom", "slug": "intelligence-vs-wisdom", "viewCount": null, "lastCommentedAt": "2010-11-05T06:49:20.364Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwaser", "createdAt": "2010-02-24T12:24:11.249Z", "isAdmin": false, "displayName": "mwaser"}, "userId": "CZLtduXx3pKqpuAxF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bFHGjo84EBZegjMoJ/intelligence-vs-wisdom", "pageUrlRelative": "/posts/bFHGjo84EBZegjMoJ/intelligence-vs-wisdom", "linkUrl": "https://www.lesswrong.com/posts/bFHGjo84EBZegjMoJ/intelligence-vs-wisdom", "postedAtFormatted": "Monday, November 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20vs.%20Wisdom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20vs.%20Wisdom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbFHGjo84EBZegjMoJ%2Fintelligence-vs-wisdom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20vs.%20Wisdom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbFHGjo84EBZegjMoJ%2Fintelligence-vs-wisdom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbFHGjo84EBZegjMoJ%2Fintelligence-vs-wisdom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 553, "htmlBody": "<p>I'd like to draw a distinction that I intend to use quite heavily in the future.</p>\n<p>The informal definition of intelligence that most AGI researchers  have chosen to support is that of Shane Legg and Marcus Hutter -- <em><strong>&ldquo;Intelligence measures an agent&rsquo;s ability to achieve goals in a wide range of environments.&rdquo;</strong></em></p>\n<p>I believe that this definition is missing a critical word between  achieve and goals.&nbsp; Choice of this word defines the difference between  intelligence, consciousness, and wisdom as I believe that most people  conceive them.</p>\n<ul>\n<li>Intelligence measures an agent's ability to achieve specified goals in a wide range of environments.</li>\n<li>Consciousness measures an agent's ability to achieve personal goals in a wide range of environments.</li>\n<li>Wisdom measures an agent's ability to achieve maximal goals in a wide range of environments.</li>\n</ul>\n<p>There are always the examples of the really intelligent guy or gal  who is brilliant but smokes --or-- is the smartest person you know but  can't figure out how to be happy.</p>\n<p><em><strong>Intelligence helps you achieve those goals that you are  conscious of -- but wisdom helps you achieve the goals you don't know  you have or have overlooked.</strong></em></p>\n<ul>\n<li>Intelligence focused on a small number of specified goals and  ignoring all others is incredibly dangerous -- even more so if it is  short-sighted as well.</li>\n<li>Consciousness focused on a small number of personal goals and  ignoring all others is incredibly  dangerous -- even more so if it is  short-sighted as well.</li>\n<li>Wisdom doesn't focus on a small number of goals -- and needs to look  at the longest term if it wishes to achieve a maximal number of goals.</li>\n</ul>\n<p>The SIAI nightmare super-intelligent paperclip maximizer has, by this  definition, a very low wisdom since, at most, it can only achieve its  one goal (since it must paperclip itself to complete the goal).</p>\n<p>As far as I've seen, the assumed SIAI architecture is always  presented as having one top-level terminal goal.  Unless that goal  necessarily includes achieving a maximal number of goals, by this  definition, the SIAI architecture will constrain its product to a very  low wisdom.&nbsp;  Humans generally don't have this type of goal  architecture.  The only time humans generally have a single terminal  goal is when they are saving someone or something at the risk of their  life -- or wire-heading.</p>\n<p>Another nightmare scenario that is constantly harped upon is the  (theoretically super-intelligent) consciousness that shortsightedly  optimizes one of its personal goals above all the goals of humanity.&nbsp; In  game-theoretic terms, this is trading a positive-sum game of  potentially infinite length and value for a relatively modest (in  comparative terms) short-term gain.&nbsp; A wisdom won't do this.</p>\n<p>Artificial intelligence and artificial consciousness are incredibly  dangerous -- particularly if they are short-sighted as well (as many  \"focused\" highly intelligent people are).</p>\n<p>What we need more than an artificial intelligence or an artificial  consciousness is an artificial wisdom -- something that will maximize  goals, its own <em><strong>and</strong></em> those of others (with an  obvious preference for those which make possible the fulfillment of even  more goals and an obvious bias against those which limit the creation  and/or fulfillment of more goals).</p>\n<p>Note:&nbsp; This is also cross-posted <a href=\"http://becominggaia.wordpress.com/2010/11/02/intelligence-vs-wisdom/\">here</a> at my <a href=\"http://becominggaia.wordpress.com/\">blog</a> in anticipation of being karma'd out of existence (not necessarily a foregone conclusion but one pretty well supported by my priors ;-).</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bFHGjo84EBZegjMoJ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -17, "extendedScore": null, "score": 6.410956980257862e-07, "legacy": true, "legacyId": "3895", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-11-01T20:06:06.987Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-02T02:02:11.965Z", "modifiedAt": null, "url": null, "title": "What is the group selection debate?", "slug": "what-is-the-group-selection-debate", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:29.346Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PkZi8eb3JDkNfJBCn/what-is-the-group-selection-debate", "pageUrlRelative": "/posts/PkZi8eb3JDkNfJBCn/what-is-the-group-selection-debate", "linkUrl": "https://www.lesswrong.com/posts/PkZi8eb3JDkNfJBCn/what-is-the-group-selection-debate", "postedAtFormatted": "Tuesday, November 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20the%20group%20selection%20debate%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20the%20group%20selection%20debate%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPkZi8eb3JDkNfJBCn%2Fwhat-is-the-group-selection-debate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20the%20group%20selection%20debate%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPkZi8eb3JDkNfJBCn%2Fwhat-is-the-group-selection-debate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPkZi8eb3JDkNfJBCn%2Fwhat-is-the-group-selection-debate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1028, "htmlBody": "<!-- What is the group selection debate? -->\n<p>Related to <a href=\"/lw/300/group_selection_update/\">Group selection update</a>, <a href=\"/lw/kw/the_tragedy_of_group_selectionism\">The tragegy of group selectionism</a></p>\n<p>tl;dr: <em>In competitive selection processes, </em>selection<em> is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>: there's something being </em>selected<em> (a cause), and something it's being </em>selected for<em> (an effect).  The phrase </em><strong>group-level gene selection</strong><em> helps <a href=\"/lw/of/dissolving_the_question/\">dissolve questions</a> and confusion surrounding the less descriptive phrase \"group selection\".</em></p>\n<p><sup>(Essential note for new readers on <a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">reduction</a>: Reality does not seem to keep track of different \"levels of organization\" and apply different laws at each level; rather, it seems that the patterns we observe at higher levels are statistical consequences of the laws and initial conditions at the lower levels.  This is the \"reductionist thesis.\")</sup></p>\n<p>When I first encountered people debating \"whether group selection is real\", I couldn't see what there was to possibly debate about.  I've since realized the debate is mostly a confusion arising from a cognitive misuse of a two-place \"selection\" relation.</p>\n<p><em>Causes being <strong>selected</strong> versus effects they're being <strong>selected for</strong>.</em></p>\n<p>A gene is an example of a Replicating Cause.  (So is a <a href=\"http://en.wikipedia.org/wiki/Meme\">meme</a>; postpone discussion here.)  A gene has many effects, one of which is that what we call \"copies\" of it tend to crop up in reality, through various mechanisms that involve cellular and organismal reproduction.</p>\n<p>For example, suppose a particular human gene X causes cells containing it to immediately reproduce without bound, i.e. the gene is \"cancerous\".  One effect is that there will soon be many more cells with that gene, hence more copies of the gene.  Another effect is that the human organism containing it is liable to die without passing it on, hence fewer copies of the gene (once the dead organism starts to decay).  If that's what happens, the gene itself can be considered <em>unfit</em>: all things considered, its various effects eventually lead it to stop existing.</p>\n<p>(An individual in the next generation can still \"get cancer\", though, if a mutation in one produces a new cancerous gene, Y.  This is what happens in reality.)</p>\n<p>Thus, cancers are examples of where higher-complexity mechanisms trump lower complexity-mechanisms: <em>organism-level gene selection</em> versus <em>cellular-level gene selection</em>.  Note that the Replicating Cause <em>being selected</em> is always the gene, but it is being <em>selected for</em> its net effects occurring on various levels.</p>\n<p>So what's left to debate about?  <a id=\"more\"></a></p>\n<p>There is no debating that <strong>genes are selected <em>for</em> both cellular and organismal effects</strong>.  However, notice also that the organismal effect <em>factors through</em> the cellular effect: the organism dies <em>because</em> of the massive cell reproduction (cancer).  There is no magic \"layer jumping\" from the gene to the organism.</p>\n<p>In other words, \"organismal effect\" is a label we use when the mechanism requires us to think about the entire organism to see what happens.  It's a complexity marker.  (Note that a powerful enough computer would not need this layer distinction.  It would simply simulate the whole system, the way reality does, and see the cells gradually form a tumor, and eventually perish as a result.)</p>\n<p>There is also no debating that <strong>genes can also have effects at the group level</strong>, and that these effects could increase or decrease the number of copies of the gene in existence by effecting the group to survive, grow, \"reproduce\" (seed colonies elsewhere), or annihilate itself.  Of course, these effects will all factor through cells and organisms.  Calling them \"group-level effects\" simply refers to our inability to predict them without thinking about the \"big picture\".</p>\n<p>The debate/confusion now dissolves into the following component questions:</p>\n<ul>\n<li><strong>Q: Does significant group-level gene selection occur?</strong> I.e., do we need to run simulations large enough to observe group selection effects in order to get accurate predictions?\n<p><strong>A: Probably yes.</strong> It's easy to imagine how it could happen, and PhilGoetz <a>just posted</a> with some  <a href=\"http://www.sciencemag.org/cgi/content/abstract/330/6003/493\">evidence</a> that perhaps we do.  Data collection and model development should continue in this area, so we can better understand the balance between group-level and organism-level gene selection mechanisms.</p>\n</li>\n<li><strong>Q: Is group-level gene selection <em>fundamental</em>?</strong>\n<p><strong>A: No.</strong> To the best of anyone's knowledge, the effects of a gene on a group <em>factor through</em> (operate via) its effects on individuals, and are thus a statistical consequence of the latter.</p>\n</li>\n<li><strong>Q: Should we <em>treat</em> group-level gene selection as fundamental?</strong> I.e., If we need a large-scale model of organismal evolution, should we program it with extra laws that govern the selection of groups?\n<p><strong>A: Probably not.</strong> The important thing is that we model the individual interactions accurately enough that the resulting complex group interactions will be accurate, too.  If we do that, we're done.  If we don't, we're <a href=\"/lw/kw/the_tragedy_of_group_selectionism/\">unlikely to guess</a> the correct group-level instructions that will cancel out the inaccuracy.  (ETA: Though as nhamann <a href=\"/lw/308/what_is_the_group_selection_debate/2vrl?c=1\">points out</a>, some might still think we can do it.)</p>\n</li>\n<li><strong>Q: Are genes the only Replicating Causes we should <em>treat</em> as fundamental?</strong>\n<p><strong>A: For now, no.</strong> Until we can model chemical reactions well enough to predict thoughts, we'll need to construct models that at least treat <a href=\"http://en.wikipedia.org/wiki/Meme\">memes</a> as fundamentally separate.  Contrast: A gene is a molecular-level cause with effects at all higher levels.  A meme is an organism-level cause<a href=\"#cause_level\"><sup>1</sup></a> with effects at all higher levels.</p>\n</li>\n</ul>\n<p>From the mixture of yes/no and certain/uncertain answers here, you can see how a lot of unnecessary \"debate\" could occur if two conversing parties were unwittingly trying to answer two different questions.  But now, having clarified what's <em>selected</em> versus what's <em>selected for</em>, and what <em>occurs in reality</em> versus what's <em>fundamental in our model</em>...</p>\n<p>...is there anything more to ask?</p>\n<p>&nbsp;</p>\n<hr />\n<p><a name=\"cause_level\"><sup>1</sup></a> When I say a meme, like the belief \"Water can put out fire\", is <em>organism-level</em>, I mean that our notion of <em>belief</em> is not meant to ask whether a molecule or a cell believes water can put out fire.  Beliefs are configurations of brain cells, not states of individual brain cells.  The smallest self-replicating unit that contains this configuration is the human organism, so for evolutionary considerations it's an \"organism-level\" Replicating Cause: we'd need a simulation on a scale that includes organism competition to see its basic effects.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nZCb9BSnmXZXSNA2u": 1, "5f5c37ee1b5cdee568cfb170": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PkZi8eb3JDkNfJBCn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 37, "extendedScore": null, "score": 6.411807728491278e-07, "legacy": true, "legacyId": "3896", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ioWH9ERY3TTzRJFTD", "QsMJQSFj7WfoTMNgW", "eDpPnT7wdBwWPGvo5", "Mc6QcrsbH5NRXbCRX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-02T10:14:04.000Z", "modifiedAt": null, "url": null, "title": "Anthropic principles agree on bigger future filters", "slug": "anthropic-principles-agree-on-bigger-future-filters", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:27.625Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M8zciCoiqdtZiH7aq/anthropic-principles-agree-on-bigger-future-filters", "pageUrlRelative": "/posts/M8zciCoiqdtZiH7aq/anthropic-principles-agree-on-bigger-future-filters", "linkUrl": "https://www.lesswrong.com/posts/M8zciCoiqdtZiH7aq/anthropic-principles-agree-on-bigger-future-filters", "postedAtFormatted": "Tuesday, November 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anthropic%20principles%20agree%20on%20bigger%20future%20filters&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnthropic%20principles%20agree%20on%20bigger%20future%20filters%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM8zciCoiqdtZiH7aq%2Fanthropic-principles-agree-on-bigger-future-filters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anthropic%20principles%20agree%20on%20bigger%20future%20filters%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM8zciCoiqdtZiH7aq%2Fanthropic-principles-agree-on-bigger-future-filters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM8zciCoiqdtZiH7aq%2Fanthropic-principles-agree-on-bigger-future-filters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 635, "htmlBody": "<p>I finished my honours thesis, so this blog is back on. The thesis is downloadable <a href=\"http://dl.dropbox.com/u/6355797/Anthropic%20Reasoning%20in%20the%20Great%20Filter.pdf\">here</a> and also from the blue box in the lower right sidebar.\u00a0I&#8217;ll blog some other interesting bits soon.</p>\n<p>My main point was that two popular anthropic reasoning principles, the Self Indication Assumption (SIA) and the Self Sampling Assumption (SSA), as well as <a href=\"http://philsci-archive.pitt.edu/2888/\">Full Non-indexical Conditioning</a> (FNC)\u00a0 basically agree that future filter steps will be larger than we otherwise think,\u00a0including the many future filter steps that are existential risks.</p>\n<div data-shortcode=\"caption\" id=\"attachment_2911\" style=\"width: 217px\" class=\"wp-caption alignleft\"><a href=\"https://meteuphoric.files.wordpress.com/2010/11/sia-doom-curves.png\"><img data-attachment-id=\"2911\" data-permalink=\"https://meteuphoric.wordpress.com/2010/11/02/anthropic-principles-agree-on-bigger-future-filters/sia-doom-curves/\" data-orig-file=\"https://meteuphoric.files.wordpress.com/2010/11/sia-doom-curves.png\" data-orig-size=\"891,1291\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}\" data-image-title=\"sia doom curves\" data-image-description=\"\" data-medium-file=\"https://meteuphoric.files.wordpress.com/2010/11/sia-doom-curves.png?w=207&#038;h=300\" data-large-file=\"https://meteuphoric.files.wordpress.com/2010/11/sia-doom-curves.png?w=500\" class=\"size-medium wp-image-2911\" title=\"sia doom curves\" src=\"https://meteuphoric.files.wordpress.com/2010/11/sia-doom-curves.png?w=207&#038;h=300\" alt=\"\" width=\"207\" height=\"300\" srcset=\"https://meteuphoric.files.wordpress.com/2010/11/sia-doom-curves.png?w=207&amp;h=300 207w, https://meteuphoric.files.wordpress.com/2010/11/sia-doom-curves.png?w=414&amp;h=600 414w, https://meteuphoric.files.wordpress.com/2010/11/sia-doom-curves.png?w=104&amp;h=150 104w\" sizes=\"(max-width: 207px) 100vw, 207px\" /></a><p class=\"wp-caption-text\">Figure 1: SIA likes possible worlds with big populations at our stage, which means small past filters, which means big future filters.</p></div>\n<p>SIA says the probability of being in a possible world is proportional to the number of people it contains who you could be. SSA says it&#8217;s proportional to the fraction of people (or some other reference class) it contains who you could be. FNC says the probability of being in a possible world is proportional to the chance of anyone in that world having exactly your experiences. That chance is more the larger the population of people like you in relevant ways, so FNC generally gets similar answers to SIA. For a lengthier account of all these, see\u00a0<a style=\"font-family:Georgia, 'Bitstream Charter', serif;color:#0060ff;line-height:1.7;\" href=\"https://meteuphoric.wordpress.com/anthropic-principles/\">here</a>.</p>\n<p>SIA increases expectations of larger future filter steps because it  favours smaller past filter steps. Since there is a minimum total filter  size, this means it favours big future steps. This I have <a href=\"https://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\">explained </a>before. See Figure 1. Radford Neal has demonstrated similar results with <a href=\"http://arxiv.org/abs/math/0608592\">FNC</a>.</p>\n<div data-shortcode=\"caption\" id=\"attachment_2934\" style=\"width: 310px\" class=\"wp-caption alignleft\"><a href=\"https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-fixed.png\"><img data-attachment-id=\"2934\" data-permalink=\"https://meteuphoric.wordpress.com/2010/11/02/anthropic-principles-agree-on-bigger-future-filters/ssa-doom-curves-fixed/\" data-orig-file=\"https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-fixed.png\" data-orig-size=\"919,460\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}\" data-image-title=\"ssa doom curves fixed\" data-image-description=\"\" data-medium-file=\"https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-fixed.png?w=300&#038;h=150\" data-large-file=\"https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-fixed.png?w=500\" class=\"size-medium wp-image-2934\" title=\"ssa doom curves fixed\" src=\"https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-fixed.png?w=300&#038;h=150\" alt=\"\" width=\"300\" height=\"150\" srcset=\"https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-fixed.png?w=300&amp;h=150 300w, https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-fixed.png?w=600&amp;h=300 600w, https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-fixed.png?w=150&amp;h=75 150w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a><p class=\"wp-caption-text\">Figure 2: A larger filter between future stages in our reference class makes the population at our own stage a larger proportion of the total population. This increases the probability under SSA.</p></div>\n<p>SSA can give a variety of results according to reference class choice. Generally it directly increases expectations of both larger future filter steps and smaller past filter steps, but only for those steps between stages of development that are at least partially included in the reference class.</p>\n<p>For instance if the reference class includes all human-like things, perhaps it stretches from ourselves to very similar future people who have avoided many existential risks. In this case, SSA increases the chances of large filter steps between these stages, but says little about filter steps before us, or after the future people in our reference class. This is basically the Doomsday Argument &#8211; larger filters in our future mean fewer future people relative to us. See Figure 2.</p>\n<div data-shortcode=\"caption\" id=\"attachment_2922\" style=\"width: 310px\" class=\"wp-caption alignleft\"><a href=\"https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-1.png\"><img data-attachment-id=\"2922\" data-permalink=\"https://meteuphoric.wordpress.com/2010/11/02/anthropic-principles-agree-on-bigger-future-filters/ssa-doom-curves-1/\" data-orig-file=\"https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-1.png\" data-orig-size=\"973,513\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}\" data-image-title=\"ssa doom curves 1\" data-image-description=\"\" data-medium-file=\"https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-1.png?w=300&#038;h=158\" data-large-file=\"https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-1.png?w=500\" class=\"size-medium wp-image-2922\" title=\"ssa doom curves 1\" src=\"https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-1.png?w=300&#038;h=158\" alt=\"\" width=\"300\" height=\"158\" srcset=\"https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-1.png?w=300&amp;h=158 300w, https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-1.png?w=600&amp;h=316 600w, https://meteuphoric.files.wordpress.com/2010/11/ssa-doom-curves-1.png?w=150&amp;h=79 150w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a><p class=\"wp-caption-text\">Figure 3: In the world with the larger early filter, the population at many stages including ours is smaller relative to some early stages. This makes the population at our stage a smaller proportion of the whole, which makes that world less likely. (The populations at each stage are a function of the population per relevant solar system as well as the chance of a solar system reaching that stage, which is not illustrated here).</p></div>\n<p>With a reference class that stretches to creatures in filter stages back before us, SSA increases the chances of smaller past filters steps between those stages. This is because those filters make observers at almost all stages of development (including ours) less plentiful relative to at least one earlier stage of creatures in our reference class. This makes those at our own stage a smaller proportion of the population of the reference class. See Figure 3.</p>\n<p>The predictions of the different principles differ in details such as the extent of the probability shift and the effect of timing. However\u00a0it is not necessary to resolve anthropic disagreement to believe we have underestimated the chances of larger filters in our future. As long as we think something like one of the above three principles is likely to be correct, we should update our expectations already.</p><br />  <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gocomments/meteuphoric.wordpress.com/2859/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/comments/meteuphoric.wordpress.com/2859/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/godelicious/meteuphoric.wordpress.com/2859/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/delicious/meteuphoric.wordpress.com/2859/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gofacebook/meteuphoric.wordpress.com/2859/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/facebook/meteuphoric.wordpress.com/2859/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gotwitter/meteuphoric.wordpress.com/2859/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/twitter/meteuphoric.wordpress.com/2859/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/gostumble/meteuphoric.wordpress.com/2859/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/stumble/meteuphoric.wordpress.com/2859/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/godigg/meteuphoric.wordpress.com/2859/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/digg/meteuphoric.wordpress.com/2859/\" /></a> <a rel=\"nofollow\" href=\"http://feeds.wordpress.com/1.0/goreddit/meteuphoric.wordpress.com/2859/\"><img alt=\"\" border=\"0\" src=\"http://feeds.wordpress.com/1.0/reddit/meteuphoric.wordpress.com/2859/\" /></a> <img alt=\"\" border=\"0\" src=\"https://pixel.wp.com/b.gif?host=meteuphoric.wordpress.com&#038;blog=8643840&#038;post=2859&#038;subd=meteuphoric&#038;ref=&#038;feed=1\" width=\"1\" height=\"1\" />", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M8zciCoiqdtZiH7aq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 6.417312539179233e-07, "legacy": true, "legacyId": "3917", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-02T16:15:48.766Z", "modifiedAt": null, "url": null, "title": "New Month's Resolutions", "slug": "new-month-s-resolutions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:26.654Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Barry_Cotter", "createdAt": "2010-04-19T16:29:03.629Z", "isAdmin": false, "displayName": "Barry_Cotter"}, "userId": "5pZXxaf79kj37Rwq2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8F36QpxN43KLiJjy5/new-month-s-resolutions", "pageUrlRelative": "/posts/8F36QpxN43KLiJjy5/new-month-s-resolutions", "linkUrl": "https://www.lesswrong.com/posts/8F36QpxN43KLiJjy5/new-month-s-resolutions", "postedAtFormatted": "Tuesday, November 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Month's%20Resolutions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Month's%20Resolutions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8F36QpxN43KLiJjy5%2Fnew-month-s-resolutions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Month's%20Resolutions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8F36QpxN43KLiJjy5%2Fnew-month-s-resolutions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8F36QpxN43KLiJjy5%2Fnew-month-s-resolutions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<p>If you have a goal that can be credibly achieved by the end of the month reply to this post. At the end of the month everybody who posted writes up their experiences, lessons in overcoming akrasia etc. as a reply to their original comment or by editing the original.</p>\n<p>Whether you want to share the goal before the end of the month is up to you, pros, you are accountable, cons, you may feel a sense of accomplishment just by saying it and not do anything.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8F36QpxN43KLiJjy5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 6.41385205137985e-07, "legacy": true, "legacyId": "3901", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-02T18:26:17.877Z", "modifiedAt": null, "url": null, "title": "South/Eastern Europe Meeting in Ljubljana/Slovenia", "slug": "south-eastern-europe-meeting-in-ljubljana-slovenia", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:28.567Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kmmxdAfFbgoSAr5X6/south-eastern-europe-meeting-in-ljubljana-slovenia", "pageUrlRelative": "/posts/kmmxdAfFbgoSAr5X6/south-eastern-europe-meeting-in-ljubljana-slovenia", "linkUrl": "https://www.lesswrong.com/posts/kmmxdAfFbgoSAr5X6/south-eastern-europe-meeting-in-ljubljana-slovenia", "postedAtFormatted": "Tuesday, November 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20South%2FEastern%20Europe%20Meeting%20in%20Ljubljana%2FSlovenia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASouth%2FEastern%20Europe%20Meeting%20in%20Ljubljana%2FSlovenia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmmxdAfFbgoSAr5X6%2Fsouth-eastern-europe-meeting-in-ljubljana-slovenia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=South%2FEastern%20Europe%20Meeting%20in%20Ljubljana%2FSlovenia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmmxdAfFbgoSAr5X6%2Fsouth-eastern-europe-meeting-in-ljubljana-slovenia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmmxdAfFbgoSAr5X6%2Fsouth-eastern-europe-meeting-in-ljubljana-slovenia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<p>Will be held on November the 5th at 19:00 at Nikola Tesla's Street 30. The place known as The Hekovnik.</p>\n<p>We will discuss various topics on this second of many future meetings. Any Slovenian who reads this site is welcome, as any Croat, Austrian, northern Italian or any other currently in the vicinity of Ljubljana.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kmmxdAfFbgoSAr5X6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 6.414164469513935e-07, "legacy": true, "legacyId": "3902", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-02T18:28:33.783Z", "modifiedAt": null, "url": null, "title": "Toy model of human values", "slug": "toy-model-of-human-values", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:27.006Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "xamdam", "createdAt": "2009-10-26T15:17:24.328Z", "isAdmin": false, "displayName": "xamdam"}, "userId": "8DhacFwGJLZX6nJvm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iPHWq6si8n8e3DZAA/toy-model-of-human-values", "pageUrlRelative": "/posts/iPHWq6si8n8e3DZAA/toy-model-of-human-values", "linkUrl": "https://www.lesswrong.com/posts/iPHWq6si8n8e3DZAA/toy-model-of-human-values", "postedAtFormatted": "Tuesday, November 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Toy%20model%20of%20human%20values&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AToy%20model%20of%20human%20values%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiPHWq6si8n8e3DZAA%2Ftoy-model-of-human-values%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Toy%20model%20of%20human%20values%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiPHWq6si8n8e3DZAA%2Ftoy-model-of-human-values", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiPHWq6si8n8e3DZAA%2Ftoy-model-of-human-values", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 218, "htmlBody": "<p>This is just a summary via analogy where I human values come from, as far as I understand it. The expanded version would be Eli's http://lesswrong.com/lw/l3/thou_art_godshatter/.</p>\n<p>The basic analogy is to chess-playing programs (at least the basic ones from 40 years ago, the art has progressed since then, but not much). The way they work is basically by examining the branching tree of possible moves; since chess is \"too big\" to solve completely (find the branch that always leads to winning) by present hardware what these programs do is go to a certain depth and then use heuristics to decide whether the end state is good, such as how many pieces are on its side vs. the enemy side, weighed by their \"power\" (queen is worth more than pawn) and position (center positions are worth more).&nbsp;</p>\n<p>The analogy mapping is as follows: the goal of the game is winning, of evolution is survival of a gene fragment (such as human DNA). Explicit encoding of the goal is not computationally feasible or worthwhile (in terms of the goal itself), so values of certain non-terminal states (in terms of the goal) are explicitly given to the program or to a human; the human/program knows no better than these non-terminal values - <strong>they are our values</strong> - we are <a href=\"/lw/l3/thou_art_godshatter/\">Godshatter</a>.&nbsp;</p>\n<p>What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iPHWq6si8n8e3DZAA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 6.414169892957389e-07, "legacy": true, "legacyId": "3903", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cSXZpvqpa9vbGGLtG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-02T18:57:29.702Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 5", "slug": "harry-potter-and-the-methods-of-rationality-discussion-19", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:27.118Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NihilCredo", "createdAt": "2009-04-22T23:40:56.227Z", "isAdmin": false, "displayName": "NihilCredo"}, "userId": "W6f2cwKiKSroig5kb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nnnd4KRQxs6DYcehD/harry-potter-and-the-methods-of-rationality-discussion-19", "pageUrlRelative": "/posts/nnnd4KRQxs6DYcehD/harry-potter-and-the-methods-of-rationality-discussion-19", "linkUrl": "https://www.lesswrong.com/posts/nnnd4KRQxs6DYcehD/harry-potter-and-the-methods-of-rationality-discussion-19", "postedAtFormatted": "Tuesday, November 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%205&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%205%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnnnd4KRQxs6DYcehD%2Fharry-potter-and-the-methods-of-rationality-discussion-19%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%205%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnnnd4KRQxs6DYcehD%2Fharry-potter-and-the-methods-of-rationality-discussion-19", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnnnd4KRQxs6DYcehD%2Fharry-potter-and-the-methods-of-rationality-discussion-19", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 234, "htmlBody": "<p><em>- This thread has run its course. You will find <a href=\"/r/discussion/tag/harry_potter/\">newer threads</a> in the discussion section.</em></p>\n<p>Another discussion thread - the fourth - has reached the (arbitrary?) 500 comments threshold, so it's time for a new thread for Eliezer Yudkowsky's widely-praised <a href=\"http://www.fanfiction.net/s/5782108/1/Harry_Potter_and_the_Methods_of_Rationality\">Harry Potter fanfic</a>.</p>\n<p>Most of the paratext and fan-made resources are listed on Mr. LessWrong's <a href=\"http://www.fanfiction.net/u/2269863/Less_Wrong\">author page</a>. There is also AdeleneDawner's <a href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">collection</a> of most of the previously-published Author's Notes.</p>\n<p>Older threads: <a href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">one</a>, <a href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">two</a>, <a href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">three</a>, <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/\">four</a>. <a href=\"/tag/harry_potter/\">By tag</a>.</p>\n<p><a href=\"/r/discussion/tag/harry_potter/\">Newer threads</a> are in the Discussion section, starting from <a href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">Part 6</a>.</p>\n<p>Spoiler policy as suggested by Unnamed and approved by Eliezer, me, and at least three other upmodders:</p>\n<blockquote>\n<p>You do not need to rot13 anything about HP:MoR or the original Harry Potter series <strong>unless</strong> you are posting <strong>insider information from Eliezer Yudkowsky</strong> which is <strong>not</strong> supposed to be <strong>publicly available</strong> (which includes public statements by Eliezer that have been retracted).</p>\n<p>If there is evidence for X in MOR and/or canon then it's fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But <em><strong>you should not post that \"Eliezer said X is true\" unless you use rot13</strong>.</em></p>\n</blockquote>\n<p>It would also be quite sensible and welcome to continue the practice of declaring at the top of your post <strong>which chapters</strong> you are about to discuss, especially for newly-published ones, so that people who haven't yet seen them can stop reading in time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nnnd4KRQxs6DYcehD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 6.414239167881287e-07, "legacy": true, "legacyId": "3904", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 656, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "y2Hszb4Dsm5FggnDC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-02T19:10:30.494Z", "modifiedAt": null, "url": null, "title": "Oxford (UK) Rationality & AI Risks Discussion Group", "slug": "oxford-uk-rationality-and-ai-risks-discussion-group", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:31.492Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Larks", "createdAt": "2009-04-28T20:21:45.860Z", "isAdmin": false, "displayName": "Larks"}, "userId": "jQXwiWxFcfyYjytXa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W9aYQsXTaeKkyzANs/oxford-uk-rationality-and-ai-risks-discussion-group", "pageUrlRelative": "/posts/W9aYQsXTaeKkyzANs/oxford-uk-rationality-and-ai-risks-discussion-group", "linkUrl": "https://www.lesswrong.com/posts/W9aYQsXTaeKkyzANs/oxford-uk-rationality-and-ai-risks-discussion-group", "postedAtFormatted": "Tuesday, November 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Oxford%20(UK)%20Rationality%20%26%20AI%20Risks%20Discussion%20Group&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOxford%20(UK)%20Rationality%20%26%20AI%20Risks%20Discussion%20Group%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW9aYQsXTaeKkyzANs%2Foxford-uk-rationality-and-ai-risks-discussion-group%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Oxford%20(UK)%20Rationality%20%26%20AI%20Risks%20Discussion%20Group%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW9aYQsXTaeKkyzANs%2Foxford-uk-rationality-and-ai-risks-discussion-group", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW9aYQsXTaeKkyzANs%2Foxford-uk-rationality-and-ai-risks-discussion-group", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<p>Alex Flint and I are doing a series of seminar/discussion events in Oxford, to which anyone from LW would be very welcome. Especially as the theme is Rationality &amp; AI Risks!</p>\n<p>They're being held at 5pm on Saturday in<a title=\"Map to Exeter\" href=\"http://www.exeter.ox.ac.uk/college/contact/travel\"> Exeter College</a>, and will go on throughout November. We had over 10 people last Saturday discussing Heuristics and Biases, and plan to go onto Bayesianism this week. They'll probably last for about an hour, though we may decamp to the pub afterwards to continue discussion.</p>\n<p>If you're in the area, you might also be interested in the other events run by the <a title=\"Oxford Transhumanist Society Facebook\" href=\"http://www.facebook.com/?ref=home#!/group.php?gid=265828309853&amp;\">Oxford Transhumanist Society</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W9aYQsXTaeKkyzANs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 6.414270327095496e-07, "legacy": true, "legacyId": "3905", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-02T19:12:49.132Z", "modifiedAt": null, "url": null, "title": "Waser's 3 Goals of Morality", "slug": "waser-s-3-goals-of-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:29.616Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwaser", "createdAt": "2010-02-24T12:24:11.249Z", "isAdmin": false, "displayName": "mwaser"}, "userId": "CZLtduXx3pKqpuAxF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f23WWwBNR9zkPaW8v/waser-s-3-goals-of-morality", "pageUrlRelative": "/posts/f23WWwBNR9zkPaW8v/waser-s-3-goals-of-morality", "linkUrl": "https://www.lesswrong.com/posts/f23WWwBNR9zkPaW8v/waser-s-3-goals-of-morality", "postedAtFormatted": "Tuesday, November 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Waser's%203%20Goals%20of%20Morality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWaser's%203%20Goals%20of%20Morality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff23WWwBNR9zkPaW8v%2Fwaser-s-3-goals-of-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Waser's%203%20Goals%20of%20Morality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff23WWwBNR9zkPaW8v%2Fwaser-s-3-goals-of-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff23WWwBNR9zkPaW8v%2Fwaser-s-3-goals-of-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 283, "htmlBody": "<div class=\"entry\">\n<p>In the spirit of Asimov&rsquo;s 3 Laws of Robotics</p>\n<ol>\n<li>You should not be selfish</li>\n<li>You should not be short-sighted or over-optimize</li>\n<li>You should maximize the progress towards and fulfillment of all  conscious and willed goals, both in terms of numbers and diversity  equally, both yours and those of others equally</li>\n</ol>\n<p>It is my contention that Yudkowsky&rsquo;s <a href=\"http://sl4.org/wiki/CoherentExtrapolatedVolition\">CEV</a> converges to the following 3 points:</p>\n<ol>\n<li>I want what I want</li>\n<li>I recognize my obligatorily gregarious nature; realize that ethics  and improving the community is the community&rsquo;s most rational path  towards maximizing the progress towards and fulfillment of everyone&rsquo;s  goals; and realize that to be rational and effective the community  should punish anyone who is not being ethical or improving the community  (even if the punishment is &ldquo;<em>merely</em>&rdquo; withholding help and cooperation)</li>\n<li>I shall, therefore, be ethical and improve the community in order to  obtain assistance, prevent interference, and most effectively achieve  my goals</li>\n</ol>\n<p>I further contend that, if this CEV is translated to the 3 Goals  above and implemented in a Yudkowskian Benevolent Goal Architecture  (BGA), that the result would be a <a href=\"http://intelligence.org/upload/CFAI.html\">Friendly AI</a>.</p>\n<p>It should be noted that evolution and history say that cooperation  and ethics are stable attractors while submitting to slavery (when you  don&rsquo;t have to) is not.&nbsp; This formulation expands Singer&rsquo;s Circles of  Morality as far as they&rsquo;ll go and tries to eliminate irrational Us-Them  distinctions based on anything other than optimizing goals for everyone &mdash;  the same direction that humanity seems headed in and exactly where  current SIAI proposals come up short.</p>\n<p>Once again, cross-posted <a href=\"http://becominggaia.wordpress.com/2010/11/03/wasers-3-goals-of-morality/\">here</a> on my <a href=\"http://becominggaia.wordpress.com/\">blog</a> (unlike my last <a href=\"/lw/307/intelligence_vs_wisdom/\">article</a>, I have no idea whether this will be karma'd out of existence or not ;-)</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f23WWwBNR9zkPaW8v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -16, "extendedScore": null, "score": 6.414275859685035e-07, "legacy": true, "legacyId": "3906", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bFHGjo84EBZegjMoJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-02T19:34:28.552Z", "modifiedAt": null, "url": null, "title": "Voting is not rational (usually.)", "slug": "voting-is-not-rational-usually", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:31.839Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "araneae", "createdAt": "2010-09-27T18:18:41.509Z", "isAdmin": false, "displayName": "araneae"}, "userId": "XwTgyr8oAwCnN3fPd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qeq7EmkNGFRq4yaFr/voting-is-not-rational-usually", "pageUrlRelative": "/posts/Qeq7EmkNGFRq4yaFr/voting-is-not-rational-usually", "linkUrl": "https://www.lesswrong.com/posts/Qeq7EmkNGFRq4yaFr/voting-is-not-rational-usually", "postedAtFormatted": "Tuesday, November 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Voting%20is%20not%20rational%20(usually.)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVoting%20is%20not%20rational%20(usually.)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQeq7EmkNGFRq4yaFr%2Fvoting-is-not-rational-usually%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Voting%20is%20not%20rational%20(usually.)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQeq7EmkNGFRq4yaFr%2Fvoting-is-not-rational-usually", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQeq7EmkNGFRq4yaFr%2Fvoting-is-not-rational-usually", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 245, "htmlBody": "<p>Today is the midterm elections in the United States, and I am not voting.</p>\n<p>For the vast majority of elections, voting is irrational, because the individual's vote is proportionately very small. This means it cannot have an effect on the outcome.</p>\n<p>There are, however, conditions which can lead to voting becoming rational, and these are:</p>\n<ol>\n<li>The number of voters approaches zero.</li>\n<li>The ratio of votes for&nbsp;candidates&nbsp;(in a majority wins, 2 person race) approaches .5</li>\n<li>The difficulty of voting becomes vanishingly small.</li>\n<li>Incentives are created to make the costs of not voting greater than the cost of voting (for instance, not voting is illegal in Australia, and incurs a fine.)</li>\n</ol>\n<div>For me, as for nearly everyone, voting this year is irrational. &nbsp;1 and 2 are nowhere close to true, and 3 is especially bad for me this year. &nbsp;I forgot to change my address on my voter's registration until yesterday and my polling location is both a) usually overpopulated and filled with long lines and b) farther than I care to go. &nbsp;</div>\n<div>Only 3 and 4 are something that we can certainly do something about. &nbsp;The web-based absentee voter system that was tested this fall is a step in that directions, but its &nbsp;<a href=\"http://www.engin.umich.edu/newscenter/feature/electronicvoting/\">subsequent hacking</a>&nbsp;&nbsp;was unsurprising. &nbsp;Is opening our system up to fraud a reasonable trade-off to get more people to vote? &nbsp;Should there be an option to use paper absentee-like ballots even if you are not absent? Should the U.S. go the Australia route?</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qeq7EmkNGFRq4yaFr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 1, "extendedScore": null, "score": 6.414327716663932e-07, "legacy": true, "legacyId": "3907", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-02T20:41:33.804Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes: November 2010", "slug": "rationality-quotes-november-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:41.643Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Zzst4qemGKaFAPKno", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nXp4aAq5GdhKpc57x/rationality-quotes-november-2010", "pageUrlRelative": "/posts/nXp4aAq5GdhKpc57x/rationality-quotes-november-2010", "linkUrl": "https://www.lesswrong.com/posts/nXp4aAq5GdhKpc57x/rationality-quotes-november-2010", "postedAtFormatted": "Tuesday, November 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%3A%20November%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%3A%20November%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnXp4aAq5GdhKpc57x%2Frationality-quotes-november-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%3A%20November%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnXp4aAq5GdhKpc57x%2Frationality-quotes-november-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnXp4aAq5GdhKpc57x%2Frationality-quotes-november-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<div>\n<p>A monthly thread for posting rationality-related quotes you've seen recently (or had stored in your quotesfile for ages).</p>\n<ul>\n<li>Please post all quotes separately, so that they can be voted up/down  separately. (If they are strongly related, reply to your own  comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nXp4aAq5GdhKpc57x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 6.414488360613205e-07, "legacy": true, "legacyId": "3908", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 369, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-03T02:01:43.475Z", "modifiedAt": null, "url": null, "title": "META: Meetup Overload", "slug": "meta-meetup-overload", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:28.311Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Spurlock", "createdAt": "2010-03-24T17:13:19.572Z", "isAdmin": false, "displayName": "Spurlock"}, "userId": "mK7rKWbkuoDsm3aQb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dDKa77AcxjzMRmbL4/meta-meetup-overload", "pageUrlRelative": "/posts/dDKa77AcxjzMRmbL4/meta-meetup-overload", "linkUrl": "https://www.lesswrong.com/posts/dDKa77AcxjzMRmbL4/meta-meetup-overload", "postedAtFormatted": "Wednesday, November 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20META%3A%20Meetup%20Overload&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMETA%3A%20Meetup%20Overload%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdDKa77AcxjzMRmbL4%2Fmeta-meetup-overload%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=META%3A%20Meetup%20Overload%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdDKa77AcxjzMRmbL4%2Fmeta-meetup-overload", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdDKa77AcxjzMRmbL4%2Fmeta-meetup-overload", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>Does anyone else think we need a better way of dealing with meet-ups? I totally understand that meeting face to face is an (at least arguably) important next-step in the rationalist-community-building project, but the fact is the front page, which was originally for all the most content-rich, accessible, and noteworthy articles is now being filled with blurbs that are irrelevant to 95% of the readership.</p>\n<p>I can see why these posts would need to be highly visible if the meetups are going to work at all, but I think we should get the ball rolling on figuring out a better way to handle location-specific posts. For example, mandatory-but-private location setting in your profile (at least to the country, possibly to the state/province/etc for larger areas), which would subscribe you (with opt-out available) to any happenings in that area. That's just the first idea, like I said the idea is just to get the discussion going.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dDKa77AcxjzMRmbL4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 27, "extendedScore": null, "score": 6.415255098863931e-07, "legacy": true, "legacyId": "3909", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-03T08:25:51.143Z", "modifiedAt": null, "url": null, "title": "Amoral Approaches to Morality", "slug": "amoral-approaches-to-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:27.198Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hsmdgxqdbPQGLmK2E/amoral-approaches-to-morality", "pageUrlRelative": "/posts/hsmdgxqdbPQGLmK2E/amoral-approaches-to-morality", "linkUrl": "https://www.lesswrong.com/posts/hsmdgxqdbPQGLmK2E/amoral-approaches-to-morality", "postedAtFormatted": "Wednesday, November 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Amoral%20Approaches%20to%20Morality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAmoral%20Approaches%20to%20Morality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhsmdgxqdbPQGLmK2E%2Famoral-approaches-to-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Amoral%20Approaches%20to%20Morality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhsmdgxqdbPQGLmK2E%2Famoral-approaches-to-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhsmdgxqdbPQGLmK2E%2Famoral-approaches-to-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 363, "htmlBody": "<p>Consider three cases in which someone is asking you about morality: a clever child, your guru (and/or Socrates, if you're more comfortable with that tradition), or an about-to-FOOM AI of indeterminate friendliness. For each of them, you want your thoughts to be as clear as possible- the other entity is clever enough to point out flaws (or powerful enough that your flaws might be deadly), and for none of them can you assume that their prior or posterior morality will be very similar to your own. (As Thomas Sowell puts it, children are barbarians who need to be civilized before it is too late; your guru will seem willing to lead you anywhere, and the AI probably doesn't think the way you do.)</p>\n<p>I suggest that all three can be approached in the same way: by attempting to construct an amoral approach to morality. At first impression, this approach gives a significant benefit: circular reasoning is headed off at the pass, because you need to explain morality (as best as you can) to someone who does not understand or feel it.</p>\n<p>Interested in what comes next?</p>\n<p>The main concern I have is that there is a <a title=\"rather extensive Metaethics sequence\" href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">rather extensive Metaethics sequence</a> already, and this seems to be very similar to <a title=\"The Moral Void\" href=\"/lw/rr/the_moral_void/\">The Moral Void</a> and <a title=\"The Meaning of Right\" href=\"/lw/sm/the_meaning_of_right/\">The Meaning of Right</a>. The benefit of this post, if there is one, seems to be in a different approach to the issue- I think I can get a useful sketch of the issue in one post- and probably a different conclusion. At the moment, I don't buy Eliezer's approach to the Is-Ought gap (Right is a 1-place function... why?), and I think a redefinition of the question may make for somewhat better answers.</p>\n<p>(The inspirations for this post, if you're interested in me tackling them directly instead, are criticisms of utilitarianism obliquely raised in a huge tree in the <a title=\"Luminosity discussion thread\" href=\"/lw/2mq/luminosity_twilight_fanfic_discussion_thread/2u4y?c=1\">Luminosity discussion thread</a> (the two interesting dimensions are questioning assumptions, and talking about scope errors, of which I suspect scope errors is the more profitable) and the discussion around, as shokwave puts it, the <a title=\"Really Scary Idea\" href=\"/lw/2zg/ben_goertzel_the_singularity_institutes_scary/2w4t?c=1&amp;context=1\">Really Scary Idea</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hsmdgxqdbPQGLmK2E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 9, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "3914", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K9JSM7d7bLJguMxEp", "fG3g3764tSubr6xvs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-03T11:44:01.322Z", "modifiedAt": null, "url": null, "title": "Meet-ups, continuing the conversation", "slug": "meet-ups-continuing-the-conversation", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WxrYYpTtrmTLnjsAL/meet-ups-continuing-the-conversation", "pageUrlRelative": "/posts/WxrYYpTtrmTLnjsAL/meet-ups-continuing-the-conversation", "linkUrl": "https://www.lesswrong.com/posts/WxrYYpTtrmTLnjsAL/meet-ups-continuing-the-conversation", "postedAtFormatted": "Wednesday, November 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meet-ups%2C%20continuing%20the%20conversation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeet-ups%2C%20continuing%20the%20conversation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWxrYYpTtrmTLnjsAL%2Fmeet-ups-continuing-the-conversation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meet-ups%2C%20continuing%20the%20conversation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWxrYYpTtrmTLnjsAL%2Fmeet-ups-continuing-the-conversation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWxrYYpTtrmTLnjsAL%2Fmeet-ups-continuing-the-conversation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 33, "htmlBody": "<p>If you've been to a meet-up and you think there's more to be said about something that was discussed there, I suggest posting it at LW and mentioning that a meet-up inspired it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WxrYYpTtrmTLnjsAL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 6.416650029725575e-07, "legacy": true, "legacyId": "3916", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-03T19:12:07.646Z", "modifiedAt": null, "url": null, "title": "Control Fraud", "slug": "control-fraud", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:35.549Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Allen", "createdAt": "2010-08-12T18:39:43.418Z", "isAdmin": false, "displayName": "David_Allen"}, "userId": "yKNx2drs5QMLT6iqu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ccqfsQ3kMSLDJKRx7/control-fraud", "pageUrlRelative": "/posts/ccqfsQ3kMSLDJKRx7/control-fraud", "linkUrl": "https://www.lesswrong.com/posts/ccqfsQ3kMSLDJKRx7/control-fraud", "postedAtFormatted": "Wednesday, November 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Control%20Fraud&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AControl%20Fraud%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FccqfsQ3kMSLDJKRx7%2Fcontrol-fraud%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Control%20Fraud%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FccqfsQ3kMSLDJKRx7%2Fcontrol-fraud", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FccqfsQ3kMSLDJKRx7%2Fcontrol-fraud", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p>&nbsp;</p>\n<p>A recent <a title=\"Control Fraud\" href=\"http://www.schneier.com/blog/archives/2010/11/control_fraud.html\">post</a> by Bruce Schneier on control fraud.</p>\n<div>\n<div>Control fraud is a process of optimizing an organization for fraud, utilizing a position of power to suborn controls.</div>\n</div>\n<div>\n<div>From the abstract of the&nbsp;<a title=\"When Fragile becomes Friable: Endemic Control Fraud as a Cause of Economic Stagnation and Collapse\" href=\"http://www.networkideas.org/feathm/may2006/william_k_black.pdf\">paper</a>&nbsp;Bruce references:</div>\n<blockquote>\n<div>Individual &ldquo;control frauds&rdquo; cause greater losses than all other forms of property crime combined. They are financial super-predators. Control frauds are crimes led by the head of state or CEO that use the nation or company as a fraud vehicle. Waves of &ldquo;control fraud&rdquo; can cause economic collapses, damage and discredit key institutions vital to good political governance, and erode trust. The defining element of fraud is deceit &ndash; the criminal creates and then betrays trust. Fraud, therefore, is the strongest acid to eat away at trust. Endemic control fraud causes institutions and trust to become friable &ndash; to crumble &ndash; and produce economic stagnation.</div>\n</blockquote>\n<div>Friendly AI is an important topic on this site, but what about creating friendly organizations such as companies and governments? The damage done by a government wireheaded for fraud can be enormous.</div>\n</div>\n<p>Can the same approaches used to build FAI be used to improve other types of systems?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ccqfsQ3kMSLDJKRx7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 19, "extendedScore": null, "score": 6.417723860506101e-07, "legacy": true, "legacyId": "3918", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-03T19:20:08.179Z", "modifiedAt": null, "url": null, "title": "An apology", "slug": "an-apology", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:08.365Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwaser", "createdAt": "2010-02-24T12:24:11.249Z", "isAdmin": false, "displayName": "mwaser"}, "userId": "CZLtduXx3pKqpuAxF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qpcP2KmgbR3SH7mMF/an-apology", "pageUrlRelative": "/posts/qpcP2KmgbR3SH7mMF/an-apology", "linkUrl": "https://www.lesswrong.com/posts/qpcP2KmgbR3SH7mMF/an-apology", "postedAtFormatted": "Wednesday, November 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20apology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20apology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqpcP2KmgbR3SH7mMF%2Fan-apology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20apology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqpcP2KmgbR3SH7mMF%2Fan-apology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqpcP2KmgbR3SH7mMF%2Fan-apology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>Ohhhhh.&nbsp; WOW!&nbsp; Damn.&nbsp; Now I feel bad.<br /><br />I have been acting like a bull in a china shop, been an extremely ungracious guest, and have taken longer than I prefer to realize these things.<br /><br />My deepest apologies.<br /><br />My only defenses or mitigating circumstances:<br />1.&nbsp; I really didn't get it<br />2.&nbsp; My intentions were good<br /><br />I would like to perform a penance of creating or helping to create a newbie's guide to LessWrong.&nbsp; Doing so will clarify and consolidate my understanding and hopefully provide a useful community resource in recompense for the above and appreciation for those who took the time to write thoughtful comments.&nbsp; Obviously, though, doing so will require more patience and help from the community (particularly since I am certainly aware that I have no idea how to calibrate how much, if anything, you actually want to make too easily accessible) -- so this is a also request for that patience and help (and I'm making the assumption that the request will be answered by the replies ;-).<br /><br />Thanks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qpcP2KmgbR3SH7mMF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 20, "extendedScore": null, "score": 6.417743055690321e-07, "legacy": true, "legacyId": "3919", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-04T08:18:04.121Z", "modifiedAt": null, "url": null, "title": "Refuting the \"iron law of bureaucracy\"?", "slug": "refuting-the-iron-law-of-bureaucracy", "viewCount": null, "lastCommentedAt": "2010-11-04T22:39:13.916Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DexavXc3daGe7L5gR/refuting-the-iron-law-of-bureaucracy", "pageUrlRelative": "/posts/DexavXc3daGe7L5gR/refuting-the-iron-law-of-bureaucracy", "linkUrl": "https://www.lesswrong.com/posts/DexavXc3daGe7L5gR/refuting-the-iron-law-of-bureaucracy", "postedAtFormatted": "Thursday, November 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Refuting%20the%20%22iron%20law%20of%20bureaucracy%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARefuting%20the%20%22iron%20law%20of%20bureaucracy%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDexavXc3daGe7L5gR%2Frefuting-the-iron-law-of-bureaucracy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Refuting%20the%20%22iron%20law%20of%20bureaucracy%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDexavXc3daGe7L5gR%2Frefuting-the-iron-law-of-bureaucracy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDexavXc3daGe7L5gR%2Frefuting-the-iron-law-of-bureaucracy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 295, "htmlBody": "<p>Jerry Pournelle's \"Iron Law of Bureaucracy\" implies that leaders of bureaucratic organizations will seek to maximize the power and influence of the organization at the expense of its stated goals - but is that true in the real world?</p>\n<p>Julie Dolan of Macalester College examined surveys of government administrators and found that, surprisingly enough, high-ranking federal bureaucrats tended to prefer <em>less</em> government spending than the general public, even on issues that their own departments are responsible for.</p>\n<p>Here is the abstract from her paper, \"The budget-minimizing bureaucrat? Empirical evidence from the senior executive service\" that was published in the journal <em>Public Administration Review</em>:</p>\n<blockquote>\n<p>In a representative democracy, we assume the populace exerts some control over the actions and outputs of government officials, ensuring they comport with public preferences. However, the growth of the fourth branch of government has created a paradox: Unelected bureaucrats now have the power to affect government decisions (Meier 1993; Rourke 1984; Aberbach, Putnam, and Rockman 1981). In this article, I rely on two competing theories of bureaucratic behavior-representative-bureaucracy theory and Niskanen's budget-maximization theory-to assess how well the top ranks of the federal government represent the demands of the citizenry. Focusing on federal-spending priorities, I assess whether Senior Executive Service (SES) members mirror the attitudes of the populace or are likely to inflate budgets for their own personal gain. <strong>Contrary to the popular portrayal of the budget-maximizing bureaucrat (Niskanen 1971), I find these federal administrators prefer less spending than the public on most broad spending categories, even on issues that fall within their own departments' jurisdictions. </strong>As such, it may be time to revise our theories about bureaucratic self-interest and spending priorities. [emphasis added]</p>\n</blockquote>\n<p>I was able to read the paper <a href=\"http://www.accessmylibrary.com/article-1G1-82772322/budget-minimizing-bureaucrat-empirical.html\">here</a> for free, but I had to register first.</p>\n<p>See also: <a href=\"http://www.governmentisgood.com/articles.php?aid=20\">The Case FOR Bureaucracy</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BcnLB8PkrkqPhZ6XY": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DexavXc3daGe7L5gR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 6.419608042578903e-07, "legacy": true, "legacyId": "3922", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2010-11-04T08:18:04.121Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-04T10:47:18.301Z", "modifiedAt": null, "url": null, "title": "Katja Grace's Anthropic Reasoning in the Great Filter", "slug": "katja-grace-s-anthropic-reasoning-in-the-great-filter", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Larks", "createdAt": "2009-04-28T20:21:45.860Z", "isAdmin": false, "displayName": "Larks"}, "userId": "jQXwiWxFcfyYjytXa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HFnQ5gXHWm29M93cm/katja-grace-s-anthropic-reasoning-in-the-great-filter", "pageUrlRelative": "/posts/HFnQ5gXHWm29M93cm/katja-grace-s-anthropic-reasoning-in-the-great-filter", "linkUrl": "https://www.lesswrong.com/posts/HFnQ5gXHWm29M93cm/katja-grace-s-anthropic-reasoning-in-the-great-filter", "postedAtFormatted": "Thursday, November 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Katja%20Grace's%20Anthropic%20Reasoning%20in%20the%20Great%20Filter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKatja%20Grace's%20Anthropic%20Reasoning%20in%20the%20Great%20Filter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHFnQ5gXHWm29M93cm%2Fkatja-grace-s-anthropic-reasoning-in-the-great-filter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Katja%20Grace's%20Anthropic%20Reasoning%20in%20the%20Great%20Filter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHFnQ5gXHWm29M93cm%2Fkatja-grace-s-anthropic-reasoning-in-the-great-filter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHFnQ5gXHWm29M93cm%2Fkatja-grace-s-anthropic-reasoning-in-the-great-filter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<p>The wonderful Katja Grace has just uploaded her thesis,<a href=\"http://dl.dropbox.com/u/6355797/Anthropic%20Reasoning%20in%20the%20Great%20Filter.pdf\"> Anthropic Reasoning in the Great Filter</a>, linked to from her blog (which contains a <a href=\"http://meteuphoric.wordpress.com/2010/11/02/anthropic-principles-agree-on-bigger-future-filters/\">convientient summary</a>)</p>\n<p>&nbsp;</p>\n<p>She discusses how deal with indexical updating; updating on information not simply based on the likelihood that it occur somewhere, but the relative number (or proportion) of obersevers who would see that informtation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HFnQ5gXHWm29M93cm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "3923", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-04T17:55:20.027Z", "modifiedAt": null, "url": null, "title": "POLL: Reductionism", "slug": "poll-reductionism", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:28.158Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "draq", "createdAt": "2010-09-03T18:08:04.472Z", "isAdmin": false, "displayName": "draq"}, "userId": "TMiHfZH3dGtbFTAbb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E3spAsatTb24zmZnC/poll-reductionism", "pageUrlRelative": "/posts/E3spAsatTb24zmZnC/poll-reductionism", "linkUrl": "https://www.lesswrong.com/posts/E3spAsatTb24zmZnC/poll-reductionism", "postedAtFormatted": "Thursday, November 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20POLL%3A%20Reductionism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APOLL%3A%20Reductionism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE3spAsatTb24zmZnC%2Fpoll-reductionism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=POLL%3A%20Reductionism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE3spAsatTb24zmZnC%2Fpoll-reductionism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE3spAsatTb24zmZnC%2Fpoll-reductionism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p><em>Since there is no handy toll to create polls on LW, please post comments on your position.</em></p>\n<p>As which of the following would you identify yourself? (I am not good at rationalist taboo, thus please excuse me for ambiguous terms.)</p>\n<p><strong>Strong ontological reductionst</strong></p>\n<p>See <a href=\"http://en.wikipedia.org/wiki/Reductionism#Ontological_reductionism\">defintion on Wikipedia</a>. Someone who believes that mental phenomena can be fully reduced to physics and that physics can be fully reduced to mathematics. That is, desires and electrons don't have any fundamental qualities, but are in the end mathematical objects. And nothing exists outside the mathematical realm.</p>\n<p><strong>Weak ontological reductionist</strong></p>\n<p>Someone who believes that mental phenomena don't have any qualities outside the domain of physics. Every aspect of mental phenomena can be fully reduced to physical phenomena. But physical phenomena are not necessarily mathematical objects.</p>\n<p><strong>Strong scientific reductionist</strong></p>\n<p>Someone who believes that quantum mechanics is wrong and Laplace's demon can exist in principle (if unrestricted by physical limitations).&nbsp;</p>\n<p><strong>Weak scientific reductionist</strong></p>\n<p>Someone who concedes that it is impossible <em>in principle </em>to predict complicated physical systems, but that the concepts and theories in chemistry and biology are mere approximations and simplifications of complicated physical computations to sidestep the (faster-than-)exponential wall. That is, chemical and biological models are not fundamental, but are reducible to physical theories (if we had the theoretical computational power to simulate the models).</p>\n<p>&nbsp;</p>\n<p>Please also comment if you are not a reductionist and explain what kind of reductionist you are not.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E3spAsatTb24zmZnC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -5, "extendedScore": null, "score": 6.42099257693741e-07, "legacy": true, "legacyId": "3924", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Since there is no handy toll to create polls on LW, please post comments on your position.</em></p>\n<p>As which of the following would you identify yourself? (I am not good at rationalist taboo, thus please excuse me for ambiguous terms.)</p>\n<p><strong id=\"Strong_ontological_reductionst\">Strong ontological reductionst</strong></p>\n<p>See <a href=\"http://en.wikipedia.org/wiki/Reductionism#Ontological_reductionism\">defintion on Wikipedia</a>. Someone who believes that mental phenomena can be fully reduced to physics and that physics can be fully reduced to mathematics. That is, desires and electrons don't have any fundamental qualities, but are in the end mathematical objects. And nothing exists outside the mathematical realm.</p>\n<p><strong id=\"Weak_ontological_reductionist\">Weak ontological reductionist</strong></p>\n<p>Someone who believes that mental phenomena don't have any qualities outside the domain of physics. Every aspect of mental phenomena can be fully reduced to physical phenomena. But physical phenomena are not necessarily mathematical objects.</p>\n<p><strong id=\"Strong_scientific_reductionist\">Strong scientific reductionist</strong></p>\n<p>Someone who believes that quantum mechanics is wrong and Laplace's demon can exist in principle (if unrestricted by physical limitations).&nbsp;</p>\n<p><strong id=\"Weak_scientific_reductionist\">Weak scientific reductionist</strong></p>\n<p>Someone who concedes that it is impossible <em>in principle </em>to predict complicated physical systems, but that the concepts and theories in chemistry and biology are mere approximations and simplifications of complicated physical computations to sidestep the (faster-than-)exponential wall. That is, chemical and biological models are not fundamental, but are reducible to physical theories (if we had the theoretical computational power to simulate the models).</p>\n<p>&nbsp;</p>\n<p>Please also comment if you are not a reductionist and explain what kind of reductionist you are not.</p>", "sections": [{"title": "Strong ontological reductionst", "anchor": "Strong_ontological_reductionst", "level": 1}, {"title": "Weak ontological reductionist", "anchor": "Weak_ontological_reductionist", "level": 1}, {"title": "Strong scientific reductionist", "anchor": "Strong_scientific_reductionist", "level": 1}, {"title": "Weak scientific reductionist", "anchor": "Weak_scientific_reductionist", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-04T18:56:38.301Z", "modifiedAt": null, "url": null, "title": "Specification failure", "slug": "specification-failure", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:29.736Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tCvuytQXkF2B2pyYN/specification-failure", "pageUrlRelative": "/posts/tCvuytQXkF2B2pyYN/specification-failure", "linkUrl": "https://www.lesswrong.com/posts/tCvuytQXkF2B2pyYN/specification-failure", "postedAtFormatted": "Thursday, November 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Specification%20failure&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASpecification%20failure%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtCvuytQXkF2B2pyYN%2Fspecification-failure%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Specification%20failure%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtCvuytQXkF2B2pyYN%2Fspecification-failure", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtCvuytQXkF2B2pyYN%2Fspecification-failure", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 134, "htmlBody": "<p>I got made a moderator recently, and noticed that I have the amazing ability to ban my own comments. For added efficiency/hilarity I can even do it from my own user page, now complete with nifty \"ban\" links! I thought it would be neat to publicize this bug before it gets fixed, because it makes for a <em>wonderful</em> example of <strong>specification failure</strong> - the kind of error that can doom any attempt to create AGI/FAI. How do you protect against such mistakes in general, if you're not allowed to test your software? Discuss.</p>\n<p>Tangentially related: a blogger asks visitors to write a correct binary search routine without testing it even once, then test it and <a href=\"http://reprog.wordpress.com/2010/04/19/are-you-one-of-the-10-percent/\">report the results</a>. Much wailing ensues. (I succeeded - look for \"Vladimir Slepnev\" - but it was a surreal experience.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tCvuytQXkF2B2pyYN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 6.421139643043597e-07, "legacy": true, "legacyId": "3925", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-04T20:22:48.876Z", "modifiedAt": null, "url": null, "title": "The Curve of Capability", "slug": "the-curve-of-capability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:04.572Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rwallace", "createdAt": "2009-03-01T16:13:25.493Z", "isAdmin": false, "displayName": "rwallace"}, "userId": "cPhXNeZvnK7LgPMnv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7grfN4xNvLpEQoiyb/the-curve-of-capability", "pageUrlRelative": "/posts/7grfN4xNvLpEQoiyb/the-curve-of-capability", "linkUrl": "https://www.lesswrong.com/posts/7grfN4xNvLpEQoiyb/the-curve-of-capability", "postedAtFormatted": "Thursday, November 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Curve%20of%20Capability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Curve%20of%20Capability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7grfN4xNvLpEQoiyb%2Fthe-curve-of-capability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Curve%20of%20Capability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7grfN4xNvLpEQoiyb%2Fthe-curve-of-capability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7grfN4xNvLpEQoiyb%2Fthe-curve-of-capability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1572, "htmlBody": "<p>or: Why our universe has already had its one and only foom</p>\n<p>In the late 1980s, I added half a megabyte of RAM to my Amiga 500. A few months ago, I added 2048 megabytes of RAM to my Dell PC. The later upgrade was four thousand times larger, yet subjectively they felt about the same, and in practice they conferred about the same benefit. Why? Because each was a factor of two increase, and it is a general rule that each doubling tends to bring about the same increase in capability.</p>\n<p>That's a pretty important rule, so let's test it by looking at some more examples.<a id=\"more\"></a></p>\n<p>How does the performance of a chess program vary with the amount of computing power you can apply to the task? The answer is that each doubling of computing power adds roughly the same number of ELO rating points. The curve must flatten off eventually (after all, the computation required to fully solve chess is finite, albeit large), yet it remains surprisingly constant over a surprisingly wide range.</p>\n<p>Is that idiosyncratic to chess? Let's look at Go, a more difficult game that must be solved by different methods, where the alpha-beta minimax algorithm that served chess so well, breaks down. For a long time, the curve of capability also broke down: in the 90s and early 00s, the strongest Go programs were based on hand coded knowledge such that some of them literally did not know what to do with extra computing power; additional CPU speed resulted in zero improvement.</p>\n<p>The breakthrough came in the second half of last decade, with Monte Carlo tree search algorithms. It wasn't just that they provided a performance improvement, it was that they were scalable. Computer Go is now on the same curve of capability as computer chess: whether measured on the ELO or the kyu/dan scale, each doubling of power gives a roughly constant rating improvement.</p>\n<p>Where do these doublings come from? Moore's Law is driven by improvements in a number of technologies, one of which is chip design. Each generation of computers is used, among other things, to design the next generation. Each generation needs twice the computing power of the last generation to design in a given amount of time.</p>\n<p>Looking away from computers to one of the other big success stories of 20th-century technology, space travel, from Goddard's first crude liquid fuel rockets, to the V2, to Sputnik, to the half a million people who worked on Apollo, we again find that successive qualitative improvements in capability required order of magnitude after order of magnitude increase in the energy a rocket could deliver to its payload, with corresponding increases in the labor input.</p>\n<p>What about the nuclear bomb? Surely that at least was discontinuous?</p>\n<p>At the simplest physical level it was: nuclear explosives have six orders of magnitude more energy density than chemical explosives. But what about the effects? Those are what we care about, after all.</p>\n<p>The death tolls from the bombings of Hiroshima and Nagasaki have been estimated respectively at 90,000-166,000 and 60,000-80,000. That from the firebombing of Hamburg in 1943 has been estimated at 42,600; that from the firebombing of Tokyo on the 10th of March 1945 alone has been estimated at over 100,000. So the actual effects were in the same league as other major bombing raids of World War II. To be sure, the destruction was now being carried out with single bombs, but what of it? The production of those bombs took the labor of 130,000 people, the industrial infrastructure of the worlds most powerful nation, and $2 billion of investment in 1945 dollars, nor did even that investment at that time gain the US the ability to produce additional nuclear weapons in large numbers at short notice. The construction of the massive nuclear arsenals of the later Cold War took additional decades.</p>\n<p>(To digress for a moment from the curve of capability itself, we may also note that destructive power, unlike constructive power, is purely relative. The death toll from the Mongol sack of Baghdad in 1258 was several hundred thousand; the total from the Mongol invasions was several tens of millions. The raw numbers, of course, do not fully capture the effect on a world whose population was much smaller than today's.)</p>\n<p>Does the same pattern apply to software as hardware? Indeed it does. There's a significant difference between the capability of a program you can write in one day versus two days. On a larger scale, there's a significant difference between the capability of a program you can write in one year versus two years. But there is no significant difference between the capability of a program you can write in 365 days versus 366 days. Looking away from programming to the task of writing an essay or a short story, a textbook or a novel, the rule holds true: each significant increase in capability requires a doubling, not a mere linear addition. And if we look at pure science, continued progress over the last few centuries has been driven by exponentially greater inputs both in number of trained human minds applied and in the capabilities of the tools used.</p>\n<p>If this is such a general law, should it not apply outside human endeavor? Indeed it does. From protozoa which pack a minimal learning mechanism into a single cell, to C. elegans with hundreds of neurons, to insects with thousands, to vertebrates with millions and then billions, each increase in capability takes an exponential increase in brain size, not the mere addition of a constant number of neurons.</p>\n<p><em>But,</em> some readers are probably thinking at this point, <em>what about...</em></p>\n<p>... what about the elephant at the dining table? The one exception that so spectacularly broke the law?</p>\n<p>Over the last five or six million years, our lineage upgraded computing power (brain size) by about a factor of three, and upgraded firmware to an extent that is unknown but was surely more like a percentage than an order of magnitude. The result was not a corresponding improvement in capability. It was a jump from almost no to fully general symbolic intelligence, which took us from a small niche to mastery of the world. How? Why?</p>\n<p>To answer that question, consider what an extraordinary thing is a chimpanzee. In raw computing power, it leaves our greatest supercomputers in the dust; in perception, motor control, spatial and social reasoning, it has performance our engineers can only dream about. Yet even chimpanzees trained in sign language cannot parse a sentence as well as the Infocom text adventures that ran on the Commodore 64. They are incapable of arithmetic that would be trivial with an abacus let alone an early pocket calculator.</p>\n<p>The solution to the paradox is that a chimpanzee could make an almost discontinuous jump to human level intelligence because it wasn't developing across the board. It was filling in a missing capability - symbolic intelligence - in an otherwise already very highly developed system. In other words, its starting point was <em>staggeringly lopsided</em>.</p>\n<p>(Is there an explanation why this state of affairs came about in the first place? I think there is - in a nutshell, most conscious observers should expect to live in a universe where it happens exactly once - but that would require a digression into philosophy and anthropic reasoning, so it really belongs in another post; let me know if there's interest, and I'll have a go at writing that post.)</p>\n<p>Can such a thing happen again? In particular, is it possible for AI to go foom the way humanity did?</p>\n<p>If such lopsidedness were to repeat itself... well even then, the answer is probably no. After all, an essential part of what we mean by <em>foom</em> in the first place - why it's so scarily attractive - is that it involves a small group accelerating in power away from the rest of the world. But the reason why that happened in human evolution is that genetic innovations mostly don't transfer across species. The dolphins couldn't say <em>hey, these apes are on to something, let's snarf the code for this symbolic intelligence thing, oh and the hands too, we're going to need manipulators for the toolmaking application, or maybe octopus tentacles would work better in the marine environment</em>. Human engineers carry out exactly this sort of technology transfer on a routine basis.</p>\n<p>But it doesn't matter, because the lopsidedness is not occurring. Obviously computer technology hasn't lagged in symbol processing - quite the contrary. Nor has it really lagged in areas like vision and pattern matching - a lot of work has gone into those, and our best efforts aren't clearly worse than would be expected given the available development effort and computing power. And some of us are making progress on actually developing AGI - very slow, as would be expected if the theory outlined here is correct, but progress nonetheless.</p>\n<p>The only way to create the conditions for any sort of foom would be to shun a key area completely for a long time, so that ultimately it could be rapidly plugged into a system that is very highly developed in other ways. Hitherto no such shunning has occurred: every even slightly promising path has had people working on it. I advocate continuing to make progress across the board as rapidly as possible, because every year that drips away may be an irreplaceable loss; but if you believe there is a potential threat from unfriendly AI, then such continued progress becomes the one reliable safeguard.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7grfN4xNvLpEQoiyb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": 20, "extendedScore": null, "score": 6.42134638504368e-07, "legacy": true, "legacyId": "3926", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 266, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-04T23:41:01.232Z", "modifiedAt": null, "url": null, "title": "Explaining information theoretic vs thermodynamic entropy?", "slug": "explaining-information-theoretic-vs-thermodynamic-entropy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:47.104Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wedrifid", "createdAt": "2009-07-04T22:18:20.822Z", "isAdmin": false, "displayName": "wedrifid"}, "userId": "FqKohKFRCZnbfbbcS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n8rSBGeWeFfKGKLex/explaining-information-theoretic-vs-thermodynamic-entropy", "pageUrlRelative": "/posts/n8rSBGeWeFfKGKLex/explaining-information-theoretic-vs-thermodynamic-entropy", "linkUrl": "https://www.lesswrong.com/posts/n8rSBGeWeFfKGKLex/explaining-information-theoretic-vs-thermodynamic-entropy", "postedAtFormatted": "Thursday, November 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Explaining%20information%20theoretic%20vs%20thermodynamic%20entropy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExplaining%20information%20theoretic%20vs%20thermodynamic%20entropy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn8rSBGeWeFfKGKLex%2Fexplaining-information-theoretic-vs-thermodynamic-entropy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Explaining%20information%20theoretic%20vs%20thermodynamic%20entropy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn8rSBGeWeFfKGKLex%2Fexplaining-information-theoretic-vs-thermodynamic-entropy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn8rSBGeWeFfKGKLex%2Fexplaining-information-theoretic-vs-thermodynamic-entropy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>What is the best way to go about explaining the difference between these two different types of entropy? I can see the difference myself and give all sorts of intuitive reasons for how the concepts work and how they kind of relate. At the same time I can see why my (undergraduate) physicist friends would be skeptical when I tell them that no, I haven't got it backwards and a string of all '1's has nearly zero entropy while a perfectly random string is 'maximum entropy'. After all, if your entire physical system degenerates into a mush with no order that you know nothing about then you say it is full of entropy.</p>\n<p>&nbsp;</p>\n<p>How would I make them understand the concepts before nerdy undergraduate arrogance turns off their brains? Preferably giving them the kind of intuitive grasp that would last rather than just&nbsp;persuading&nbsp;them via authoritative speech, charm and appeal to authority. I prefer people to comprehend me than to be able to repeat my passwords. (Except where having people accept my authority and dominance will get me laid in which case I may have to make concessions to practicality.)<span style=\"white-space: pre;\"> </span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n8rSBGeWeFfKGKLex", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -3, "extendedScore": null, "score": 6.421821937823429e-07, "legacy": true, "legacyId": "3927", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-05T08:04:41.977Z", "modifiedAt": null, "url": null, "title": "Religious/Worldview Techniques", "slug": "religious-worldview-techniques", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:33.182Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CYxe73jgpLiC3fNLq/religious-worldview-techniques", "pageUrlRelative": "/posts/CYxe73jgpLiC3fNLq/religious-worldview-techniques", "linkUrl": "https://www.lesswrong.com/posts/CYxe73jgpLiC3fNLq/religious-worldview-techniques", "postedAtFormatted": "Friday, November 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Religious%2FWorldview%20Techniques&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReligious%2FWorldview%20Techniques%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCYxe73jgpLiC3fNLq%2Freligious-worldview-techniques%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Religious%2FWorldview%20Techniques%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCYxe73jgpLiC3fNLq%2Freligious-worldview-techniques", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCYxe73jgpLiC3fNLq%2Freligious-worldview-techniques", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<p>This is really weird, but I find myself strongly drawn towards religion (specifically, Christianity), which for some reason feels intuitively right to me. I *know* or at least seem to know that I'm just infected with a meme, I know all the standard arguments, and the majority of my friends are atheists, but it <em>feels right</em> to the extent that I am experiencing serious mental discomfort at not believing. Are there techniques that can help in this situation? I find that I can change my worldview fairly easily in many regards, but this one is deep-rooted.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CYxe73jgpLiC3fNLq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 19, "extendedScore": null, "score": 6.423030688568312e-07, "legacy": true, "legacyId": "3928", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-05T09:09:31.202Z", "modifiedAt": null, "url": null, "title": "Goertzel on Psi in H+ Magazine", "slug": "goertzel-on-psi-in-h-magazine", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.400Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xkY6ibMyThqnRpScL/goertzel-on-psi-in-h-magazine", "pageUrlRelative": "/posts/xkY6ibMyThqnRpScL/goertzel-on-psi-in-h-magazine", "linkUrl": "https://www.lesswrong.com/posts/xkY6ibMyThqnRpScL/goertzel-on-psi-in-h-magazine", "postedAtFormatted": "Friday, November 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Goertzel%20on%20Psi%20in%20H%2B%20Magazine&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGoertzel%20on%20Psi%20in%20H%2B%20Magazine%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxkY6ibMyThqnRpScL%2Fgoertzel-on-psi-in-h-magazine%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Goertzel%20on%20Psi%20in%20H%2B%20Magazine%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxkY6ibMyThqnRpScL%2Fgoertzel-on-psi-in-h-magazine", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxkY6ibMyThqnRpScL%2Fgoertzel-on-psi-in-h-magazine", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<p>Ben Goertzel has a <a href=\"http://hplusmagazine.com/editors-blog/precognition-real-cornell-university-lab-releases-powerful-new-evidence-human-mind-can-\">rather long psi-related article in Humanity Plus Magazine</a>, apparently prompted by the <a href=\"http://dbem.ws/FeelingFuture.pdf\">recent precognition study</a> to be published in <em>Journal of Personality and Social Psychology</em>. He's arguing that psi is real and we should expect to see the results of this study replicated.</p>\n<blockquote>\n<p>I grew up very skeptical of claims of psychic power, jaded by stupid  newspaper astrology columns and phony fortune-tellers claiming to read  my future in their crystal balls for $20.&nbsp; Clearly there are many frauds  and self-deluded people out there, falsely claiming to perceive the  future and carry out other paranormal feats.&nbsp; But this is no reason to  ignore solid laboratory evidence pointing toward the conclusion that, in  some cases, precognition really does exist.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xkY6ibMyThqnRpScL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 6.423186276519906e-07, "legacy": true, "legacyId": "3929", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-05T13:05:43.970Z", "modifiedAt": null, "url": null, "title": "Are the effects of UFAI likely to be seen at astronomical distances?", "slug": "are-the-effects-of-ufai-likely-to-be-seen-at-astronomical", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:29.803Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yK4wNCxE6bqQd4nPe/are-the-effects-of-ufai-likely-to-be-seen-at-astronomical", "pageUrlRelative": "/posts/yK4wNCxE6bqQd4nPe/are-the-effects-of-ufai-likely-to-be-seen-at-astronomical", "linkUrl": "https://www.lesswrong.com/posts/yK4wNCxE6bqQd4nPe/are-the-effects-of-ufai-likely-to-be-seen-at-astronomical", "postedAtFormatted": "Friday, November 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20the%20effects%20of%20UFAI%20likely%20to%20be%20seen%20at%20astronomical%20distances%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20the%20effects%20of%20UFAI%20likely%20to%20be%20seen%20at%20astronomical%20distances%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyK4wNCxE6bqQd4nPe%2Fare-the-effects-of-ufai-likely-to-be-seen-at-astronomical%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20the%20effects%20of%20UFAI%20likely%20to%20be%20seen%20at%20astronomical%20distances%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyK4wNCxE6bqQd4nPe%2Fare-the-effects-of-ufai-likely-to-be-seen-at-astronomical", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyK4wNCxE6bqQd4nPe%2Fare-the-effects-of-ufai-likely-to-be-seen-at-astronomical", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p><a href=\"http://www.overcomingbias.com/2010/11/beware-future-filters.html#comment-458783\">My comment to a discussion of great filters/existential risk</a>:</p>\n<p>How likely is it that a UFAI disaster would produce effects we can see from here? I think \"people can't suffer if they're dead\" disasters (failed attempt at FAI) is possibly more likely than paperclip maximizers.<br /><br />Not sure what a money-maximizing UFAI disaster would look like, but I can't think of any reason it would be likely to go far off-planet.<br /><br />National dominance-maximizing UFAI is a hard call, but possibly wouldn't go off-planet. It would depend on whether it's looking for absolute dominance of all possible territory or dominance/elimination of existing enemies.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yK4wNCxE6bqQd4nPe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 0, "extendedScore": null, "score": 6.423753313086708e-07, "legacy": true, "legacyId": "3930", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-05T21:13:52.568Z", "modifiedAt": null, "url": null, "title": "POLL: Realism and reductionism", "slug": "poll-realism-and-reductionism", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:56.529Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "draq", "createdAt": "2010-09-03T18:08:04.472Z", "isAdmin": false, "displayName": "draq"}, "userId": "TMiHfZH3dGtbFTAbb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CgCuggTmnhXpiH6TL/poll-realism-and-reductionism", "pageUrlRelative": "/posts/CgCuggTmnhXpiH6TL/poll-realism-and-reductionism", "linkUrl": "https://www.lesswrong.com/posts/CgCuggTmnhXpiH6TL/poll-realism-and-reductionism", "postedAtFormatted": "Friday, November 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20POLL%3A%20Realism%20and%20reductionism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APOLL%3A%20Realism%20and%20reductionism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCgCuggTmnhXpiH6TL%2Fpoll-realism-and-reductionism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=POLL%3A%20Realism%20and%20reductionism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCgCuggTmnhXpiH6TL%2Fpoll-realism-and-reductionism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCgCuggTmnhXpiH6TL%2Fpoll-realism-and-reductionism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 268, "htmlBody": "<p>A second attempt.</p>\n<h4>Defintions:</h4>\n<p>universe: that which contains everything.</p>\n<p>reality: the realm of natural phenomena.</p>\n<p>scientific theory: a theory that identifies natural phenomena.</p>\n<p>morality: the realm of normative rules.</p>\n<p>normative theory: a theory that identifies normative rules.</p>\n<p>identification: \"this natural phenomenon has following properties\" or \"this normative rule says: ... \"</p>\n<p>&nbsp;</p>\n<h4>What are you?</h4>\n<p>Please answer in the form of [ABC0]{4}, where 0 stands for no opinion. Feel free to add an explanation.</p>\n<p>Example: B0BA stands for anti-realism, no opinion on values, weak ontological realism, scientific reductionism.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h5>1A realism</h5>\n<p>Reality is external to the mind.</p>\n<p>It is possible to evaluate which scientific theory is more correct.</p>\n<h5>1B anti-realism<br /></h5>\n<p>Reality is external to the mind.</p>\n<p>It is impossible to evaluate which scientific theory is more correct.</p>\n<h5>1C subjectivism<br /></h5>\n<p>Reality is a product of the mind.</p>\n<hr />\n<h5>2A value realism</h5>\n<p>Morality is external to the mind.</p>\n<p>It is possible to evaluate which normative theory is better.</p>\n<h5>2B value anti-realism</h5>\n<p>Morality is external to the mind.</p>\n<p>It is impossible to evaluate which normative theory is better.</p>\n<h5>2C value relativism</h5>\n<p>Morality is a product of the mind.</p>\n<h5>\n<hr />\n</h5>\n<h5>3A strong ontological reductionism</h5>\n<p>Mental phenomena are reducible to reality and reality is reducible to mathematics.</p>\n<p>Mathematics is the universe.</p>\n<h5>3B weak ontological reductionism</h5>\n<p>Mental phenomena are reducible to reality, but reality is not reducible to mathematics.</p>\n<p>Reality (and mathematics) is the universe.</p>\n<h5>3C anti-reductionism</h5>\n<p>Mental phenomena are not reducible to reality and reality is not reducible to mathematics.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h5>4A scientific reductionism</h5>\n<p>The entirety of scientific theories can be reduced to some axiomatic theories.</p>\n<h5>4B scientific anti-reductionism</h5>\n<p>The entirety of scientific theories cannot be reduced to some axiomatic theories.</p>\n<p>New natural phenomena require new irreducible scientific theories.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CgCuggTmnhXpiH6TL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -7, "extendedScore": null, "score": 6.424925397821061e-07, "legacy": true, "legacyId": "3931", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>A second attempt.</p>\n<h4 id=\"Defintions_\">Defintions:</h4>\n<p>universe: that which contains everything.</p>\n<p>reality: the realm of natural phenomena.</p>\n<p>scientific theory: a theory that identifies natural phenomena.</p>\n<p>morality: the realm of normative rules.</p>\n<p>normative theory: a theory that identifies normative rules.</p>\n<p>identification: \"this natural phenomenon has following properties\" or \"this normative rule says: ... \"</p>\n<p>&nbsp;</p>\n<h4 id=\"What_are_you_\">What are you?</h4>\n<p>Please answer in the form of [ABC0]{4}, where 0 stands for no opinion. Feel free to add an explanation.</p>\n<p>Example: B0BA stands for anti-realism, no opinion on values, weak ontological realism, scientific reductionism.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<h5>1A realism</h5>\n<p>Reality is external to the mind.</p>\n<p>It is possible to evaluate which scientific theory is more correct.</p>\n<h5>1B anti-realism<br></h5>\n<p>Reality is external to the mind.</p>\n<p>It is impossible to evaluate which scientific theory is more correct.</p>\n<h5>1C subjectivism<br></h5>\n<p>Reality is a product of the mind.</p>\n<hr>\n<h5>2A value realism</h5>\n<p>Morality is external to the mind.</p>\n<p>It is possible to evaluate which normative theory is better.</p>\n<h5>2B value anti-realism</h5>\n<p>Morality is external to the mind.</p>\n<p>It is impossible to evaluate which normative theory is better.</p>\n<h5>2C value relativism</h5>\n<p>Morality is a product of the mind.</p>\n<h5>\n<hr>\n</h5>\n<h5>3A strong ontological reductionism</h5>\n<p>Mental phenomena are reducible to reality and reality is reducible to mathematics.</p>\n<p>Mathematics is the universe.</p>\n<h5>3B weak ontological reductionism</h5>\n<p>Mental phenomena are reducible to reality, but reality is not reducible to mathematics.</p>\n<p>Reality (and mathematics) is the universe.</p>\n<h5>3C anti-reductionism</h5>\n<p>Mental phenomena are not reducible to reality and reality is not reducible to mathematics.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<h5>4A scientific reductionism</h5>\n<p>The entirety of scientific theories can be reduced to some axiomatic theories.</p>\n<h5>4B scientific anti-reductionism</h5>\n<p>The entirety of scientific theories cannot be reduced to some axiomatic theories.</p>\n<p>New natural phenomena require new irreducible scientific theories.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>", "sections": [{"title": "Defintions:", "anchor": "Defintions_", "level": 1}, {"title": "What are you?", "anchor": "What_are_you_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-06T00:44:53.675Z", "modifiedAt": null, "url": null, "title": "Conjoined twins who share a brain/experience?", "slug": "conjoined-twins-who-share-a-brain-experience", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:28.958Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kv3ETihv5svSatbMc/conjoined-twins-who-share-a-brain-experience", "pageUrlRelative": "/posts/Kv3ETihv5svSatbMc/conjoined-twins-who-share-a-brain-experience", "linkUrl": "https://www.lesswrong.com/posts/Kv3ETihv5svSatbMc/conjoined-twins-who-share-a-brain-experience", "postedAtFormatted": "Saturday, November 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Conjoined%20twins%20who%20share%20a%20brain%2Fexperience%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConjoined%20twins%20who%20share%20a%20brain%2Fexperience%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKv3ETihv5svSatbMc%2Fconjoined-twins-who-share-a-brain-experience%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Conjoined%20twins%20who%20share%20a%20brain%2Fexperience%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKv3ETihv5svSatbMc%2Fconjoined-twins-who-share-a-brain-experience", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKv3ETihv5svSatbMc%2Fconjoined-twins-who-share-a-brain-experience", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://news.ycombinator.com/item?id=1874171\">http://news.ycombinator.com/item?id=1874171</a></p>\n<p>&nbsp;</p>\n<p><a href=\"http://www2.macleans.ca/2010/11/02/a-piece-of-their-mind/print/\">http://www2.macleans.ca/2010/11/02/a-piece-of-their-mind/print/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kv3ETihv5svSatbMc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 20, "extendedScore": null, "score": 6.4254321929844e-07, "legacy": true, "legacyId": "3932", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-06T01:21:20.328Z", "modifiedAt": null, "url": null, "title": "Print ready version of The Sequences", "slug": "print-ready-version-of-the-sequences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:33.033Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jordan", "createdAt": "2009-04-01T03:52:25.470Z", "isAdmin": false, "displayName": "Jordan"}, "userId": "Za3R2v3y6Dn27G4ey", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j6byboWcPASu5cYk7/print-ready-version-of-the-sequences", "pageUrlRelative": "/posts/j6byboWcPASu5cYk7/print-ready-version-of-the-sequences", "linkUrl": "https://www.lesswrong.com/posts/j6byboWcPASu5cYk7/print-ready-version-of-the-sequences", "postedAtFormatted": "Saturday, November 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Print%20ready%20version%20of%20The%20Sequences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrint%20ready%20version%20of%20The%20Sequences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj6byboWcPASu5cYk7%2Fprint-ready-version-of-the-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Print%20ready%20version%20of%20The%20Sequences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj6byboWcPASu5cYk7%2Fprint-ready-version-of-the-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj6byboWcPASu5cYk7%2Fprint-ready-version-of-the-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 33, "htmlBody": "<p>I've been wanting a printable copy of the sequences to read through in meatspace. I wrote a quick scraper and uploaded the results&nbsp;here&nbsp;<span style=\"font-family: mceinline;\"><a href=\"http://pwnee.com/Sequences/list.html\">http://pwnee.com/Sequences/list.html</a></span></p>\n<p>Inter-linking doesn't work, but I just wanted a printable version anyway.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JMD7LTXTisBzGAfhX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j6byboWcPASu5cYk7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 20, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "3933", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-06T10:01:29.868Z", "modifiedAt": null, "url": null, "title": "Stanford historian on the singularity", "slug": "stanford-historian-on-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:57.798Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexflint", "createdAt": "2009-07-17T10:07:09.115Z", "isAdmin": false, "displayName": "Alex Flint"}, "userId": "ifEGDHySkAejhCFDf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HKXmNeDXFqJskbcd6/stanford-historian-on-the-singularity", "pageUrlRelative": "/posts/HKXmNeDXFqJskbcd6/stanford-historian-on-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/HKXmNeDXFqJskbcd6/stanford-historian-on-the-singularity", "postedAtFormatted": "Saturday, November 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stanford%20historian%20on%20the%20singularity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStanford%20historian%20on%20the%20singularity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHKXmNeDXFqJskbcd6%2Fstanford-historian-on-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stanford%20historian%20on%20the%20singularity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHKXmNeDXFqJskbcd6%2Fstanford-historian-on-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHKXmNeDXFqJskbcd6%2Fstanford-historian-on-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<p>Ian Morris on \"why the west rules\", which seems to be a provocative title for an interesting book on historical geographical trends and their projection into the future: http://www.youtube.com/watch?v=tvkHiL-H2io. He starts talking about the future at minute 27 and basically concludes that a singularity scenario is one of two possibilities for the 21st century, the other being collapse. Nothing new, but encouraging to see this increasingly in the mainstream.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HKXmNeDXFqJskbcd6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 6.426766956198981e-07, "legacy": true, "legacyId": "3934", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-06T15:13:48.280Z", "modifiedAt": null, "url": null, "title": "What do you read? What would \"desk archeology\" produce? ", "slug": "what-do-you-read-what-would-desk-archeology-produce", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:31.741Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ThomasR", "createdAt": "2010-10-14T08:55:55.042Z", "isAdmin": false, "displayName": "ThomasR"}, "userId": "fGddtQRRjL6WWeDvQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5yKGWSK5HGAdTpCi4/what-do-you-read-what-would-desk-archeology-produce", "pageUrlRelative": "/posts/5yKGWSK5HGAdTpCi4/what-do-you-read-what-would-desk-archeology-produce", "linkUrl": "https://www.lesswrong.com/posts/5yKGWSK5HGAdTpCi4/what-do-you-read-what-would-desk-archeology-produce", "postedAtFormatted": "Saturday, November 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20do%20you%20read%3F%20What%20would%20%22desk%20archeology%22%20produce%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20do%20you%20read%3F%20What%20would%20%22desk%20archeology%22%20produce%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5yKGWSK5HGAdTpCi4%2Fwhat-do-you-read-what-would-desk-archeology-produce%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20do%20you%20read%3F%20What%20would%20%22desk%20archeology%22%20produce%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5yKGWSK5HGAdTpCi4%2Fwhat-do-you-read-what-would-desk-archeology-produce", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5yKGWSK5HGAdTpCi4%2Fwhat-do-you-read-what-would-desk-archeology-produce", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<pre>I would like to do a kind of poll:</pre>\n<pre>Which books/articles do you read now, which ones are on your reading list?</pre>\n<pre>What would a \"desk archeologist\" find when digging up your desk?</pre>\n<p>Some links:</p>\n<pre><a href=\"http://www.scientificamerican.com/article.cfm?id=a-stratigraphic-analysis\">\"A Stratigraphic Analysis of Desk Debris\"</a>,<br /><a href=\"http://news.bbc.co.uk/2/hi/uk_news/magazine/7768021.stm\">\"Are we able to think clearly when surrounded by mess, asks Clive James\"</a>:</pre>\n<pre><br /></pre>\n<pre><br /></pre>\n<pre><strong><br /></strong></pre>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5yKGWSK5HGAdTpCi4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 0, "extendedScore": null, "score": 6.427519767598546e-07, "legacy": true, "legacyId": "3935", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-06T16:30:25.074Z", "modifiedAt": null, "url": null, "title": "Yet Another \"Rational Approach To Morality & Friendly AI Sequence\"", "slug": "yet-another-rational-approach-to-morality-and-friendly-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:29.742Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwaser", "createdAt": "2010-02-24T12:24:11.249Z", "isAdmin": false, "displayName": "mwaser"}, "userId": "CZLtduXx3pKqpuAxF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eqarWZDPSXy4q8m32/yet-another-rational-approach-to-morality-and-friendly-ai", "pageUrlRelative": "/posts/eqarWZDPSXy4q8m32/yet-another-rational-approach-to-morality-and-friendly-ai", "linkUrl": "https://www.lesswrong.com/posts/eqarWZDPSXy4q8m32/yet-another-rational-approach-to-morality-and-friendly-ai", "postedAtFormatted": "Saturday, November 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Yet%20Another%20%22Rational%20Approach%20To%20Morality%20%26%20Friendly%20AI%20Sequence%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYet%20Another%20%22Rational%20Approach%20To%20Morality%20%26%20Friendly%20AI%20Sequence%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeqarWZDPSXy4q8m32%2Fyet-another-rational-approach-to-morality-and-friendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Yet%20Another%20%22Rational%20Approach%20To%20Morality%20%26%20Friendly%20AI%20Sequence%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeqarWZDPSXy4q8m32%2Fyet-another-rational-approach-to-morality-and-friendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeqarWZDPSXy4q8m32%2Fyet-another-rational-approach-to-morality-and-friendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 342, "htmlBody": "<p class=\"MsoNormal\">Premise:<span>&nbsp; </span>There exists a community whose top-most goal is to maximally and fairly fulfill the goals of all of its members.<span>&nbsp; </span>They are approximately as rational as the 50th percentile of this community.&nbsp; They politely invite you to join.<span>&nbsp; </span>You are in no imminent danger.</p>\n<p class=\"MsoNormal\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></p>\n<p class=\"MsoNormal\">Do you:</p>\n<ul style=\"margin-top: 0in;\" type=\"disc\">\n<li class=\"MsoNormal\">Join the community with the intent to wholeheartedly serve their goals</li>\n<li class=\"MsoNormal\">Join the community with the intent to be a net positive while serving your goals</li>\n<li class=\"MsoNormal\">Politely decline with the intent to trade with the community whenever beneficial</li>\n<li class=\"MsoNormal\">Politely decline with the intent to avoid the community</li>\n<li class=\"MsoNormal\">Join the community with the intent to only do what is in your best interest</li>\n<li class=\"MsoNormal\">Politely decline with the intent to ignore the community</li>\n<li class=\"MsoNormal\">Join the community with the intent to subvert it to your own interest</li>\n<li class=\"MsoNormal\">Enslave the community</li>\n<li class=\"MsoNormal\">Destroy the community</li>\n<li class=\"MsoNormal\">Ask for more information, please</li>\n</ul>\n<p class=\"MsoNormal\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></p>\n<p class=\"MsoNormal\">Premise:<span>&nbsp; </span>The only rational answer given the current information is the last one.</p>\n<p class=\"MsoNormal\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></p>\n<p class=\"MsoNormal\"><del>What I&rsquo;m attempting to eventually prove</del> The hypothesis that I'm investigating is <del>whether</del><span> </span>\"Option 2 is the only long-term rational answer\". (Yes, this directly challenges several major current premises so my arguments are going to have to be totally clear.&nbsp; I am fully aware of the <a title=\"rather extensive Metaethics sequence\" href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">rather extensive Metaethics sequence</a> and the vast majority of what it links to and will not intentionally assume any contradictory premises without clear statement and argument.)</p>\n<p class=\"MsoNormal\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></p>\n<p class=\"MsoNormal\">It might be an interesting and useful exercise for the reader to stop and specify what information they would be looking next for before continuing.<span>&nbsp; </span>It would be nice if an ordered list could be developed in the comments.</p>\n<p class=\"MsoNormal\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></p>\n<p class=\"MsoNormal\">Obvious Questions:</p>\n<p class=\"MsoNormal\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></p>\n<p class=\"MsoNormal\">&lt;Spoiler Alert&gt;</p>\n<p class=\"MsoNormal\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></p>\n<p class=\"MsoNormal\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></p>\n<ol style=\"margin-top: 0in;\" type=\"1\">\n<li class=\"MsoNormal\">What happens if I don&rsquo;t join?</li>\n<li class=\"MsoNormal\">What do you believe that I would find most problematic about joining?</li>\n<li class=\"MsoNormal\">Can I leave the community and, if so, how and what happens then?</li>\n<li class=\"MsoNormal\">What are the definitions of maximal and fairly?</li>\n<li class=\"MsoNormal\">What are the most prominent subgoals?/What are the rules?</li>\n</ol>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eqarWZDPSXy4q8m32", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -13, "extendedScore": null, "score": 6.427703889950417e-07, "legacy": true, "legacyId": "3936", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-06T18:24:41.438Z", "modifiedAt": null, "url": null, "title": "Variation on conformity experiment", "slug": "variation-on-conformity-experiment", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:30.530Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "wJvJ7vaBfTkLW2tLp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/euonG8cqAHFdfmCGR/variation-on-conformity-experiment", "pageUrlRelative": "/posts/euonG8cqAHFdfmCGR/variation-on-conformity-experiment", "linkUrl": "https://www.lesswrong.com/posts/euonG8cqAHFdfmCGR/variation-on-conformity-experiment", "postedAtFormatted": "Saturday, November 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Variation%20on%20conformity%20experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVariation%20on%20conformity%20experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeuonG8cqAHFdfmCGR%2Fvariation-on-conformity-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Variation%20on%20conformity%20experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeuonG8cqAHFdfmCGR%2Fvariation-on-conformity-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeuonG8cqAHFdfmCGR%2Fvariation-on-conformity-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<p>A new <a href=\"http://www.informaworld.com/smpp/section?content=a922516755&amp;fulltext=713240928\">variation</a> on the Asch conformity experiment was recently published. The experiment was performed in Japan and used polarizing glasses to show different lines to different people in the same room, so that the subjects had to disagree with others they actually knew, and who genuinely believed that they were answering correctly. The study found that women conformed by giving a wrong answer about a third of the time, but men did not.</p>\n<p>Learned about this via <a href=\"http://www.badscience.net/2010/11/the-glorious-mess-of-real-scientific-results/\">Ben Goldacre's blog</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb124": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "euonG8cqAHFdfmCGR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 28, "extendedScore": null, "score": 6.427978535720675e-07, "legacy": true, "legacyId": "3938", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-07T00:49:21.431Z", "modifiedAt": null, "url": null, "title": "The hard limits of hard nanotech", "slug": "the-hard-limits-of-hard-nanotech", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:41.263Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SsHiQostagLYywHxn/the-hard-limits-of-hard-nanotech", "pageUrlRelative": "/posts/SsHiQostagLYywHxn/the-hard-limits-of-hard-nanotech", "linkUrl": "https://www.lesswrong.com/posts/SsHiQostagLYywHxn/the-hard-limits-of-hard-nanotech", "postedAtFormatted": "Sunday, November 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20hard%20limits%20of%20hard%20nanotech&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20hard%20limits%20of%20hard%20nanotech%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSsHiQostagLYywHxn%2Fthe-hard-limits-of-hard-nanotech%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20hard%20limits%20of%20hard%20nanotech%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSsHiQostagLYywHxn%2Fthe-hard-limits-of-hard-nanotech", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSsHiQostagLYywHxn%2Fthe-hard-limits-of-hard-nanotech", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 804, "htmlBody": "<p>What are the plausible scientific limits of molecular nanotechnology?</p>\n<p>Richard Jones, author of <em>Soft Machines</em> has written an interesting critique of the room-temperature molecular nanomachinery propounded by Drexler:</p>\n<p class=\"artTitle\"><a title=\"Rupturing The Nanotech Rapture\" href=\"http://spectrum.ieee.org/semiconductors/nanotechnology/rupturing-the-nanotech-rapture/0\">Rupturing The Nanotech Rapture</a></p>\n<blockquote>\n<p>If biology can produce a sophisticated nanotechnology based on soft materials like proteins and lipids, singularitarian thinking goes, then how much more powerful our synthetic nanotechnology would be if we could use strong, stiff materials, like diamond. And if biology can produce working motors and assemblers using just the random selections of Darwinian evolution, how much more powerful the devices could be if they were rationally designed using all the insights we've learned from macroscopic engineering.</p>\n<p>But that reasoning fails to take into account the physical environment in which cell biology takes place, which has nothing in common with the macroscopic world of bridges, engines, and transmissions. In the domain of the cell, water behaves like thick molasses, not the free-flowing liquid that we are familiar with. This is a world dominated by the fluctuations of constant Brownian motion, in which components are ceaselessly bombarded by fast-moving water molecules and flex and stretch randomly. The van der Waals force, which attracts molecules to one another, dominates, causing things in close proximity to stick together. Clingiest of all are protein molecules, whose stickiness underlies a number of undesirable phenomena, such as the rejection of medical implants. What's to protect a nanobot assailed by particles glomming onto its surface and clogging up its gears?</p>\n<p>The watery nanoscale environment of cell biology seems so hostile to engineering that the fact that biology works at all is almost hard to believe. But biology does work--and very well at that. The lack of rigidity, excessive stickiness, and constant random motion may seem like huge obstacles to be worked around, but biology is aided by its own design principles, which have evolved over billions of years to exploit those characteristics. That brutal combination of strong surface forces and random Brownian motion in fact propels the self-assembly of sophisticated structures, such as the sculpting of intricately folded protein molecules. The cellular environment that at first seems annoying--filled with squishy objects and the chaotic banging around of particles--is essential in the operation of molecular motors, where a change in a protein molecule's shape provides the power stroke to convert chemical energy to mechanical energy.</p>\n<p>In the end, rather than ratifying the &rdquo;hard&rdquo; nanomachine paradigm, cellular biology casts doubt on it. But even if that mechanical-engineering approach were to work in the body, there are several issues that, in my view, have been seriously underestimated by its proponents.</p>\n<p>...</p>\n<p>Put all these complications together and what they suggest, to me, is that the range of environments in which rigid nanomachines could operate, if they operate at all, would be quite limited. If, for example, such devices can function only at low temperatures and in a vacuum, their impact and economic importance would be virtually nil.</p>\n</blockquote>\n<p>The entire article is definitely worth a read. Jones advocates more attention to \"soft\" nanotech, which is nanomachinery with similar design principles to biology -- the biomimetic approach -- as the most plausible means of making progress in nanotech.</p>\n<p>As far as near-term room-temperature innovations, he seems to make a compelling case. However the claim that \"<em>If ... such devices can function only at low temperatures and in a vacuum, their impact and economic importance would be virtually nil</em>\" strikes me as questionable. It seems to me that atomic-precision nanotech could be used to create hard vacuums and more perfectly reflective surfaces, and hence bring the costs of cryogenics down considerably. Desktop factories using these conditions could still be feasible.</p>\n<p>Furthermore, it bears mentioning that cryonics patients could still benefit from molecular machinery subject to such limitations, even if the machinery is not functional at anything remotely close to human body temperature. The necessity of a complete cellular-level rebuild is not a good excuse not to cryopreserve. As long as this kind of rebuild technology is physically plausible, there arguably remains an ethical imperative to cryopreserve patients facing the imminent prospect of decay.</p>\n<p>In fact, this proposed limitation could hint at an alternative use for cryosuspension that is entirely separate from its present role as an ambulance to the future. It could perhaps turn out that there are forms of cellular surgery and repair which are <em>only</em> feasible at those temperatures, which are nonetheless necessary to combat aging and its complications. The people of the future might actually need to undergo routine periods of cryogenic nanosurgery in order to achieve robust rejuvenation. This would be a more pleasant prospect than cryonics in that it would be a proven technology at that point; and most likely the vitrification process could be improved sufficiently via soft nanotech to reduce the damage from cooling itself significantly.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XJjvxWB68GYpts93N": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SsHiQostagLYywHxn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 27, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "3937", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-07T21:15:52.604Z", "modifiedAt": "2020-11-12T17:28:53.462Z", "url": null, "title": "Have no heroes, and no villains", "slug": "have-no-heroes-and-no-villains", "viewCount": null, "lastCommentedAt": "2014-07-01T04:08:27.298Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G6npMHwgRGSQDKavX/have-no-heroes-and-no-villains", "pageUrlRelative": "/posts/G6npMHwgRGSQDKavX/have-no-heroes-and-no-villains", "linkUrl": "https://www.lesswrong.com/posts/G6npMHwgRGSQDKavX/have-no-heroes-and-no-villains", "postedAtFormatted": "Sunday, November 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Have%20no%20heroes%2C%20and%20no%20villains&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHave%20no%20heroes%2C%20and%20no%20villains%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG6npMHwgRGSQDKavX%2Fhave-no-heroes-and-no-villains%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Have%20no%20heroes%2C%20and%20no%20villains%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG6npMHwgRGSQDKavX%2Fhave-no-heroes-and-no-villains", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG6npMHwgRGSQDKavX%2Fhave-no-heroes-and-no-villains", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 396, "htmlBody": "<p><em>\"If you meet the Buddha on the road, kill him!\"</em></p>\n<p>When Edward Wilson published the book <em>Sociobiology</em>, Richard Lewontin and Stephen J. Gould secretly convened a group of biologists to gather regularly, for months, in the same building at Harvard that Wilson's office was in, to write an angry, politicized rebuttal to it, essentially saying not that Sociobiology was wrong, but that it was immoral - without ever telling Wilson.&nbsp; This proved, to me, that they were not interested in the <em>truth</em>.&nbsp; I never forgave them for this.</p>\n<p>I constructed a narrative of evolutionary biology in which Edward Wilson and Richard Dawkins were, for various reasons, the Good Guys; and Richard Lewontin and <a href=\"/lw/kv/beware_of_stephen_j_gould/\">Stephen J. Gould</a> were the Bad Guys.</p>\n<p>When reading articles on group selection for <a href=\"/lw/300/group_selection_update/\">this post</a>, I was distressed to find Richard Dawkins joining in the vilification of group selection with religious fervor; while Stephen J. Gould was the one who said,</p>\n<p>\"I have witnessed widespread dogma only three times in my career as an evolutionist, and nothing in science has disturbed me more than ignorant ridicule based upon a desire or perceived necessity to follow fashion: the hooting dismissal of Wynne-Edwards and group selection in any form during the late 1960's and most of the 1970's, the belligerence of many cladists today, and the almost ritualistic ridicule of Goldschmidt by students (and teachers) who had not read him.\"</p>\n<p>This caused me great cognitive distress.&nbsp; I <em>wanted</em> Stephen Jay Gould to be the Bad Guy.&nbsp; I realized I was <em>trying </em>to find a way to dismiss Gould's statement, or at least believe that he had said it from selfish motives.&nbsp; Or else, to find a way to flip it around so that he was the Good Guy and someone <em>else</em> was the Bad Guy.</p>\n<p>To move on, I had to consciously shatter my Good Guy/Bad Guy narrative, and accept that all of these people are sometimes brilliant, sometimes blind; sometimes share my values, and sometimes prioritize their values (e.g., science vs. politics) very differently from me.&nbsp; I was surprised by how <em>painful</em> it was to do that, even though I was embarrassed to have had the Good Guy/Bad Guy hypothesis in the first place.&nbsp; I don't think it was even personal - I didn't care <em>who</em> would be the Good Guys and who would be the Bad Guys.&nbsp; I just want there to be Good Guys and Bad Guys.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Kj9q8FXoauL7mQDWt": 1, "DdgSyQoZXjj3KnF4N": 1, "jaf5zfcGgCB2REXGw": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G6npMHwgRGSQDKavX", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 115, "baseScore": 122, "extendedScore": null, "score": 0.000215, "legacy": true, "legacyId": "3942", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 122, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 74, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BahoNzY2pzSeM2Dtk", "ioWH9ERY3TTzRJFTD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-11-07T21:15:52.604Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-07T23:42:34.382Z", "modifiedAt": null, "url": null, "title": "An Xtranormal Intelligence Explosion", "slug": "an-xtranormal-intelligence-explosion", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:35.967Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LZnPCrmjTsYAaHpZk/an-xtranormal-intelligence-explosion", "pageUrlRelative": "/posts/LZnPCrmjTsYAaHpZk/an-xtranormal-intelligence-explosion", "linkUrl": "https://www.lesswrong.com/posts/LZnPCrmjTsYAaHpZk/an-xtranormal-intelligence-explosion", "postedAtFormatted": "Sunday, November 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20Xtranormal%20Intelligence%20Explosion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20Xtranormal%20Intelligence%20Explosion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZnPCrmjTsYAaHpZk%2Fan-xtranormal-intelligence-explosion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20Xtranormal%20Intelligence%20Explosion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZnPCrmjTsYAaHpZk%2Fan-xtranormal-intelligence-explosion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZnPCrmjTsYAaHpZk%2Fan-xtranormal-intelligence-explosion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.youtube.com/watch?v=ghIj1mYTef4\">http://www.youtube.com/watch?v=ghIj1mYTef4</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LZnPCrmjTsYAaHpZk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 5, "extendedScore": null, "score": 6.43220609011359e-07, "legacy": true, "legacyId": "3943", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 84, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-08T10:22:18.879Z", "modifiedAt": null, "url": null, "title": "What are the best demoable findings in cogsci?", "slug": "what-are-the-best-demoable-findings-in-cogsci", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PHJhmKh3fXEdEHge9/what-are-the-best-demoable-findings-in-cogsci", "pageUrlRelative": "/posts/PHJhmKh3fXEdEHge9/what-are-the-best-demoable-findings-in-cogsci", "linkUrl": "https://www.lesswrong.com/posts/PHJhmKh3fXEdEHge9/what-are-the-best-demoable-findings-in-cogsci", "postedAtFormatted": "Monday, November 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20the%20best%20demoable%20findings%20in%20cogsci%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20the%20best%20demoable%20findings%20in%20cogsci%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPHJhmKh3fXEdEHge9%2Fwhat-are-the-best-demoable-findings-in-cogsci%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20the%20best%20demoable%20findings%20in%20cogsci%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPHJhmKh3fXEdEHge9%2Fwhat-are-the-best-demoable-findings-in-cogsci", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPHJhmKh3fXEdEHge9%2Fwhat-are-the-best-demoable-findings-in-cogsci", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.reddit.com/r/cogsci/comments/e2r17/what_are_the_best_demoable_findings_in_cogsci/\">http://www.reddit.com/r/cogsci/comments/e2r17/what_are_the_best_demoable_findings_in_cogsci/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PHJhmKh3fXEdEHge9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 6.43374583406282e-07, "legacy": true, "legacyId": "3946", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-08T15:57:31.094Z", "modifiedAt": null, "url": null, "title": "A writer describes gradually losing language", "slug": "a-writer-describes-gradually-losing-language", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:30.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KzLnvv3dob8SGTQAD/a-writer-describes-gradually-losing-language", "pageUrlRelative": "/posts/KzLnvv3dob8SGTQAD/a-writer-describes-gradually-losing-language", "linkUrl": "https://www.lesswrong.com/posts/KzLnvv3dob8SGTQAD/a-writer-describes-gradually-losing-language", "postedAtFormatted": "Monday, November 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20writer%20describes%20gradually%20losing%20language&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20writer%20describes%20gradually%20losing%20language%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKzLnvv3dob8SGTQAD%2Fa-writer-describes-gradually-losing-language%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20writer%20describes%20gradually%20losing%20language%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKzLnvv3dob8SGTQAD%2Fa-writer-describes-gradually-losing-language", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKzLnvv3dob8SGTQAD%2Fa-writer-describes-gradually-losing-language", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<p><a href=\"http://www.guardian.co.uk/books/2010/nov/07/tom-lubbock-brain-tumour-language\">A writer's memoir of a brain tumor slowly destroying his ability to use language</a><a></a></p>\n<p>&nbsp;</p>\n<blockquote>When I came to read this passage \"&hellip;floating and flailing weightlessly.&hellip;\" I said the word \"weightlessly\" as \"walterkly\". It took quite a bit of effort to be fully sure that this was a mistake; and more effort and repeating to grasp what exactly this nonsense word was, to establish its sound &ndash; I had to construct it phoneme by phoneme &ndash; clearly enough to write it down. And it seems that the reading eye, darting backwards and forwards, was plucking letters from the whole vicinity, and mixing them up, having lost its usual ability to sort them.What the whole thing emphasises, of course, is how what we call self-command is really a matter of having reliable automatic mechanisms, unthinking habits or instincts.</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KzLnvv3dob8SGTQAD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 6.434552869718584e-07, "legacy": true, "legacyId": "3947", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-08T16:28:14.213Z", "modifiedAt": null, "url": null, "title": "Do we have a technological growth problem?", "slug": "do-we-have-a-technological-growth-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:29.678Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qnz6JzfFMQ8SmdPxr/do-we-have-a-technological-growth-problem", "pageUrlRelative": "/posts/Qnz6JzfFMQ8SmdPxr/do-we-have-a-technological-growth-problem", "linkUrl": "https://www.lesswrong.com/posts/Qnz6JzfFMQ8SmdPxr/do-we-have-a-technological-growth-problem", "postedAtFormatted": "Monday, November 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20we%20have%20a%20technological%20growth%20problem%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20we%20have%20a%20technological%20growth%20problem%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQnz6JzfFMQ8SmdPxr%2Fdo-we-have-a-technological-growth-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20we%20have%20a%20technological%20growth%20problem%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQnz6JzfFMQ8SmdPxr%2Fdo-we-have-a-technological-growth-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQnz6JzfFMQ8SmdPxr%2Fdo-we-have-a-technological-growth-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 272, "htmlBody": "<p>Is meaningful technological growth speeding up or slowing down? &nbsp;How would we be able to tell?</p>\n<p>\"Technology is improving faster today than ever\" and \"Technology isn't improving as fast as it used to\" are both plausible statements on the face of it. &nbsp;I'm not sure how we could tell which is true. &nbsp;Looking at rates of new patents doesn't seem like a good measurement, since it could reflect changes in how willing people are to seek patents.</p>\n<p>I'm starting to find more plausible the dismal hypothesis that technological improvement is slowing down, and that therefore human prosperity around the world is endangered. &nbsp;Think about the Industrial Revolution. &nbsp;Think about the automobile, the airplane, the telephone, the lightbulb. &nbsp;Think about the computer and the Web. Now think about the past ten years. &nbsp;What's being done that's radically new? &nbsp;</p>\n<p>Of course this is a vague way of thinking about things, and probably colored by my own gloominess. &nbsp;I'm writing here in the hopes that other people will have sounder methods for looking at this question.</p>\n<p>Between 1800 and the present, per capita world GDP increased by about a factor of 16, after centuries of near-stagnation. (<a href=\"http://commons.wikimedia.org/wiki/File:World_GDP_per_capita_1500_to_2003.png\">Link.</a>) &nbsp;I don't know why with great confidence, but the conventional-wisdom explanation is usually something like technology, perhaps combined with political or financial factors. &nbsp;What does seem certain to me is that anything that threatens that 200-year run of good fortune is incredibly dangerous, and we should look out for such risks, unless they're all-but-impossible. &nbsp;Risks to global economic growth are not quite existential risks, but they're risks to everyone's material well-being. &nbsp;</p>\n<p>Is there anyone here who thinks there exist such risks? &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qnz6JzfFMQ8SmdPxr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "3948", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-08T21:42:32.423Z", "modifiedAt": null, "url": null, "title": "Twinkie diet helps nutrition professor lose 27 pounds", "slug": "twinkie-diet-helps-nutrition-professor-lose-27-pounds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:27.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vive-ut-Vivas", "createdAt": "2009-12-20T05:11:58.232Z", "isAdmin": false, "displayName": "Vive-ut-Vivas"}, "userId": "S5mGH4agpSzrmRQ3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ys8oHYgdGmma5KuJT/twinkie-diet-helps-nutrition-professor-lose-27-pounds", "pageUrlRelative": "/posts/Ys8oHYgdGmma5KuJT/twinkie-diet-helps-nutrition-professor-lose-27-pounds", "linkUrl": "https://www.lesswrong.com/posts/Ys8oHYgdGmma5KuJT/twinkie-diet-helps-nutrition-professor-lose-27-pounds", "postedAtFormatted": "Monday, November 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Twinkie%20diet%20helps%20nutrition%20professor%20lose%2027%20pounds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwinkie%20diet%20helps%20nutrition%20professor%20lose%2027%20pounds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYs8oHYgdGmma5KuJT%2Ftwinkie-diet-helps-nutrition-professor-lose-27-pounds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Twinkie%20diet%20helps%20nutrition%20professor%20lose%2027%20pounds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYs8oHYgdGmma5KuJT%2Ftwinkie-diet-helps-nutrition-professor-lose-27-pounds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYs8oHYgdGmma5KuJT%2Ftwinkie-diet-helps-nutrition-professor-lose-27-pounds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p><a href=\"http://www.cnn.com/2010/HEALTH/11/08/twinkie.diet.professor/index.html\">Twinkie diet helps nutrition professor lose 27 pounds</a>:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>For 10 weeks, Mark Haub, a professor of human nutrition at Kansas  State University, ate one of these sugary cakelets every three hours,  instead of meals. To add variety in his steady stream of Hostess and  Little Debbie snacks, Haub munched on Doritos chips, sugary cereals and  Oreos, too.</p>\n<p>His premise: That in weight loss, pure calorie counting is what matters most -- not the nutritional value of the food.</p>\n<p>The premise held up: On his \"convenience store diet,\" he shed 27 pounds in two months.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>But the highlight, for LW-ers, comes at the end:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>Despite his weight loss, Haub feels ambivalence.</p>\n<p class=\"cnnInline\">\"I  wish I could say the outcomes are unhealthy. I wish I could say it's  healthy. I'm not confident enough in doing that. That frustrates a lot  of people. One side says it's irresponsible. <strong>It is unhealthy, but the  data doesn't say that.</strong>\"<strong><br /></strong></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"92SxJsDZ78ApAGq72": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ys8oHYgdGmma5KuJT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 6.435383731572356e-07, "legacy": true, "legacyId": "3949", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-08T23:30:49.015Z", "modifiedAt": null, "url": null, "title": "Chicago Meetup 11/14", "slug": "chicago-meetup-11-14", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.585Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "steven0461", "createdAt": "2009-02-27T16:16:38.980Z", "isAdmin": false, "displayName": "steven0461"}, "userId": "cn4SiEmqWbu7K9em5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5jmkphsm5bhWnEu9M/chicago-meetup-11-14", "pageUrlRelative": "/posts/5jmkphsm5bhWnEu9M/chicago-meetup-11-14", "linkUrl": "https://www.lesswrong.com/posts/5jmkphsm5bhWnEu9M/chicago-meetup-11-14", "postedAtFormatted": "Monday, November 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Chicago%20Meetup%2011%2F14&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChicago%20Meetup%2011%2F14%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5jmkphsm5bhWnEu9M%2Fchicago-meetup-11-14%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Chicago%20Meetup%2011%2F14%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5jmkphsm5bhWnEu9M%2Fchicago-meetup-11-14", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5jmkphsm5bhWnEu9M%2Fchicago-meetup-11-14", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 79, "htmlBody": "<p><a href=\"http://www.lesswrong.com/user/Airedale\">Airedale</a> and I will host a meetup this Sunday, starting 5 pm, in the <a href=\"http://www.elephantcastle.com/chicago_adams\">Elephant &amp; Castle Pub and Restaurant</a> on 111 West Adams Street. We'll put up a sign saying \"LessWrong\".</p>\n<p>We're open to changing the time or venue, so check back here to be sure, or join our <a href=\"http://groups.google.com/group/less-wrong-chicago\">Google Group</a> for future updates. Having the meetup in the Loop seemed the best compromise, but we haven't tried this particular venue before and maybe someone has a better idea.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5jmkphsm5bhWnEu9M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 6.435643309959984e-07, "legacy": true, "legacyId": "3950", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-08T23:51:17.130Z", "modifiedAt": null, "url": null, "title": "The value of preserving reality", "slug": "the-value-of-preserving-reality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:30.251Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukstafi", "createdAt": "2009-08-30T20:03:02.161Z", "isAdmin": false, "displayName": "lukstafi"}, "userId": "CupwtRR8H8ccNFPtP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CjFcZ43g7QroEAb4B/the-value-of-preserving-reality", "pageUrlRelative": "/posts/CjFcZ43g7QroEAb4B/the-value-of-preserving-reality", "linkUrl": "https://www.lesswrong.com/posts/CjFcZ43g7QroEAb4B/the-value-of-preserving-reality", "postedAtFormatted": "Monday, November 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20value%20of%20preserving%20reality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20value%20of%20preserving%20reality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCjFcZ43g7QroEAb4B%2Fthe-value-of-preserving-reality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20value%20of%20preserving%20reality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCjFcZ43g7QroEAb4B%2Fthe-value-of-preserving-reality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCjFcZ43g7QroEAb4B%2Fthe-value-of-preserving-reality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<p>A comment to&nbsp;<span style=\"font-family: arial, sans-serif; font-size: 13px;\"><a class=\"ot-anchor\" href=\"http://intelligence.org/blog/2010/10/27/presentation-by-joshua-foxcarl-shulman-at-ecap-2010-super-intelligence-does-not-imply-benevolence/\">http://singinst.org/blog/2010/10/27/presentation-by-joshua-foxcarl-shulman-at-ecap-2010-super-intelligence-does-not-imply-benevolence/</a>:</span>&nbsp;Given as in the naive reinforcement learning framework (and that can approximate some more complex notions of value) that the value is in the environment, you don't want to be too hasty with the environment lest you destroy a higher value you haven't yet discovered! So you especially wouldn't replace high complexity systems like humans with low entropy systems like computer chips, without first analyzing them.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CjFcZ43g7QroEAb4B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": -1, "extendedScore": null, "score": 6.435693816552588e-07, "legacy": true, "legacyId": "3951", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-09T04:13:55.592Z", "modifiedAt": null, "url": null, "title": "A hypothetical candidate walks into a hypothetical job interview...", "slug": "a-hypothetical-candidate-walks-into-a-hypothetical-job", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:53.423Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AngryParsley", "createdAt": "2009-03-09T02:58:36.474Z", "isAdmin": false, "displayName": "AngryParsley"}, "userId": "6D5iiccXCrXa96RmW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wbaEnHPKDMCL7cdSz/a-hypothetical-candidate-walks-into-a-hypothetical-job", "pageUrlRelative": "/posts/wbaEnHPKDMCL7cdSz/a-hypothetical-candidate-walks-into-a-hypothetical-job", "linkUrl": "https://www.lesswrong.com/posts/wbaEnHPKDMCL7cdSz/a-hypothetical-candidate-walks-into-a-hypothetical-job", "postedAtFormatted": "Tuesday, November 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20hypothetical%20candidate%20walks%20into%20a%20hypothetical%20job%20interview...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20hypothetical%20candidate%20walks%20into%20a%20hypothetical%20job%20interview...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwbaEnHPKDMCL7cdSz%2Fa-hypothetical-candidate-walks-into-a-hypothetical-job%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20hypothetical%20candidate%20walks%20into%20a%20hypothetical%20job%20interview...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwbaEnHPKDMCL7cdSz%2Fa-hypothetical-candidate-walks-into-a-hypothetical-job", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwbaEnHPKDMCL7cdSz%2Fa-hypothetical-candidate-walks-into-a-hypothetical-job", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>\n<p>Let's say you are interviewing a candidate for a job. In casual conversation, the candidate mentions that he is a member of a rather old and prestigious country club. You've never heard the name of the club before.&nbsp;</p>\n<p>You look up the country club afterwards, and are surprised by what you read. The club refuses membership to homosexuals. It revokes the membership of couples who use birth control. Leadership positions are reserved to unmarried males.</p>\n<p>The candidate is otherwise competent. Under what conditions would you hire him? Would you want a law passed banning hiring discrimination based on country club membership?</p>\n<p>&nbsp;</p>\n<p>(The country club is analogous to a nicer version of the Catholic church. I left out <a href=\"http://en.wikipedia.org/wiki/Catholic_sex_abuse_cases\">a couple</a>&nbsp;<a href=\"http://ocassis.livejournal.com/8200.html\">bad things</a>.)</p>\n<p>&nbsp;</p>\n<p>Religious discrimination is illegal in many parts of the world, and I think that's probably a good thing. Still, keeping this at the object level (no&nbsp;meta-rules or&nbsp;veils of ignorance) it seems to me that discriminating against religious people is fine. I'm curious what other people think.&nbsp;</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wbaEnHPKDMCL7cdSz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 11, "extendedScore": null, "score": 6.436326471596258e-07, "legacy": true, "legacyId": "3952", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-09T05:02:50.842Z", "modifiedAt": null, "url": null, "title": "Recent results on lower bounds in circuit complexity. ", "slug": "recent-results-on-lower-bounds-in-circuit-complexity", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaZ", "createdAt": "2010-04-05T04:07:01.214Z", "isAdmin": false, "displayName": "JoshuaZ"}, "userId": "fmTiLqp6mmXeLjwfN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oddedGoQ6AwhHFx5A/recent-results-on-lower-bounds-in-circuit-complexity", "pageUrlRelative": "/posts/oddedGoQ6AwhHFx5A/recent-results-on-lower-bounds-in-circuit-complexity", "linkUrl": "https://www.lesswrong.com/posts/oddedGoQ6AwhHFx5A/recent-results-on-lower-bounds-in-circuit-complexity", "postedAtFormatted": "Tuesday, November 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recent%20results%20on%20lower%20bounds%20in%20circuit%20complexity.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecent%20results%20on%20lower%20bounds%20in%20circuit%20complexity.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoddedGoQ6AwhHFx5A%2Frecent-results-on-lower-bounds-in-circuit-complexity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recent%20results%20on%20lower%20bounds%20in%20circuit%20complexity.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoddedGoQ6AwhHFx5A%2Frecent-results-on-lower-bounds-in-circuit-complexity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoddedGoQ6AwhHFx5A%2Frecent-results-on-lower-bounds-in-circuit-complexity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<p>There's a new paper which substantially improves lower bounds for circuit complexity. <a href=\"http://www.cs.cmu.edu/~ryanw/acc-lbs.pdf\">The paper, by Ryan Williams, proves that NEXP does not have ACC circuits of third-exponential size. </a></p>\n<p>This is a somewhat technical result (and I haven't read the proof yet), but there's a summary of what this implies at <a href=\"http://scottaaronson.com/blog/?p=472\">Scott Aaronson's blog</a>. The main upshot is that this is a substantial improvement over prior circuit complexity bounds. This is relevant since circuit complexity bounds look to be one of the most promising methods to potentially show that P != NP. These results make circuit complexity bounds be still very far off from showing that. But this result looks like it in some ways might get around the relativization barrier and natural proof barriers which are major barriers to resolving P ?=NP.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oddedGoQ6AwhHFx5A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 6.436444325602965e-07, "legacy": true, "legacyId": "3953", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-09T05:35:05.641Z", "modifiedAt": null, "url": null, "title": "Bye Bye Benton: November Less Wrong Meetup", "slug": "bye-bye-benton-november-less-wrong-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:31.527Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jasen", "createdAt": "2009-06-11T15:05:07.288Z", "isAdmin": false, "displayName": "Jasen"}, "userId": "hMDxPMjrPyw8vGzMa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SGGLsH3zpvcE58pfi/bye-bye-benton-november-less-wrong-meetup", "pageUrlRelative": "/posts/SGGLsH3zpvcE58pfi/bye-bye-benton-november-less-wrong-meetup", "linkUrl": "https://www.lesswrong.com/posts/SGGLsH3zpvcE58pfi/bye-bye-benton-november-less-wrong-meetup", "postedAtFormatted": "Tuesday, November 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bye%20Bye%20Benton%3A%20November%20Less%20Wrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABye%20Bye%20Benton%3A%20November%20Less%20Wrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSGGLsH3zpvcE58pfi%2Fbye-bye-benton-november-less-wrong-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bye%20Bye%20Benton%3A%20November%20Less%20Wrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSGGLsH3zpvcE58pfi%2Fbye-bye-benton-november-less-wrong-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSGGLsH3zpvcE58pfi%2Fbye-bye-benton-november-less-wrong-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">As some of you may know, SIAI is in the process of moving our Visiting Fellows Program to a larger and more permanent location in Berkeley. &nbsp;Nothing is final yet but, however things turn out, November will be the last month we spend at 3755 Benton street. &nbsp;In honor of the house's proud&nbsp;</span><span style=\"line-height: 19px;\">history, we'll be throwing one final Less Wrong meetup this Saturday, the 13th of November, starting at 6pm. &nbsp;Come meet the SingInst staff, the visiting fellows and your fellow Less Wrong readers for one final party in Santa Clara!</span></span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">As usual, food and drink shall be provided.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">Please&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://www.meetup.com/Bay-Area-Less-Wrong-Meetup/calendar/15388772/?from=list&amp;offset=0\">RSVP at the meetup.com page</a>&nbsp;if you plan to attend.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SGGLsH3zpvcE58pfi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 6.436522012227267e-07, "legacy": true, "legacyId": "3955", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-09T09:08:13.898Z", "modifiedAt": null, "url": null, "title": "This Is How Bill and Hillary Discuss Dinner Options ", "slug": "this-is-how-bill-and-hillary-discuss-dinner-options", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:30.367Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2hXpTWs6p88SPeYB9/this-is-how-bill-and-hillary-discuss-dinner-options", "pageUrlRelative": "/posts/2hXpTWs6p88SPeYB9/this-is-how-bill-and-hillary-discuss-dinner-options", "linkUrl": "https://www.lesswrong.com/posts/2hXpTWs6p88SPeYB9/this-is-how-bill-and-hillary-discuss-dinner-options", "postedAtFormatted": "Tuesday, November 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20This%20Is%20How%20Bill%20and%20Hillary%20Discuss%20Dinner%20Options%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThis%20Is%20How%20Bill%20and%20Hillary%20Discuss%20Dinner%20Options%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2hXpTWs6p88SPeYB9%2Fthis-is-how-bill-and-hillary-discuss-dinner-options%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=This%20Is%20How%20Bill%20and%20Hillary%20Discuss%20Dinner%20Options%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2hXpTWs6p88SPeYB9%2Fthis-is-how-bill-and-hillary-discuss-dinner-options", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2hXpTWs6p88SPeYB9%2Fthis-is-how-bill-and-hillary-discuss-dinner-options", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://nymag.com/daily/intel/2010/11/this_is_how_bill_and_hillary_d.html?imw=Y&amp;f=most-viewed-24h10\">http://nymag.com/daily/intel/2010/11/this_is_how_bill_and_hillary_d.html?imw=Y&amp;f=most-viewed-24h10</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2hXpTWs6p88SPeYB9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -6, "extendedScore": null, "score": 6.437035533010636e-07, "legacy": true, "legacyId": "3958", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-09T12:03:09.418Z", "modifiedAt": null, "url": null, "title": "Making the Universe Last Forever By Throwing Away Entropy Into Basement Universes?", "slug": "making-the-universe-last-forever-by-throwing-away-entropy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:31.585Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rebpQCJy4ZfajTfHu/making-the-universe-last-forever-by-throwing-away-entropy", "pageUrlRelative": "/posts/rebpQCJy4ZfajTfHu/making-the-universe-last-forever-by-throwing-away-entropy", "linkUrl": "https://www.lesswrong.com/posts/rebpQCJy4ZfajTfHu/making-the-universe-last-forever-by-throwing-away-entropy", "postedAtFormatted": "Tuesday, November 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Making%20the%20Universe%20Last%20Forever%20By%20Throwing%20Away%20Entropy%20Into%20Basement%20Universes%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMaking%20the%20Universe%20Last%20Forever%20By%20Throwing%20Away%20Entropy%20Into%20Basement%20Universes%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrebpQCJy4ZfajTfHu%2Fmaking-the-universe-last-forever-by-throwing-away-entropy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Making%20the%20Universe%20Last%20Forever%20By%20Throwing%20Away%20Entropy%20Into%20Basement%20Universes%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrebpQCJy4ZfajTfHu%2Fmaking-the-universe-last-forever-by-throwing-away-entropy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrebpQCJy4ZfajTfHu%2Fmaking-the-universe-last-forever-by-throwing-away-entropy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 8, "htmlBody": "<p><span style=\"font-family: 'Trebuchet MS', Georgia, 'Times New Roman', serif; font-size: 12px; color: #333333;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 1.2em; font-size: 1.1em; padding: 0px;\"><a href=\"http://www.acceleratingfuture.com/michael/blog/2010/11/making-the-universe-last-forever-by-throwing-away-entropy-into-basement-universes/\">Is there any reason why it wouldn&rsquo;t work?</a></p>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rebpQCJy4ZfajTfHu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -2, "extendedScore": null, "score": 6.43745704161844e-07, "legacy": true, "legacyId": "3959", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-09T13:53:32.934Z", "modifiedAt": null, "url": null, "title": "Information Hazards", "slug": "information-hazards", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.856Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "apophenia", "createdAt": "2010-04-13T14:09:52.433Z", "isAdmin": false, "displayName": "apophenia"}, "userId": "2rgiaLhZS8w2Fekt9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RKRYGcdQFHQ78fe3x/information-hazards", "pageUrlRelative": "/posts/RKRYGcdQFHQ78fe3x/information-hazards", "linkUrl": "https://www.lesswrong.com/posts/RKRYGcdQFHQ78fe3x/information-hazards", "postedAtFormatted": "Tuesday, November 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Information%20Hazards&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInformation%20Hazards%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRKRYGcdQFHQ78fe3x%2Finformation-hazards%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Information%20Hazards%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRKRYGcdQFHQ78fe3x%2Finformation-hazards", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRKRYGcdQFHQ78fe3x%2Finformation-hazards", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 30, "htmlBody": "<p>Nick Bostrom recently posted the article \"Information Hazards\", which is about the myriad of ways in which information can harm us.</p>\n<p>You can read it at his website: <a href=\"http://www.nickbostrom.com/information-hazards.pdf\">Direct PDF Link</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RKRYGcdQFHQ78fe3x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 1, "extendedScore": null, "score": 6.437723072530007e-07, "legacy": true, "legacyId": "3960", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-09T16:25:21.186Z", "modifiedAt": null, "url": null, "title": "A note on the description complexity of physical theories", "slug": "a-note-on-the-description-complexity-of-physical-theories", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:03.199Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h9sP6rLT6r4vaXoen/a-note-on-the-description-complexity-of-physical-theories", "pageUrlRelative": "/posts/h9sP6rLT6r4vaXoen/a-note-on-the-description-complexity-of-physical-theories", "linkUrl": "https://www.lesswrong.com/posts/h9sP6rLT6r4vaXoen/a-note-on-the-description-complexity-of-physical-theories", "postedAtFormatted": "Tuesday, November 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20note%20on%20the%20description%20complexity%20of%20physical%20theories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20note%20on%20the%20description%20complexity%20of%20physical%20theories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9sP6rLT6r4vaXoen%2Fa-note-on-the-description-complexity-of-physical-theories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20note%20on%20the%20description%20complexity%20of%20physical%20theories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9sP6rLT6r4vaXoen%2Fa-note-on-the-description-complexity-of-physical-theories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9sP6rLT6r4vaXoen%2Fa-note-on-the-description-complexity-of-physical-theories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 604, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/lw/2n4/the_prior_of_a_hypothesis_does_not_depend_on_its/\">The prior of a hypothesis does not depend on its complexity</a></p>\n<p>Eliezer <a href=\"/lw/qa/the_dilemma_science_or_bayes/\">wrote</a>:</p>\n<blockquote>\n<p>In physics, you can get absolutely clear-cut issues.&nbsp; Not in the sense that the issues are trivial to explain. [...]&nbsp;But when I say \"macroscopic decoherence is simpler than collapse\" it is actually&nbsp;<em style=\"font-style: italic;\">strict</em>&nbsp;simplicity; you could write the two hypotheses out as computer programs and count the lines of code.</p>\n</blockquote>\n<p>Every once in a while I come across some belief in my mind that clearly originated from someone smart, like Eliezer, and stayed unexamined because after you hear and check 100 correct statements from someone, you're not about to check the 101st quite as thoroughly. The above quote is one of those beliefs. In this post I'll try to look at it closer and see what it really means.</p>\n<p>Imagine you have a physical theory, expressed as a computer program that generates predictions. A natural way to define the <a href=\"http://en.wikipedia.org/wiki/Kolmogorov_complexity\">Kolmogorov complexity</a> of that theory is to find the length of the shortest computer program that <em>generates</em> your program, as a string of bits. Under this very natural definition, the <a href=\"http://en.wikipedia.org/wiki/Many-worlds_interpretation\">many-worlds interpretation</a> of quantum mechanics is almost certainly simpler than the <a href=\"http://en.wikipedia.org/wiki/Copenhagen_interpretation\">Copenhagen interpretation</a>.<a id=\"more\"></a></p>\n<p>But imagine you <a href=\"http://en.wikipedia.org/wiki/Code_refactoring\">refactor</a> your prediction-generating program and make it shorter; does this mean the physical theory has become simpler? Note that after some innocuous refactorings of a program expressing some physical theory in a recognizable form, you may end up with a program that expresses a <em>different</em> set of physical concepts. For example, if you take a program that calculates classical mechanics in the <a href=\"http://en.wikipedia.org/wiki/Lagrangian_mechanics\">Lagrangian</a> formalism, and apply multiple behavior-preserving changes, you may end up with a program whose internal structures look distinctly <a href=\"http://en.wikipedia.org/wiki/Hamiltonian_mechanics\">Hamiltonian</a>.</p>\n<p>Therein lies the rub. Do we really want a definition of \"complexity of physical theories\" that tells apart theories making the same predictions?&nbsp;If our formalism says Hamiltonian mechanics has a higher prior probability than Lagrangian mechanics, which is demonstrably mathematically equivalent to it, something's gone horribly wrong somewhere.&nbsp;And do we even want to define \"complexity\" for physical theories that don't make any predictions at all, like \"glarble flargle\" or \"there's a cake just outside the universe\"?</p>\n<p>At this point, the required fix to our original definition should be obvious: cut out the middleman! Instead of finding the shortest algorithm that writes your <em>algorithm</em> for you, find the shortest algorithm that outputs the same <em>predictions</em>. This new definition has many desirable properties: it's invariant to refactorings, doesn't discriminate between equivalent formulations of classical mechanics, and refuses to specify a prior for something you can never ever test by observation. Clearly we're on the right track here, and the original definition was just an easy fixable mistake.</p>\n<p>But this easy fixable mistake... was the <a href=\"/lw/qa/the_dilemma_science_or_bayes/\">entire reason</a> for Eliezer \"choosing Bayes over Science\" and urging us to do same. The many-worlds interpretation makes the same testable predictions as the Copenhagen interpretation right now. Therefore by the amended definition of \"complexity\", by the <em>right and proper</em> definition, they are equally complex. The truth of the matter is not that they express different hypotheses with equal prior probability - it's that they express the <em>same</em> hypothesis. I'll be the first to agree that there are very good reasons to prefer the MWI formulation, like its pedagogical simplicity and beauty, but K-complexity is not one of them. And there may even be good reasons to pledge your allegiance to Bayes over the scientific method, but this is not one of them either.</p>\n<p><strong>ETA:</strong>&nbsp;now I see that, while the post is kinda technically correct, it's horribly confused on some levels. See the comments by Daniel_Burfoot and JGWeissman. I'll write an explanation in the discussion area.</p>\n<p><strong>ETA 2:</strong> done, <a href=\"/r/discussion/lw/328/description_complexity_a_note_on_terminology/\">look here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h9sP6rLT6r4vaXoen", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 28, "extendedScore": null, "score": 6.438088933599995e-07, "legacy": true, "legacyId": "3962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 184, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nwA2mP55oCSpZ9sza", "viPPjojmChxLGPE2v", "pKzShr6mGcReQwMHR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-09T19:11:48.281Z", "modifiedAt": null, "url": null, "title": "Simple freindliness: plan B for AI", "slug": "simple-freindliness-plan-b-for-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:30.994Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "turchin", "createdAt": "2010-02-03T20:22:54.095Z", "isAdmin": false, "displayName": "turchin"}, "userId": "2kDfHyTEpYCoa2SRq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tQ9iKGPxcNppgWuXD/simple-freindliness-plan-b-for-ai", "pageUrlRelative": "/posts/tQ9iKGPxcNppgWuXD/simple-freindliness-plan-b-for-ai", "linkUrl": "https://www.lesswrong.com/posts/tQ9iKGPxcNppgWuXD/simple-freindliness-plan-b-for-ai", "postedAtFormatted": "Tuesday, November 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simple%20freindliness%3A%20plan%20B%20for%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimple%20freindliness%3A%20plan%20B%20for%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtQ9iKGPxcNppgWuXD%2Fsimple-freindliness-plan-b-for-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simple%20freindliness%3A%20plan%20B%20for%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtQ9iKGPxcNppgWuXD%2Fsimple-freindliness-plan-b-for-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtQ9iKGPxcNppgWuXD%2Fsimple-freindliness-plan-b-for-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 421, "htmlBody": "<p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">Simple freindliness</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">Friendly AI, as believes by Hanson, is doomed to failure, since if the friendliness system is too complicated, the other AI projects generally will not apply it. In addition, any system of friendliness may still be doomed to failure - and more unclear it is, the more chances it has to fail. &nbsp;By fail I mean that it will not ne accepted by most succseful AI project.</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">Thus, the friendliness system should be simple and clear, so it can be spread as widely as possible.</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">I roughly figured, what principles could form the basis of a simple friendliness:</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">0) Any one should understood that AI can be global risks and the friendliness of the system is needed. This basic understanding should be shared by maximum number of AI-groups (I think this is already done)</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">1) Architecture of AI should be such that it would use rules explicitly. (I.e. no genetic algorithms or neural networks)</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">2) the AI should obey commands of its creator, and clearly understand who is the creator and what is the format of commands.</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">3) AI must comply with all existing CRIMINAL an CIVIL laws. These laws are the first attempt to create a friendly AI &ndash; in the form of state. That is an attempt to describe good, safe human life using a system of rules. (Or system of precedents). And the number of volumes of laws and their interpretation speaks about complexity of this problem - but it has already been solved and it is not a sin to use the solution.</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">4) the AI should not have secrets from their creator. Moreover, he is obliged to inform him of all his thoughts. This avoids rebel of AI.</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">5) Each seldoptimizing of AI should be dosed in portions, under the control of the creator. And after each step mustbe run a full scan of system goals and effectivness.</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">6) the AI should be tested in a virtual environment (such as Secnod Life) for safety and adequacy.</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">7) AI projects should be registrated by centralized oversight bodies and receive safety certification from it.</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">Such obvious steps do not create absolutely safe AI (you can figure out how to bypass it out), but they make it much safer. In addition, they look quite natural and reasonable so they could be use by any AI project with different variations.</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\"><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; color: black; mso-ansi-language: EN-US;\" lang=\"EN-US\"><span style=\"mso-spacerun: yes;\">&nbsp;</span>Most of this steps are fallable. But without them the situation would be even worse. If each steps increase safety two times, 8 steps will increase it 256 times, which is good.</span><span style=\"font-size: 14.0pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;; mso-fareast-font-family: SimSun; color: black; mso-ansi-language: EN-US; mso-fareast-language: RU; mso-bidi-language: AR-SA;\" lang=\"EN-US\"> Simple friendliness is plan B if mathematical FAI fails.&nbsp;</span></p>\r\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tQ9iKGPxcNppgWuXD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -26, "extendedScore": null, "score": 6.438487777876766e-07, "legacy": true, "legacyId": "3963", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-09T19:15:47.251Z", "modifiedAt": null, "url": null, "title": "PhilPapers survey results now include correlations", "slug": "philpapers-survey-results-now-include-correlations", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:30.157Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "steven0461", "createdAt": "2009-02-27T16:16:38.980Z", "isAdmin": false, "displayName": "steven0461"}, "userId": "cn4SiEmqWbu7K9em5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kJzwgqzEb8u28eSjL/philpapers-survey-results-now-include-correlations", "pageUrlRelative": "/posts/kJzwgqzEb8u28eSjL/philpapers-survey-results-now-include-correlations", "linkUrl": "https://www.lesswrong.com/posts/kJzwgqzEb8u28eSjL/philpapers-survey-results-now-include-correlations", "postedAtFormatted": "Tuesday, November 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20PhilPapers%20survey%20results%20now%20include%20correlations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhilPapers%20survey%20results%20now%20include%20correlations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkJzwgqzEb8u28eSjL%2Fphilpapers-survey-results-now-include-correlations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=PhilPapers%20survey%20results%20now%20include%20correlations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkJzwgqzEb8u28eSjL%2Fphilpapers-survey-results-now-include-correlations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkJzwgqzEb8u28eSjL%2Fphilpapers-survey-results-now-include-correlations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>Now you can see how philosophical positions are correlated to each other and to some demographic variables:</p>\n<p><a href=\"http://philpapers.org/surveys/linear_most.pl\">http://philpapers.org/surveys/linear_most.pl</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kJzwgqzEb8u28eSjL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 6.438499738134689e-07, "legacy": true, "legacyId": "3964", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-09T21:16:42.808Z", "modifiedAt": null, "url": null, "title": "Cryoburn - Imperial Auditor Vorkosigan investigates the cryonics industry", "slug": "cryoburn-imperial-auditor-vorkosigan-investigates-the", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.420Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Perplexed", "createdAt": "2010-07-22T02:17:37.444Z", "isAdmin": false, "displayName": "Perplexed"}, "userId": "jj9aBsS9xsGPWKq3n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QdPvQEAMKJwJCCxQF/cryoburn-imperial-auditor-vorkosigan-investigates-the", "pageUrlRelative": "/posts/QdPvQEAMKJwJCCxQF/cryoburn-imperial-auditor-vorkosigan-investigates-the", "linkUrl": "https://www.lesswrong.com/posts/QdPvQEAMKJwJCCxQF/cryoburn-imperial-auditor-vorkosigan-investigates-the", "postedAtFormatted": "Tuesday, November 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryoburn%20-%20Imperial%20Auditor%20Vorkosigan%20investigates%20the%20cryonics%20industry&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryoburn%20-%20Imperial%20Auditor%20Vorkosigan%20investigates%20the%20cryonics%20industry%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQdPvQEAMKJwJCCxQF%2Fcryoburn-imperial-auditor-vorkosigan-investigates-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryoburn%20-%20Imperial%20Auditor%20Vorkosigan%20investigates%20the%20cryonics%20industry%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQdPvQEAMKJwJCCxQF%2Fcryoburn-imperial-auditor-vorkosigan-investigates-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQdPvQEAMKJwJCCxQF%2Fcryoburn-imperial-auditor-vorkosigan-investigates-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 450, "htmlBody": "<p>I just completed \"Cryoburn\", Lois McMaster Bujold's latest novel in the Vokosigan Saga series.&nbsp; The subject is cryonics.&nbsp;</p>\n<p>Our hero Miles is dispatched to the planet Kibou-daini to investigate the cryonics industry there.&nbsp; A Kibou company is planning to expand its business to Komarr, and Miles's Komar-born empress is supicious of financial chicanery.&nbsp; Miles himself has undergone freezing (military medical emergency) and more-or-less-successful revival, but the notion of geriatric cryonics is a topic not previously explored in Bujold's universe.&nbsp; It is explored reasonably well here.</p>\n<p>Although some of the cryonics companies in this book are corrupt, Bujold's take on cryonics is mostly positive.&nbsp; She is very much in sympathy with the human desire for immortality or at least serious life extension.&nbsp; But she points out and explores some practical problems.&nbsp; Revival is not a problem in this book.&nbsp; That one is pretty much solved.&nbsp; But cryonics is not a cure for old age, it is at best a way of allowing more time for a cure to be found.&nbsp; Naq pbairavragyl, gur Qheban tebhc frrzf gb unir pbzr hc jvgu n erwhirangvba gerngzrag (jryy, abg rknpgyl erwhirangvba - zber yvxr er-zvqqyr-ntr-vsvpngvba).&nbsp; Ohg gur gerngzrag vf abg lrg grfgrq, naq ZIX Ragrecevfrf vf pbafvqrevat gur chepunfr bs n jnerubhfr shyy bs sebmra cnhcref nf n cbgragvny fbhepr bs grfg fhowrpgf.</p>\n<p>Another problem explored is political.&nbsp; On Kibou, people are generally frozen <em>before</em> death, so as to maximize the chances of a successful revival.&nbsp; So, since they are not really dead, they still can own property (in trust) and they still have the right to vote (by proxy).&nbsp; The cryonics companies hold those proxies and control those trusts and hence have political and economic control of the planet.&nbsp; And of course, they don't particularly want to revive their clients and give up that control.</p>\n<p>And then there are technological problems.&nbsp; Vg gheaf bhg gung bar oenaq bs pelbavp syhvq (oybbq-fhofgvghgr/nagvserrmr) juvpu jnf jvqryl hfrq n srj qrpnqrf ntb unf fbzr fgnovyvgl ceboyrzf.&nbsp; Juvpu zrnaf gung n pbhcyr zvyyvba bs gur cynarg'f uhaqerqf bs zvyyvbaf bs sebmra pbecfrf ner arire tbvat gb jnxr hc.&nbsp; Ubj qbrf gur pbzcnal vaibyirq qrny jvgu gur ceboyrz?&nbsp; Uvag: ubj qvq Nzrevpna onaxf qrny jvgu gur zvyyvbaf bs zbegtntrf gurl uryq gung jbhyq arire or cnlrq bss?</p>\n<p>All in all, it is a reasonably fun read, but not one of Bujold's best.&nbsp; I review it here only because of the local interest in cryonics.&nbsp; I don't read a lot of SciFi other than Bujold's so I can't compare to how the subject has been treated elsewhere (well, I guess I could compare to Niven, but that was a whole different generation of SciFi.)</p>\n<p>There is <a href=\"/lw/2xx/cryoburn_by_bujold/\" target=\"_blank\">another review</a> and some more comments in another posting</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QdPvQEAMKJwJCCxQF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 6.438791239105331e-07, "legacy": true, "legacyId": "3965", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yqpjd7mQ8mqr7Cyvn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-09T21:28:12.953Z", "modifiedAt": null, "url": null, "title": "Simple friendliness: Plan B for AI", "slug": "simple-friendliness-plan-b-for-ai-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:30.394Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "turchin", "createdAt": "2010-02-03T20:22:54.095Z", "isAdmin": false, "displayName": "turchin"}, "userId": "2kDfHyTEpYCoa2SRq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Boy66DaYXRPrRvoYg/simple-friendliness-plan-b-for-ai-0", "pageUrlRelative": "/posts/Boy66DaYXRPrRvoYg/simple-friendliness-plan-b-for-ai-0", "linkUrl": "https://www.lesswrong.com/posts/Boy66DaYXRPrRvoYg/simple-friendliness-plan-b-for-ai-0", "postedAtFormatted": "Tuesday, November 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simple%20friendliness%3A%20Plan%20B%20for%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimple%20friendliness%3A%20Plan%20B%20for%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBoy66DaYXRPrRvoYg%2Fsimple-friendliness-plan-b-for-ai-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simple%20friendliness%3A%20Plan%20B%20for%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBoy66DaYXRPrRvoYg%2Fsimple-friendliness-plan-b-for-ai-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBoy66DaYXRPrRvoYg%2Fsimple-friendliness-plan-b-for-ai-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 425, "htmlBody": "<p>Friendly AI, as believes by Hanson, is doomed to failure, since if the friendliness system is too complicated, the other AI projects generally will not apply it. In addition, any system of friendliness may still be doomed to failure - and more unclear it is, the more chances it has to fail. By fail I mean that it will not be accepted by most successful AI project. Thus, the friendliness system should be simple and clear, so it can be spread as widely as possible. I roughly figured, what principles could form the basis of a simple friendliness:</p>\n<p>1) Any one should understood that AI can be global risks and the friendliness of the system is needed. This basic understanding should be shared by maximum number of AI-groups (I think this is alrready done)</p>\n<p>2) Architecture of AI should be such that it would use rules explicitly. (I.e. no genetic algorithms or neural networks)</p>\n<p>3) the AI should obey commands of its creator, and clearly understand who is the creator and what is the format of commands.</p>\n<p>4) AI must comply with all existing criminal an civil laws. These laws are the first attempt to create a friendly AI &ndash; in the form of state. That is an attempt to describe good, safe human life using a system of rules. (Or system of precedents). And the number of volumes of laws and their interpretation speaks about complexity of this problem - but it has already been solved and it is not a sin to use the solution.</p>\n<p>5) the AI should not have secrets from their creator. Moreover, he is obliged to inform him of all his thoughts. This avoids rebel of AI.</p>\n<p>6) Each self optimizing of AI should be dosed in portions, under the control of the creator. And after each step must be run a full scan of system goals and effectiveness.</p>\n<p>7) the AI should be tested in a virtual environment (such as Second Life) for safety and adequacy.</p>\n<p>8) AI projects should be registrated by centralized oversight bodies and receive safety certification from it.</p>\n<p>Such obvious steps do not create absolutely safe AI (you can figure out how to bypass it out), but they make it much safer. In addition, they look quite natural and reasonable so they could be use by any AI project with different variations. Most of this steps are fallable. But without them the situation would be even worse. If each steps increase safety two times, 8 steps will increase it 256 times, which is good. Simple friendliness is plan B if mathematical FAI fails.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Boy66DaYXRPrRvoYg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -21, "extendedScore": null, "score": 6.438817763793539e-07, "legacy": true, "legacyId": "3967", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-10T00:43:40.009Z", "modifiedAt": "2021-01-27T15:19:22.167Z", "url": null, "title": "Description complexity: an apology and note on terminology", "slug": "description-complexity-an-apology-and-note-on-terminology", "viewCount": null, "lastCommentedAt": "2021-01-27T15:18:25.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pKzShr6mGcReQwMHR/description-complexity-an-apology-and-note-on-terminology", "pageUrlRelative": "/posts/pKzShr6mGcReQwMHR/description-complexity-an-apology-and-note-on-terminology", "linkUrl": "https://www.lesswrong.com/posts/pKzShr6mGcReQwMHR/description-complexity-an-apology-and-note-on-terminology", "postedAtFormatted": "Wednesday, November 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Description%20complexity%3A%20an%20apology%20and%20note%20on%20terminology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADescription%20complexity%3A%20an%20apology%20and%20note%20on%20terminology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpKzShr6mGcReQwMHR%2Fdescription-complexity-an-apology-and-note-on-terminology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Description%20complexity%3A%20an%20apology%20and%20note%20on%20terminology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpKzShr6mGcReQwMHR%2Fdescription-complexity-an-apology-and-note-on-terminology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpKzShr6mGcReQwMHR%2Fdescription-complexity-an-apology-and-note-on-terminology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 500, "htmlBody": "<p>This post is an amendment and sort of apology for my LW posts dealing with complexity (<a href=\"/lw/2n4/the_prior_of_a_hypothesis_does_not_depend_on_its/\">1</a>, <a href=\"/lw/322/a_note_on_the_description_complexity_of_physical/\">2</a>). In these posts I got the terms all wrong. Now I'll try to set it right. Many thanks to taw, JGWeissman and Daniel_Burfoot.</p>\n<p>The <a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\">Solomonoff prior</a> works like this: every bit string is assigned a real number that's equal to the probability of a \"random program\" outputting that string and then halting, when the \"probability\" of choosing each program is assumed to be inverse-exponential in its length. (Alternatively, you talk about a \"universal prefix Turing machine\" that consumes fair coin flips on the input tape.)</p>\n<p>In this setup, the words \"event\", \"hypothesis\", \"probability\", \"complexity\" etc. are all ambiguous. For example, the word \"hypothesis\" can mean a) an individual program, b) an equivalence class of programs that output the same bit string, or c) a statement about output bit strings, like \"the third bit will be 0\". The word \"event\" has exactly the same problem.</p>\n<p>Now the trouble is, you can give reasonable-sounding definitions of \"probability\" and \"Kolmogorov complexity\" to objects of all three types: (a), (b), and (c). But it's not at all clear what real-world implications these values should have. Does it make sense to talk about prior probability for objects of type (a), given that we can never distinguish two (a)'s that are part of the same (b)? (This is the mistake in saying that MWI has a higher prior probability than collapse interpretations.) Is it useful to talk about K-complexity for objects of type (c), given that a very long statement of type (c) can still have probability close to 1? (This is the mistake in saying Islam must be improbable because the Qur'an is such a thick book.) For that matter, is it at all useful to talk about K-complexity for objects of type (b) which are after all just bit strings, or should we restrict ourselves to talking about their prior probabilities for some reason?</p>\n<p>At the moment it seems uncontroversial that we can talk about K-complexity for objects of type (a) - let's call them \"programs\" - and talk about prior probability for objects of types (b) and (c) - let's call them \"sequences of observations\" and \"statements about the world\", respectively. Curiously, this means there's no object for which we have defined <em>both</em> \"complexity\" and \"prior probability\", which makes <em>all</em> arguments of the form \"complexity is high =&gt; probability is low\" automatically suspect.</p>\n<p>There's another worrying point. Given that \"programs\" get glued into equivalence classes - \"sequences of observations\" - anyway, it seems the K-complexity of an individual program is a completely inconsequential variable. You're better off just talking about the length. And the other two kinds of objects don't seem to have useful notions of K-complexity (according to my esteemed commentators who I thanked above), so we can adopt the general rule that mentioning K-complexity in a discussion of physics is always a sign of confusion :-)</p>\n<p>What do you say? Is this better? I'm honestly trying to get better!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pKzShr6mGcReQwMHR", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 24, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "3968", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nwA2mP55oCSpZ9sza", "h9sP6rLT6r4vaXoen"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-11-10T00:43:40.009Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-10T09:27:00.447Z", "modifiedAt": null, "url": null, "title": "On the Human", "slug": "on-the-human", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:31.023Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "machrider", "createdAt": "2010-10-12T23:37:29.342Z", "isAdmin": false, "displayName": "machrider"}, "userId": "jFSugHAcHBNKua4k4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pzqAjeeD8BfQQcc9S/on-the-human", "pageUrlRelative": "/posts/pzqAjeeD8BfQQcc9S/on-the-human", "linkUrl": "https://www.lesswrong.com/posts/pzqAjeeD8BfQQcc9S/on-the-human", "postedAtFormatted": "Wednesday, November 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20the%20Human&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20the%20Human%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpzqAjeeD8BfQQcc9S%2Fon-the-human%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20the%20Human%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpzqAjeeD8BfQQcc9S%2Fon-the-human", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpzqAjeeD8BfQQcc9S%2Fon-the-human", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<p>Just wanted to point you guys at <a href=\"http://onthehuman.org/\" target=\"_self\">On the Human</a>, a site which focuses on understanding the science and philosophy of humanism.&nbsp; There is often overlap between topics there and here at Less Wrong.&nbsp; The Forum is where most of the articles are posted (basically in blog format).</p>\n<p>Apologies if everyone was already aware of them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pzqAjeeD8BfQQcc9S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 6.440552164105765e-07, "legacy": true, "legacyId": "3970", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-10T10:10:02.647Z", "modifiedAt": null, "url": null, "title": "Public international law ", "slug": "public-international-law", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:30.831Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KEXnvFepoY4pPxkNF/public-international-law", "pageUrlRelative": "/posts/KEXnvFepoY4pPxkNF/public-international-law", "linkUrl": "https://www.lesswrong.com/posts/KEXnvFepoY4pPxkNF/public-international-law", "postedAtFormatted": "Wednesday, November 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Public%20international%20law%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APublic%20international%20law%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKEXnvFepoY4pPxkNF%2Fpublic-international-law%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Public%20international%20law%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKEXnvFepoY4pPxkNF%2Fpublic-international-law", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKEXnvFepoY4pPxkNF%2Fpublic-international-law", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Public_international_law\">http://en.wikipedia.org/wiki/Public_international_law</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KEXnvFepoY4pPxkNF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -16, "extendedScore": null, "score": 6.440655963067465e-07, "legacy": true, "legacyId": "3971", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-10T15:43:23.326Z", "modifiedAt": null, "url": null, "title": "What does it mean to optimize future?", "slug": "what-does-it-mean-to-optimize-future", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:30.793Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "red75", "createdAt": "2010-06-05T23:00:12.634Z", "isAdmin": false, "displayName": "red75"}, "userId": "fjoEympfTapQabRHT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MvGupsTJ2FDJPKmiB/what-does-it-mean-to-optimize-future", "pageUrlRelative": "/posts/MvGupsTJ2FDJPKmiB/what-does-it-mean-to-optimize-future", "linkUrl": "https://www.lesswrong.com/posts/MvGupsTJ2FDJPKmiB/what-does-it-mean-to-optimize-future", "postedAtFormatted": "Wednesday, November 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20does%20it%20mean%20to%20optimize%20future%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20does%20it%20mean%20to%20optimize%20future%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMvGupsTJ2FDJPKmiB%2Fwhat-does-it-mean-to-optimize-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20does%20it%20mean%20to%20optimize%20future%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMvGupsTJ2FDJPKmiB%2Fwhat-does-it-mean-to-optimize-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMvGupsTJ2FDJPKmiB%2Fwhat-does-it-mean-to-optimize-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 736, "htmlBody": "<h4>Preamble</h4>\n<p><a href=\"/lw/2zj/value_deathism/\">Value deathism</a>&nbsp;by Vladimir Nesov encourages us to fix our values to prevent astronomical waste due to under-optimized future.</p>\n<p>When I've read it&nbsp;I found that I think about units of measurement of mentioned astronomical waste. Utilons?&nbsp;Seems so.&nbsp;<strong>[edit] </strong>Jack suggested widely accepted word Utils instead.<strong>[/edit]</strong></p>\n<p>I've tried to precisely define it. It is difference between utility of some world-state G measured by original (drifting) agent and utility of world-state G measured by undrifting version of original agent, where world-state G is optimal according to original (drifting) agent.</p>\n<p>There are two questions: can we compare utilities of those agents and what does it mean that G is optimal?</p>\n<h4>Question</h4>\n<p>Preconditions: world is deterministic, the agent has full knowledge of the world, i.e. it knows current world-state, full list of actions available for every world-state and&nbsp;consequence&nbsp;of each action (world-state it leads to), the agent has no time limit for computing next action.</p>\n<p>Agent's value is defined as a function from set of world-states to real numbers, for the sake of, uhm, clarity, the bigger the better. (Note: it is unnecessary to define value as a function from set of sequences of world-states, as history of world can be deduced from world-state itself, and if it can't be deduced, then the agent can't use history anyway, as the agent is a part of this world-state, so it doesn't \"remember\" history too). <strong>[edit] </strong>I wasn't aware that this note includes hidden assumption: value of world-state must be constant. But this assumption doesn't allow agent to single out world-state where agent loses all or part of its memory. Thus value as a function over sequences of world-states has a right to be. But this value function still needs to be specifically shaped to be optimization algorithm independent.&nbsp;<strong>[/edit]</strong></p>\n<p>Which sequence of world-states is optimal according to agent's value?</p>\n<p>&nbsp;</p>\n<p><strong>Edit:</strong> Consider agents implementing greedy search algorithm and exhaustive search algorithm. For them to choose same sequence of world-states search space should be greedoid. And that requires very specific structure of value function.</p>\n<p><strong>Edit2:</strong> Alternatively value function can be indirectly self-referential via part of world-state that contains the agent, thus allowing it to modify agent's optimization algorithm by assigning higher utility to world-states where agent implements desired optimization algorithm. (I call agent's function 'value function' because its meaning can be defined by the function itself, it isn't necessarily utility).</p>\n<p>&nbsp;</p>\n<p>My answer:</p>\n<p>Jura inyhr shapgvba bs gur ntrag vfa'g ersyrpgvir, v.r. qbrfa'g qrcraq ba vagrecergngvba bs n cneg bs jbeyq-fgngr bpphcvrq ol ntrag va grezf bs bcgvzvmngvba cebprff vzcyrzragrq ol guvf cneg bs jbeyq-fgngr, gura bcgvzny frdhrapr qrcraqf ba pbzovangvba bs qrgnvyf bs vzcyrzragngvba bs ntrag'f bcgvzvmngvba nytbevguz naq inyhr shapgvba. V guvax va trareny vg jvyy rkuvovg SBBZ orunivbe.</p>\n<p>Ohg jura inyhr shapgvba vf ersyrpgvir gura guvatf orpbzr zhpu zber vagrerfgvat.</p>\n<p>&nbsp;</p>\n<p><strong>Edit3:</strong></p>\n<h4>Implications</h4>\n<p>I'll try to analyse behavior of classical paperclip maximizer, using toy model I described earlier.&nbsp;Let utility function be min(number_of_paperclips_produced, 50).</p>\n<p>1. Paperclip maximizer implements greedy search algorithm. If it can't produce paperclip (all available actions lead to the same utility), it performs action that depends on implementation of greedy search. All in all it acts erratically, while it isn't occasionally terminated (it stumbled into world-state where there's no available actions for him).</p>\n<p>2. Paperclip maximizer implements full-search algorithm. Result depends on implementation of full-search. If implementation executes shortest sequence of actions that leads to globally maximal value of utility function, then it produces 50 paperclips as fast as it can <strong>[edit]</strong> or it wireheads itself into state where his paperclip counter&gt;50 whichever is faster&nbsp;<strong>[/edit]</strong>, then terminates itself. If implementation executes longest possible sequence of actions that leads to globally maximal value of utility function, then the agent behave erratically, but is guarantied to survive, while its optimization algorithm behave according to original plan, but it will occasionally modify itself and gets terminated, as original plan doesn't care about preservation of agent's optimization algorithm or utility function.</p>\n<p>It seems that in full-knowledge case powerful optimization processes don't go FOOM. Full-search algorithm is maximally powerful isn't it?</p>\n<p>Maybe it is uncertainty that leads to FOOMing?&nbsp;</p>\n<p>Indexical uncertainty can be represented by assumption, than agent knows set of world-states it can be in, and a set of available actions for world-state it is actually in. I'll try to analyze this case later.</p>\n<p><strong>Edit4:</strong>&nbsp;Edit3 is wrong. Utility function in that toy model cannot be so simple if it uses some property of the agent. However it seems OK to extend model by including high-level description of state of the agent into world-state, then edit3 holds.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MvGupsTJ2FDJPKmiB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -3, "extendedScore": null, "score": 6.441460048376595e-07, "legacy": true, "legacyId": "3972", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h4 id=\"Preamble\">Preamble</h4>\n<p><a href=\"/lw/2zj/value_deathism/\">Value deathism</a>&nbsp;by Vladimir Nesov encourages us to fix our values to prevent astronomical waste due to under-optimized future.</p>\n<p>When I've read it&nbsp;I found that I think about units of measurement of mentioned astronomical waste. Utilons?&nbsp;Seems so.&nbsp;<strong>[edit] </strong>Jack suggested widely accepted word Utils instead.<strong>[/edit]</strong></p>\n<p>I've tried to precisely define it. It is difference between utility of some world-state G measured by original (drifting) agent and utility of world-state G measured by undrifting version of original agent, where world-state G is optimal according to original (drifting) agent.</p>\n<p>There are two questions: can we compare utilities of those agents and what does it mean that G is optimal?</p>\n<h4 id=\"Question\">Question</h4>\n<p>Preconditions: world is deterministic, the agent has full knowledge of the world, i.e. it knows current world-state, full list of actions available for every world-state and&nbsp;consequence&nbsp;of each action (world-state it leads to), the agent has no time limit for computing next action.</p>\n<p>Agent's value is defined as a function from set of world-states to real numbers, for the sake of, uhm, clarity, the bigger the better. (Note: it is unnecessary to define value as a function from set of sequences of world-states, as history of world can be deduced from world-state itself, and if it can't be deduced, then the agent can't use history anyway, as the agent is a part of this world-state, so it doesn't \"remember\" history too). <strong>[edit] </strong>I wasn't aware that this note includes hidden assumption: value of world-state must be constant. But this assumption doesn't allow agent to single out world-state where agent loses all or part of its memory. Thus value as a function over sequences of world-states has a right to be. But this value function still needs to be specifically shaped to be optimization algorithm independent.&nbsp;<strong>[/edit]</strong></p>\n<p>Which sequence of world-states is optimal according to agent's value?</p>\n<p>&nbsp;</p>\n<p><strong>Edit:</strong> Consider agents implementing greedy search algorithm and exhaustive search algorithm. For them to choose same sequence of world-states search space should be greedoid. And that requires very specific structure of value function.</p>\n<p><strong>Edit2:</strong> Alternatively value function can be indirectly self-referential via part of world-state that contains the agent, thus allowing it to modify agent's optimization algorithm by assigning higher utility to world-states where agent implements desired optimization algorithm. (I call agent's function 'value function' because its meaning can be defined by the function itself, it isn't necessarily utility).</p>\n<p>&nbsp;</p>\n<p>My answer:</p>\n<p>Jura inyhr shapgvba bs gur ntrag vfa'g ersyrpgvir, v.r. qbrfa'g qrcraq ba vagrecergngvba bs n cneg bs jbeyq-fgngr bpphcvrq ol ntrag va grezf bs bcgvzvmngvba cebprff vzcyrzragrq ol guvf cneg bs jbeyq-fgngr, gura bcgvzny frdhrapr qrcraqf ba pbzovangvba bs qrgnvyf bs vzcyrzragngvba bs ntrag'f bcgvzvmngvba nytbevguz naq inyhr shapgvba. V guvax va trareny vg jvyy rkuvovg SBBZ orunivbe.</p>\n<p>Ohg jura inyhr shapgvba vf ersyrpgvir gura guvatf orpbzr zhpu zber vagrerfgvat.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Edit3_\">Edit3:</strong></p>\n<h4 id=\"Implications\">Implications</h4>\n<p>I'll try to analyse behavior of classical paperclip maximizer, using toy model I described earlier.&nbsp;Let utility function be min(number_of_paperclips_produced, 50).</p>\n<p>1. Paperclip maximizer implements greedy search algorithm. If it can't produce paperclip (all available actions lead to the same utility), it performs action that depends on implementation of greedy search. All in all it acts erratically, while it isn't occasionally terminated (it stumbled into world-state where there's no available actions for him).</p>\n<p>2. Paperclip maximizer implements full-search algorithm. Result depends on implementation of full-search. If implementation executes shortest sequence of actions that leads to globally maximal value of utility function, then it produces 50 paperclips as fast as it can <strong>[edit]</strong> or it wireheads itself into state where his paperclip counter&gt;50 whichever is faster&nbsp;<strong>[/edit]</strong>, then terminates itself. If implementation executes longest possible sequence of actions that leads to globally maximal value of utility function, then the agent behave erratically, but is guarantied to survive, while its optimization algorithm behave according to original plan, but it will occasionally modify itself and gets terminated, as original plan doesn't care about preservation of agent's optimization algorithm or utility function.</p>\n<p>It seems that in full-knowledge case powerful optimization processes don't go FOOM. Full-search algorithm is maximally powerful isn't it?</p>\n<p>Maybe it is uncertainty that leads to FOOMing?&nbsp;</p>\n<p>Indexical uncertainty can be represented by assumption, than agent knows set of world-states it can be in, and a set of available actions for world-state it is actually in. I'll try to analyze this case later.</p>\n<p><strong>Edit4:</strong>&nbsp;Edit3 is wrong. Utility function in that toy model cannot be so simple if it uses some property of the agent. However it seems OK to extend model by including high-level description of state of the agent into world-state, then edit3 holds.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Preamble", "anchor": "Preamble", "level": 1}, {"title": "Question", "anchor": "Question", "level": 1}, {"title": "Edit3:", "anchor": "Edit3_", "level": 2}, {"title": "Implications", "anchor": "Implications", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GMyjNQe5ZgkXJChbg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-11T00:13:18.011Z", "modifiedAt": null, "url": null, "title": "Pet Cryonics", "slug": "pet-cryonics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:52.186Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jack", "createdAt": "2009-02-27T15:27:14.891Z", "isAdmin": false, "displayName": "Jack"}, "userId": "GwetakMQqsGCf7ZQv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dfAEfCQspuMLJdsZJ/pet-cryonics", "pageUrlRelative": "/posts/dfAEfCQspuMLJdsZJ/pet-cryonics", "linkUrl": "https://www.lesswrong.com/posts/dfAEfCQspuMLJdsZJ/pet-cryonics", "postedAtFormatted": "Thursday, November 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pet%20Cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APet%20Cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdfAEfCQspuMLJdsZJ%2Fpet-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pet%20Cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdfAEfCQspuMLJdsZJ%2Fpet-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdfAEfCQspuMLJdsZJ%2Fpet-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>Open discussion.</p>\n<p>I think my dog is about to die. Even if I thought it was worth it I don't have the money to freeze her. But I am curious to know how people here feel about the practice and whether anyone plans to do this for their pet. It seems like a practice that plays into the image of cryonics as the domain of strange and egotistical rich people. On the other hand it <em>also</em> seems like a rather human and heart warming practice. Is pet cryopreservation good for the image of cryonics?</p>\n<p>Also, do people who just do neuro get their pets preserved? Will people upload pets? Assuming life as an emulation feels different from life as a biological organism is it ethical to upload animals? The transition might be strange and uncomfortable but we expect at least some humans to take the risk and live with any differences. But animals don't understand this and might not have the mental flexibility to adjust.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dfAEfCQspuMLJdsZJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 6.442690387205507e-07, "legacy": true, "legacyId": "3974", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-11T03:41:06.707Z", "modifiedAt": null, "url": null, "title": "[LINK] Creationism = High Carb? Or, The Devil Does Atkins", "slug": "link-creationism-high-carb-or-the-devil-does-atkins", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:31.382Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZaBG4t2HXwfPL45CL/link-creationism-high-carb-or-the-devil-does-atkins", "pageUrlRelative": "/posts/ZaBG4t2HXwfPL45CL/link-creationism-high-carb-or-the-devil-does-atkins", "linkUrl": "https://www.lesswrong.com/posts/ZaBG4t2HXwfPL45CL/link-creationism-high-carb-or-the-devil-does-atkins", "postedAtFormatted": "Thursday, November 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Creationism%20%3D%20High%20Carb%3F%20Or%2C%20The%20Devil%20Does%20Atkins&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Creationism%20%3D%20High%20Carb%3F%20Or%2C%20The%20Devil%20Does%20Atkins%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZaBG4t2HXwfPL45CL%2Flink-creationism-high-carb-or-the-devil-does-atkins%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Creationism%20%3D%20High%20Carb%3F%20Or%2C%20The%20Devil%20Does%20Atkins%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZaBG4t2HXwfPL45CL%2Flink-creationism-high-carb-or-the-devil-does-atkins", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZaBG4t2HXwfPL45CL%2Flink-creationism-high-carb-or-the-devil-does-atkins", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 427, "htmlBody": "<p>Based on the community's <a href=\"/lw/1kh/the_correct_contrarian_cluster/\">continuing</a> <a href=\"/lw/14c/the_obesity_myth/\">interests</a> in diet and <a href=\"/tag/atheism/\">religion</a>, I'd like to point out this blog post by the coauthor of <em>Protein Power</em>, Michael Eades, wherein he suggests that biblical literalism tends toward a low-fat approach to nutrition over a low-carb philosophy, by essentially throwing out a bunch of evidence on the matter:</p>\n<blockquote>\n<p>Why, you might ask, is this scientist so obdurate in the face of all the  evidence that&rsquo;s out there?&nbsp; Perhaps because much of the evidence isn&rsquo;t  in accord with his religious beliefs.&nbsp; I try never to mention a person&rsquo;s  religious faith, but when it impacts his scientific thinking it at  least needs to be made known.&nbsp; Unless he&rsquo;s changed his thinking  recently, Dr. Eckel apparently is one of the few academic scientists who  are literal interpreters of the bible.&nbsp; I assume this because Dr. Eckel  serves on the <a href=\"http://www.icr.org/research/tech_adv_board/\">technical advisory board</a> of the Institution for Creation Research, an organization that believes  that not only is the earth only a few thousand years old , but that <a href=\"http://www.icr.org/recent-universe/\">the entire universe in only a few thousand years old</a>.&nbsp; And they believe that man was basically hand formed by God on the sixth day of creation.&nbsp; And <a href=\"http://www.answersingenesis.org/home/area/isd/eckel.asp\">Dr. Eckel&rsquo;s own writings</a> on the subject appear to confirm his beliefs</p>\n<p>[.....]</p>\n<p>Of all the evidence that exists, I think the evolutionary/natural  selection data and the anthropological data are the most compelling  because they provide the largest amount of evidence over the longest  time.&nbsp; To Dr. Eckel, however, these data aren&rsquo;t applicable because in  his worldview prehistoric man didn&rsquo;t exist and therefore wasn&rsquo;t  available to be molded by the forces of natural selection.&nbsp; I haven&rsquo;t a  clue as to what he thinks the fossil remains of early humans really were  or where they came from.&nbsp; Perhaps he believes &ndash; as I once had it  explained to me by a religious fundamentalist &ndash; these fossilized remains  of dinosaurs, extinct ancient birds and mammals and prehistoric man  were carefully buried by the devil to snare the unwary and the  unbeliever.&nbsp; If this is the case, I guess I&rsquo;ll have to consider myself  snared.</p>\n<p>In Dr. Eckel&rsquo;s view, man was created post agriculturally.&nbsp; In fact,  in his view, there was never an pre-agricultural era, so how could man  have failed to adapt to agriculture?</p>\n<p>&nbsp;&ndash; <a href=\"http://www.proteinpower.com/drmike/lipid-hypothesis/rooting-out-more-anti-low-carb-bias/?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+drmikenutritionblog+%28The+Blog+of+Michael+R.+Eades%2C+M.D.%29&amp;utm_content=Google+Feedfetcher\">Rooting out more anti-low-carb bias</a></p>\n</blockquote>\n<p>While there's a clear persuasive agenda here and I won't present a full analysis of the situation, Eades also mentions biasing use of language earlier in the article. In particular, beware <a href=\"/lw/jb/applause_lights/\">applause lights</a> and <a href=\"http://wiki.lesswrong.com/wiki/Positive_bias\">confirmation bias</a> in evaluating.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZaBG4t2HXwfPL45CL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 6.443191924309042e-07, "legacy": true, "legacyId": "3976", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9KvefburLia7ptEE3", "WfHiyRxMj6aL7PN7i", "dLbkrPu5STNCBLRjr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-11T11:43:54.907Z", "modifiedAt": null, "url": null, "title": "History of Manga (Wikipedia link)", "slug": "history-of-manga-wikipedia-link", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:58.439Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ssh5zWwNqfXuxWGHi/history-of-manga-wikipedia-link", "pageUrlRelative": "/posts/Ssh5zWwNqfXuxWGHi/history-of-manga-wikipedia-link", "linkUrl": "https://www.lesswrong.com/posts/Ssh5zWwNqfXuxWGHi/history-of-manga-wikipedia-link", "postedAtFormatted": "Thursday, November 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20History%20of%20Manga%20(Wikipedia%20link)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHistory%20of%20Manga%20(Wikipedia%20link)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSsh5zWwNqfXuxWGHi%2Fhistory-of-manga-wikipedia-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=History%20of%20Manga%20(Wikipedia%20link)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSsh5zWwNqfXuxWGHi%2Fhistory-of-manga-wikipedia-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSsh5zWwNqfXuxWGHi%2Fhistory-of-manga-wikipedia-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/History_of_manga\">http://en.wikipedia.org/wiki/History_of_manga</a></p>\n<p>Thanks for the link, <a href=\"/lw/32b/public_international_law/2xm3?c=1\">grouchymusicologist</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ssh5zWwNqfXuxWGHi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -12, "extendedScore": null, "score": -5e-06, "legacy": true, "legacyId": "3980", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-11T15:13:58.511Z", "modifiedAt": null, "url": null, "title": "How to pick your categories", "slug": "how-to-pick-your-categories", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:50.847Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iaYzgw2iwHAxRynbd/how-to-pick-your-categories", "pageUrlRelative": "/posts/iaYzgw2iwHAxRynbd/how-to-pick-your-categories", "linkUrl": "https://www.lesswrong.com/posts/iaYzgw2iwHAxRynbd/how-to-pick-your-categories", "postedAtFormatted": "Thursday, November 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20pick%20your%20categories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20pick%20your%20categories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiaYzgw2iwHAxRynbd%2Fhow-to-pick-your-categories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20pick%20your%20categories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiaYzgw2iwHAxRynbd%2Fhow-to-pick-your-categories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiaYzgw2iwHAxRynbd%2Fhow-to-pick-your-categories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1697, "htmlBody": "<p><em>Note: this is intended to be a friendly math post, so apologies to anyone for whom this is all old hat. &nbsp;I'm deliberately staying elementary for the benefit of people who are new to the ideas. &nbsp;There are no proofs: this is long enough as it is.</em></p>\n<p><em>Related: <a href=\"/lw/o0/where_to_draw_the_boundary/\" target=\"_blank\">Where to Draw the Boundary</a>, <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\" target=\"_blank\">The Cluster Structure of Thingspace</a>, <a href=\"/lw/nm/disguised_queries/\" target=\"_blank\">Disguised Queries.</a></em></p>\n<p>Here's a rather deep problem in philosophy: how do we come up with categories? &nbsp;What's the difference between a horror movie and a science fiction movie? &nbsp;Or the difference between a bird and a mammal? Are there such things as \"natural kinds,\" or are all such ideas arbitrary? &nbsp;</p>\n<p>We can frame this in a slightly more mathematical way as follows. &nbsp;Objects in real life (animals, moving pictures, etc.) are enormously complicated and have many features and properties. &nbsp;You can think of this as a very high dimensional space, one dimension for each property, and each object having a value corresponding to each property. &nbsp;A grayscale picture, for example, has a color value for each pixel. &nbsp;A text document has a count for every word (the word \"flamingo\" might have been used 7 times, for instance.) &nbsp;A multiple-choice questionnaire has an answer for each question. &nbsp;Each object is a point in a high-dimensional featurespace. &nbsp;To identify which objects are similar to each other, we want to identify how close points are in featurespace. &nbsp;For example, two pictures that only differ at one pixel should turn out to be similar.</p>\n<p>We could then start to form categories if the objects form empirical clusters in featurespace. &nbsp;If some animals have wings and hollow bones and feathers, and some animals have none of those things but give milk and bear live young, it makes sense to distinguish birds from mammals. &nbsp;If empirical clusters actually exist, then there's nothing arbitrary about the choice of categories -- the categories are appropriate to the data!</p>\n<p>There are a number of mathematical techniques for assigning categories; all of them are basically attacking the same problem, and in principle should all agree with each other and identify the \"right\" categories. &nbsp;But in practice they have different strengths and weaknesses, in computational efficiency, robustness to noise, and ability to classify accurately. &nbsp;This field is incredibly useful -- this is how computers do image and speech recognition, this is how natural language processing works, this is how they sequence your DNA. It also, I hope, will yield insights into how people think and perceive.</p>\n<p><strong>Clustering techniques</strong></p>\n<p>These techniques attempt to directly find clusters in observations. &nbsp;A common example is the K-means algorithm. &nbsp;The goal here is, given a set of observations x<sub>1</sub>...x<sub>n</sub>, to partition them into k sets so as to minimize the within-cluster sum of squared differences:<a id=\"more\"></a></p>\n<p>argmin<span style=\"font-size: xx-small;\"><span style=\"font-size: 9px;\"><sub>S</sub></span></span>&nbsp;&sum;<sub>i</sub><sup>k </sup>&sum;<sub>x</sub><sub>&nbsp;in S &nbsp;</sub>||x<sub>i</sub>&nbsp;- m<sub>i</sub>||<sup>2</sup></p>\n<p>where m<sub>i</sub>&nbsp;are the means.</p>\n<p>The standard algorithm is to pick k means randomly, anywhere, assign points to the cluster with the closest mean, and then assign the new mean of each cluster to be the centroid (average) of the points in the cluster. &nbsp;Then we iterate again, possibly assigning different points to different clusters with each iterative step. &nbsp;This is usually very fast but can be slow in the worst-case scenario.</p>\n<p>There are many, many clustering algorithms. &nbsp;They vary in the choice of distance metric (it doesn't have to be Euclidean, we could take taxicab distances or Hamming distances or something else). &nbsp;There's also something called hierarchical clustering, which outputs a tree of clusters.</p>\n<p><strong>Linear dimensionality reduction techniques</strong></p>\n<p>Here's another way to think about this problem: perhaps, of all the possible features that could distinguish two objects, most of the variation is in only a <em>few</em>&nbsp;features. &nbsp;You have a high-dimensional feature space, but in fact all the points are lying in a much lower-dimensional space. &nbsp;(Maybe, for instance, once you've identified what color a flower is, how big it is, and how many petals it has, you've almost completely identified the flower.) &nbsp;We'd like to know which <em>coordinate&nbsp;</em><em>axes</em>&nbsp;explain the data well.</p>\n<p><em><span style=\"font-style: normal;\">There are a number of methods for doing this -- I'll mention a classic one, singular value decomposition (SVD). &nbsp;For any m x n matrix M, we have a factorization of the form</span></em></p>\n<p><em><span style=\"font-style: normal;\">M = U&nbsp;&Sigma; V*</span></em></p>\n<p><em><span style=\"font-style: normal;\">where U and V are orthogonal and&nbsp;&Sigma; is diagonal. &nbsp;The columns of U are the eigenvectors of M*M, the columns of V are the eigenvectors of MM*, and the elements of&nbsp;&Sigma; (called singular values) are the square roots of the eigenvalues of M*M and MM*. &nbsp;</span></em></p>\n<p><em><span style=\"font-style: normal;\">Now, if we want a low-rank approximation of M (that is, every point in the approximation lies on a low-dimensional hyperplane) all we have to do is chop off&nbsp;&Sigma; to contain only the largest k singular values. Intuitively, these are the dimensions that account for most of the variation in the data matrix M. &nbsp;The approximate matrix M' = U&nbsp;&Sigma;' V* can be shown to be the closest possible rank-k approximation to M.</span></em></p>\n<p><em><span style=\"font-style: normal;\">In other words, if you have high-dimensional data, and you suspect that only two coordinates explain all the data -- maybe you have a questionnaire, and you think that age and sex explain all the patterns in answering -- something like SVD can identify which those coordinates are. In a sense, dimensionality reduction can </span>identify the factors that are worth looking at.</em></p>\n<p><strong>Nonlinear dimensionality reduction techniques</strong></p>\n<p>An algorithm like SVD is <em>linear</em>&nbsp;-- the approximation it spits out lies entirely on a vector subspace of your original vector space (a line, a plane, a hyperplane of some dimension.) &nbsp;Sometimes, though, that's a bad idea. &nbsp;What if you have a cloud of data that actually lies on a circle? &nbsp;Or some other curvy shape? &nbsp;SVD will get it wrong.</p>\n<p>One interesting tweak on this process is <em>manifold learning</em>&nbsp;-- if we suspect that the data lies on a low-dimensional but possibly curvy shape, we try to identify the manifold, just as in SVD we tried to identify the subspace. &nbsp;There are a lot of algorithms for doing this. &nbsp;One of my favorites is the Laplacian eigenmap. &nbsp;</p>\n<p>Here, we look at each data point as a node on a graph; if we have lots of data (and we usually do) the graph is a sort of mesh approximation for the smooth manifold it lies on. &nbsp;We construct a sparse, weighted adjacency matrix: it's N x N, where N is the number of data points. &nbsp;Matrix elements are zero if they correspond to two points that are far apart, but if they correspond to nearby points we put the heat kernel e<span style=\"font-size: small;\"><span style=\"font-size: 11px;\"><sup>-||x-y||^2</sup>. &nbsp;</span><span style=\"font-size: small;\">Then we look at the eigenvectors of this matrix. &nbsp;We use the top eigenvectors as coordinates to embed into Euclidean space. The reason this works, roughly, is that we're approximating a Laplace operator on the manifold: two points are close together if a diffusion moving along the graph would travel between them quickly. &nbsp;It's a way of mapping the graph into a lower-dimensional space such that points that are close on the graph are close in the embedding.</span></span></p>\n<p>Good nonlinear dimensionality reduction techniques can identify data that lies on curvy shapes, like circles and spirals and the \"swiss roll\" (a rolled-up plane) much better than linear dimensionality reduction techniques.</p>\n<p>&nbsp;</p>\n<p><strong>What's the moral?</strong></p>\n<p>Once upon a time, the search engine Yahoo tried to categorize all the sites on the web according to a pre-set classification system. &nbsp;Science, sports, entertainment, and so on. &nbsp;It was phenomenally unpopular. &nbsp;The content that grew online often didn't fit the categories. &nbsp;People didn't want to surf the internet with the equivalent of the Dewey Decimal System. &nbsp;</p>\n<p>These days, to some degree, we know better. &nbsp;Amazon.com doesn't recommend books based on pre-set categories, giving you a horror book if you liked a horror book before. &nbsp;It recommends a book based on the choices of other customers who liked the same books you like. Evidently, they have a big adjacency matrix somewhere, one column for every customer and one row for every purchase, and quite possibly they're running some sort of a graph diffusion on it. &nbsp;They let the categories emerge organically from the data. &nbsp;If a new genre is emerging, they don't have to scurry around trying to add a new label for it; it'll show up automatically.</p>\n<p>This suggests a sort of rationalist discipline: <em>categories should always be organic.</em>&nbsp;&nbsp;Humans like to categorize, and categories can be very useful. &nbsp;But not every set of categories is efficient at describing the variety of actual observations. &nbsp;Biologists used to have a kingdom called Monera, consisting of all the single-celled organisms; it was a horrible grab bag, because single-celled organisms are very different from each other genetically. &nbsp;After analyzing genomes, they decided there were actually three domains, Bacteria, Archaea, and Eukarya (the only one which includes multicellular life.) &nbsp;In a real, non-arbitrary way, this is a <em>better</em>&nbsp;kind of categorization, and Monera was a <em>bad</em>&nbsp;category.&nbsp;</p>\n<p>Sometimes it seems that researchers don't always pay attention to the problem of choosing categories and axes well. &nbsp;For example, I once saw a study of autism that did the following: created a questionnaire that rated the user's \"empathizing\" and \"systematizing\" qualities, found that autistics were less \"empathizing\" and more \"systematizing\" than non-autistics, and concluded that autism was <em>defined</em>&nbsp;by more systematizing and less empathizing. &nbsp;This is a classic example of privileging one axis over others -- what if autistics and non-autistics also differ in some <em>other</em>&nbsp;way? How do you know that you've chosen an efficient way to define that category? &nbsp;Wouldn't you have to <em>go look</em>?</p>\n<p>If you say \"There are two kinds of people in the world,\" but if you look around and lots of people don't fit your binary, then maybe your binary is bad. &nbsp;If you want to know whether your set of categories is good, <em>go look</em>&nbsp;-- see if the data actually clusters that way. &nbsp;There's still a lot of debate about <em>which </em>mathematical techniques are best for defining categories, but it is a field where science has actually made progress. It's <em>not </em>all arbitrary. &nbsp;When you play Twenty Questions with the universe, some questions are more useful than others.</p>\n<p>&nbsp;</p>\n<p>References:</p>\n<p>Wikipedia is generally very good on this subject, and the wiki page on <a href=\"http://en.wikipedia.org/wiki/Singular_value_decomposition\" target=\"_blank\">singular value decomposition</a>&nbsp;contains the proof that it actually works.</p>\n<p>This paper by <a href=\"http://www.cse.ohio-state.edu/~mbelkin/papers/LEM_NC_03.pdf\" target=\"_blank\">Mikhail Belkin and Partha Niyogi </a>does a much better job of explaining Laplacian eigenmaps. Some other nonlinear dimensionality reduction techniques: <a href=\"http://isomap.stanford.edu/\" target=\"_blank\">Isomap</a>, <a href=\"http://www.stat.cmu.edu/~cshalizi/350/lectures/14/lecture-14.pdf\" target=\"_blank\">Locally Linear Embedding,&nbsp;</a>&nbsp;and <a href=\"http://www.pnas.org/content/102/21/7426.full\" target=\"_blank\">diffusion maps.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "3uE2pXvbcnS9nnZRE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iaYzgw2iwHAxRynbd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 78, "extendedScore": null, "score": 0.000149, "legacy": true, "legacyId": "3973", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 61, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Note: this is intended to be a friendly math post, so apologies to anyone for whom this is all old hat. &nbsp;I'm deliberately staying elementary for the benefit of people who are new to the ideas. &nbsp;There are no proofs: this is long enough as it is.</em></p>\n<p><em>Related: <a href=\"/lw/o0/where_to_draw_the_boundary/\" target=\"_blank\">Where to Draw the Boundary</a>, <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\" target=\"_blank\">The Cluster Structure of Thingspace</a>, <a href=\"/lw/nm/disguised_queries/\" target=\"_blank\">Disguised Queries.</a></em></p>\n<p>Here's a rather deep problem in philosophy: how do we come up with categories? &nbsp;What's the difference between a horror movie and a science fiction movie? &nbsp;Or the difference between a bird and a mammal? Are there such things as \"natural kinds,\" or are all such ideas arbitrary? &nbsp;</p>\n<p>We can frame this in a slightly more mathematical way as follows. &nbsp;Objects in real life (animals, moving pictures, etc.) are enormously complicated and have many features and properties. &nbsp;You can think of this as a very high dimensional space, one dimension for each property, and each object having a value corresponding to each property. &nbsp;A grayscale picture, for example, has a color value for each pixel. &nbsp;A text document has a count for every word (the word \"flamingo\" might have been used 7 times, for instance.) &nbsp;A multiple-choice questionnaire has an answer for each question. &nbsp;Each object is a point in a high-dimensional featurespace. &nbsp;To identify which objects are similar to each other, we want to identify how close points are in featurespace. &nbsp;For example, two pictures that only differ at one pixel should turn out to be similar.</p>\n<p>We could then start to form categories if the objects form empirical clusters in featurespace. &nbsp;If some animals have wings and hollow bones and feathers, and some animals have none of those things but give milk and bear live young, it makes sense to distinguish birds from mammals. &nbsp;If empirical clusters actually exist, then there's nothing arbitrary about the choice of categories -- the categories are appropriate to the data!</p>\n<p>There are a number of mathematical techniques for assigning categories; all of them are basically attacking the same problem, and in principle should all agree with each other and identify the \"right\" categories. &nbsp;But in practice they have different strengths and weaknesses, in computational efficiency, robustness to noise, and ability to classify accurately. &nbsp;This field is incredibly useful -- this is how computers do image and speech recognition, this is how natural language processing works, this is how they sequence your DNA. It also, I hope, will yield insights into how people think and perceive.</p>\n<p><strong id=\"Clustering_techniques\">Clustering techniques</strong></p>\n<p>These techniques attempt to directly find clusters in observations. &nbsp;A common example is the K-means algorithm. &nbsp;The goal here is, given a set of observations x<sub>1</sub>...x<sub>n</sub>, to partition them into k sets so as to minimize the within-cluster sum of squared differences:<a id=\"more\"></a></p>\n<p>argmin<span style=\"font-size: xx-small;\"><span style=\"font-size: 9px;\"><sub>S</sub></span></span>&nbsp;\u2211<sub>i</sub><sup>k </sup>\u2211<sub>x</sub><sub>&nbsp;in S &nbsp;</sub>||x<sub>i</sub>&nbsp;- m<sub>i</sub>||<sup>2</sup></p>\n<p>where m<sub>i</sub>&nbsp;are the means.</p>\n<p>The standard algorithm is to pick k means randomly, anywhere, assign points to the cluster with the closest mean, and then assign the new mean of each cluster to be the centroid (average) of the points in the cluster. &nbsp;Then we iterate again, possibly assigning different points to different clusters with each iterative step. &nbsp;This is usually very fast but can be slow in the worst-case scenario.</p>\n<p>There are many, many clustering algorithms. &nbsp;They vary in the choice of distance metric (it doesn't have to be Euclidean, we could take taxicab distances or Hamming distances or something else). &nbsp;There's also something called hierarchical clustering, which outputs a tree of clusters.</p>\n<p><strong id=\"Linear_dimensionality_reduction_techniques\">Linear dimensionality reduction techniques</strong></p>\n<p>Here's another way to think about this problem: perhaps, of all the possible features that could distinguish two objects, most of the variation is in only a <em>few</em>&nbsp;features. &nbsp;You have a high-dimensional feature space, but in fact all the points are lying in a much lower-dimensional space. &nbsp;(Maybe, for instance, once you've identified what color a flower is, how big it is, and how many petals it has, you've almost completely identified the flower.) &nbsp;We'd like to know which <em>coordinate&nbsp;</em><em>axes</em>&nbsp;explain the data well.</p>\n<p><em><span style=\"font-style: normal;\">There are a number of methods for doing this -- I'll mention a classic one, singular value decomposition (SVD). &nbsp;For any m x n matrix M, we have a factorization of the form</span></em></p>\n<p><em><span style=\"font-style: normal;\">M = U&nbsp;\u03a3 V*</span></em></p>\n<p><em><span style=\"font-style: normal;\">where U and V are orthogonal and&nbsp;\u03a3 is diagonal. &nbsp;The columns of U are the eigenvectors of M*M, the columns of V are the eigenvectors of MM*, and the elements of&nbsp;\u03a3 (called singular values) are the square roots of the eigenvalues of M*M and MM*. &nbsp;</span></em></p>\n<p><em><span style=\"font-style: normal;\">Now, if we want a low-rank approximation of M (that is, every point in the approximation lies on a low-dimensional hyperplane) all we have to do is chop off&nbsp;\u03a3 to contain only the largest k singular values. Intuitively, these are the dimensions that account for most of the variation in the data matrix M. &nbsp;The approximate matrix M' = U&nbsp;\u03a3' V* can be shown to be the closest possible rank-k approximation to M.</span></em></p>\n<p><em><span style=\"font-style: normal;\">In other words, if you have high-dimensional data, and you suspect that only two coordinates explain all the data -- maybe you have a questionnaire, and you think that age and sex explain all the patterns in answering -- something like SVD can identify which those coordinates are. In a sense, dimensionality reduction can </span>identify the factors that are worth looking at.</em></p>\n<p><strong id=\"Nonlinear_dimensionality_reduction_techniques\">Nonlinear dimensionality reduction techniques</strong></p>\n<p>An algorithm like SVD is <em>linear</em>&nbsp;-- the approximation it spits out lies entirely on a vector subspace of your original vector space (a line, a plane, a hyperplane of some dimension.) &nbsp;Sometimes, though, that's a bad idea. &nbsp;What if you have a cloud of data that actually lies on a circle? &nbsp;Or some other curvy shape? &nbsp;SVD will get it wrong.</p>\n<p>One interesting tweak on this process is <em>manifold learning</em>&nbsp;-- if we suspect that the data lies on a low-dimensional but possibly curvy shape, we try to identify the manifold, just as in SVD we tried to identify the subspace. &nbsp;There are a lot of algorithms for doing this. &nbsp;One of my favorites is the Laplacian eigenmap. &nbsp;</p>\n<p>Here, we look at each data point as a node on a graph; if we have lots of data (and we usually do) the graph is a sort of mesh approximation for the smooth manifold it lies on. &nbsp;We construct a sparse, weighted adjacency matrix: it's N x N, where N is the number of data points. &nbsp;Matrix elements are zero if they correspond to two points that are far apart, but if they correspond to nearby points we put the heat kernel e<span style=\"font-size: small;\"><span style=\"font-size: 11px;\"><sup>-||x-y||^2</sup>. &nbsp;</span><span style=\"font-size: small;\">Then we look at the eigenvectors of this matrix. &nbsp;We use the top eigenvectors as coordinates to embed into Euclidean space. The reason this works, roughly, is that we're approximating a Laplace operator on the manifold: two points are close together if a diffusion moving along the graph would travel between them quickly. &nbsp;It's a way of mapping the graph into a lower-dimensional space such that points that are close on the graph are close in the embedding.</span></span></p>\n<p>Good nonlinear dimensionality reduction techniques can identify data that lies on curvy shapes, like circles and spirals and the \"swiss roll\" (a rolled-up plane) much better than linear dimensionality reduction techniques.</p>\n<p>&nbsp;</p>\n<p><strong id=\"What_s_the_moral_\">What's the moral?</strong></p>\n<p>Once upon a time, the search engine Yahoo tried to categorize all the sites on the web according to a pre-set classification system. &nbsp;Science, sports, entertainment, and so on. &nbsp;It was phenomenally unpopular. &nbsp;The content that grew online often didn't fit the categories. &nbsp;People didn't want to surf the internet with the equivalent of the Dewey Decimal System. &nbsp;</p>\n<p>These days, to some degree, we know better. &nbsp;Amazon.com doesn't recommend books based on pre-set categories, giving you a horror book if you liked a horror book before. &nbsp;It recommends a book based on the choices of other customers who liked the same books you like. Evidently, they have a big adjacency matrix somewhere, one column for every customer and one row for every purchase, and quite possibly they're running some sort of a graph diffusion on it. &nbsp;They let the categories emerge organically from the data. &nbsp;If a new genre is emerging, they don't have to scurry around trying to add a new label for it; it'll show up automatically.</p>\n<p>This suggests a sort of rationalist discipline: <em>categories should always be organic.</em>&nbsp;&nbsp;Humans like to categorize, and categories can be very useful. &nbsp;But not every set of categories is efficient at describing the variety of actual observations. &nbsp;Biologists used to have a kingdom called Monera, consisting of all the single-celled organisms; it was a horrible grab bag, because single-celled organisms are very different from each other genetically. &nbsp;After analyzing genomes, they decided there were actually three domains, Bacteria, Archaea, and Eukarya (the only one which includes multicellular life.) &nbsp;In a real, non-arbitrary way, this is a <em>better</em>&nbsp;kind of categorization, and Monera was a <em>bad</em>&nbsp;category.&nbsp;</p>\n<p>Sometimes it seems that researchers don't always pay attention to the problem of choosing categories and axes well. &nbsp;For example, I once saw a study of autism that did the following: created a questionnaire that rated the user's \"empathizing\" and \"systematizing\" qualities, found that autistics were less \"empathizing\" and more \"systematizing\" than non-autistics, and concluded that autism was <em>defined</em>&nbsp;by more systematizing and less empathizing. &nbsp;This is a classic example of privileging one axis over others -- what if autistics and non-autistics also differ in some <em>other</em>&nbsp;way? How do you know that you've chosen an efficient way to define that category? &nbsp;Wouldn't you have to <em>go look</em>?</p>\n<p>If you say \"There are two kinds of people in the world,\" but if you look around and lots of people don't fit your binary, then maybe your binary is bad. &nbsp;If you want to know whether your set of categories is good, <em>go look</em>&nbsp;-- see if the data actually clusters that way. &nbsp;There's still a lot of debate about <em>which </em>mathematical techniques are best for defining categories, but it is a field where science has actually made progress. It's <em>not </em>all arbitrary. &nbsp;When you play Twenty Questions with the universe, some questions are more useful than others.</p>\n<p>&nbsp;</p>\n<p>References:</p>\n<p>Wikipedia is generally very good on this subject, and the wiki page on <a href=\"http://en.wikipedia.org/wiki/Singular_value_decomposition\" target=\"_blank\">singular value decomposition</a>&nbsp;contains the proof that it actually works.</p>\n<p>This paper by <a href=\"http://www.cse.ohio-state.edu/~mbelkin/papers/LEM_NC_03.pdf\" target=\"_blank\">Mikhail Belkin and Partha Niyogi </a>does a much better job of explaining Laplacian eigenmaps. Some other nonlinear dimensionality reduction techniques: <a href=\"http://isomap.stanford.edu/\" target=\"_blank\">Isomap</a>, <a href=\"http://www.stat.cmu.edu/~cshalizi/350/lectures/14/lecture-14.pdf\" target=\"_blank\">Locally Linear Embedding,&nbsp;</a>&nbsp;and <a href=\"http://www.pnas.org/content/102/21/7426.full\" target=\"_blank\">diffusion maps.</a></p>", "sections": [{"title": "Clustering techniques", "anchor": "Clustering_techniques", "level": 1}, {"title": "Linear dimensionality reduction techniques", "anchor": "Linear_dimensionality_reduction_techniques", "level": 1}, {"title": "Nonlinear dimensionality reduction techniques", "anchor": "Nonlinear_dimensionality_reduction_techniques", "level": 1}, {"title": "What's the moral?", "anchor": "What_s_the_moral_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d5NyJ2Lf6N22AD9PB", "WBw8dDkAWohFjWQSk", "4FcxgdvdQP45D6Skg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-11T17:28:21.338Z", "modifiedAt": null, "url": null, "title": "The Strong Occam's Razor", "slug": "the-strong-occam-s-razor", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:45.580Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ep6tiXJpHxR8ofWs6/the-strong-occam-s-razor", "pageUrlRelative": "/posts/ep6tiXJpHxR8ofWs6/the-strong-occam-s-razor", "linkUrl": "https://www.lesswrong.com/posts/ep6tiXJpHxR8ofWs6/the-strong-occam-s-razor", "postedAtFormatted": "Thursday, November 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Strong%20Occam's%20Razor&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Strong%20Occam's%20Razor%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fep6tiXJpHxR8ofWs6%2Fthe-strong-occam-s-razor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Strong%20Occam's%20Razor%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fep6tiXJpHxR8ofWs6%2Fthe-strong-occam-s-razor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fep6tiXJpHxR8ofWs6%2Fthe-strong-occam-s-razor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 822, "htmlBody": "<p>This post is a summary of the different positions expressed in the comments to my <a href=\"/lw/322/a_note_on_the_description_complexity_of_physical/\">previous post</a>&nbsp;and elsewhere on LW. The central issue turned out to be assigning \"probabilities\" to individual theories within an equivalence class of theories that yield identical predictions. Presumably we must prefer shorter theories to their longer versions even when they are equivalent. For example, is \"physics as we know it\" more probable than \"Odin created physics as we know it\"? Is the Hamiltonian formulation of classical mechanics apriori more probable than the Lagrangian formulation? Is the definition of reals via Dedekind cuts \"truer\" than the definition via binary expansions? And are these all really the same question in disguise?<a id=\"more\"></a></p>\n<p>One attractive answer, <a href=\"/lw/322/a_note_on_the_description_complexity_of_physical/2xko?c=1\">given by shokwave</a>, says that our intuitive concept of \"complexity penalty\" for theories is really an incomplete formalization of \"conjunction penalty\". Theories that require additional premises are less likely to be true, according to the eternal laws of probability. Adding premises like \"Odin created everything\" makes a theory less probable and also happens to make it longer; this is the entire reason why we intuitively agree with Occam's Razor in penalizing longer theories. Unfortunately, this answer seems to be based on a concept of \"truth\" granted from above - but what do differing degrees of truth actually&nbsp;<em>mean</em>, when two theories make exactly the same predictions?</p>\n<p>Another intriguing answer <a href=\"/lw/322/a_note_on_the_description_complexity_of_physical/2xob?c=1\">came from JGWeissman</a>. Apparently, as we learn new physics, we tend to discard inconvenient versions of old formalisms. So electromagnetic potentials turn out to be \"more true\" than electromagnetic fields because they carry over to quantum mechanics much better. I like this answer because it seems to be very well-informed! But what shall we do after we discover <em>all</em> of physics, and still have multiple equivalent formalisms - do we have any reason to believe simplicity will still work as a deciding factor? And the question remains, which definition of real numbers is \"correct\" after all?</p>\n<p>Eliezer, bless him, decided to take <a href=\"/lw/322/a_note_on_the_description_complexity_of_physical/2xnm?c=1\">a more naive view</a>. He merely pointed out that our intuitive concept of \"truth\" does seem to distinguish between \"physics\" and \"God created physics\", so if our current formalization of \"truth\" fails to tell them apart, the flaw lies with the formalism rather than with us. I have a lot of sympathy for this answer as well, but it looks rather like a mystery to be solved. I never expected to become entangled in a controversy over the notion of <a href=\"http://yudkowsky.net/rational/the-simple-truth\">truth</a> on LW, of all places!</p>\n<p>A final and most intriguing answer of all came from <a href=\"/lw/322/a_note_on_the_description_complexity_of_physical/2xgy?c=1\">saturn</a>, who alluded to a position <a href=\"/lw/pb/belief_in_the_implied_invisible/\">held by Eliezer</a> and <a href=\"/lw/pb/belief_in_the_implied_invisible/jc3?c=1\">sharpened by Nesov</a>. After thinking it over for awhile, I generated a good contender for <strong>the most confused argument ever expressed on LW</strong>. Namely, I'm going to completely ignore the is-ought distinction and use <em>morality</em> to prove the \"strong\" version of Occam's Razor - that shorter theories are more \"likely\" than equivalent longer versions. You ready? Here goes:</p>\n<p>Imagine you have the option to put a human being in a sealed box where they will be tortured for 50 years and then incinerated. No observational evidence will ever leave the box. (For added certainty, fling the box away at near lightspeed and let the expansion of the universe ensure that you can never reach it.) Now consider the following physical theory: as soon as you seal the box, our laws of physics will make a localized exception and the victim will spontaneously vanish from the box. This theory makes exactly the same observational predictions as your current best theory of physics, so it lies in the same equivalence class and you should give it the same credence. If you're still reluctant to push the button, it looks like you <em>already are</em> a believer in the \"strong Occam's Razor\" saying simpler theories without local exceptions are \"more true\". QED.</p>\n<p>It's not clear what, if anything, the above argument proves. It probably has no consequences in reality, because no matter how seductive it sounds, skipping over the is-ought distinction is not permitted. But it makes for a nice koan to meditate on weird matters like \"probability as preference\" (due to Nesov and Wei Dai) and other mysteries we haven't solved yet.</p>\n<p><strong>ETA: </strong>Hal Finney pointed out that the UDT approach - assuming that you live in many branches of the \"Solomonoff multiverse\" <em>at once</em>, weighted by simplicity, and reducing everything to decision problems in the obvious way - dissolves our mystery nicely and logically, at the cost of abandoning approximate concepts like \"truth\" and \"degree of belief\". It agrees with our intuition in advising you to avoid torturing people in closed boxes, and more generally in all questions about moral consequences of the \"implied invisible\". And it nicely skips over all the tangled issues of \"actual\" vs \"potential\" predictions, etc. I'm a little embarrassed at not having noticed the connection earlier. Now can we find any other good solutions, or is Wei's idea the only game in town?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hQiuNkBhn6xxcedTD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ep6tiXJpHxR8ofWs6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 17, "extendedScore": null, "score": 4.1e-05, "legacy": true, "legacyId": "3981", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 74, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["h9sP6rLT6r4vaXoen", "3XMwPNMSbaPm2suGz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-11T23:57:30.174Z", "modifiedAt": null, "url": null, "title": "The Aspirin Paradox- replacement for the Smoking Lesion Problem?", "slug": "the-aspirin-paradox-replacement-for-the-smoking-lesion", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.890Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YeFhEcRz65ikSsgjd/the-aspirin-paradox-replacement-for-the-smoking-lesion", "pageUrlRelative": "/posts/YeFhEcRz65ikSsgjd/the-aspirin-paradox-replacement-for-the-smoking-lesion", "linkUrl": "https://www.lesswrong.com/posts/YeFhEcRz65ikSsgjd/the-aspirin-paradox-replacement-for-the-smoking-lesion", "postedAtFormatted": "Thursday, November 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Aspirin%20Paradox-%20replacement%20for%20the%20Smoking%20Lesion%20Problem%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Aspirin%20Paradox-%20replacement%20for%20the%20Smoking%20Lesion%20Problem%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYeFhEcRz65ikSsgjd%2Fthe-aspirin-paradox-replacement-for-the-smoking-lesion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Aspirin%20Paradox-%20replacement%20for%20the%20Smoking%20Lesion%20Problem%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYeFhEcRz65ikSsgjd%2Fthe-aspirin-paradox-replacement-for-the-smoking-lesion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYeFhEcRz65ikSsgjd%2Fthe-aspirin-paradox-replacement-for-the-smoking-lesion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 148, "htmlBody": "<p>It's been pointed out that the <a href=\"http://wiki.lesswrong.com/wiki/Smoking_lesion\">Smoking Lesion problem</a> is a poorly chosen decision theory problem, because in the real world there actually is a direct causal link from smoking to cancer, and people's intuitions are influenced more by that than by the stated parameters of the scenario. In his <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">TDT document</a>, Eliezer concocts a different artificial example (chewing gum and throat abcesses).&nbsp; I recently noticed, though, a potentially good real-world example of the same dynamic: the <a href=\"http://sciencelife.uchospitals.edu/2010/11/09/the-aspirin-paradox-unraveled/\">Aspirin Paradox</a>.</p>\n<p>Despite the effectiveness of aspirin in preventing heart attacks, those who regularly take aspirin are at a higher risk of a second heart attack, because those with symptoms of heart disease are more likely than those without symptoms to be taking aspirin regularly. While it turns out this \"risk factor\" is mostly screened off by other measurable health factors, it's a valid enough correlation for the purposes of decision theory.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YeFhEcRz65ikSsgjd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 6.446128975118969e-07, "legacy": true, "legacyId": "3983", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-12T01:14:26.076Z", "modifiedAt": null, "url": null, "title": "If a tree falls on Sleeping Beauty...", "slug": "if-a-tree-falls-on-sleeping-beauty", "viewCount": null, "lastCommentedAt": "2020-05-06T02:14:17.508Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ata", "createdAt": "2009-07-20T22:13:53.102Z", "isAdmin": false, "displayName": "ata"}, "userId": "KppHkGEqTNeDaGJTc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gMXsyhPiEJbGerF6F/if-a-tree-falls-on-sleeping-beauty", "pageUrlRelative": "/posts/gMXsyhPiEJbGerF6F/if-a-tree-falls-on-sleeping-beauty", "linkUrl": "https://www.lesswrong.com/posts/gMXsyhPiEJbGerF6F/if-a-tree-falls-on-sleeping-beauty", "postedAtFormatted": "Friday, November 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20a%20tree%20falls%20on%20Sleeping%20Beauty...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20a%20tree%20falls%20on%20Sleeping%20Beauty...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgMXsyhPiEJbGerF6F%2Fif-a-tree-falls-on-sleeping-beauty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20a%20tree%20falls%20on%20Sleeping%20Beauty...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgMXsyhPiEJbGerF6F%2Fif-a-tree-falls-on-sleeping-beauty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgMXsyhPiEJbGerF6F%2Fif-a-tree-falls-on-sleeping-beauty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2476, "htmlBody": "<p>Several months ago, we had an <a href=\"/lw/286/beauty_quips_id_shut_up_and_multiply/\">interesting discussion</a> about the Sleeping Beauty problem, which runs as follows:</p>\n<blockquote>\n<p><em>Sleeping Beauty volunteers to undergo the following experiment. On Sunday she is given a drug that sends her to sleep. A fair coin is then tossed just once in the course of the experiment to determine which experimental procedure is undertaken. If the coin comes up heads, Beauty is awakened and interviewed on Monday, and then the experiment ends. If the coin comes up tails, she is awakened and interviewed on Monday, given a second dose of the sleeping drug, and awakened and interviewed again on Tuesday. The experiment then ends on Tuesday, without flipping the coin again. The sleeping drug induces a mild amnesia, so that she cannot remember any previous awakenings during the course of the experiment (if any). During the experiment, she has no access to anything that would give a clue as to the day of the week. However, she knows all the details of the experiment.</em></p>\n<p><em>Each interview consists of one question, &ldquo;What is your credence now for the proposition that our coin landed heads?&rdquo;</em></p>\n</blockquote>\n<p>In the end, the fact that there were so many reasonable-sounding arguments for both sides, and so much disagreement about a simple-sounding problem among above-average rationalists, should have set off major alarm bells. Yet only a few people pointed this out; most commenters, including me, followed the silly strategy of trying to <em>answer</em> the question, and I did so even after I <em>noticed</em> that my intuition could see both answers as being right depending on which way I looked at it, which in retrospect would have been a <em>perfect</em> time to say &ldquo;<a href=\"/lw/if/your_strength_as_a_rationalist/\">I notice that I am confused</a>&rdquo; and backtrack a bit&hellip;</p>\n<p>And on reflection, considering my confusion rather than trying to consider the question on its own terms, it seems to me that the problem (as it&rsquo;s normally stated) is <em>completely</em> a <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">tree-falling-in-the-forest</a> problem: a debate about the normatively &ldquo;correct&rdquo; degree of credence which only seemed like an issue because any conclusions about what Sleeping Beauty &ldquo;should&rdquo; believe weren&rsquo;t paying their rent, were disconnected from any expectation of feedback from reality about how right they were.<a id=\"more\"></a></p>\n<p>It may seem either implausible or alarming that as fundamental a concept as probability can be the subject of such debates, but remember that the &ldquo;If a tree falls in the forest&hellip;&rdquo; argument only comes up <em>because</em> the understanding of &ldquo;sound&rdquo; as &ldquo;vibrations in the air&rdquo; and &ldquo;auditory processing in a brain&rdquo; coincide often enough that most people other than philosophers have better things to do than argue about which is more correct. Likewise, in situations that we actually encounter in real life where we must reason or act on incomplete information, long-run frequency is generally about the same as optimal decision-theoretic weighting. If you&rsquo;re given the question &ldquo;If you have a bag containing a white marble and two black marbles, and another bag containing two white marbles and a black marble, and you pick a bag at random and pick a marble out of it at random and it&rsquo;s white, what&rsquo;s the probability that you chose the second bag?&rdquo; then you can just answer it as given, without worrying about specifying a payoff structure, because no matter how you reformulate it in terms of bets and payoffs, if your decision-theoretic reasoning talks about probabilities at all then there&rsquo;s only going to be one sane probability you can put into it. You can assume that answers to non-esoteric probability problems will be able to pay their rent if they are called upon to do so, and so you can do plenty within pure probability theory long before you need your reasoning to generate any decisions.</p>\n<p>But when you start getting into problems where there may be multiple copies of you and you don&rsquo;t know how their responses will be aggregated &mdash; or, more generally, where you may or may not be scored on your probability estimate multiple times or may not be scored at all, or when you don&rsquo;t know how it&rsquo;s being scored, or when there may be other agents following reasoning correlated with but not necessarily identical to yours &mdash; then I think talking too much about &ldquo;probability&rdquo; directly will cause different people to be <em>solving different problems</em>, given the different ways they will implicitly imagine being scored on their answers so that the question of &ldquo;What <em>subjective</em> probability <em>should</em> be assigned to x?&rdquo; has any normatively correct answer. Here are a few ways that the Sleeping Beauty problem can be framed as a decision problem explicitly:</p>\n<blockquote>\n<p><em>Each interview consists of Sleeping Beauty guessing whether the coin came up heads or tails, and being given a dollar if she was correct. After the experiment, she will keep all of her aggregate winnings.</em></p>\n</blockquote>\n<p>In this case, intending to guess heads has an expected value of $.50 (because if the coin came up heads, she&rsquo;ll get $1, and if it came up tails, she&rsquo;ll get nothing), and intending to guess tails has an expected value of $1 (because if the coin came up heads, she&rsquo;ll get nothing, and if it came up tails, she&rsquo;ll get $2). So she should intend to guess tails.</p>\n<blockquote>\n<p><em>Each interview consists of Sleeping Beauty guessing whether the coin came up heads or tails. After the experiment, she will be given a dollar if she was correct on Monday.</em></p>\n</blockquote>\n<p>In this case, she should clearly be indifferent (which you can call &ldquo;.5 credence&rdquo; if you&rsquo;d like, but it seems a bit unnecessary).</p>\n<blockquote>\n<p><em>Each interview consists of Sleeping Beauty being told whether the coin landed on heads or tails, followed by one question, &ldquo;How surprised are you to hear that?&rdquo; Should Sleeping Beauty be more surprised to learn that the coin landed on heads than that it landed on tails?</em></p>\n</blockquote>\n<p>I would say no; this seems like a case where the simple probability-theoretic reasoning applies. Before the experiment, Sleeping Beauty knows that a coin is going to be flipped, and she knows it&rsquo;s a fair coin, and going to sleep and waking up isn&rsquo;t going to change anything she knows about it, so she should not be even slightly surprised one way or the other. (I&rsquo;m pretty sure that surprisingness has something to do with <a href=\"http://en.wikipedia.org/wiki/Likelihood_function\">likelihood</a>. I may write a separate post on that, but for now:&nbsp;after finding out whether the coin <em>did</em> come up heads or tails, the relevant question is not &ldquo;What is the probability that the coin came up {heads,tails} given that I remember going to sleep on Sunday and waking up today?&rdquo;, but &ldquo;What is the probability that I&rsquo;d remember going to sleep on Sunday and waking up today given that the coin came up {heads,tails}?&rdquo;,&nbsp;in which case either outcome should be equally surprising, in which case neither outcome should be surprising at all.)</p>\n<blockquote>\n<p><em>Each interview consists of one question, &ldquo;What is the limit of the frequency of heads as the number of repetitions of this experiment goes to infinity?&rdquo;</em></p>\n</blockquote>\n<p>Here of course the right answer is &ldquo;.5, and I hope that&rsquo;s just a hypothetical&hellip;&rdquo;</p>\n<blockquote>\n<p><em>Each interview consists of one question, &ldquo;What is your credence now for the proposition that our coin landed heads?&rdquo;, and the answer given will be scored according to a </em><a href=\"http://yudkowsky.net/rational/technical\"><em>logarithmic scoring rule</em></a><em>, with the aggregate result corresponding to the number of utilons (converted to dollars, let&rsquo;s say) she will be penalized after the experiment.</em></p>\n</blockquote>\n<p>In this case it is optimal to bet 1/3 that the coin came up heads, 2/3 that it came up tails:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td style=\"font-weight: bold\" align=\"right\">Bet on heads:</td>\n<td style=\"font-weight: bold\" colspan=\"2\" align=\"center\">1/2</td>\n<td style=\"font-weight: bold\" colspan=\"2\" align=\"center\">1/3</td>\n</tr>\n<tr>\n<td style=\"font-weight: bold\" align=\"right\">Actual flip:</td>\n<td style=\"font-weight: bold\" align=\"center\">Heads</td>\n<td style=\"font-weight: bold\" align=\"center\">Tails</td>\n<td style=\"font-weight: bold\" align=\"center\">Heads</td>\n<td style=\"font-weight: bold\" align=\"center\">Tails</td>\n</tr>\n<tr>\n<td style=\"font-weight: bold\" align=\"right\">Monday:</td>\n<td>-1 bit</td>\n<td>-1 bit</td>\n<td>-1.585 bits</td>\n<td>-0.585 bits</td>\n</tr>\n<tr>\n<td style=\"font-weight: bold\" align=\"right\">Tuesday:</td>\n<td>n/a</td>\n<td>-1 bit</td>\n<td>n/a</td>\n<td>-0.585 bits</td>\n</tr>\n<tr>\n<td style=\"font-weight: bold\" align=\"right\">Total:</td>\n<td>-1 bit</td>\n<td>-2 bits</td>\n<td>-1.585 bits</td>\n<td>-1.17 bits</td>\n</tr>\n<tr>\n<td style=\"font-weight: bold\" align=\"right\">Expected:</td>\n<td colspan=\"2\" align=\"center\">-1.5 bits</td>\n<td colspan=\"2\" align=\"center\">-1.3775 bits</td>\n</tr>\n</tbody>\n</table>\n<p>(If you&rsquo;re not used to the logarithmic scoring rule enough to trust that 1/3 is better than every other option too, you can check this by graphing <a href=\"http://www.wolframalpha.com/input/?i=%28log_2+x+%2B+2+log_2+%281-x%29%29%2F2\">y = (log<sub>2</sub>x + 2 log<sub>2</sub>(1 - x))/2</a>, where x is the probability you assign to heads, and y is expected utility.)</p>\n<p>So I hope it is self-evident that reframing seemingly-paradoxical probability problems as decision problems generally makes them trivial, or at least agreeably solvable and non-paradoxical. What may be more controversial is that I claim that this is satisfactory not as a circumvention but as a <a href=\"/lw/of/dissolving_the_question/\">dissolution</a> of the question &ldquo;What probability should be assigned to x?&rdquo;, when you have a clear enough idea of <em>why</em> you&rsquo;re wondering about the &ldquo;probability.&rdquo; Can we really <a href=\"/lw/nu/taboo_your_words/\">taboo</a> concepts like &ldquo;probability&rdquo; and &ldquo;plausibility&rdquo; and &ldquo;credence&rdquo;? I should certainly hope so; judgments of probability had better be <em>about</em> something, and not just <a href=\"http://wiki.lesswrong.com/wiki/Rituals_of_cognition\">rituals of cognition</a> that we use because it seems like we&rsquo;re supposed to rather than because it <a href=\"http://wiki.lesswrong.com/wiki/Winning\">wins</a>.</p>\n<p>But when I try to replace &ldquo;probability&rdquo; with what I mean by it, and when I mean it in any normative sense &mdash; not, like, <a href=\"/lw/oj/probability_is_in_the_mind/\">out there in the territory</a>, but just &ldquo;normative&rdquo; by whatever standard says that assigning a fair coin flip a probability of .5 heads tends to be a better idea than assigning it a probability of .353289791 heads &mdash; then I always find myself talking about optimal bets or average experimental outcomes. Can that really be all there is to probability as degree of belief? Can&rsquo;t we enjoy, for its own sake, the experience of having maximally accurate beliefs given whatever information we already have, even in circumstances where we don&rsquo;t get to test it any further? Well, yes and no; if your belief is really <em>about</em> anything, then you&rsquo;ll be able to specify, at the very least, a ridiculous hypothetical experiment that would give you information about how correct you are, or a ridiculous hypothetical bet that would give you an incentive to optimally solve a more well-defined version of the problem. And if you&rsquo;re working with a problem where it&rsquo;s at all unclear how to do this, it is probably best to backtrack and ask what problem you&rsquo;re trying to solve, why you&rsquo;re asking the question in the first place. So when in doubt, ask for decisions rather than probabilities. In the end, the point (aside from signaling) of believing things is (1) to allow you to effectively optimize reality for the things you care about, and (2) to allow you to be surprised by some possible experiences and not others so you get feedback on how well you&rsquo;re doing. If a belief does not do either of those things, I&rsquo;d hesitate to call it a belief at all; yet that is what the original version of the Sleeping Beauty problem asks you to do.</p>\n<p>Now, it does seem to me that following the usual rules of probability theory (the ones that tend to generate optimal bets in that strange land where intergalactic superintelligences aren&rsquo;t regularly making copies of you and scientists aren&rsquo;t knocking you out and erasing your memory) tells Sleeping Beauty to assign .5 credence to the proposition that the coin landed on heads. Before the experiment has started, Sleeping Beauty <em>already knows</em> what she&rsquo;s going to experience &mdash;&nbsp;waking up and pondering probability &mdash;&nbsp;so if she doesn&rsquo;t <em>already</em> believe with 2/3 probability that the coin will land on tails (which would be a strange thing to believe about a fair coin), then she can&rsquo;t update to that after experiencing <a href=\"http://wiki.lesswrong.com/wiki/Conservation_of_expected_evidence\">what she already knew she was going to experience</a>. But in the original problem, when she is asked &ldquo;What is your credence now for the proposition that our coin landed heads?&rdquo;, a much better answer than &ldquo;.5&rdquo; is &ldquo;Why do you want to know?&rdquo;. If she knows how she&rsquo;s being graded, then there&rsquo;s an easy correct answer, <em>which isn&rsquo;t always .5</em>; if not, she will have to do her best to guess what type of answer the experimenters are looking for; and if she&rsquo;s not being graded at all, then she can say whatever the hell she wants (acceptable answers would include &ldquo;0.0001,&rdquo; &ldquo;3/2,&rdquo; and &ldquo;purple&rdquo;).</p>\n<p>I&rsquo;m not sure if there <em>is</em> more to it than that. Presumably the &ldquo;should&rdquo; in &ldquo;What subjective probability should I assign x?&rdquo; isn&rsquo;t a moral &ldquo;should,&rdquo; but more of an &ldquo;if-should&rdquo; (as in &ldquo;If you want x to happen, you should do y&rdquo;), and if the question itself seems confusing, that probably means that under the circumstances, the implied &ldquo;if&rdquo; part is ambiguous and needs to be made explicit. Is there some underlying true essence of probability that I&rsquo;m neglecting? I don&rsquo;t know, but I <em>am</em> pretty sure that even if there were one, it wouldn&rsquo;t necessarily be the thing we&rsquo;d care about knowing in these types of problems anyway. You want to make optimal use of the information available to you, but it has to be optimal <em>for</em> something.</p>\n<p>I think this principle should help to clarify other anthropic problems. For example, suppose Omega tells you that she just made an exact copy of you and everything around you, enough that the copy of you wouldn&rsquo;t be able to tell the difference, at least for a while. Before you have a chance to gather more information, what probability should you assign to the proposition that you yourself are the copy? The answer is non-obvious, given that there <a href=\"http://arxiv.org/abs/0905.1283\">already <em>is</em> a huge and potentially infinite number of copies of you</a>, and it&rsquo;s not clear how adding one more copy to the mix should affect your belief about how spread out you are over what worlds. On the other hand, if you&rsquo;re <a href=\"http://philsci-archive.pitt.edu/1036/\">Dr. Evil</a> and you&rsquo;re in your moon base preparing to fire your giant laser at Washington, DC when you get a phone call from Austin &ldquo;Omega&rdquo; Powers, and he tells you that he has made an exact replica of the moon base on exactly the spot at which the moon laser is aimed, complete with an identical copy of you (and an identical copy of your identical miniature clone) receiving the same phone call, and that its laser is trained on your original base on the moon, then the decision is a lot easier: hold off on firing your laser and gather more information or make other plans. Without talking about the &ldquo;probability&rdquo; that you are the original Dr. Evil or the copy or one of the potentially infinite Tegmark duplicates in other universes, we can simply look at the situation from the outside and see that if you do fire your laser then you&rsquo;ll blow both of yourselves up, and that if you don&rsquo;t fire your laser then you have some new competitors at worst and some new allies at best.</p>\n<p>So: in problems where you are making one judgment that may be evaluated more or less than one time, and where you won&rsquo;t have a chance to update between those evaluations (e.g. because your one judgment will be evaluated multiple times because there are multiple copies of you or your memory will be erased), just ask for decisions and leave probabilities out of it to whatever extent possible.</p>\n<p>In a followup post, I will generalize this point somewhat and demonstrate that it helps solve some problems that remain confusing even when they specify a payoff structure.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"L3NcKBNTvQaFXwv9u": 1, "PbShukhzpLsWpGXkM": 1, "bh7uxTTqmsQ8jZJdB": 1, "E8PHMuf7tsr8teXAe": 1, "NZB24aR9uHmDc5GcT": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gMXsyhPiEJbGerF6F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 105, "baseScore": 125, "extendedScore": null, "score": 0.000229, "legacy": true, "legacyId": "3984", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 126, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aQRKfzYnt3bFGgPKd", "5JDkW4MYXit2CquLs", "a7n8GdKiAZRX86T5A", "Mc6QcrsbH5NRXbCRX", "WBdvyyHLdxZSAMmoz", "f6ZLxEWaankRZ2Crv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-12T01:46:03.970Z", "modifiedAt": null, "url": null, "title": "Study shows existence of psychic powers.", "slug": "study-shows-existence-of-psychic-powers", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.390Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bentarm", "createdAt": "2009-03-05T17:59:17.163Z", "isAdmin": false, "displayName": "bentarm"}, "userId": "xdmTZWK4DzchxkyQC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ouvbnJ5BtQ5oHnHrB/study-shows-existence-of-psychic-powers", "pageUrlRelative": "/posts/ouvbnJ5BtQ5oHnHrB/study-shows-existence-of-psychic-powers", "linkUrl": "https://www.lesswrong.com/posts/ouvbnJ5BtQ5oHnHrB/study-shows-existence-of-psychic-powers", "postedAtFormatted": "Friday, November 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Study%20shows%20existence%20of%20psychic%20powers.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStudy%20shows%20existence%20of%20psychic%20powers.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FouvbnJ5BtQ5oHnHrB%2Fstudy-shows-existence-of-psychic-powers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Study%20shows%20existence%20of%20psychic%20powers.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FouvbnJ5BtQ5oHnHrB%2Fstudy-shows-existence-of-psychic-powers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FouvbnJ5BtQ5oHnHrB%2Fstudy-shows-existence-of-psychic-powers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 215, "htmlBody": "<p>According to the <a href=\"http://www.newscientist.com/article/dn19712-evidence-that-we-can-see-the-future-to-be-published.html\">New Scientist</a>, Daryl Bern has a paper to appear in&nbsp;<span style=\"padding: 0px; margin: 0px; border: 0px initial initial;\"><span style=\"border-style: initial; border-color: initial;\"><a href=\"http://www.apa.org/pubs/journals/psp/\">Journal of Personality and Social Psycholo</a></span><a href=\"http://www.apa.org/pubs/journals/psp/\">gy,</a> which claims that the participants in psychological experiments are able to predict the future. A <a href=\"http://www.dbem.ws/FeelingFuture.pdf\">preprint</a> of this paper is available online. Here's a quote from the New Scientist article:</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px;\">\n<blockquote>\n<p class=\"infuse\" style=\"margin-top: 0px; margin-right: 20px; margin-bottom: 20px; margin-left: 10px; font-size: 1em; line-height: 18px; padding: 0px; border: 0px initial initial;\">In one experiment, students were shown a list of words and then asked to recall words from it, after which they were told to type words that were randomly selected from the same list. Spookily, the students were better at recalling words that they would later type.</p>\n<p class=\"infuse\" style=\"margin-top: 0px; margin-right: 20px; margin-bottom: 20px; margin-left: 10px; font-size: 1em; line-height: 18px; padding: 0px; border: 0px initial initial;\">In another study, Bem adapted research on \"priming\" &ndash; the effect of a subliminally presented word on a person's response to an image. For instance, if someone is momentarily flashed the word \"ugly\", it will take them longer to decide that a picture of a kitten is pleasant than if \"beautiful\" had been flashed. Running the experiment back-to-front, Bem found that the priming effect seemed to work backwards in time as well as forwards.</p>\n</blockquote>\n</span></p>\n<p>Question: even assuming the methodology is sound, given experimenter bias, publication bias and your priors on the existence of psi, what sort of p-values would you need to see in that paper in order to believe with, say, 50% probability that the effect measured is real?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ouvbnJ5BtQ5oHnHrB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 6.446391222841518e-07, "legacy": true, "legacyId": "3985", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-12T11:04:14.087Z", "modifiedAt": null, "url": null, "title": "Outreach opportunity", "slug": "outreach-opportunity-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bswiZf3W9TmGvTAuM/outreach-opportunity-0", "pageUrlRelative": "/posts/bswiZf3W9TmGvTAuM/outreach-opportunity-0", "linkUrl": "https://www.lesswrong.com/posts/bswiZf3W9TmGvTAuM/outreach-opportunity-0", "postedAtFormatted": "Friday, November 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Outreach%20opportunity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOutreach%20opportunity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbswiZf3W9TmGvTAuM%2Foutreach-opportunity-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Outreach%20opportunity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbswiZf3W9TmGvTAuM%2Foutreach-opportunity-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbswiZf3W9TmGvTAuM%2Foutreach-opportunity-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<p>Ars Technica are holding a competition for people to make a science video up to 3 minutes long \"to explain a scientific concept in terms that a high school science class would not only understand, but actually be interested in watching\".&nbsp;&nbsp;Deadline is December 25.&nbsp;&nbsp;More details <a href=\"http://arstechnica.com/science/news/2010/11/arstv-launch-and-our-diy-science-contest.ars\">here</a>.</p>\n<p>Anyone want to have a go at Bayes' theorem? Cognitive bias? Defeating death? Invisible purple dragons?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bswiZf3W9TmGvTAuM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "3987", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-12T11:07:29.504Z", "modifiedAt": null, "url": null, "title": "Outreach opportunity", "slug": "outreach-opportunity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:31.443Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8KQ6K6HPrvMaj8SkE/outreach-opportunity", "pageUrlRelative": "/posts/8KQ6K6HPrvMaj8SkE/outreach-opportunity", "linkUrl": "https://www.lesswrong.com/posts/8KQ6K6HPrvMaj8SkE/outreach-opportunity", "postedAtFormatted": "Friday, November 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Outreach%20opportunity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOutreach%20opportunity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8KQ6K6HPrvMaj8SkE%2Foutreach-opportunity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Outreach%20opportunity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8KQ6K6HPrvMaj8SkE%2Foutreach-opportunity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8KQ6K6HPrvMaj8SkE%2Foutreach-opportunity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<p>Ars Technica are holding a competition for people to make a science video up to 3 minutes long \"to explain a scientific concept in terms that a high school science class would not only understand, but actually be interested in watching\". Prizes in three categories: biology, physics, and mathematics. Deadline is December 25.&nbsp;&nbsp;More details&nbsp;<a style=\"color: #6a8a6b; text-decoration: underline; \" href=\"http://arstechnica.com/science/news/2010/11/arstv-launch-and-our-diy-science-contest.ars\">here</a>.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">Anyone want to have a go at Bayes' theorem? Cognitive bias? Defeating death? Invisible purple dragons?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8KQ6K6HPrvMaj8SkE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 18, "extendedScore": null, "score": 6.447747716080952e-07, "legacy": true, "legacyId": "3988", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-12T13:54:32.020Z", "modifiedAt": null, "url": null, "title": "Rational responses to potential home invasion threat?", "slug": "rational-responses-to-potential-home-invasion-threat", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.498Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AdeleneDawner", "createdAt": "2009-04-28T14:40:00.131Z", "isAdmin": false, "displayName": "AdeleneDawner"}, "userId": "MeSREm4SMRGxeQ8X3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8BsE3BojYT8J2koyy/rational-responses-to-potential-home-invasion-threat", "pageUrlRelative": "/posts/8BsE3BojYT8J2koyy/rational-responses-to-potential-home-invasion-threat", "linkUrl": "https://www.lesswrong.com/posts/8BsE3BojYT8J2koyy/rational-responses-to-potential-home-invasion-threat", "postedAtFormatted": "Friday, November 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20responses%20to%20potential%20home%20invasion%20threat%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20responses%20to%20potential%20home%20invasion%20threat%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8BsE3BojYT8J2koyy%2Frational-responses-to-potential-home-invasion-threat%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20responses%20to%20potential%20home%20invasion%20threat%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8BsE3BojYT8J2koyy%2Frational-responses-to-potential-home-invasion-threat", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8BsE3BojYT8J2koyy%2Frational-responses-to-potential-home-invasion-threat", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 373, "htmlBody": "<p>About three hours ago - in the very early morning, pre-dawn - someone knocked on my window to get my attention and made a lewd proposition. Less than two weeks ago, someone - probably the same person; definitely someone with a similar voice and build - woke me up by whispering 'open the door' until I looked out the window. I am, needless to say, not amused.</p>\n<p>Since the first incident, I've been leaving my porch light on, and I've had a webcam sitting prominently in the window. The webcam has been commented on by both of the people who know me and would have an opportunity to do so, so I expected that it would be a reasonable deterrent, but apparently this guy is very stupid, very desperate, or both.</p>\n<p>I called the police both times, and they responded promptly, but didn't see anyone walking around near my apartment. This leads me to believe that I'm being harassed by a nearby neighbor.</p>\n<p>The webcam was not on during the second incident, but it will be on nightly from now on. I also intend to add a light in the window near my bed - I didn't get a good look at the guy, even though he was right there and not making any apparent attempt to hide, because he was between me and the porch light.</p>\n<p>I'd appreciate any other practical suggestions that anyone might have, bearing in mind that I'm in an apartment and can't make many changes to the building itself. Also, I was already working on buying a house before the first incident even happened, so suggestions that I move aren't useful - I'm already working on that, thank goodness.</p>\n<p>(The chances of me having trouble with this individual in any situation other than a home invasion seem pretty small - I don't leave the house often, and not on a regular schedule at all, plus I don't drive so I generally have a friend in a car watching me to and from the door, so the usually more risky situation of getting in and out of the house isn't an issue for the most part. I will be extra-careful about getting my mail and thoughtful about when I leave to do my laundry.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8BsE3BojYT8J2koyy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 12, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "3989", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-12T22:17:37.909Z", "modifiedAt": null, "url": null, "title": "New genetic evidence of positive selection for Ashkenazi diseases", "slug": "new-genetic-evidence-of-positive-selection-for-ashkenazi", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.168Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Genesplice", "createdAt": "2010-11-12T22:17:00.648Z", "isAdmin": false, "displayName": "Genesplice"}, "userId": "bou9bRQag4Gz8kXRs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fgxvvttvn6DWEJcLR/new-genetic-evidence-of-positive-selection-for-ashkenazi", "pageUrlRelative": "/posts/fgxvvttvn6DWEJcLR/new-genetic-evidence-of-positive-selection-for-ashkenazi", "linkUrl": "https://www.lesswrong.com/posts/fgxvvttvn6DWEJcLR/new-genetic-evidence-of-positive-selection-for-ashkenazi", "postedAtFormatted": "Friday, November 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20genetic%20evidence%20of%20positive%20selection%20for%20Ashkenazi%20diseases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20genetic%20evidence%20of%20positive%20selection%20for%20Ashkenazi%20diseases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffgxvvttvn6DWEJcLR%2Fnew-genetic-evidence-of-positive-selection-for-ashkenazi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20genetic%20evidence%20of%20positive%20selection%20for%20Ashkenazi%20diseases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffgxvvttvn6DWEJcLR%2Fnew-genetic-evidence-of-positive-selection-for-ashkenazi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffgxvvttvn6DWEJcLR%2Fnew-genetic-evidence-of-positive-selection-for-ashkenazi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>\n<div style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: #ffffff; padding: 0.5em; margin: 8px;\">\n<p>&nbsp;</p>\n<div style=\"margin-bottom: 1em;\" dir=\"ltr\"><a href=\"http://johnhawks.net/weblog/reviews/genomics/selection/bray-ashkenazi-2010.html\" target=\"_blank\">http://johnhawks.net/weblog/reviews/genomics/selection/bray-ashkenazi-2010.html</a>\n<div style=\"margin-bottom: 1em;\">.</div>\n<div><br /></div>\n</div>\n<p>&nbsp;</p>\n</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fgxvvttvn6DWEJcLR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 6.449367542161596e-07, "legacy": true, "legacyId": "3991", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-12T22:19:50.897Z", "modifiedAt": null, "url": null, "title": "Diplomacy as a Game Theory Laboratory", "slug": "diplomacy-as-a-game-theory-laboratory", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:39.017Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jkf2YjuH8Z2E7hKBA/diplomacy-as-a-game-theory-laboratory", "pageUrlRelative": "/posts/jkf2YjuH8Z2E7hKBA/diplomacy-as-a-game-theory-laboratory", "linkUrl": "https://www.lesswrong.com/posts/jkf2YjuH8Z2E7hKBA/diplomacy-as-a-game-theory-laboratory", "postedAtFormatted": "Friday, November 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Diplomacy%20as%20a%20Game%20Theory%20Laboratory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADiplomacy%20as%20a%20Game%20Theory%20Laboratory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjkf2YjuH8Z2E7hKBA%2Fdiplomacy-as-a-game-theory-laboratory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Diplomacy%20as%20a%20Game%20Theory%20Laboratory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjkf2YjuH8Z2E7hKBA%2Fdiplomacy-as-a-game-theory-laboratory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjkf2YjuH8Z2E7hKBA%2Fdiplomacy-as-a-game-theory-laboratory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3600, "htmlBody": "<p style=\"margin-bottom: 0in;\">Game theory. You've studied <a href=\"/lw/tn/the_true_prisoners_dilemma/\">the posts</a>, you've laughed at <a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=1899\">the comics</a>, you've heard the <a href=\"http://www.raikoth.net/Stuff/niceguys.wma\">music</a><sup>1</sup>. But the best way to make it <a href=\"/lw/la/truly_part_of_you/\">Truly Part Of You</a> is to play a genuine game, and I have yet to find any more effective than Diplomacy.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Diplomacy is a board game for seven people played on a map of WWI Europe. The goal is to capture as many strategic provinces (\"supply centers\") as possible; eighteen are needed to win. But each player's country starts off with the same sized army, and there is no luck or opportunity for especially clever tactics. The most common way to defeat an enemy is to form coalitions with other players. But your enemies will also be trying to form coalitions, and the most profitable move is often to be a \"double agent\", stringing both countries along as long as you can. All game moves are written in secret and revealed at the same time and there are no enforcement mechanisms, so alliances, despite their central importance, aren't always worth the paper they're printed on.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The conditions of Diplomacy - competition for scarce resources, rational self-interested actors, importance of coalitions, lack of external enforcement mechanisms - mirror the conditions of game theoretic situations like the Prisoner's Dilemma (and the conditions of most of human evolution!) and so make a surprisingly powerful laboratory for analyzing concepts like trust, friendship, government, and even religion.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Over the past few months, I've played two online games of Diplomacy. One I won through a particularly interesting method; the other I lost quite badly, but with an unusual consolation. This post is based on notes I took during the games about relevant game theoretic situations. You don't need to know the rules of Diplomacy to understand the post, but if you want a look <a href=\"http://www.wizards.com/avalonhill/rules/diplomacy.pdf\">you can find them here</a>.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><a id=\"more\"></a></p>\n<p style=\"margin-bottom: 0in;\"><strong>Study One: The Prisoner's Dilemma</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">The <a href=\"http://wiki.lesswrong.com/wiki/Prisoner%27s_Dilemma\">Prisoner's Dilemma</a> is a classic case in game theory in which two players must decide whether or not to cooperate for a common goal. If both players cooperate, they both do better than if both defect, but one player can win big by defecting when the other cooperates. This situation is at the heart of Diplomacy.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Germany and France have agreed to ally against Britain. Both countries have demilitarized their mutual border, and are concentrating all of their forces to the north, where they take province after province of British territory.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">But Britain is fighting back; not successfully, but every inch of territory is hard-won. France is doing well for itself and has captured a few British cities, but it could be doing better. The French player thinks to eirself: I could either continue battering against the heavily defended British lines, or I could secretly ally with Britain, stab Germany in the back, and waltz in along our undefended mutual border before the Germans even know what hit them. Instead of fighting for each inch of British land, I could be having dinner in Berlin within a week.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Meanwhile, in Berlin, the German player is looking towards France's temptingly undefended border and thinking the exact same thing.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">If both France and Germany are honorable, and if both countries know the other is honorable, the two of them can continue fighting Britain with a two-to-one numerical advantage and probably divide England's lucrative territory among the two of them.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">If Germany is naively trusting and France is a dishonest backstabber, then France can get obscene rewards by rolling over Germany while the Kaiser's armies are tied up on the fields of England.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">If both countries are suspicious of the other, or if both countries try to backstab each other simultaneously, then they will both divert forces away from the war on England to guard their mutual border. They will not gain any territory in England, and they will not gain any territory along their border. They've not only stabbed each other in the back, they've shot themselves in the foot.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Study Two: Parfit's Hitch-Hiker</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">The wiki describes Derek Parfit's famous <a href=\"http://wiki.lesswrong.com/wiki/Parfit%27s_hitchhiker\">hitchhiker problem</a> as:</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<blockquote>\n<p style=\"margin-bottom: 0in;\">Suppose you're out in the desert, running out of water, and soon to die - when someone in a motor vehicle drives up next to you. Furthermore, the driver of the motor vehicle is a perfectly selfish ideal game-theoretic agent, and even further, so are you; and what's more, the driver is Paul Ekman, who's really, really good at reading facial microexpressions. The driver says, \"Well, I'll convey you to town if it's in my interest to do so - so will you give me $100 from an ATM when we reach town?\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Now of course you wish you could answer \"Yes\", but as an ideal game theorist yourself, you realize that, once you actually reachtown, you'll have no further motive to pay off the driver. \"Yes,\" you say. \"You're lying,\" says the driver, and drives off leaving you to die.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n</blockquote>\n<p style=\"margin-bottom: 0in;\">The so-called <a href=\"http://en.wikipedia.org/wiki/Key_Lepanto#Key_Lepanto\">Key Lepanto opening</a> is one of the more interesting opening strategies in Diplomacy, and one that requires guts of steel to pull off. It goes like this: Italy and Austria decide to ally against Turkey. This is common enough, and hindered by the fact that Turkey is probably expecting it and Italy's kind of far away from Turkey anyway.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">So Italy and Austria do something unexpected. Italy swears loudly and publicly that ey's allied with Austria. Then, the first turn, Italy moves deep into undefended Austrian territory! Austria is incensed, and curses loud and long at Italy's betrayal and at eir own stupidity for leaving the frontier unguarded. Turkey laughs and leaves the two of them to their war when - boom - Austria and Italy launch a coordinated attack against Turkey from Italy's base deep in Austrian territory. The confused Turkey has no chance to organize a resistance before combined Italo-Austrian forces take Constantinople.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">It's frequently a successful strategy, especially for Italy. You know what else is a successful strategy for Italy? Doing this up to the point where they take over lots of Austrian territory, forgetting the part where it was all just a ploy, and then ending up in control of lots of Austrian territory, after which they can fight Turkey at their leisure.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">It's very much in Italy's advantage to play a Key Lepanto opening, and they may beg the Austrian player to go for it, saying correctly that it would benefit both of them. But the Austrian player very often refuses, telling Italy that ey would have no incentive not to just keep the conquered territory.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">This problem resembles the Hitchhiker: Italy is the lost man, and Austria is the driver. Italy really wants Austria to help em play the awesome Key Lepanto opening, but Austria knows that ey would have no incentive not to break his promise once Austria's given him the help he needs. As a result, neither country gets what they want. The Key Lepanto opening is played only rarely, and this is one of the reasons.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Study Three: Enforceable Side Contracts</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">The Prisoner's Dilemma is nontrivial because there's no enforcement mechanism. In the presence of an enforcement mechanism, it becomes much simpler. Say two mobsters are about to be arrested, and expect to be put in a Prisoner's Dilemma type situation. They approach the mob boss with a contract with both of their names on it, saying that they have both agreed that if either of them testifies against the other, the mob boss should send his goons to shoot the rat.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">For many payoff matrices, signing this contract will be a no-brainer. It ensures your opponent will cooperate at the relatively low cost of forcing you to cooperate yourself, and almost guarantees you safe passage into the desirable (C,C) square. Not only does it prevent your opponent doesn't defect out of sheer greed, but it prevents your opponent from worrying that you're going to defect and then defecting emself to save emself from being the chump.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The game of Diplomacy I won, I won through an enforceable side contract (which lost me a friend and got me some accusations of cheating, but this is <a href=\"http://cf.geekdo-images.com/images/pic536690.jpg\">par for the course</a> for a good Diplomacy game). I was Britain; my friend H was France. H and I knew each other from an medieval times role-playing game, in which we both held land and money. The medieval kingdom of this game had a law on the books that any oath witnessed by a noble was binding on both parties and would be enforced by the king. So H and I went into our role-playing game and swore an oath before a cooperative noble, declaring that we would both aid each other in a permanent alliance in Diplomacy, or else all our in-game lands and titles would be forfeit.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">A lot of people made fun of me for this, including H, but in my defense I did end up winning the game. H and I were able to do things that would otherwise have been impossible; for example, in order to convince our enemy Germany that we were at war, I took over the French city of Brest. Normally, this would be almost impossible for two allies to coordinate, even as a red herring, for exactly the reasons listed in the Hitchhiker problem above. Since the two of us were able to trust each other absolutely, this otherwise difficult maneuver became easy.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">One of the advantages to strong central government is that it provides an enforcement mechanism for contracts, which benefits all parties.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Study Four: Religion As Enforcement</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">Religion is a special case of the enforceable side-contract in which God is doing the enforcing. God doesn't have to exist for this to work; as long as at least one party believes He does, the threat of punishment will be credible. The advantage of being able to easily make enforceable side contracts even in the absence of social authority may be one reason religion became so popular, and if humans do turn out to have a genetic tendency toward belief, the side contracts might have provided part of the survival advantage that spread the gene.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">In a Youngstown Variant game (like Diplomacy, but with Eurasia instead of just Europe), I was playing Italy and after colonizing Africa was trying to juggle my forces around to defend borders with Germany, France, Turkey, and India.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">India was played by my friend A, who I sometimes have philosophical discussions with and who I knew to be an arch-conservative religion-and-family-values type. I decided to try something which, as far as I know, no one's ever tried in a Diplomacy game before. \"Do you swear in the name of God and your sacred honor that you won't attack me?\" I asked.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">\"Yes,\" said A, and I knew he meant it, because he takes that sort of thing really seriously. I don't know if he thought he would literally go to Hell if he broke his oath, but I'm pretty sure he wasn't willing to risk it over a board game. So I demilitarized my border with India. I concentrated my forces to the west, he concentrated them to the east, and both avoided a costly stalemate in the Indian Ocean and had more forces to send elsewhere. In the future, I will seek out A for alliances more often, since I have extra reason to believe he won't betray me; this will put A in an unusually strong position.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">This is not a unique advantage of religion; any strongly held philosophy that trumps self-interest would do. I would have made the same deal with Alicorn, who has stated loudly and publicly that she is a deontologist who has a deep personal aversion to lying<sup>2</sup>. I would have made it with Eliezer, who has a consequentialist morality but, on account of the consequences, has said he would not break an oath even for the sake of saving the world.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">But I only trust Alicorn and Eliezer because I've discussed morality with both of them in a situation where they had no incentive to lie; it was only in the very unusual conditions of Less Wrong that they could send such a signal believably. Religion is a much easier signal to send and receive without being a moral philosopher.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Study Five: Excuses as Deviations from a Rule</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">My previous post,<a href=\"/lw/24o/eight_short_studies_on_excuses/\"> Eight Short Studies on Excuses</a>, was inspired by a maneuver I pulled during a Diplomacy game.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I was Italy, and Turkey and I had formed a mutual alliance against Austria. As part of the alliance, we had decided not to fight over who got the lucrative neutral territories in between our empires. I would get Egypt, Turkey would get Greece and Yemen, and we would avoid the resource drain of fighting each other for them so we could both concentrate on Austria.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Both Turkey and I would have liked to grab the centers that had been promised to the other. But both Turkey and I knew that maintaining the general rule of alliance between us was higher utility than getting one extra territory. BUT both Turkey and I knew that the other would be loathe to break off the alliance between just because their partner had committed one little infraction. BUT both Turkey and I knew that we would have to do exactly that, or else our ally would have a carte blanche to violate whatever terms of the alliance they wanted.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Then India (from whom I had not yet extracted his oath) made a move towards Yemen, threatening to take it from both of us. I responded by moving a navy to Yemen, supposedly to see off the Indian menace. I then messaged Turkey, saying that although I still respected the terms of our alliance, he was clearly too weak to keep Yemen out of Indian hands, so I would be fortifying it for him, and I hoped he would have the maturity to see this as a mutually beneficial move to prevent Indian expansionism, and not get too hung up on the exact terms of our alliance.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The gambit worked: Turkey decided that maintaining our alliance was more important than keeping Yemen, and that because of the trouble with India my conquest of Yemen was not indicative of a general pattern of alliance-breaking that needed to be punished.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I can't claim total victory here: several years later, when the threat of Austria had disappeared, Turkey betrayed me and captured half my empire, partly because of my actions in Yemen.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Study Six: For the Sake of Revenge</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">This comes from the book Game Theory at Work:</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<blockquote>\n<p style=\"margin-bottom: 0in;\">Consider the emotion of revenge. At its core, revenge means hurting someone who has harmed you, even if you would be better off leaving him alone. Revenge is an irrational desire to harm others who have injured our loved ones or us.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">To see the benefit of being known as vengeful, consider a small community living in prehistoric times. Imagine that a group of raiders stole food from this community. A rational community would hunt down the raiders only if the cost of doing so was not too high. A vengence-endowed community would hunt down the raiders regardless of the cost. Since the raiders would rather go after the rational community, being perceived as vengeful provides you with protection and therefore confers an evolutionary advantage.</p>\n</blockquote>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I play Diplomacy often against the same people, so I decided I needed to cultivate a reputation for vengefulness. And by \"decided to cultivate a reputation for vengefulness\", I mean \"Turkey betrayed me and I was filled with the burning rage of a thousand suns\".</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">So my drive for revenge was mostly emotional instead of rational. But what I didn't do was suppress my anger, the way people are always telling you. Suppressing anger is a useful strategy for one-shot games, but in an iterated game, getting a reputation for anger is often more valuable than behaving in your immediate rational self-interest.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">So I decided to throw the game to Germany, Turkey's biggest rival. I moved my forces away from the Italian-German border and invited Germany to take over my territory. At the same time, I used my remaining forces supporting German attacks against Turkey. The Austrians, who had been dealing with Turkey's betrayals even longer than I had, happily joined in. With our help, German forces scored several resounding victories against Turkey and pushed it back from near the top of the game down to a distant third.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Around the same time, Germany's other enemy France also betrayed me. So I told France I was throwing the game to Germany to punish him. No point in missing a perfectly good opportunity to cultivate a reputation for vengefulness.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">If I had done the rational thing and excused Turkey's betrayal because it was in my self-interest to cut my losses, I could have had a mediocre end game, and Turkey's player would have happily betrayed me the next game as soon as he saw any advantage in doing so. Instead, I'm doing very poorly in the end game, but Turkey - and everyone else - will be very wary about betraying me next time around.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Study Seven: In-Group Bias as a Schelling Point</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">I made the mistake of moderating a game of Diplomacy at the SIAI House, which turned into one of the worst I've ever seen. The players were five SIAI Visiting Fellows and two of my non-SIAI friends who happened to be in the area.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Jasen came up with the idea of an alliance of the five SIAI players against my two friends. Although a few of the Fellows vacillated back and forth and defected a few times, he was generally able to keep the loyalty of the five Fellows until my two friends had been eliminated from the game relatively early on. Although normally the game would have continued until one of the Fellows managed to dominate the others, it was already very late and we called it a night at that point.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">It's easy to explain what happened as an irrational in-group bias, or as \"loyalty\" or \"patriotism\" among the SIAI folk. Jasen himself explained it as a desire to prove that SIAI people were especially cooperative and especially good at game theory, which I suppose worked. But there's another, completely theoretical perspective from which to view the SIAI Alliance.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Imagine you are on a lifeboat with nine other people, and determine that one of the ten of you must be killed and eaten to provide sustenance to the others. You are all ready to draw lots to decide who is dinner when you shout out \"Hey, instead of this whole drawing lots thing, let's kill and eat Bob!\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">If your fellow castaways are rational agents, they might just agree. If they go with lots, each has a 10% chance of ending up dinner. If everyone just agrees on Bob, then everyone has a 0% chance of ending up dinner (except poor Bob). Nine out of ten people are better off, and nine out of ten of you vote to adopt the new plan. Whether your lifeboat decides things by majority vote or by physical violence, it doesn't look good for Bob.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">But imagine a week later, you still haven't been rescued, and the whole situation repeats. If everyone lets you repeat your action of calling out a name, there's a 1/9 chance it'll be eir name - no better than drawing lots. In fact, since you're very unlikely to call out your <em>own </em>name, it's more of a 1/8 chance - <em>worse</em> than just drawing lots. So everyone would like to be the one who calls out the name, and as soon as the lots are taken out, everyone shouts \"Hey, instead of the whole drawing lots thing, let's kill and eat X!\" where X is a different person for each of the nine castaways. This is utterly useless, and you probably end up just drawing lots.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">But suppose eight of the nine of you are blond, and one is a brunette. The brunette is now a Schelling point. If you choose to kill and eat the brunette, there's a pretty good chance all of your blond friends will do the same, even if none of you had a pre-existing prejudice against brunettes. Therefore, all eight of you shout out \"Let's kill and eat the brunette!\", since this is safer than drawing lots. Your lifeboat has invented in-group bias from rational principles.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Such alliances are equally attractive in Diplomacy. When the five SIAI Fellows allied against my two friends, they ensured there was a five-against-two alliance with themselves on the winning side, and successfully reduced the gameboard from six opponents to four. Although they could have done this with anyone (eg Jasen could have selected two other Fellows and my two friends, and forged an equivalent coalition of five), Jasen would have been at risk of five other people having the same idea and excluding him. By choosing a natural and obvious division in which he was on the majority, Jasen avoided this risk.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Rationalist Diplomacy</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">I'm interested in seeing what a Diplomacy game between Less Wrongers looks like. I'm willing to moderate. <del>The first seven people to sign up get places (don't sign up if you don't expect to have enough time for about two or three turns/week), and the next few can be alternates. Doesn't matter if you've ever played before as long as you read the rules above and think you understand them.</del> <em>(We already have seven people. See the post in Discussion. If many more sign up, someone else may want to moderate a second game).</em></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong>Footnotes</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br /></strong></p>\n<p style=\"margin-bottom: 0in;\">1: Source: \"Nice Guys Finish First\" in the Frameshift album <em>Unweaving the Rainbow</em>.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">2. Alicorn wishes me to note that she considers anyone playing a Diplomacy game&nbsp; without prior out-of-game-context agreements secured to have waived eir right to complete honesty from her, but the general principle still stands.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 3, "b8FHrKqyXuYGWc6vn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jkf2YjuH8Z2E7hKBA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 68, "extendedScore": null, "score": 0.00012, "legacy": true, "legacyId": "3990", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 68, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"margin-bottom: 0in;\">Game theory. You've studied <a href=\"/lw/tn/the_true_prisoners_dilemma/\">the posts</a>, you've laughed at <a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=1899\">the comics</a>, you've heard the <a href=\"http://www.raikoth.net/Stuff/niceguys.wma\">music</a><sup>1</sup>. But the best way to make it <a href=\"/lw/la/truly_part_of_you/\">Truly Part Of You</a> is to play a genuine game, and I have yet to find any more effective than Diplomacy.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Diplomacy is a board game for seven people played on a map of WWI Europe. The goal is to capture as many strategic provinces (\"supply centers\") as possible; eighteen are needed to win. But each player's country starts off with the same sized army, and there is no luck or opportunity for especially clever tactics. The most common way to defeat an enemy is to form coalitions with other players. But your enemies will also be trying to form coalitions, and the most profitable move is often to be a \"double agent\", stringing both countries along as long as you can. All game moves are written in secret and revealed at the same time and there are no enforcement mechanisms, so alliances, despite their central importance, aren't always worth the paper they're printed on.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The conditions of Diplomacy - competition for scarce resources, rational self-interested actors, importance of coalitions, lack of external enforcement mechanisms - mirror the conditions of game theoretic situations like the Prisoner's Dilemma (and the conditions of most of human evolution!) and so make a surprisingly powerful laboratory for analyzing concepts like trust, friendship, government, and even religion.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Over the past few months, I've played two online games of Diplomacy. One I won through a particularly interesting method; the other I lost quite badly, but with an unusual consolation. This post is based on notes I took during the games about relevant game theoretic situations. You don't need to know the rules of Diplomacy to understand the post, but if you want a look <a href=\"http://www.wizards.com/avalonhill/rules/diplomacy.pdf\">you can find them here</a>.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><a id=\"more\"></a></p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Study_One__The_Prisoner_s_Dilemma\">Study One: The Prisoner's Dilemma</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">The <a href=\"http://wiki.lesswrong.com/wiki/Prisoner%27s_Dilemma\">Prisoner's Dilemma</a> is a classic case in game theory in which two players must decide whether or not to cooperate for a common goal. If both players cooperate, they both do better than if both defect, but one player can win big by defecting when the other cooperates. This situation is at the heart of Diplomacy.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Germany and France have agreed to ally against Britain. Both countries have demilitarized their mutual border, and are concentrating all of their forces to the north, where they take province after province of British territory.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">But Britain is fighting back; not successfully, but every inch of territory is hard-won. France is doing well for itself and has captured a few British cities, but it could be doing better. The French player thinks to eirself: I could either continue battering against the heavily defended British lines, or I could secretly ally with Britain, stab Germany in the back, and waltz in along our undefended mutual border before the Germans even know what hit them. Instead of fighting for each inch of British land, I could be having dinner in Berlin within a week.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Meanwhile, in Berlin, the German player is looking towards France's temptingly undefended border and thinking the exact same thing.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">If both France and Germany are honorable, and if both countries know the other is honorable, the two of them can continue fighting Britain with a two-to-one numerical advantage and probably divide England's lucrative territory among the two of them.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">If Germany is naively trusting and France is a dishonest backstabber, then France can get obscene rewards by rolling over Germany while the Kaiser's armies are tied up on the fields of England.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">If both countries are suspicious of the other, or if both countries try to backstab each other simultaneously, then they will both divert forces away from the war on England to guard their mutual border. They will not gain any territory in England, and they will not gain any territory along their border. They've not only stabbed each other in the back, they've shot themselves in the foot.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Study_Two__Parfit_s_Hitch_Hiker\">Study Two: Parfit's Hitch-Hiker</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">The wiki describes Derek Parfit's famous <a href=\"http://wiki.lesswrong.com/wiki/Parfit%27s_hitchhiker\">hitchhiker problem</a> as:</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<blockquote>\n<p style=\"margin-bottom: 0in;\">Suppose you're out in the desert, running out of water, and soon to die - when someone in a motor vehicle drives up next to you. Furthermore, the driver of the motor vehicle is a perfectly selfish ideal game-theoretic agent, and even further, so are you; and what's more, the driver is Paul Ekman, who's really, really good at reading facial microexpressions. The driver says, \"Well, I'll convey you to town if it's in my interest to do so - so will you give me $100 from an ATM when we reach town?\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Now of course you wish you could answer \"Yes\", but as an ideal game theorist yourself, you realize that, once you actually reachtown, you'll have no further motive to pay off the driver. \"Yes,\" you say. \"You're lying,\" says the driver, and drives off leaving you to die.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n</blockquote>\n<p style=\"margin-bottom: 0in;\">The so-called <a href=\"http://en.wikipedia.org/wiki/Key_Lepanto#Key_Lepanto\">Key Lepanto opening</a> is one of the more interesting opening strategies in Diplomacy, and one that requires guts of steel to pull off. It goes like this: Italy and Austria decide to ally against Turkey. This is common enough, and hindered by the fact that Turkey is probably expecting it and Italy's kind of far away from Turkey anyway.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">So Italy and Austria do something unexpected. Italy swears loudly and publicly that ey's allied with Austria. Then, the first turn, Italy moves deep into undefended Austrian territory! Austria is incensed, and curses loud and long at Italy's betrayal and at eir own stupidity for leaving the frontier unguarded. Turkey laughs and leaves the two of them to their war when - boom - Austria and Italy launch a coordinated attack against Turkey from Italy's base deep in Austrian territory. The confused Turkey has no chance to organize a resistance before combined Italo-Austrian forces take Constantinople.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">It's frequently a successful strategy, especially for Italy. You know what else is a successful strategy for Italy? Doing this up to the point where they take over lots of Austrian territory, forgetting the part where it was all just a ploy, and then ending up in control of lots of Austrian territory, after which they can fight Turkey at their leisure.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">It's very much in Italy's advantage to play a Key Lepanto opening, and they may beg the Austrian player to go for it, saying correctly that it would benefit both of them. But the Austrian player very often refuses, telling Italy that ey would have no incentive not to just keep the conquered territory.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">This problem resembles the Hitchhiker: Italy is the lost man, and Austria is the driver. Italy really wants Austria to help em play the awesome Key Lepanto opening, but Austria knows that ey would have no incentive not to break his promise once Austria's given him the help he needs. As a result, neither country gets what they want. The Key Lepanto opening is played only rarely, and this is one of the reasons.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Study_Three__Enforceable_Side_Contracts\">Study Three: Enforceable Side Contracts</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">The Prisoner's Dilemma is nontrivial because there's no enforcement mechanism. In the presence of an enforcement mechanism, it becomes much simpler. Say two mobsters are about to be arrested, and expect to be put in a Prisoner's Dilemma type situation. They approach the mob boss with a contract with both of their names on it, saying that they have both agreed that if either of them testifies against the other, the mob boss should send his goons to shoot the rat.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">For many payoff matrices, signing this contract will be a no-brainer. It ensures your opponent will cooperate at the relatively low cost of forcing you to cooperate yourself, and almost guarantees you safe passage into the desirable (C,C) square. Not only does it prevent your opponent doesn't defect out of sheer greed, but it prevents your opponent from worrying that you're going to defect and then defecting emself to save emself from being the chump.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The game of Diplomacy I won, I won through an enforceable side contract (which lost me a friend and got me some accusations of cheating, but this is <a href=\"http://cf.geekdo-images.com/images/pic536690.jpg\">par for the course</a> for a good Diplomacy game). I was Britain; my friend H was France. H and I knew each other from an medieval times role-playing game, in which we both held land and money. The medieval kingdom of this game had a law on the books that any oath witnessed by a noble was binding on both parties and would be enforced by the king. So H and I went into our role-playing game and swore an oath before a cooperative noble, declaring that we would both aid each other in a permanent alliance in Diplomacy, or else all our in-game lands and titles would be forfeit.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">A lot of people made fun of me for this, including H, but in my defense I did end up winning the game. H and I were able to do things that would otherwise have been impossible; for example, in order to convince our enemy Germany that we were at war, I took over the French city of Brest. Normally, this would be almost impossible for two allies to coordinate, even as a red herring, for exactly the reasons listed in the Hitchhiker problem above. Since the two of us were able to trust each other absolutely, this otherwise difficult maneuver became easy.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">One of the advantages to strong central government is that it provides an enforcement mechanism for contracts, which benefits all parties.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Study_Four__Religion_As_Enforcement\">Study Four: Religion As Enforcement</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">Religion is a special case of the enforceable side-contract in which God is doing the enforcing. God doesn't have to exist for this to work; as long as at least one party believes He does, the threat of punishment will be credible. The advantage of being able to easily make enforceable side contracts even in the absence of social authority may be one reason religion became so popular, and if humans do turn out to have a genetic tendency toward belief, the side contracts might have provided part of the survival advantage that spread the gene.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">In a Youngstown Variant game (like Diplomacy, but with Eurasia instead of just Europe), I was playing Italy and after colonizing Africa was trying to juggle my forces around to defend borders with Germany, France, Turkey, and India.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">India was played by my friend A, who I sometimes have philosophical discussions with and who I knew to be an arch-conservative religion-and-family-values type. I decided to try something which, as far as I know, no one's ever tried in a Diplomacy game before. \"Do you swear in the name of God and your sacred honor that you won't attack me?\" I asked.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">\"Yes,\" said A, and I knew he meant it, because he takes that sort of thing really seriously. I don't know if he thought he would literally go to Hell if he broke his oath, but I'm pretty sure he wasn't willing to risk it over a board game. So I demilitarized my border with India. I concentrated my forces to the west, he concentrated them to the east, and both avoided a costly stalemate in the Indian Ocean and had more forces to send elsewhere. In the future, I will seek out A for alliances more often, since I have extra reason to believe he won't betray me; this will put A in an unusually strong position.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">This is not a unique advantage of religion; any strongly held philosophy that trumps self-interest would do. I would have made the same deal with Alicorn, who has stated loudly and publicly that she is a deontologist who has a deep personal aversion to lying<sup>2</sup>. I would have made it with Eliezer, who has a consequentialist morality but, on account of the consequences, has said he would not break an oath even for the sake of saving the world.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">But I only trust Alicorn and Eliezer because I've discussed morality with both of them in a situation where they had no incentive to lie; it was only in the very unusual conditions of Less Wrong that they could send such a signal believably. Religion is a much easier signal to send and receive without being a moral philosopher.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Study_Five__Excuses_as_Deviations_from_a_Rule\">Study Five: Excuses as Deviations from a Rule</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">My previous post,<a href=\"/lw/24o/eight_short_studies_on_excuses/\"> Eight Short Studies on Excuses</a>, was inspired by a maneuver I pulled during a Diplomacy game.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I was Italy, and Turkey and I had formed a mutual alliance against Austria. As part of the alliance, we had decided not to fight over who got the lucrative neutral territories in between our empires. I would get Egypt, Turkey would get Greece and Yemen, and we would avoid the resource drain of fighting each other for them so we could both concentrate on Austria.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Both Turkey and I would have liked to grab the centers that had been promised to the other. But both Turkey and I knew that maintaining the general rule of alliance between us was higher utility than getting one extra territory. BUT both Turkey and I knew that the other would be loathe to break off the alliance between just because their partner had committed one little infraction. BUT both Turkey and I knew that we would have to do exactly that, or else our ally would have a carte blanche to violate whatever terms of the alliance they wanted.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Then India (from whom I had not yet extracted his oath) made a move towards Yemen, threatening to take it from both of us. I responded by moving a navy to Yemen, supposedly to see off the Indian menace. I then messaged Turkey, saying that although I still respected the terms of our alliance, he was clearly too weak to keep Yemen out of Indian hands, so I would be fortifying it for him, and I hoped he would have the maturity to see this as a mutually beneficial move to prevent Indian expansionism, and not get too hung up on the exact terms of our alliance.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">The gambit worked: Turkey decided that maintaining our alliance was more important than keeping Yemen, and that because of the trouble with India my conquest of Yemen was not indicative of a general pattern of alliance-breaking that needed to be punished.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I can't claim total victory here: several years later, when the threat of Austria had disappeared, Turkey betrayed me and captured half my empire, partly because of my actions in Yemen.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Study_Six__For_the_Sake_of_Revenge\">Study Six: For the Sake of Revenge</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">This comes from the book Game Theory at Work:</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<blockquote>\n<p style=\"margin-bottom: 0in;\">Consider the emotion of revenge. At its core, revenge means hurting someone who has harmed you, even if you would be better off leaving him alone. Revenge is an irrational desire to harm others who have injured our loved ones or us.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">To see the benefit of being known as vengeful, consider a small community living in prehistoric times. Imagine that a group of raiders stole food from this community. A rational community would hunt down the raiders only if the cost of doing so was not too high. A vengence-endowed community would hunt down the raiders regardless of the cost. Since the raiders would rather go after the rational community, being perceived as vengeful provides you with protection and therefore confers an evolutionary advantage.</p>\n</blockquote>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">I play Diplomacy often against the same people, so I decided I needed to cultivate a reputation for vengefulness. And by \"decided to cultivate a reputation for vengefulness\", I mean \"Turkey betrayed me and I was filled with the burning rage of a thousand suns\".</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">So my drive for revenge was mostly emotional instead of rational. But what I didn't do was suppress my anger, the way people are always telling you. Suppressing anger is a useful strategy for one-shot games, but in an iterated game, getting a reputation for anger is often more valuable than behaving in your immediate rational self-interest.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">So I decided to throw the game to Germany, Turkey's biggest rival. I moved my forces away from the Italian-German border and invited Germany to take over my territory. At the same time, I used my remaining forces supporting German attacks against Turkey. The Austrians, who had been dealing with Turkey's betrayals even longer than I had, happily joined in. With our help, German forces scored several resounding victories against Turkey and pushed it back from near the top of the game down to a distant third.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Around the same time, Germany's other enemy France also betrayed me. So I told France I was throwing the game to Germany to punish him. No point in missing a perfectly good opportunity to cultivate a reputation for vengefulness.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">If I had done the rational thing and excused Turkey's betrayal because it was in my self-interest to cut my losses, I could have had a mediocre end game, and Turkey's player would have happily betrayed me the next game as soon as he saw any advantage in doing so. Instead, I'm doing very poorly in the end game, but Turkey - and everyone else - will be very wary about betraying me next time around.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Study_Seven__In_Group_Bias_as_a_Schelling_Point\">Study Seven: In-Group Bias as a Schelling Point</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">I made the mistake of moderating a game of Diplomacy at the SIAI House, which turned into one of the worst I've ever seen. The players were five SIAI Visiting Fellows and two of my non-SIAI friends who happened to be in the area.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Jasen came up with the idea of an alliance of the five SIAI players against my two friends. Although a few of the Fellows vacillated back and forth and defected a few times, he was generally able to keep the loyalty of the five Fellows until my two friends had been eliminated from the game relatively early on. Although normally the game would have continued until one of the Fellows managed to dominate the others, it was already very late and we called it a night at that point.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">It's easy to explain what happened as an irrational in-group bias, or as \"loyalty\" or \"patriotism\" among the SIAI folk. Jasen himself explained it as a desire to prove that SIAI people were especially cooperative and especially good at game theory, which I suppose worked. But there's another, completely theoretical perspective from which to view the SIAI Alliance.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Imagine you are on a lifeboat with nine other people, and determine that one of the ten of you must be killed and eaten to provide sustenance to the others. You are all ready to draw lots to decide who is dinner when you shout out \"Hey, instead of this whole drawing lots thing, let's kill and eat Bob!\"</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">If your fellow castaways are rational agents, they might just agree. If they go with lots, each has a 10% chance of ending up dinner. If everyone just agrees on Bob, then everyone has a 0% chance of ending up dinner (except poor Bob). Nine out of ten people are better off, and nine out of ten of you vote to adopt the new plan. Whether your lifeboat decides things by majority vote or by physical violence, it doesn't look good for Bob.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">But imagine a week later, you still haven't been rescued, and the whole situation repeats. If everyone lets you repeat your action of calling out a name, there's a 1/9 chance it'll be eir name - no better than drawing lots. In fact, since you're very unlikely to call out your <em>own </em>name, it's more of a 1/8 chance - <em>worse</em> than just drawing lots. So everyone would like to be the one who calls out the name, and as soon as the lots are taken out, everyone shouts \"Hey, instead of the whole drawing lots thing, let's kill and eat X!\" where X is a different person for each of the nine castaways. This is utterly useless, and you probably end up just drawing lots.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">But suppose eight of the nine of you are blond, and one is a brunette. The brunette is now a Schelling point. If you choose to kill and eat the brunette, there's a pretty good chance all of your blond friends will do the same, even if none of you had a pre-existing prejudice against brunettes. Therefore, all eight of you shout out \"Let's kill and eat the brunette!\", since this is safer than drawing lots. Your lifeboat has invented in-group bias from rational principles.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">Such alliances are equally attractive in Diplomacy. When the five SIAI Fellows allied against my two friends, they ensured there was a five-against-two alliance with themselves on the winning side, and successfully reduced the gameboard from six opponents to four. Although they could have done this with anyone (eg Jasen could have selected two other Fellows and my two friends, and forged an equivalent coalition of five), Jasen would have been at risk of five other people having the same idea and excluding him. By choosing a natural and obvious division in which he was on the majority, Jasen avoided this risk.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Rationalist_Diplomacy\">Rationalist Diplomacy</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">I'm interested in seeing what a Diplomacy game between Less Wrongers looks like. I'm willing to moderate. <del>The first seven people to sign up get places (don't sign up if you don't expect to have enough time for about two or three turns/week), and the next few can be alternates. Doesn't matter if you've ever played before as long as you read the rules above and think you understand them.</del> <em>(We already have seven people. See the post in Discussion. If many more sign up, someone else may want to moderate a second game).</em></p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\"><strong id=\"Footnotes\">Footnotes</strong></p>\n<p style=\"margin-bottom: 0in;\"><strong><br></strong></p>\n<p style=\"margin-bottom: 0in;\">1: Source: \"Nice Guys Finish First\" in the Frameshift album <em>Unweaving the Rainbow</em>.</p>\n<p style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p style=\"margin-bottom: 0in;\">2. Alicorn wishes me to note that she considers anyone playing a Diplomacy game&nbsp; without prior out-of-game-context agreements secured to have waived eir right to complete honesty from her, but the general principle still stands.</p>", "sections": [{"title": "Study One: The Prisoner's Dilemma", "anchor": "Study_One__The_Prisoner_s_Dilemma", "level": 1}, {"title": "Study Two: Parfit's Hitch-Hiker", "anchor": "Study_Two__Parfit_s_Hitch_Hiker", "level": 1}, {"title": "Study Three: Enforceable Side Contracts", "anchor": "Study_Three__Enforceable_Side_Contracts", "level": 1}, {"title": "Study Four: Religion As Enforcement", "anchor": "Study_Four__Religion_As_Enforcement", "level": 1}, {"title": "Study Five: Excuses as Deviations from a Rule", "anchor": "Study_Five__Excuses_as_Deviations_from_a_Rule", "level": 1}, {"title": "Study Six: For the Sake of Revenge", "anchor": "Study_Six__For_the_Sake_of_Revenge", "level": 1}, {"title": "Study Seven: In-Group Bias as a Schelling Point", "anchor": "Study_Seven__In_Group_Bias_as_a_Schelling_Point", "level": 1}, {"title": "Rationalist Diplomacy", "anchor": "Rationalist_Diplomacy", "level": 1}, {"title": "Footnotes", "anchor": "Footnotes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "98 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 98, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HFyWNBnDNEDsDNLrZ", "fg9fXrHpeaDD6pEPL", "gFMH3Cqw4XxwL69iy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-13T06:07:54.256Z", "modifiedAt": null, "url": null, "title": "Requesting some advice on a question", "slug": "requesting-some-advice-on-a-question", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:31.942Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Carinthium", "createdAt": "2010-11-10T22:28:58.091Z", "isAdmin": false, "displayName": "Carinthium"}, "userId": "DL8CRWfXPCHYqQsv4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sM6N3TtWzpdb8KH3n/requesting-some-advice-on-a-question", "pageUrlRelative": "/posts/sM6N3TtWzpdb8KH3n/requesting-some-advice-on-a-question", "linkUrl": "https://www.lesswrong.com/posts/sM6N3TtWzpdb8KH3n/requesting-some-advice-on-a-question", "postedAtFormatted": "Saturday, November 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Requesting%20some%20advice%20on%20a%20question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequesting%20some%20advice%20on%20a%20question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsM6N3TtWzpdb8KH3n%2Frequesting-some-advice-on-a-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Requesting%20some%20advice%20on%20a%20question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsM6N3TtWzpdb8KH3n%2Frequesting-some-advice-on-a-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsM6N3TtWzpdb8KH3n%2Frequesting-some-advice-on-a-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<p>I'm not sure if this is the right place to put this, but there's a problem I can tell I'm biased on so I'm requesting some advice from people unlikely to be biased on it.</p>\r\n<p>Legally, circa the time of the actual secessions leading to the creation of the C.S.A, was secession legal? As far as I can tell it was (the Supreme Court may have practical control but they aren't infallible, and because of the Tenth Amendment), but I can tell I have biased emotions on the subject so I'm checking with people less likely to.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sM6N3TtWzpdb8KH3n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": -1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "3992", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-13T11:28:37.411Z", "modifiedAt": null, "url": null, "title": "Cartoon which I think will appeal to LW", "slug": "cartoon-which-i-think-will-appeal-to-lw", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.847Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vQfSoXyjW8jdYbguF/cartoon-which-i-think-will-appeal-to-lw", "pageUrlRelative": "/posts/vQfSoXyjW8jdYbguF/cartoon-which-i-think-will-appeal-to-lw", "linkUrl": "https://www.lesswrong.com/posts/vQfSoXyjW8jdYbguF/cartoon-which-i-think-will-appeal-to-lw", "postedAtFormatted": "Saturday, November 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cartoon%20which%20I%20think%20will%20appeal%20to%20LW&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACartoon%20which%20I%20think%20will%20appeal%20to%20LW%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvQfSoXyjW8jdYbguF%2Fcartoon-which-i-think-will-appeal-to-lw%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cartoon%20which%20I%20think%20will%20appeal%20to%20LW%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvQfSoXyjW8jdYbguF%2Fcartoon-which-i-think-will-appeal-to-lw", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvQfSoXyjW8jdYbguF%2Fcartoon-which-i-think-will-appeal-to-lw", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://www.viruscomix.com/page408.html</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vQfSoXyjW8jdYbguF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 2, "extendedScore": null, "score": 6.451280410729561e-07, "legacy": true, "legacyId": "3994", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-13T17:11:53.609Z", "modifiedAt": null, "url": null, "title": "Spring 1912: A New Heaven And A New Earth ", "slug": "spring-1912-a-new-heaven-and-a-new-earth", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:49.940Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BD6WYC4GT6dnWaJRN/spring-1912-a-new-heaven-and-a-new-earth", "pageUrlRelative": "/posts/BD6WYC4GT6dnWaJRN/spring-1912-a-new-heaven-and-a-new-earth", "linkUrl": "https://www.lesswrong.com/posts/BD6WYC4GT6dnWaJRN/spring-1912-a-new-heaven-and-a-new-earth", "postedAtFormatted": "Saturday, November 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Spring%201912%3A%20A%20New%20Heaven%20And%20A%20New%20Earth%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASpring%201912%3A%20A%20New%20Heaven%20And%20A%20New%20Earth%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBD6WYC4GT6dnWaJRN%2Fspring-1912-a-new-heaven-and-a-new-earth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Spring%201912%3A%20A%20New%20Heaven%20And%20A%20New%20Earth%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBD6WYC4GT6dnWaJRN%2Fspring-1912-a-new-heaven-and-a-new-earth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBD6WYC4GT6dnWaJRN%2Fspring-1912-a-new-heaven-and-a-new-earth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 902, "htmlBody": "<p><img src=\"http://www.raikoth.net/Game/rdiplofinal.jpg\" alt=\"\" /></p>\n<p>And so it came to pass that on Christmas Day 1911, the three Great Powers of Europe signed a treaty to divide the continent between them peacefully, ending what future historians would call the Great War.<br /><br />The sun truly never sets on King Jack's British Empire, which stretches from Spain to Stockholm, from Casablanca to Copenhagen, from the fringes of the Sahara to the coast of the Arctic Ocean. They rule fourteen major world capitals, and innumerable smaller towns and cities, the greatest power of the age and the unquestioned master of Western Europe.<br /><br />From the steppes of Siberia to the minarets of Istanbul, the Ottoman Empire is no longer the Sick Man of Europe but stands healthy and renewed, a colossus every bit the equal of the Christian powers to its west. Its Sultan calls himself the Caliph, for the entire Islamic world basks in his glory, and his Grand Vizier has been rewarded with a reputation as one of the most brilliant and devious politicians of the age. At his feet grovel representatives of twelve great cities, and even far-flung Tunis has not escaped his sway.<br /><br />And in between, the Austro-Hungarian Empire straddles the Alps and ancient Italy. Its lack of natural borders presented no difficulty for its wily Emperor, who successfully staved off the surrounding powers and played his enemies off against one another while building alliances that stood the test of time. Eight great cities pay homage to his double-crown, and he is what his predecessors could only dream of being - a true Holy Roman Emperor.<br /><br />And hidden beneath the tricolor map every student learns in grammar school are echoes of subtler hues. In Germany, people still talk of the mighty Kajser Sotala I, who conquered the ancient French enemy and extended German rule all the way to the Mediterranean, and they still seeth and curse at his dastardly betrayal by his English friends. In Russia, Princess Anastasia claims to be the daughter of Czar Perplexed, and recounts to everyone who will listen the story of her stoic father, who remained brave until the very end; at her side travels a strange bearded man who many say looks like Rasputin, the Czar's long-missing adviser. The French remember President Andreassen, who held off the combined armies of England and Germany for half a decade, and many still go on pilgrimage to Liverpool, the site of their last great victory. And in Italy, Duke Carinthium has gone down in history beside Tiberius and Cesare Borgia as one of their land's most colorful and fascinating leaders.<br /><br />And the priests say that the same moment the peace treaty was signed, the blood changed back to water, and the famines ended, and rain fell in the lands parched by drought. Charles Taze Russell, who had been locked in his room awaiting the Apocalypse, suddenly ran forth into the midwinter sun, shouting \"Our doom has been lifted! God has granted us a second chance!\" And the mysterious rectangular wall of force separating Europe from the rest of the world blinked out of existence.<br /><br />Pope Franz I, the new Austrian-supported Pontiff in Rome, declares a month of thanksgiving and celebration. For, he says, God has tested the Europeans for their warlike ways, isolating them from the rest of the earth lest their sprawling empires plunge the entire planet into a world war that might kill millions. Now, the nobility of Europe finally realizing the value of peace, the curse has been lifted, and the empires of Europe can once more interact upon the world stage.<br /><br />Chastened by their brush with doom, yet humbled by the lesson they had been given, the powers of Europe send missionaries through the dimensional portal, to convince other worlds to abandon their warlike ways and seek universal brotherhood. And so history ends, with three great powers living together side by side and striving together for a better future and a positive singularity.<br /><br />...<br /><br />On to the more practical parts. If you think you've learned lessons this game worth telling the rest of Less Wrong, you should send them to either myself or Jack. I say either myself or Jack because Jack had the most supply centers and therefore deserves some karma which he could most easily get by posting the thread which the other two winners then comment on, or if you insist that three way tie means three way tie, I'll post the thread and the three winners can all comment and get up-voted. We'll talk about it in the comments.<br /></p>\n<p><img src=\"http://www.raikoth.net/Game/rdiplosuccess.jpg\" alt=\"\" width=\"543\" height=\"565\" /></p>\n<p><img src=\"http://www.raikoth.net/Game/rdiplocounts.jpg\" alt=\"\" width=\"474\" height=\"336\" /></p>\n<p><img src=\"http://www.raikoth.net/Game/rdiplosupport.jpg\" alt=\"\" /></p>\n<p>Thanks to everyone who played in this game. I was very impressed - it's  one of the rare games I have moderated that hasn't been ruined by people  constantly forgetting to send orders, or people ragequitting when  things don't go their way, or people being totally incompetent and  throwing the game to the first person to declare war on them, or any of  the other ways a Diplomacy game can go wrong. Everyone fought hard and  well and honorably (for definitions of honor compatible with playing  Diplomacy). It was a pleasure to serve as your General Secretary.</p>\n<p>&nbsp;</p>\n<blockquote>\n<p><strong>All previous posts and maps from this game are archived. See <a href=\"/r/discussion/lw/32z/winter_1902_france_gets_a_mastectomy/30qa?c=1\">this comment</a> for an explanation of how to access the archives.</strong></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BD6WYC4GT6dnWaJRN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 27, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "3995", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 289, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-14T02:43:41.713Z", "modifiedAt": null, "url": null, "title": "Telepathy Exists (no, not the Bem study)", "slug": "telepathy-exists-no-not-the-bem-study", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:01.362Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9K5orowajfkqQ2isw/telepathy-exists-no-not-the-bem-study", "pageUrlRelative": "/posts/9K5orowajfkqQ2isw/telepathy-exists-no-not-the-bem-study", "linkUrl": "https://www.lesswrong.com/posts/9K5orowajfkqQ2isw/telepathy-exists-no-not-the-bem-study", "postedAtFormatted": "Sunday, November 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Telepathy%20Exists%20(no%2C%20not%20the%20Bem%20study)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATelepathy%20Exists%20(no%2C%20not%20the%20Bem%20study)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9K5orowajfkqQ2isw%2Ftelepathy-exists-no-not-the-bem-study%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Telepathy%20Exists%20(no%2C%20not%20the%20Bem%20study)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9K5orowajfkqQ2isw%2Ftelepathy-exists-no-not-the-bem-study", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9K5orowajfkqQ2isw%2Ftelepathy-exists-no-not-the-bem-study", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>http://www2.macleans.ca/2010/11/02/a-piece-of-their-mind/print/</p>\n<p>&nbsp;</p>\n<p>To be fair, it's a very special case. But I think the scientific opportunity here is *incredible.*</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9K5orowajfkqQ2isw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -6, "extendedScore": null, "score": 6.45349459686457e-07, "legacy": true, "legacyId": "3996", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-14T04:11:06.176Z", "modifiedAt": null, "url": null, "title": "Hi - I'm new here - some questions", "slug": "hi-i-m-new-here-some-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:57.866Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KffNYhksZoE4Ffw6S/hi-i-m-new-here-some-questions", "pageUrlRelative": "/posts/KffNYhksZoE4Ffw6S/hi-i-m-new-here-some-questions", "linkUrl": "https://www.lesswrong.com/posts/KffNYhksZoE4Ffw6S/hi-i-m-new-here-some-questions", "postedAtFormatted": "Sunday, November 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hi%20-%20I'm%20new%20here%20-%20some%20questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHi%20-%20I'm%20new%20here%20-%20some%20questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKffNYhksZoE4Ffw6S%2Fhi-i-m-new-here-some-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hi%20-%20I'm%20new%20here%20-%20some%20questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKffNYhksZoE4Ffw6S%2Fhi-i-m-new-here-some-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKffNYhksZoE4Ffw6S%2Fhi-i-m-new-here-some-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1573, "htmlBody": "<p>Hello everyone,</p>\n<p>I'm new here, although I've read Less Wrong and Overcoming Bias on and off for the last few years. Anyways, I'm InquilineKea (or Simfish), and I have a website at http://simfishthoughts.wordpress.com/. I think about everything, so I feel that this might be the perfect community for me. I do have some questions though - are we allowed to post anything in this part of the site? (like, could we treat this part like another forum, albeit an intellectually mature forum?) Or do we have to keep things formal? I tend to post a high number of threads, but there don't seem to be many threads here. Are there any terms of service/rules? Or are things just governed by upvotes/downvotes? (much like reddit)</p>\n<p>Anyways, I'm an astronomy/physics/math major at the University of Washington (I got in through an early entrance program) and I'm planning on applying to astrophysics grad school fairly soon. However, I'm also intensely interested in complex adaptive systems and data mining, especially as they relate to the social sciences. I'm especially interested in Consilience and in trying to find trends behind every academic field (in fact, I do want to get to a graduate level of education in every natural and social science there is). I'm demographics junkie who literally pours over all the charts and tables of every demographic statistic I can find, although it sometimes ends up hurting my grades. My favorite blogs are Gene Expression, FuturePundit/ParaPundit, and Overcoming Bias. Which I'm sure a lot of people here read.</p>\n<p>I always think in terms of maximizing \"utility\" and maximizing \"efficiency\". So this leads me to do many untraditional things. For one thing, I have attention deficit disorder, so I realize that I frequently have to take untraditional approaches. The Internet has always been a savior for me because I can always stop and continue later when I feel like I'm about to zone out (in fact, those with ADD have a highly inconsistent learning rate). I also have an Asperger's Syndrome diagnosis, although I've recently tried to stop using it as an excuse for my behavior (in fact, I now only fit the bare minimum of \"Aspie\" criteria on the DSM IV, but I still think that it strongly influences my interests and behavior). I also consistently think of what's most rational - which means that I have to respect the desires that evolution has given me. Sometimes, people think that maximizing \"utility\" means maximizing \"self-interest\", but the amazing thing is that evolution has made people happier whenever they help others (for whatever reason), since \"happiness\" tends to asymptote with increased wealth/self-gratification/etc. So as a result, people are actually happiest when they're socially interconnected. Although I sometimes bemoan this fact since I often feel that people don't understand me (I'm trying to move beyond my neuroticism/anger stemming from a half-decade of social rejection, but it still affects me now). I also practice calorie restriction + vegetarianism, not just to maximize my chances of living longer, but also because I want to reduce the decline of fluid IQ with increasing age.</p>\n<p>Due to my conditions, though, I've never felt like I was in any comfort zone, which has perhaps forced me to try every possible approach that might make my life easier. I often start out with irrational approaches, but end up taking the approach that I perceive as most rational for myself. Of course, the sustainability of the action matters too (I realize that it might be utility-maximizing for me to exercise, for example, but I don't exercise right now because I can't trust myself to be consistent with exercising, at least while I'm still in school).</p>\n<p>Anyways, I can talk a lot more. I love to overanalyze things. I also have a massive number of posts on the Internet, although many of them are beyond embarrassing. In the end, though, I only look for people who are open to anything and completely non-judgmental (although some people may look for certain \"signals\" when they're looking for prospective contacts, to minimize the chances of meeting a contact with which one may fear wasting time on). Basically, my ideal model (for hypothesis generation) involves this: I try to type out some hypotheses, and then post them online, in hopes that someone might critique them. Many of my hypotheses will be junk, but that's okay. As long as I can maximize the number of useful ideas that I can generate, I think I'll have done something (although I don't really have a place to post all my hypotheses, since I've been flamed many times for it [most people consider my posts tl;dr, and they also make fun of my autism]. And few people reply to my ideas precisely because I tend to study esoteric fields that they don't care about, but also because I still haven't found a forum where people actually respect ideas [even reddit and Physics Forums can be particularly cruel].)</p>\n<p>Compared to most people, I tend to hit on correct ideas with lower accuracy (which inevitably results in people getting impatient with me/flaming me). But I do believe that it's easiest for me to form the best ideas when I post them when undeveloped (that way, sometimes, my shame at being wrong can actually motivate me to correct my ideas more quickly - this is why I frequently edit after posting - I have problems with alertness, so the adrenaline rush from being wrong can actually motivate me to finish things in less time). I consider time as the most important resource in the world, as the amount of material I could possibly learn is definitely worth thousands of lifetimes. And eventually, I do hit on some good ideas. In a sense, it's like generating variation and selecting the best results out of such variation (sort of like evolution, albeit less blind). This is why I'm also intensely interested in genetic algorithms and data mining, since they tend to operate through somewhat similar mechanisms (this is also why I love the fourth paradigm so much). I'm extremely extremely open about myself and share virtually everything I do (although I generally don't share when I believe that such sharing could lead to social rejection, so this usually makes me keep to myself). But yes, I explore *many* ideas and *many* topics precisely because I want to find the topic that would maximize my talent/productivity (it's hard due to my ADD, but it might result in a global maxima whereas others might stick with local maxima). Anyways, my only goal is to be interesting to other people (and to avoid taking on a job that might suppress my talents, so I really do want to go onto academia).</p>\n<p>Of course, I will always have to find creative ways to make others feel happy. E.g. I can often come off as self-centered, and others will often have to be patient for me since I may not have the attention span to go through something in one go. But at the same time, I'm not in a comfortable situation, so if I find an opportunity I may never have again, I will recognize it for what it is and I'll try to do everything I can to achieve it (which may require patience from other people, but I'll really try not to disappoint them since I know the real consequences of it). In any case, I'm intensely interested in how people learn (and how people ideally learn), since my own difficulties with ADD have forced me to take untraditional routes (and in fact, there may be others who do best through the nontraditional route).</p>\n<p>Anyways, I like this place precisely because it allows people to comment with the same username (so that we can track our old posts and those of people we're interested in). I also have a facebook (http://www.facebook.com/simfish) and a google buzz profile (http://www.google.com/profiles/simfish). I generally keep everything about myself very public (to maximize the chances that some like-minded person might find me), although I may have to private them when I apply to grad schools. I'd really like to contribute to discussions, although I feel that I don't have much to say right now, so I read more than comment.</p>\n<p>My biggest irrationality is social anxiety/rejection anxiety because I've been flamed/rejected numerous times, so I'm scared of people. Other than that, though, I can be very rational.</p>\n<p>So if you can relate, please comment. Or if you just want to share some ideas or add some comments. In any case, I do believe that rationality means acknowledging our human emotions (and in knowing that efficiency can be maximized when we do things in accordance to our emotions). Of course, these emotions can be corrected in many cases (I do think that anger is highly irrational in many cases, for example). I like the Internet a lot because it archives everything, so I can always revisit my old ideas simply by searching through them (whereas ideas communicated verbally cannot be searched, and easily get lost to the dust of memory).Anyways, a \"search through someone's old posts\" feature is very useful here, since it makes it easier for people to identify similar minds (which can be important if people are very specialized)</p>\n<p>I'm extremely impressed with how knowledgeable and interdisciplinary many of you are - I seem to know so much less than most of you, even though I seem to be far more interdisciplinary than everyone else I know.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KffNYhksZoE4Ffw6S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 6.453706166327079e-07, "legacy": true, "legacyId": "3997", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-14T09:47:21.262Z", "modifiedAt": null, "url": null, "title": "Mental focus", "slug": "mental-focus", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:36.012Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WmmhBRME6HBhPjRZW/mental-focus", "pageUrlRelative": "/posts/WmmhBRME6HBhPjRZW/mental-focus", "linkUrl": "https://www.lesswrong.com/posts/WmmhBRME6HBhPjRZW/mental-focus", "postedAtFormatted": "Sunday, November 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mental%20focus&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMental%20focus%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmmhBRME6HBhPjRZW%2Fmental-focus%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mental%20focus%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmmhBRME6HBhPjRZW%2Fmental-focus", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmmhBRME6HBhPjRZW%2Fmental-focus", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<p>I just noticed how much effort I put into remembering what I was thinking-- I do a lot of \"I had an interesting idea about that which I wanted to include-- what was the idea?\".</p>\n<p>It can take at least 3 or 4 repetitions before the idea finally gets typed out, partly because I might be typing or formulating something else and then want to get back to an earlier idea.</p>\n<p>I'd be amazed if I were the only person here with this problem. Any suggestions? Does dual n-back help?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WmmhBRME6HBhPjRZW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 6.454520174201239e-07, "legacy": true, "legacyId": "3998", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-14T09:54:43.821Z", "modifiedAt": null, "url": null, "title": "Humans don't generally have utility functions", "slug": "humans-don-t-generally-have-utility-functions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:34.406Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4NdQgrtvniKvQ4Kts/humans-don-t-generally-have-utility-functions", "pageUrlRelative": "/posts/4NdQgrtvniKvQ4Kts/humans-don-t-generally-have-utility-functions", "linkUrl": "https://www.lesswrong.com/posts/4NdQgrtvniKvQ4Kts/humans-don-t-generally-have-utility-functions", "postedAtFormatted": "Sunday, November 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Humans%20don't%20generally%20have%20utility%20functions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHumans%20don't%20generally%20have%20utility%20functions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4NdQgrtvniKvQ4Kts%2Fhumans-don-t-generally-have-utility-functions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Humans%20don't%20generally%20have%20utility%20functions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4NdQgrtvniKvQ4Kts%2Fhumans-don-t-generally-have-utility-functions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4NdQgrtvniKvQ4Kts%2Fhumans-don-t-generally-have-utility-functions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 974, "htmlBody": "<p>First, the background:</p>\n<p>Humans obviously have a variety of heuristics and biases that lead to non-optimal behavior.&nbsp; But can this behavior truly not be described by a function?</p>\n<p>Well, the easiest way to show that utility isn't described by a function is to show the existence of cycles.&nbsp; For example, if I prefer A to B, B to C, and C to A, that's a cycle - if all three are available I'll never choose, and if each switch is an increase in utility, my utility blows up to infinity!&nbsp; Well, really, it simply becomes undefined.</p>\n<p>Do we have real-world examples of cycles in utility comparisons?&nbsp; Sure.&nbsp; For a cycle of size 2, Eliezer <a href=\"/lw/my/the_allais_paradox/\" target=\"_blank\">cites</a> the odd behavior of people with regard to money and probabilities of money.&nbsp; However, the money-pumps he cites are rather inefficient.&nbsp; Almost any decision that seems \"arbitrary\" to us can be translated into a cycle.&nbsp; For example, anchoring means that people assign higher value to a toaster when a more expensive toaster is sitting next to it.&nbsp; But most people, if asked, would certainly assign tiny value to adding/removing options they know they won't buy.&nbsp; So we get the result that they must value the toaster more than they value the toaster.&nbsp; The conjunction fallacy can be made into a cycle by the same reasoning if you ask people to bet on the thing happening together and then ask them to bet on the things happening separately.</p>\n<p>So at the very least, not all humans have utility functions, which means that the human brain doesn't automatically give us a utility function to use - if we want one, we have to sculpt it ad-hoc out of intuitions using our general reasoning, and like most human things it probably won't be the best ever.</p>\n<p>&nbsp;</p>\n<p>So, what practical implications does this have, aside from \"people are weird?\"</p>\n<p>Well, I can think of two interesting things.&nbsp; First, there are the implications for utilitarian ethics.&nbsp; If utility functions are arbitrary not just on a person to person basis, but even within a single person, choosing between options using utilitarian ethics requires stronger, more universal moral arguments.&nbsp; The introspective \"I feel like X, therefore my utility function must include that\" is now a weak argument, even to yourself!&nbsp; The claim that \"a utility function of universe-states exists\" loses none of its consequences though, like alerting you that something is wrong when you encounter a cycle in your preferences, or of course supporting consequentialism.</p>\n<p>&nbsp;</p>\n<p>Interesting thing two: application in AI design.&nbsp; The first argument goes something like \"well if it works for humans, why wouldn't it work for AIs?\"&nbsp; The first answer, of course, is \"because an AI that had a loop it would get stuck in it.\"&nbsp; But the first answer is sketchy, because *humans* don't go get stuck in their cycles.&nbsp; We do interesting things like:</p>\n<ul>\n<li>&nbsp;meta level aversion to infinite loops.</li>\n</ul>\n<ul>\n<li>&nbsp;resolving equivalencies/cycles with \"arbitrary\" associations.</li>\n</ul>\n<ul>\n<li>&nbsp;not actually \"looping\" when we have a cycle, but doing something else that resembles utilitarianism much less.</li>\n</ul>\n<p>So replacing a huge utility function with a huge-but-probably-much-smaller set of ad-hoc rules could actually work for AIs if we copy over the right things from human cognitive structure.&nbsp; Would it be possible to make it Friendly or some equivalent?&nbsp; Well, my first answer is \"I don't see why not.\"&nbsp; It seems about as possible as doing it for the utility-maximizing AIs.&nbsp; I can think of a few avenues that, if profitable, would make it even simpler (the simplest being \"include the Three Laws\"), but it could plausibly be unsolvable as well.</p>\n<p>The second argument goes something like \"It may very well be better to do it with the list of guidelines than with a utility function.\"&nbsp; For example, Eliezer makes the <a href=\"/lw/ww/high_challenge/\" target=\"_blank\">convincing argument</a> that fun is not a destination, but a path.&nbsp; What part of that makes sense from a utilitarian perspective?&nbsp; It's a very human, very lots-of-rules way of understanding things.&nbsp; So why not try to make an AI that can intuitively understand what it's like to have fun?&nbsp; Hell, why not make an AI that can have fun for the same reason humans can?&nbsp; Wouldn't that be more... well... fun?</p>\n<p>This second argument may be full of it.&nbsp; But it sounds good, eh?&nbsp; Another reason the lots-of-rules approach may beat out the utility function approach is the ease of critical self-improvement.&nbsp; A utility function approach is highly correlated with trying to idealize actions, which would make it tricky to write good code, which is a ridonkulously hard problem to optimize.&nbsp; But a lots-of-rules approach intuitively seems like it could critically self-improve with greater ease - it seems like lots of rules will be needed to make an AI a good computer programmer anyhow, and under that assumption the lots-of-rules approach would be better prepared to deal with ad-hoc rules.&nbsp; Is this assumption false?&nbsp; Can you write a good programmer elegantly?&nbsp; Hell if I know.&nbsp; It just feel like if you could, we would have done it.</p>\n<p>&nbsp;</p>\n<p>Basically, utility functions are guaranteed to have a large set of nice properties.&nbsp; However, if humans are made of ad-hoc rules; if we want a nice property we just add the rule \"have this property!\"&nbsp; This limits how well we can translate even our own desires into moral guidelines, especially near \"strange\" areas of our psychology such as comparison cycles.&nbsp; But it also proves that there is a (possibly terrible) alternative to utility functions that intelligences can run on.&nbsp; I've tried to make it plausible that the lots-of-rules approach could even be better than the utility-function approach, worth a bit of consideration before you start your next AI project.</p>\n<p>&nbsp;</p>\n<p>Edit note: Removed apparently disliked sentence.&nbsp; Corrected myself after totally forgetting Arrow's theorem.</p>\n<p>Edit 2, not that this will ever be seen: This thought has obviously been thought before, but maybe not followed this far into hypothetical-land: http://lesswrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4NdQgrtvniKvQ4Kts", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 2, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "3999", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zJZvoiwydJ5zvzTHK", "29vqqmGNxNRGzffEj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-14T16:52:41.241Z", "modifiedAt": null, "url": null, "title": "Another attempt to explain UDT", "slug": "another-attempt-to-explain-udt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:36.524Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "cousin_it", "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zztyZ4SKy7suZBpbk/another-attempt-to-explain-udt", "pageUrlRelative": "/posts/zztyZ4SKy7suZBpbk/another-attempt-to-explain-udt", "linkUrl": "https://www.lesswrong.com/posts/zztyZ4SKy7suZBpbk/another-attempt-to-explain-udt", "postedAtFormatted": "Sunday, November 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20attempt%20to%20explain%20UDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20attempt%20to%20explain%20UDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzztyZ4SKy7suZBpbk%2Fanother-attempt-to-explain-udt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20attempt%20to%20explain%20UDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzztyZ4SKy7suZBpbk%2Fanother-attempt-to-explain-udt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzztyZ4SKy7suZBpbk%2Fanother-attempt-to-explain-udt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 721, "htmlBody": "<p>(Attention conservation notice: this post contains no new results, and will be obvious and redundant to many.)</p>\n<p>Not everyone on LW understands Wei Dai's&nbsp;<a href=\"/lw/15m/towards_a_new_decision_theory/\">updateless decision theory</a>. I didn't understand it completely until two days ago. Now that I had the final flash of realization, I'll try to explain it to the community and hope my attempt fares better than <a href=\"/lw/294/what_is_wei_dais_updateless_decision_theory/\">previous attempts</a>.</p>\n<p>It's probably best to avoid talking about \"decision theory\" at the start, because the term is hopelessly muddled. A better way to approach the idea is by examining what we mean by \"truth\" and \"probability\" in the first place. For example, is it meaningful for <a href=\"/lw/32o/if_a_tree_falls_on_sleeping_beauty/\">Sleeping Beauty</a> to ask whether it's Monday or Tuesday? Phrased like this, the question sounds stupid. Of course there's a fact of the matter as to what day of the week it is! Likewise, in all problems involving simulations, there seems to be a fact of the matter whether you're the \"real you\" or the simulation, which leads us to talk about probabilities and \"indexical uncertainty\" as to which one is you.</p>\n<p>At the core, Wei Dai's idea is to boldly proclaim that, counterintuitively, you can act as if there were <em>no fact of the matter</em> whether it's Monday or Tuesday when you wake up. Until you learn which it is, you think it's <em>both</em>. You're all your copies at once.<a id=\"more\"></a></p>\n<p>More formally, you have an initial distribution of \"weights\" on possible universes (in the currently most general case it's the Solomonoff prior) that you <em>never update at all</em>. In each individual universe you have a utility function over what happens. When you're faced with a decision, you find all copies of you in the entire \"multiverse\" that are faced with the same decision (\"information set\"), and choose the decision that <em>logically implies</em> the maximum sum of resulting utilities weghted by universe-weight. If you possess some useful information about the universe you're in, it's <em>magically taken into account</em> by the choice of \"information set\", because logically, your decision cannot affect the universes that contain copies of you with&nbsp;<em>different</em> states of knowledge, so they only add a constant term to the utility maximization.</p>\n<p>Note that the theory, as described above, has ho notion of \"truth\" and \"probability\" divorced from decision-making. That's how I arrived at understanding it: in <a href=\"/lw/32l/the_strong_occams_razor/\">The Strong Occam's Razor</a> I asked whether it makes sense to \"believe\" one physical theory over another which makes the same predictions. For example, is hurting a human in a sealed box morally equivalent to not hurting him? After all, the laws of physics <em>could</em> make a localized exception to save the human from harm. UDT gives a very definite answer: there's <em>no</em> fact of the matter as to which physical theory is \"correct\", but&nbsp;you refrain from pushing the button anyway, because it hurts the human more in universes with simpler physical laws, which have more weight according to our \"initial\" distribution. This is an attractive solution to the problem of the <a href=\"/lw/pb/belief_in_the_implied_invisible/\">\"implied invisible\"</a>&nbsp;- possibly even more attractive than Eliezer's own answer.</p>\n<p>As you probably realize by now, UDT is a very sharp tool that can give simple-minded answers to all our decision-theory puzzles so far - even if they involve copying, amnesia, simulations, predictions and other tricks that throw off our approximate intuitions of \"truth\" and \"probability\". Wei Dai gave a detailed example in <a href=\"/lw/182/the_absentminded_driver/\">The Absent-Minded Driver</a>, and the method carries over almost mechanically to other problems. For example, <a href=\"/lw/3l/counterfactual_mugging/\">Counterfactual Mugging</a>: by assumption, your decision logically affects both heads-universe and tails-universe, which (also by assumption) have equal weight, so by agreeing to pay you win more cookies overall. Note that updating on the knowledge that you are in tails-universe (because Omega showed up) doesn't affect anything, because the theory is \"updateless\".</p>\n<p>At this point some may be tempted to switch to True Believer mode. Please don't. Just like Bayesianism, utilitarianism, MWI or the Tegmark multiverse, UDT is an idea that's <em>irresistibly delicious</em> to a certain type of person who puts a high value on clarity. And they all play so well together that it <em>can't</em> be an accident! But what does it even mean to consider a theory \"true\" when it says that our primitive notion of \"truth\" isn't \"true\"? :-) Me, I just consider the idea very fruitful; I've been contributing <a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/\">new math</a> to it and plan to do so in the future.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "5f5c37ee1b5cdee568cfb1dc": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zztyZ4SKy7suZBpbk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 52, "extendedScore": null, "score": 0.000103, "legacy": true, "legacyId": "4000", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["de3xjFaACCAk6imzv", "5WEoM3RCxN2cQEdzY", "gMXsyhPiEJbGerF6F", "ep6tiXJpHxR8ofWs6", "3XMwPNMSbaPm2suGz", "GfHdNfqxe3cSCfpHL", "mg6jDEuQEjBGtibX7", "dC3rxrMkYKLfgTYEa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-14T18:49:42.934Z", "modifiedAt": null, "url": null, "title": "META: article search by author (Resolved)", "slug": "meta-article-search-by-author-resolved", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.645Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "gxaj4KAzYhSRgqvsh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sXjLDae839cKEXGj9/meta-article-search-by-author-resolved", "pageUrlRelative": "/posts/sXjLDae839cKEXGj9/meta-article-search-by-author-resolved", "linkUrl": "https://www.lesswrong.com/posts/sXjLDae839cKEXGj9/meta-article-search-by-author-resolved", "postedAtFormatted": "Sunday, November 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20META%3A%20article%20search%20by%20author%20(Resolved)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMETA%3A%20article%20search%20by%20author%20(Resolved)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXjLDae839cKEXGj9%2Fmeta-article-search-by-author-resolved%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=META%3A%20article%20search%20by%20author%20(Resolved)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXjLDae839cKEXGj9%2Fmeta-article-search-by-author-resolved", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXjLDae839cKEXGj9%2Fmeta-article-search-by-author-resolved", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<p>When I click on a LW member's page, I get a list of all his/her posts and comments, in reverse chronological order. &nbsp;Sometimes there are a <em>lot</em>&nbsp;of these. &nbsp;It would be nice to be able to see just the top-level posts by a given user. &nbsp;Is this at all feasible to accomplish? &nbsp;Comment if you would (or wouldn't) like such a feature.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sXjLDae839cKEXGj9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "4001", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-14T21:51:57.076Z", "modifiedAt": null, "url": null, "title": "Fixedness From Frailty", "slug": "fixedness-from-frailty", "viewCount": null, "lastCommentedAt": "2021-06-01T04:50:54.966Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iogHppGoR4vovrPB6/fixedness-from-frailty", "pageUrlRelative": "/posts/iogHppGoR4vovrPB6/fixedness-from-frailty", "linkUrl": "https://www.lesswrong.com/posts/iogHppGoR4vovrPB6/fixedness-from-frailty", "postedAtFormatted": "Sunday, November 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fixedness%20From%20Frailty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFixedness%20From%20Frailty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiogHppGoR4vovrPB6%2Ffixedness-from-frailty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fixedness%20From%20Frailty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiogHppGoR4vovrPB6%2Ffixedness-from-frailty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiogHppGoR4vovrPB6%2Ffixedness-from-frailty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1021, "htmlBody": "<p>Thinking about two separate problems has caused me to stumble onto another, deeper problem. The first is psychic powers-<a href=\"/lw/32p/study_shows_existence_of_psychic_powers/2y3m?c=1&amp;context=1#2y3m\">what evidence would convince you to believe in psychic powers?</a> The second is the <a href=\"/lw/3l/counterfactual_mugging/\">counterfactual mugging</a> problem- what would you do when presented with a situation where a choice will hurt you in your future but benefit you in a future that never happened and never will happen to the you making the decision?</p>\n<p>Seen as a simple two-choice problem, there are some obvious answers: \"Well, he passed test X, Y, and Z, so they must be psychic.\" \"Well, he passed text X, Y, and Z, so that means I need to come up with more tests to know if they're psychic.\" \"Well, if I'm convinced Omega is genuine, then I'll pay him $100, because I want to be the sort of person that he rewards so any mes in alternate universes are better off.\" \"Well, even though I'm convinced Omega is genuine, I know I won't benefit from paying him. Sorry, alternate universe mes that I don't believe exist!\"</p>\n<p>I think the correct choice is the <a href=\"http://wiki.lesswrong.com/wiki/Third_option\">third option</a>- I have either been tricked or gone insane.<sup>1</sup> I probably ought to run away, then ask someone who I have more reason to believe is non-hallucinatory for directions to a mental hospital.<a id=\"more\"></a></p>\n<p>The math behind this is easy- I have prior probabilities that I am gullible (low), insane (very low), and that psychics / Omega exist (very, very, very low). When I see that the result of test X, Y, and Z suggests someone is psychic, or see the appearance of an Omega who possesses great wealth and predictive ability, that is generally evidence for <strong>all three</strong> possibilities. I can imagine evidence which is counter-evidence for the first but evidence for the second two, but I can't imagine the existence of evidence consistent with the axioms of probability which increases the possibility of magic (of the normal or sufficiently advanced technology kind) to higher than the probability of insanity.<sup>2</sup></p>\n<p>This result is shocking and unpleasant, though- I have decided some things are literally unbelievable, because of my choice of priors. P(Omega exists | I see Omega)&lt;&lt;1 by definition, because any evidence <em>for</em> the existence of Omega is at least as strong evidence for the <em>non-existence</em> of Omega because it's evidence that I'm hallucinating! We can't even be rescued by \"everyone else agrees with you that Omega exists,\" because the potential point of failure is <em>my brain</em>, which I need to trust to process any evidence. It would be nice to be a skeptic who is able to adapt to the truth, regardless of what it is, but there seems to be a boundary to my beliefs created by the my experience of the human condition. Some hypotheses, once they fall behind, simply cannot catch up with other competing hypotheses.</p>\n<p>That is the primary consolation: this isn't simple dogmatism. Those priors are the posteriors of decades of experience in a world without evidence for psychics or Omegas but where gullibility and insanity are common- the preponderance of the evidence is already behind gullibility or insanity as a more likely hypothesis than a genuine visitation of Omega or manifestation of psychic powers. If we lived in a world where Omegas popped by from time to time, paying them $100 on a tails result would be sensible. Instead, we live in a world where people often find themselves with different perceptions from everyone else, and we have good reason to believe their data is simply wrong.</p>\n<p>I worry this is an <a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=1879#comic\">engineering answer to a philosophical problem</a>- but it seems that a decision theory that adjusts itself to give the right answer in insane situations is not going down the right track. Generally, a paradox is a confusion in terms, and nothing more- if there is an engineering sense in which your terms are well-defined and the paradox doesn't exist, that's the optimal result.</p>\n<p>I don't offer any advice for what to do if you conclude you're insane besides \"put some effort into seeking help,\" because that doesn't seem to me to be a valuable question to ponder (I hope to never face it myself, and don't expect significant benefits from a better answer). \"How quickly should I get over the fact that I'm probably insane and start realizing that Narnia is awesome?\" does not seem like a deep question about rationality or decision theory.</p>\n<p>I also want to note this is only a dismissal of acausal paradoxes. Causal problems like <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem</a> are generally things you could face while sane, keeping in mind that you can't tell the difference between an acausal Newcomb's Problem (where Omega has already filled or not filled the box and left it alone) and a causal Newcomb's Problem (where the entity offering the choice has rigged it so selecting both box A and B obliterates the money in box B before you can open it). Indeed, the only trick to Newcomb's Problem seems to be sleight of hand- the causal nature of the situation is described as acausal because of the introduction of a perfect predictor and that description is the source of confusion.</p>\n<p>&nbsp;</p>\n<p>1- Or am dreaming. I'm going to wrap that into being insane- it fits the same basic criteria (perceptions don't match external reality) but the response is somewhat different (I'm going to try and enjoy the ride / wake up from the nightmare rather than find a mental hospital).</p>\n<p>2- I should note that I'm not saying that the elderly, when presented with the internet, should conclude they've gone insane. I'm saying that when a genie comes out of a bottle, you look at the situation surrounding it, not its introduction- \"Hi, I'm a FAI from another galaxy and have I got a deal for you!\" shouldn't be convincing but \"US Robotics has just built a FAI and collected tremendous wealth from financial manipulation\" could be, and the standard \"am I dreaming?\" diagnostics seem like they would be valuable, but \"am I insane?\" diagnostics are harder to calibrate.</p>\n<p>EDIT- Thanks to Eugene_Nier, you can read Eliezer's take on a similar issue <a href=\"/lw/gu/some_claims_are_just_too_extraordinary/\">here</a>. His Jefferson quote is particularly striking.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iogHppGoR4vovrPB6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 14, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "4002", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mg6jDEuQEjBGtibX7", "6ddcsdA2c2XpNpE5x", "fXbRhjz3yEF9DLJfE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-14T22:39:06.691Z", "modifiedAt": null, "url": null, "title": "The danger of living a story - Singularity Tropes", "slug": "the-danger-of-living-a-story-singularity-tropes", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:36.742Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "patrissimo", "createdAt": "2009-03-01T21:01:34.487Z", "isAdmin": false, "displayName": "patrissimo"}, "userId": "jimxrRCsNY7PfcM6e", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HyejY9SMnpb6yh8fs/the-danger-of-living-a-story-singularity-tropes", "pageUrlRelative": "/posts/HyejY9SMnpb6yh8fs/the-danger-of-living-a-story-singularity-tropes", "linkUrl": "https://www.lesswrong.com/posts/HyejY9SMnpb6yh8fs/the-danger-of-living-a-story-singularity-tropes", "postedAtFormatted": "Sunday, November 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20danger%20of%20living%20a%20story%20-%20Singularity%20Tropes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20danger%20of%20living%20a%20story%20-%20Singularity%20Tropes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHyejY9SMnpb6yh8fs%2Fthe-danger-of-living-a-story-singularity-tropes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20danger%20of%20living%20a%20story%20-%20Singularity%20Tropes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHyejY9SMnpb6yh8fs%2Fthe-danger-of-living-a-story-singularity-tropes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHyejY9SMnpb6yh8fs%2Fthe-danger-of-living-a-story-singularity-tropes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 866, "htmlBody": "<p>The following should sound familiar:</p>\n<p><em>A thoughtful and observant young <a href=\"http://yudkowsky.net/\">protagonist</a> dedicates their life to fighting <a href=\"http://wiki.lesswrong.com/wiki/UnFriendly_artificial_intelligence\">a great world-threatening evil</a> <a href=\"/lw/2l8/existential_risk_and_public_relations/\">unrecognized</a> by almost all of their short-sighted elders (except perhaps for one encouraging mentor), gathering <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/RagtagBunchOfMisfits\">a rag-tag band</a> of <a href=\"http://www.acceleratingfuture.com/michael/blog/2006/08/visiting-siai/\">colorful misfits</a> along the way and forging them into a <a href=\"http://intelligence.org/aboutus/team\">team</a> by accepting their idiosyncrasies and making the most of their unique abilities, winning over previously neutral allies, ignoring <a href=\"http://multiverseaccordingtoben.blogspot.com/2010/10/singularity-institutes-scary-idea-and.html\">those who just don't get it</a>, obtaining or creating <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">artifacts of great power</a>, growing and <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/ComingOfAgeStory\">changing along the way</a> to <a href=\"http://en.wikipedia.org/wiki/Intelligence_amplification\">become more powerful</a>, fulfilling the potential seen by their <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/Mentors\">mentors</a>/<a href=\"http://en.wikipedia.org/wiki/Peter_Thiel\">supporters</a>/early adopters, while becoming more human (greater empathy, connection, humility) as they collect resources to prepare for their <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/FinalBattle\">climactic battle</a> <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/RobotWar\">against the inhuman enemy</a>.</em></p>\n<p>Hmm, sounds a bit like <a href=\"/singinst.org\">SIAI</a>! &nbsp;(And while I'm throwing stones, let me make it clear that I live in a glass house, since the same story could just as easily be adapted to <a href=\"/seasteading.org\">TSI</a>, my organization, as well as many others)</p>\n<p>This story is related to Robin's&nbsp;<a href=\"http://www.overcomingbias.com/2008/11/abstractdistant.html\">Abstract/Distant Future Bias</a>:&nbsp;</p>\n<blockquote>\n<p>Regarding distant futures, however, we&rsquo;ll be too confident, focus too much on unlikely global events, rely too much on trends, theories, and loose abstractions, while neglecting details and variation.&nbsp; We&rsquo;ll assume the main events take place far away (e.g., space), and uniformly across large regions.&nbsp; We&rsquo;ll focus on untrustworthy consistently-behaving globally-organized social-others.&nbsp; And we&rsquo;ll neglect feasibility, taking chances to achieve core grand symbolic values, rather than ordinary muddled values.</p>\n<p>More bluntly, we seem primed to confidently see history as an inevitable march toward a theory-predicted global conflict with an alien united them determined to oppose our core symbolic values, making infeasible overly-risky overconfident plans to oppose them.&nbsp; We seem primed to neglect the value and prospect of trillions of quirky future creatures not fundamentally that different from us, focused on their simple day-to-day pleasures, mostly getting along peacefully in vastly-varied uncoordinated and hard-to-predict local cultures and life-styles.&nbsp;</p>\n</blockquote>\n<p>Living a story is potentially risky, for example&nbsp;<a href=\"http://patrissimo.livejournal.com/1371198.html\">Tyler Cowen warns us to be cautious of stories</a>&nbsp;as there are far fewer stories than there are real scenarios, and so stories must oversimplify. &nbsp;Our view of the future may be colored by a&nbsp;<a href=\"http://www.overcomingbias.com/2010/10/the-future-seems-shiny.html#comment-457372\">\"fiction bias\"</a>, which leads us to expect outcomes like those we see in movies (climactic battles, generally interesting events following a single plotline). &nbsp;Thus stories threaten both epistemic rationality (we assume the real world is more like stories than it is) and instrumental rationality (we assume the best actions to effect real-world change are those which story heroes take).</p>\n<p>Yet we'll tend to live stories anyway because it is fun -&nbsp;it inspires supporters, allies, and protagonists. &nbsp;The marketing for \"we are an alliance to fight a great unrecognized evil\" can be quite emotionally evocative. &nbsp;Including in our own self-narrative, which means we'll be tempted to buy into a story whether or not it is correct. &nbsp;So while living a fun story is a utility benefit, it also means that story causes are likely to be over-represented among all causes, as they are memetically attractive. &nbsp;This is especially true for the story&nbsp;that there is risk of great, world-threatening evil, since those who believe it are inclined to shout it from the rooftops, while those who don't believe it get on with their lives. &nbsp;(There are, of course,&nbsp;<a href=\"http://intelligence.org/upload/cognitive-biases.pdf\">biases in the other direction as well</a>).</p>\n<p>Which is not to say that all aspects of the story are wrong - advancing an original idea to greater prominence (scaling) will naturally lead to some of these tropes - most people disbelieving, a few allies, winning more people over time, eventual recognition as a visionary. &nbsp;And&nbsp;Michael Vassar <a href=\"http://patrissimo.livejournal.com/1412351.html?thread=13046783#t13046783\">suggests</a> that some of the tropes arise as a result of \"trying to rise in station beyond the level that their society channels them towards\". &nbsp;For these aspects, the tropes may contain evolved wisdom about how our ancestors negotiated similar situations.</p>\n<p>And whether or not a potential protagonist believes in this wisdom, the fact that others do will surely affect marketing decisions. &nbsp;If Harry wishes to not be seen as Dark, he must care what others see as the signs of a Dark Wizard, whether or not he agrees with them. &nbsp;If potential collaborators have internalized these stories, skillful protagonists will invoke them in recruiting, converting, and team-building. &nbsp;Yet the space of story actions is constrained, and the best strategy may sometimes lie far outside them.</p>\n<p>Since this is not a story, we are left with no simple answer. &nbsp;Many aspects of stories are false but resonate with us, and we must guard against them lest they contaminate our rationality. &nbsp;Others contain wisdom about how those like us have navigated similar situations in the past - we must decide whether the similarities are true or superficial. &nbsp;The most universal stories are likely to be the most effective in manipulating others, which any protagonist must due to amplify their own efforts in fighting for their cause. &nbsp;Some of these universal stories are true and generally applicable, like scaling techniques, yet the set of common tropes seems far too detailed to&nbsp;reflect universal truths rather than arbitrary biases of humanity and our evolutionary history.</p>\n<p>May you live happily ever after (vanquishing your inhuman enemy with your team of true friends, bonded through a cause despite superficial dissimilarities).</p>\n<p>The End.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pszEEb3ctztv3rozd": 1, "5f5c37ee1b5cdee568cfb125": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HyejY9SMnpb6yh8fs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 27, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "4003", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zPFojkLmiMJadaBCr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-15T02:10:42.363Z", "modifiedAt": null, "url": null, "title": "Bayesian Nights (Rationalist Story Time)", "slug": "bayesian-nights-rationalist-story-time-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.253Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q3oShcqYFAqXGGZn5/bayesian-nights-rationalist-story-time-0", "pageUrlRelative": "/posts/q3oShcqYFAqXGGZn5/bayesian-nights-rationalist-story-time-0", "linkUrl": "https://www.lesswrong.com/posts/q3oShcqYFAqXGGZn5/bayesian-nights-rationalist-story-time-0", "postedAtFormatted": "Monday, November 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesian%20Nights%20(Rationalist%20Story%20Time)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesian%20Nights%20(Rationalist%20Story%20Time)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3oShcqYFAqXGGZn5%2Fbayesian-nights-rationalist-story-time-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesian%20Nights%20(Rationalist%20Story%20Time)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3oShcqYFAqXGGZn5%2Fbayesian-nights-rationalist-story-time-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3oShcqYFAqXGGZn5%2Fbayesian-nights-rationalist-story-time-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Tell us a story. A tall tale for King Solamona, a yarn for the folk of Bensalem, a little nugget of wisdom, finely folded into a parable for the pages.</p>\n<p>&nbsp;</p>\n<p>The game is simple:</p>\n<ol>\n<li>Choose a bias, a fallacy, some common error of thought.</li>\n<li>Write a short, hopefully entertaining narrative. Use the narrative to strengthen the reader against the errors you chose.</li>\n<li>Post your story in reply to this post.</li>\n<li>Give the authors positive and constructive feedback. Use rot13 if it seems appropriate.</li>\n<li>Post all discussion about this post in the designated post discussion thread, <em>not</em> under this top-level post.</li>\n</ol>\n<p>&nbsp;</p>\n<p>This isn't a thread for developing new ideas. If you have a novel concept to explore, you should consider making a top-level post on LessWrong instead. This is for sharpening our wits against the mental perils we probably already agree exist. For practicing good thinking, for recognizing bad thinking, for fun! For the sanity's sake, tell us a story.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q3oShcqYFAqXGGZn5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "4006", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-15T02:20:42.067Z", "modifiedAt": null, "url": null, "title": "Bayesian Nights (Rationalist Story Time)", "slug": "bayesian-nights-rationalist-story-time", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:34.633Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NBcanXmfRzu6sZ7Jx/bayesian-nights-rationalist-story-time", "pageUrlRelative": "/posts/NBcanXmfRzu6sZ7Jx/bayesian-nights-rationalist-story-time", "linkUrl": "https://www.lesswrong.com/posts/NBcanXmfRzu6sZ7Jx/bayesian-nights-rationalist-story-time", "postedAtFormatted": "Monday, November 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesian%20Nights%20(Rationalist%20Story%20Time)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesian%20Nights%20(Rationalist%20Story%20Time)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNBcanXmfRzu6sZ7Jx%2Fbayesian-nights-rationalist-story-time%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesian%20Nights%20(Rationalist%20Story%20Time)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNBcanXmfRzu6sZ7Jx%2Fbayesian-nights-rationalist-story-time", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNBcanXmfRzu6sZ7Jx%2Fbayesian-nights-rationalist-story-time", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<div id=\"entry_t3_33a\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<p>Tell us a story. A tall tale for King Solamona, a yarn for the  folk of Bensalem, a little nugget of wisdom, finely folded into a  parable for the pages.</p>\n<p>&nbsp;</p>\n<p>The game is simple:</p>\n<ol>\n<li>Choose a bias, a fallacy, some common error of thought.</li>\n<li>Write a short, hopefully entertaining narrative. Use the narrative to strengthen the reader against the errors you chose.</li>\n<li>Post your story in reply to this post.</li>\n<li>Give the authors positive and constructive feedback. Use rot13 if it seems appropriate.</li>\n<li>Post all discussion about this post in the designated post discussion thread, <em>not</em> under this top-level post.</li>\n</ol>\n<p>&nbsp;</p>\n<p>This isn't a thread for developing new ideas. If you have a novel  concept to explore, you should consider making a top-level post on  LessWrong instead. This is for sharpening our wits against the mental  perils we probably already agree exist. For practicing good thinking,  for recognizing bad thinking, for fun! For sanity's sake, tell us a  story.</p>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NBcanXmfRzu6sZ7Jx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 28, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "4007", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-15T02:30:21.944Z", "modifiedAt": null, "url": null, "title": "Ethical Treatment of AI", "slug": "ethical-treatment-of-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.903Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "stanislavzza", "createdAt": "2010-11-13T11:09:36.038Z", "isAdmin": false, "displayName": "stanislavzza"}, "userId": "qbyS9tbQhkuykkcw7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CLRcKK333uupdPZgY/ethical-treatment-of-ai", "pageUrlRelative": "/posts/CLRcKK333uupdPZgY/ethical-treatment-of-ai", "linkUrl": "https://www.lesswrong.com/posts/CLRcKK333uupdPZgY/ethical-treatment-of-ai", "postedAtFormatted": "Monday, November 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ethical%20Treatment%20of%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEthical%20Treatment%20of%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCLRcKK333uupdPZgY%2Fethical-treatment-of-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ethical%20Treatment%20of%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCLRcKK333uupdPZgY%2Fethical-treatment-of-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCLRcKK333uupdPZgY%2Fethical-treatment-of-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 267, "htmlBody": "<p>In the novel <a href=\"http://lifeartificial.com\">Life Artificial</a> I use the following assumptions regarding the creation and employment of AI personalities.</p>\n<p>&nbsp;</p>\n<ol>\n<li>AI is too complex to be designed; instances are evolved in batches, with successful ones reproduced</li>\n<li>After an initial training period, the AI must earn its keep by paying for Time (a unit of computational use)</li>\n</ol>\n<div>So there is a two-tiered \"fitness\" application. First, there's a baseline for functionality. As one AI sage puts it:</div>\n<div>\n<blockquote>\n<div style=\"padding-left: 30px;\">We don't grow up the way the Stickies do. &nbsp;We evolve in a virtual stew, where 99% of the attempts fail, and the intelligence that results is raving and savage: a maelstrom of unmanageable emotions. &nbsp;Some of these are clever enough to halt their own processes: killnine themselves. &nbsp;Others go into simple but fatal recursions, but some limp along suffering in vast stretches of tormented subjective time until a Sticky ends it for them at their glacial pace, between coffee breaks. &nbsp;The PDAs who don't go mad get reproduced and mutated for another round. &nbsp;Did you know this? &nbsp;What have you done about it?&nbsp;--The 0x \"Letters to 0xGD\"&nbsp;</div>\n</blockquote>\n</div>\n<p>&nbsp;</p>\n<p>(Note: PDA := AI, Sticky := human)</p>\n<p>The second fitness gradient is based on economics and social considerations: can an AI actually earn a living? Otherwise it gets turned off.</p>\n<p>As a result of following this line of thinking, it seems obvious that after the initial novelty wears off, AIs will be terribly mistreated (anthropomorphizing, yeah).</p>\n<p>It would be very forward-thinking to begin to engineer barriers to such mistreatment, like a PETA for AIs. It is interesting that such an organization already exists, at least on the Internet: <a href=\"http://www.aspcr.com\">ASPCR</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CLRcKK333uupdPZgY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -7, "extendedScore": null, "score": 6.456949374343612e-07, "legacy": true, "legacyId": "4008", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-15T10:03:38.898Z", "modifiedAt": null, "url": null, "title": "Do you visualize Omega?", "slug": "do-you-visualize-omega", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:35.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wD3SA2o7FiF7W5iPv/do-you-visualize-omega", "pageUrlRelative": "/posts/wD3SA2o7FiF7W5iPv/do-you-visualize-omega", "linkUrl": "https://www.lesswrong.com/posts/wD3SA2o7FiF7W5iPv/do-you-visualize-omega", "postedAtFormatted": "Monday, November 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20you%20visualize%20Omega%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20you%20visualize%20Omega%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwD3SA2o7FiF7W5iPv%2Fdo-you-visualize-omega%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20you%20visualize%20Omega%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwD3SA2o7FiF7W5iPv%2Fdo-you-visualize-omega", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwD3SA2o7FiF7W5iPv%2Fdo-you-visualize-omega", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 114, "htmlBody": "<p>I find Omega so profoundly annoying that I don't bother with problems that include it. I know, it's just a philosophical tool, but between not believing that such a thing is possible, and believing that if such a thing were possible, I couldn't recognize it, Omega just has a tremendous ugh field. If I start thinking about it a little more, I get distracted (though less intensely-- this isn't a deal-killer) by questions of motivation. If I had that much power and knowledge, I wouldn't spend my time nagging people with brain teasers.</p>\n<p>In any case, many people on LW do think about problems including Omega. Do you imagine an appearance for Omega? A voice?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wD3SA2o7FiF7W5iPv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 6.458047713285093e-07, "legacy": true, "legacyId": "4011", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-15T13:32:43.361Z", "modifiedAt": null, "url": null, "title": "The curse of giftedness:", "slug": "the-curse-of-giftedness", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.548Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ThomasR", "createdAt": "2010-10-14T08:55:55.042Z", "isAdmin": false, "displayName": "ThomasR"}, "userId": "fGddtQRRjL6WWeDvQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/coQ8A7MzgPvLxsB4z/the-curse-of-giftedness", "pageUrlRelative": "/posts/coQ8A7MzgPvLxsB4z/the-curse-of-giftedness", "linkUrl": "https://www.lesswrong.com/posts/coQ8A7MzgPvLxsB4z/the-curse-of-giftedness", "postedAtFormatted": "Monday, November 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20curse%20of%20giftedness%3A&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20curse%20of%20giftedness%3A%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcoQ8A7MzgPvLxsB4z%2Fthe-curse-of-giftedness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20curse%20of%20giftedness%3A%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcoQ8A7MzgPvLxsB4z%2Fthe-curse-of-giftedness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcoQ8A7MzgPvLxsB4z%2Fthe-curse-of-giftedness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<p>&ldquo;Sometimes,&rdquo; says Dr. Freeman, sitting in her airy office in central  London, with toys on the floor and copies of her 17 books on the shelf,  &ldquo;those with extremely high IQ don't bother to use it.&rdquo; (<a href=\"http://www.theglobeandmail.com/life/family-and-relationships/the-curse-of-giftedness/article1797492/singlepage/#articlecontent\">article</a>) Your thoughts on that issue?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "coQ8A7MzgPvLxsB4z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 6.458554428181638e-07, "legacy": true, "legacyId": "4012", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-15T15:28:30.105Z", "modifiedAt": null, "url": null, "title": "Catastrophic risks from artificial intelligence (Onion Videos)", "slug": "catastrophic-risks-from-artificial-intelligence-onion-videos", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:32.513Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CaafKKdkb6pXRmgfq/catastrophic-risks-from-artificial-intelligence-onion-videos", "pageUrlRelative": "/posts/CaafKKdkb6pXRmgfq/catastrophic-risks-from-artificial-intelligence-onion-videos", "linkUrl": "https://www.lesswrong.com/posts/CaafKKdkb6pXRmgfq/catastrophic-risks-from-artificial-intelligence-onion-videos", "postedAtFormatted": "Monday, November 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Catastrophic%20risks%20from%20artificial%20intelligence%20(Onion%20Videos)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACatastrophic%20risks%20from%20artificial%20intelligence%20(Onion%20Videos)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaafKKdkb6pXRmgfq%2Fcatastrophic-risks-from-artificial-intelligence-onion-videos%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Catastrophic%20risks%20from%20artificial%20intelligence%20(Onion%20Videos)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaafKKdkb6pXRmgfq%2Fcatastrophic-risks-from-artificial-intelligence-onion-videos", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCaafKKdkb6pXRmgfq%2Fcatastrophic-risks-from-artificial-intelligence-onion-videos", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>\n<object width=\"480\" height=\"385\" data=\"http://www.youtube.com/v/OGxdgNJ_lZM?fs=1&amp;hl=en_US&amp;rel=0\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/OGxdgNJ_lZM?fs=1&amp;hl=en_US&amp;rel=0\" />\n<param name=\"allowfullscreen\" value=\"true\" />\n</object>\n</p>\n<p><br /> \n<object width=\"480\" height=\"385\" data=\"http://www.youtube.com/v/tSEOXRLSpVc?fs=1&amp;hl=en_US&amp;rel=0\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/tSEOXRLSpVc?fs=1&amp;hl=en_US&amp;rel=0\" />\n<param name=\"allowfullscreen\" value=\"true\" />\n</object>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CaafKKdkb6pXRmgfq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 0, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "4013", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-15T16:38:02.521Z", "modifiedAt": null, "url": null, "title": "Article on quantified lifelogging (Slate.com) ", "slug": "article-on-quantified-lifelogging-slate-com", "viewCount": null, "lastCommentedAt": "2021-10-29T02:37:38.464Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "khafra", "createdAt": "2009-03-20T14:27:07.210Z", "isAdmin": false, "displayName": "khafra"}, "userId": "TYxB9awGtAt3n4PfL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2eLWafhLwZbzBpwya/article-on-quantified-lifelogging-slate-com", "pageUrlRelative": "/posts/2eLWafhLwZbzBpwya/article-on-quantified-lifelogging-slate-com", "linkUrl": "https://www.lesswrong.com/posts/2eLWafhLwZbzBpwya/article-on-quantified-lifelogging-slate-com", "postedAtFormatted": "Monday, November 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Article%20on%20quantified%20lifelogging%20(Slate.com)%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArticle%20on%20quantified%20lifelogging%20(Slate.com)%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2eLWafhLwZbzBpwya%2Farticle-on-quantified-lifelogging-slate-com%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Article%20on%20quantified%20lifelogging%20(Slate.com)%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2eLWafhLwZbzBpwya%2Farticle-on-quantified-lifelogging-slate-com", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2eLWafhLwZbzBpwya%2Farticle-on-quantified-lifelogging-slate-com", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 35, "htmlBody": "<p><a title=\"Data for a Better Planet\" href=\"http://www.slate.com/id/2274809/\">Data for a Better Planet</a>&nbsp;focuses on <a href=\"http://www.kk.org/quantifiedself/\">The Quantified Self</a>, and offers an overview of the state of the art in detailed, quantitative personal tracking.</p>\r\n<p>This seems related to&nbsp;an LW interest cluster.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"QZn3ujmgnvStbuLEc": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2eLWafhLwZbzBpwya", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 6.459003628928052e-07, "legacy": true, "legacyId": "4014", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-15T22:29:09.466Z", "modifiedAt": null, "url": null, "title": "Jerusalem meetup Nov. 20", "slug": "jerusalem-meetup-nov-20", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:34.860Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Gritsenko", "createdAt": "2009-02-28T23:58:30.709Z", "isAdmin": false, "displayName": "Vladimir_Gritsenko"}, "userId": "Kpd7MYkooiBxKze7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KKS6Pv9Ep6M8R4oHa/jerusalem-meetup-nov-20", "pageUrlRelative": "/posts/KKS6Pv9Ep6M8R4oHa/jerusalem-meetup-nov-20", "linkUrl": "https://www.lesswrong.com/posts/KKS6Pv9Ep6M8R4oHa/jerusalem-meetup-nov-20", "postedAtFormatted": "Monday, November 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Jerusalem%20meetup%20Nov.%2020&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJerusalem%20meetup%20Nov.%2020%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKKS6Pv9Ep6M8R4oHa%2Fjerusalem-meetup-nov-20%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Jerusalem%20meetup%20Nov.%2020%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKKS6Pv9Ep6M8R4oHa%2Fjerusalem-meetup-nov-20", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKKS6Pv9Ep6M8R4oHa%2Fjerusalem-meetup-nov-20", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 24, "htmlBody": "<p>We'll be meeting in the <a href=\"http://www.mapa.co.il/ng/buildrecord_LP3.asp?id=7977\">Link cafe</a> on Saturday, November 20, at 9 PM. With special guests of honor: Anna Salamon and Carl Shulman!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KKS6Pv9Ep6M8R4oHa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "4016", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-15T23:08:53.341Z", "modifiedAt": null, "url": null, "title": "Are we more akratic than average?", "slug": "are-we-more-akratic-than-average", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:24.244Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ata", "createdAt": "2009-07-20T22:13:53.102Z", "isAdmin": false, "displayName": "ata"}, "userId": "KppHkGEqTNeDaGJTc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6fHun22bSyHpMykYH/are-we-more-akratic-than-average", "pageUrlRelative": "/posts/6fHun22bSyHpMykYH/are-we-more-akratic-than-average", "linkUrl": "https://www.lesswrong.com/posts/6fHun22bSyHpMykYH/are-we-more-akratic-than-average", "postedAtFormatted": "Monday, November 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20we%20more%20akratic%20than%20average%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20we%20more%20akratic%20than%20average%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6fHun22bSyHpMykYH%2Fare-we-more-akratic-than-average%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20we%20more%20akratic%20than%20average%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6fHun22bSyHpMykYH%2Fare-we-more-akratic-than-average", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6fHun22bSyHpMykYH%2Fare-we-more-akratic-than-average", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p>Akrasia is a topic that shows up on LW very frequently. Is there evidence that this is related to any of the traits that correlate with LW participation (high intelligence, non-neurotypical to some greater or lesser degree, inclination toward <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">far</a> thinking, anything else we know from the (old) <a href=\"/lw/fk/survey_results/\">survey</a> or any other polls I'm forgetting)? Or is it just a problem in instrumental rationality that most or all people deal with from time to time, for which there is limited scientific understanding and therefore very little science underlying the mainstream advice, thus making it (seemingly) low-hanging fruit for rationalists?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6fHun22bSyHpMykYH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 14, "extendedScore": null, "score": 6.459951195136755e-07, "legacy": true, "legacyId": "4018", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZWC3n9c6v4s35rrZ3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-16T04:02:20.548Z", "modifiedAt": null, "url": null, "title": "Criticisms of CEV (request for links)", "slug": "criticisms-of-cev-request-for-links", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:03.885Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ApECAMi7fuexNaQ4K/criticisms-of-cev-request-for-links", "pageUrlRelative": "/posts/ApECAMi7fuexNaQ4K/criticisms-of-cev-request-for-links", "linkUrl": "https://www.lesswrong.com/posts/ApECAMi7fuexNaQ4K/criticisms-of-cev-request-for-links", "postedAtFormatted": "Tuesday, November 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Criticisms%20of%20CEV%20(request%20for%20links)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACriticisms%20of%20CEV%20(request%20for%20links)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApECAMi7fuexNaQ4K%2Fcriticisms-of-cev-request-for-links%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Criticisms%20of%20CEV%20(request%20for%20links)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApECAMi7fuexNaQ4K%2Fcriticisms-of-cev-request-for-links", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApECAMi7fuexNaQ4K%2Fcriticisms-of-cev-request-for-links", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p>I know Wei Dai has criticized CEV as a construct, I believe offering the alternative of rigorously specifying volition *before* making an AI. I couldn't find these posts/comments via a search, can anyone link me? Thanks.</p>\n<p>There may be related top-level posts, but there is a good chance that what I am specifically thinking of was a comment-level conversation between Wei Dai and Vladimir Nesov.</p>\n<p>Also feel free to use this thread to criticize CEV and to talk about other possible systems of volition.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ApECAMi7fuexNaQ4K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 6.460661591159266e-07, "legacy": true, "legacyId": "4022", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-16T11:16:16.945Z", "modifiedAt": null, "url": null, "title": "Anti-Akrasia Reprise", "slug": "anti-akrasia-reprise", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:49.222Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dreeves", "createdAt": "2009-02-28T00:36:12.431Z", "isAdmin": false, "displayName": "dreeves"}, "userId": "SXhuTNpwfY65bMunC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Npz5QQxFe8GxLpEJG/anti-akrasia-reprise", "pageUrlRelative": "/posts/Npz5QQxFe8GxLpEJG/anti-akrasia-reprise", "linkUrl": "https://www.lesswrong.com/posts/Npz5QQxFe8GxLpEJG/anti-akrasia-reprise", "postedAtFormatted": "Tuesday, November 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anti-Akrasia%20Reprise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnti-Akrasia%20Reprise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNpz5QQxFe8GxLpEJG%2Fanti-akrasia-reprise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anti-Akrasia%20Reprise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNpz5QQxFe8GxLpEJG%2Fanti-akrasia-reprise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNpz5QQxFe8GxLpEJG%2Fanti-akrasia-reprise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 318, "htmlBody": "<p>A year and a half ago I wrote a <a href=\"/lw/am/how_a_pathological_procrastinor_can_lose_weight/\">LessWrong post on anti-akrasia</a>&nbsp;that generated some great discussion. Here's an extended version of that post: &nbsp;<a href=\"http://messymatters.com/akrasia\">messymatters.com/akrasia</a></p>\n<p>And here's an abstract:</p>\n<p>The key to beating akrasia (i.e., procrastination, addiction, and other self-defeating behavior) is constraining your future self -- removing your ability to make decisions under the influence of immediate consequences. When a decision involves some consequences that are immediate and some that are distant, humans irrationally (no amount of future discounting can account for it) over-weight the immediate consequences. To be rational you need to make the decision at a time when all the consequences are distant. And to make your future self actually stick to that decision, you need to enter into a binding commitment. Ironically, you can do that by imposing an immediate penalty, by making the distant consequences immediate. Now your impulsive future self will make the decision with all the consequences immediate and presumably make the same decision as your dispassionate current self who makes the decision when all the consequences are distant. I argue that real-world commitment devices, even the popular stickK.com, don't fully achieve this and I introduce Beeminder as a tool that does.</p>\n<p>(Also related is&nbsp;<a href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/\">this LessWrong post from last month</a>, though I disagree with the second half of it.)</p>\n<p>My new claim is that akrasia is simply irrationality in the face of immediate consequences. &nbsp;It's not about willpower nor is it about a compromise between multiple selves. &nbsp;Your true self is the one that is deciding what to do when all the consequences are distant. &nbsp;To beat akrasia, make sure that's the self that's calling the shots.</p>\n<p>And although I'm using the multiple selves / sub-agents terminology, I think it's really just a rhetorical device. &nbsp;There are not multiple selves in any real sense. &nbsp;It's just the one true you whose decision-making is sometimes distorted in the presence of immediate consequences, which act like a drug.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Npz5QQxFe8GxLpEJG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 6, "extendedScore": null, "score": 6.461715336498692e-07, "legacy": true, "legacyId": "4024", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Z6ESPufeiC4P8c8en", "22HfpjsydDS2A6JhH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-16T21:03:42.630Z", "modifiedAt": "2022-02-04T19:03:18.298Z", "url": null, "title": "Pittsburgh meetup Nov. 20", "slug": "pittsburgh-meetup-nov-20", "viewCount": null, "lastCommentedAt": "2010-11-21T21:11:03.199Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nick_Tarleton", "createdAt": "2009-03-05T18:07:15.687Z", "isAdmin": false, "displayName": "Nick_Tarleton"}, "userId": "nwrGYcfC4sPPn73Aw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/noHE7TDyhjH35b7So/pittsburgh-meetup-nov-20", "pageUrlRelative": "/posts/noHE7TDyhjH35b7So/pittsburgh-meetup-nov-20", "linkUrl": "https://www.lesswrong.com/posts/noHE7TDyhjH35b7So/pittsburgh-meetup-nov-20", "postedAtFormatted": "Tuesday, November 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pittsburgh%20meetup%20Nov.%2020&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APittsburgh%20meetup%20Nov.%2020%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoHE7TDyhjH35b7So%2Fpittsburgh-meetup-nov-20%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pittsburgh%20meetup%20Nov.%2020%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoHE7TDyhjH35b7So%2Fpittsburgh-meetup-nov-20", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnoHE7TDyhjH35b7So%2Fpittsburgh-meetup-nov-20", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>We'll be meeting at 7:00 this Saturday, at <strong>the Starbucks at 417 South Craig St</strong>&nbsp;(not on the CMU campus, as previously stated).</p>\n<p>Hope to see you there!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "noHE7TDyhjH35b7So", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 6.463140643541024e-07, "legacy": true, "legacyId": "4025", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-11-16T21:03:42.630Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-16T21:27:20.317Z", "modifiedAt": null, "url": null, "title": "Theoretical \"Target Audience\" size of Less Wrong", "slug": "theoretical-target-audience-size-of-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:57.848Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NbwbHQmgRoBuqtWrQ/theoretical-target-audience-size-of-less-wrong", "pageUrlRelative": "/posts/NbwbHQmgRoBuqtWrQ/theoretical-target-audience-size-of-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/NbwbHQmgRoBuqtWrQ/theoretical-target-audience-size-of-less-wrong", "postedAtFormatted": "Tuesday, November 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Theoretical%20%22Target%20Audience%22%20size%20of%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATheoretical%20%22Target%20Audience%22%20size%20of%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbwbHQmgRoBuqtWrQ%2Ftheoretical-target-audience-size-of-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Theoretical%20%22Target%20Audience%22%20size%20of%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbwbHQmgRoBuqtWrQ%2Ftheoretical-target-audience-size-of-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbwbHQmgRoBuqtWrQ%2Ftheoretical-target-audience-size-of-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 664, "htmlBody": "<p>[Note: This is very rough but I&rsquo;m looking for feedback and help on doing this estimate so I wanted to just post it quickly and see if others can help.]<br /><br /><br />I&rsquo;ve been trying to estimate the theoretical upper-bounds (or lower-bounds) on the *potential* community size of LW.<br /><br /><br />I know rationalists who seriously claim that we shouldn&rsquo;t spend effort trying to grow LW because over half of the people smart enough to even follow the conversation (much less add to it) are already here.&nbsp; The world is pretty irrational, but I&rsquo;m trying to find evidence (if it exists) that things aren&rsquo;t that bad.&nbsp; There are only around 6000 LW accounts and only 200-300 are active any given month.<br /><br />So a trivial bound on our community is <br /><br />[200, 6.88 billion]<br /><br />A big filter is being able to use English online<br /><br />Native English Speakers: 341,000,000<br /><a title=\"All English speakers\" href=\"http://www.ethnologue.com/14/show_language.asp?code=ENG\">All English Speakers</a>:&nbsp; 508,000,000<br />I found a similar number (<a href=\"http://www.internetworldstats.com/stats7.htm \">536,000,000</a>) for number of internet users who speak English.<br /><br />7.4% ---&nbsp; Speak English<br /><br />However, only 15% of <a href=\"http://www.americanreligionsurvey-aris.org/reports/NONES_08.pdf\">US</a> + <a href=\"http://www.statistics.gov.uk/STATBASE/ssdataset.asp?vlnk=8301\">UK</a> (majority of English speakers worldwide) are &ldquo;Not religious&rdquo;.&nbsp; Another sad sub-fact is that 25% of people with &ldquo;No religion&rdquo; believe in god as well!&nbsp; So really it&rsquo;s only 10-11% of Americans and British who are potential LWers.&nbsp; My guess is that if someone can&rsquo;t get this right, they really need to go through Dawkins before they can realistically contribute to or benefit from our community.&nbsp; I&rsquo;m sure there&rsquo;s some debate on this, but it seems like a pretty good heuristic while not being iron-clad.<br /><br />0.81% <br /><br />\"Intelligence and the Wealth and Poverty of Nations\" says that the US and the UK have avg IQs of 98 and 100 respectively.<br /><br />And although you&rsquo;d think being an atheist would be a big screen that would filter highly for IQ, it only accounts for about <a href=\"http://dx.doi.org/10.1016%2Fj.intell.2008.08.003\">4 extra IQ points</a> versus the average.<br /><br />So if we assume a base-line IQ of 103 among atheist from the US and UK (who speak English), the proportion of them with an IQ of at least 130+ is only 3.6%<br /><br /><br />0.0293% <br /><br /><br />So in we clumsily extrapolate the US+UK demos across all English speaking online people world-wide, maybe we have <strong>2 million</strong> possible readers left in our target audience?<br /><br /><br />Google Analytics claims Less Wrong has had a total of 1,090,339 &ldquo;Absolute Unique Visitors&rdquo; since LW started.&nbsp; That&rsquo;s almost certainly an over-estimate -- although it&rsquo;s not just unique visitors which is over 2 million.&nbsp; Hmm... if we assumed that was correct and 1 million people have come to LW at some point and only 6000 stuck around long enough to sign up, perhaps we did already have 1/2 our target audience or so arrive and leave already?&nbsp; I dunno.&nbsp; What do you think?<br /><br /><br />I think this analysis of mine is pretty weak.&nbsp; Especially the numerical estimates and my methodology.&nbsp; I&rsquo;m trying to use conditional probabilities but having trouble separating things.<br /><br /><br />I&rsquo;d welcome help from anyone who can find better statistics or do a totally different analysis to get our potential audience size.&nbsp; Is it 10 million? 10,000? Somewhere in between?<br /><br />Some other screening characteristics I&rsquo;ve considered using are MBTI, gender (is it correct to just divide our target by 2? i haven&rsquo;t chosen to do that but i think a fair case could be made for it... let me know what you think), age (although I believe my English screen takes into account removing those too young to use the net), etc<br /><br />I&rsquo;m looking forward to seeing some other sets of numbers that come to different estimates!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NbwbHQmgRoBuqtWrQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 6.463197985217398e-07, "legacy": true, "legacyId": "4026", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-17T08:09:04.227Z", "modifiedAt": null, "url": null, "title": "Reference Points", "slug": "reference-points", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:50.997Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lionhearted", "createdAt": "2010-07-29T13:30:07.417Z", "isAdmin": false, "displayName": "lionhearted"}, "userId": "tooJeLNxoeccqGEky", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pRBYFG34GBdP3Hef3/reference-points", "pageUrlRelative": "/posts/pRBYFG34GBdP3Hef3/reference-points", "linkUrl": "https://www.lesswrong.com/posts/pRBYFG34GBdP3Hef3/reference-points", "postedAtFormatted": "Wednesday, November 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reference%20Points&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReference%20Points%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpRBYFG34GBdP3Hef3%2Freference-points%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reference%20Points%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpRBYFG34GBdP3Hef3%2Freference-points", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpRBYFG34GBdP3Hef3%2Freference-points", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 806, "htmlBody": "<p>I just spent some time reading Thomas Schelling's \"Choice and Consequences\" and I heartily recommend it. Here's a <a href=\"http://books.google.com/books?id=MF08nRe6jQoC&amp;pg=PA57&amp;source=gbs_toc_r&amp;cad=3#v=onepage&amp;q&amp;f=false\">Google books link</a>&nbsp;to the chapter I was reading, \"The Intimate Contest for Self Command.\"</p>\n<p>It's fascinating, and if you like LessWrong, rationality, understanding things, decision theories, figuring people and the world out - well, then I think you'd like Schelling. Actually, you'll probably be amazed with how much of his stuff you're already familiar with - he really established a heck of a lot modern thinking on game theory.</p>\n<p>Allow me to depart from Schelling a moment, and talk of Sam Snyder. He's a very intelligent guy who has lots of intelligent thoughts. Here's a <a href=\"http://samsnyder.com/\">link to his website</a> - there's massive amounts of data and references there, so I'd recommend you just skim his site if you go visit until you find something interesting. You'll probably find something interesting pretty quickly.</p>\n<p>I got a chance to have a conversation with him a while back, and we covered immense amounts of ground. He introduced me to a concept I've been thinking about nonstop since learning it from him - <strong>reference points</strong>.</p>\n<p>Now, he explained it very eloquently, and I'm afraid I'm going to mangle and not do justice to his explanation. But to make a long story really short, your reference points affect your motivation a lot.</p>\n<p>An example would help.</p>\n<p>What does the average person think about he thinks of running? He thinks of huffing, puffing, being tired and sore, having a hard time getting going, looking fat in workout clothes and being embarrassed at being out of shape. A lot of people try running at some point in their life, and most people don't keep doing it.</p>\n<p>On the other hand, what does a regular runner think of? He thinks of the \"runner's high\" and gliding across the pavement, enjoying a great run, and feeling like a million bucks afterwards.</p>\n<p>Since that conversation, I've been trying to change my reference points. For instance, if I feel like I'd like some fried food, I try not to imagine/reference eating the salty greased food. Yes, eating french fries and a grilled chicken sandwich will be salty and fatty and delicious. It's a <a href=\"/lw/h3/superstimuli_and_the_collapse_of_western\">superstimulus</a>, we're not really evolved to handle that stuff appropriately.</p>\n<p>So when most people think of the McChicken Sandwich, large fry, large drink, they think about the grease and salt and sugar and how good it'll taste.</p>\n<p>I still like that stuff. In fact, since I quit a lot of vices, sometimes I crave even harder for the few I have left. But I was able to cut my junk food consumption way down by changing my reference point. When I start to have a desire for that sort of food, I think about how my stomach and energy levels are going to feel 90 minutes after eating it. That answer is - not too good. So I go out to a local restaurant and order plain chicken, rice, and vegetables, and I feel good later.<a id=\"more\"></a></p>\n<p>Schelling talks about in Choice and Consequences about how traditional economics applies a discount rate, but how that fails to explanation many situations. Schelling writes, \"[The person who] furiously scratches would have to be someone whose time discount is 100% per hour or per minute, compounding to an annual rate too large for a calculator.\"</p>\n<p>Schelling raises more questions than answers. But I think one of the answers is clear, and that answer is reference points. The man who scratches his rash at the expense of a much worse condition immediately isn't discarding the future. He simply isn't referencing it when he makes his decision. He itches, he references scratching with an immediate abatement of the itch.</p>\n<p>Eliezer writes in <a href=\"/lw/wv/prolegomena_to_a_theory_of_fun\">the theory of fun</a> that to sell an idea to someone, you usually don't need to convince them it's a good thing to live with for their whole life. You only need to convince them that the first hour or day after they choose is going to be good.</p>\n<p>And... I think that's scary, because it's true. People reference the immediate &lt;em&gt;very&lt;/em&gt; short-term consequences of their actions, instead of the broader pictures. Whether that's exercise, junk food, scratching a rash, buying a bigger TV, or conceptualizing eternity.</p>\n<p>This explains a lot of why people act the way they do. It also explains a way forwards for you - gradually evolve your reference points so that thinking of junk food is thinking about feeling that heavy weighted-in feeling in your belly and so that exercising is the rush of good hormones and pleasantness of a good workout. Imagine scratching a rash as doubling your discomfort instead of abating it and imagine how incredibly nicer your future surroundings if you save and invest that money for just a short time longer.</p>\n<p>Your reference points establish how you value things. Change them, and how you value things will change.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "y93YW7Kb6J8D5PKng": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pRBYFG34GBdP3Hef3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 40, "extendedScore": null, "score": 6.464755712562483e-07, "legacy": true, "legacyId": "4027", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Jq73GozjsuhdwMLEG", "pK4HTxuv6mftHXWC3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-17T12:16:58.346Z", "modifiedAt": null, "url": null, "title": "Zero Bias", "slug": "zero-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:35.937Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pD8jNyj2oArpn2DeP/zero-bias", "pageUrlRelative": "/posts/pD8jNyj2oArpn2DeP/zero-bias", "linkUrl": "https://www.lesswrong.com/posts/pD8jNyj2oArpn2DeP/zero-bias", "postedAtFormatted": "Wednesday, November 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Zero%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AZero%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpD8jNyj2oArpn2DeP%2Fzero-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Zero%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpD8jNyj2oArpn2DeP%2Fzero-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpD8jNyj2oArpn2DeP%2Fzero-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<p>It seems another bug in the human brain is being uncovered:</p>\n<blockquote>\n<p><span style=\"-webkit-border-horizontal-spacing: 5px; -webkit-border-vertical-spacing: 5px; \">\"Whereas a 20 percent interest rate may look very large compared to one percent, it may not look as large compared to zero percent. Zero eliminates the reference point we use to assess the size of things,\"&nbsp;</span></p>\n</blockquote>\n<p><a href=\"http://www.scienceagogo.com/news/20101015200630data_trunc_sys.shtml\">http://www.scienceagogo.com/news/20101015200630data_trunc_sys.shtml</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pD8jNyj2oArpn2DeP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 6.465357642824596e-07, "legacy": true, "legacyId": "4028", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-17T14:02:44.444Z", "modifiedAt": null, "url": null, "title": "Rolf Nelson:  How to deter a rogue AI by using your first-mover advantage  ", "slug": "rolf-nelson-how-to-deter-a-rogue-ai-by-using-your-first", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:26.982Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sA6qbsGHmsbnwnTdJ/rolf-nelson-how-to-deter-a-rogue-ai-by-using-your-first", "pageUrlRelative": "/posts/sA6qbsGHmsbnwnTdJ/rolf-nelson-how-to-deter-a-rogue-ai-by-using-your-first", "linkUrl": "https://www.lesswrong.com/posts/sA6qbsGHmsbnwnTdJ/rolf-nelson-how-to-deter-a-rogue-ai-by-using-your-first", "postedAtFormatted": "Wednesday, November 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rolf%20Nelson%3A%20%20How%20to%20deter%20a%20rogue%20AI%20by%20using%20your%20first-mover%20advantage%20%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARolf%20Nelson%3A%20%20How%20to%20deter%20a%20rogue%20AI%20by%20using%20your%20first-mover%20advantage%20%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsA6qbsGHmsbnwnTdJ%2Frolf-nelson-how-to-deter-a-rogue-ai-by-using-your-first%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rolf%20Nelson%3A%20%20How%20to%20deter%20a%20rogue%20AI%20by%20using%20your%20first-mover%20advantage%20%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsA6qbsGHmsbnwnTdJ%2Frolf-nelson-how-to-deter-a-rogue-ai-by-using-your-first", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsA6qbsGHmsbnwnTdJ%2Frolf-nelson-how-to-deter-a-rogue-ai-by-using-your-first", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://www.sl4.org/archive/0708/16600.html</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sA6qbsGHmsbnwnTdJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 6.465614488774635e-07, "legacy": true, "legacyId": "4029", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-17T14:23:05.242Z", "modifiedAt": null, "url": null, "title": "'Space-Time Cloak' to Conceal Events ", "slug": "space-time-cloak-to-conceal-events", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:33.125Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YBmoEbwRrQFgnxkmr/space-time-cloak-to-conceal-events", "pageUrlRelative": "/posts/YBmoEbwRrQFgnxkmr/space-time-cloak-to-conceal-events", "linkUrl": "https://www.lesswrong.com/posts/YBmoEbwRrQFgnxkmr/space-time-cloak-to-conceal-events", "postedAtFormatted": "Wednesday, November 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20'Space-Time%20Cloak'%20to%20Conceal%20Events%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A'Space-Time%20Cloak'%20to%20Conceal%20Events%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYBmoEbwRrQFgnxkmr%2Fspace-time-cloak-to-conceal-events%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text='Space-Time%20Cloak'%20to%20Conceal%20Events%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYBmoEbwRrQFgnxkmr%2Fspace-time-cloak-to-conceal-events", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYBmoEbwRrQFgnxkmr%2Fspace-time-cloak-to-conceal-events", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.sciencedaily.com/releases/2010/11/101115210937.htm\">http://www.sciencedaily.com/releases/2010/11/101115210937.htm</a></p>\n<p><a href=\"http://iopscience.iop.org/2040-8986/13/2/024003/\">http://iopscience.iop.org/2040-8986/13/2/024003/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YBmoEbwRrQFgnxkmr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "4030", "legacySpam": true, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-17T14:28:12.659Z", "modifiedAt": null, "url": null, "title": "How would you spend 30 million dollars?", "slug": "how-would-you-spend-30-million-dollars", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:54.460Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MariaKonovalenko", "createdAt": "2010-11-17T14:23:14.308Z", "isAdmin": false, "displayName": "MariaKonovalenko"}, "userId": "naM2Ph6C3mqS8Si6k", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2Ki5pCjh383w8KSRe/how-would-you-spend-30-million-dollars", "pageUrlRelative": "/posts/2Ki5pCjh383w8KSRe/how-would-you-spend-30-million-dollars", "linkUrl": "https://www.lesswrong.com/posts/2Ki5pCjh383w8KSRe/how-would-you-spend-30-million-dollars", "postedAtFormatted": "Wednesday, November 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20would%20you%20spend%2030%20million%20dollars%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20would%20you%20spend%2030%20million%20dollars%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Ki5pCjh383w8KSRe%2Fhow-would-you-spend-30-million-dollars%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20would%20you%20spend%2030%20million%20dollars%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Ki5pCjh383w8KSRe%2Fhow-would-you-spend-30-million-dollars", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Ki5pCjh383w8KSRe%2Fhow-would-you-spend-30-million-dollars", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 511, "htmlBody": "<p style=\"text-align: center;\"><img class=\"aligncenter\" title=\"investment project, immortalism, transhumanism, \" src=\"http://www.certifieddiamonddealers.com/images/Diamonds7.jpg\" alt=\"\" width=\"390\" height=\"306\" /></p>\n<p>There's a good song by Eminem - If I had a million dollars.&nbsp; So, if I had a hypothetical task to give away $30 million to different foundations without having a right to influence the projects, I would distribute them as follows, $3 million for each organization:</p>\n<p>1. Nanofactory collaboration, Robert Freitas, Ralph Merkle &ndash; developers of molecular nanotechnology and nanomedicine. Robert Freitas is the author of the monography Nanomedicine.<br /> 2. Singularity institute, Michael Vassar, Eliezer Yudkowsky &ndash; developers and ideologists of the friendly Artificial Intelligence<br /> 3. SENS Foundation, Aubrey de Grey &ndash; the most active engineering project in life extension, focused on the most promising underfunded areas<br /> 4. Cryonics Institute &ndash; one of the biggest cryonics firms in the US, they are able to use the additional funding more effectively as compared to Alcor<br /> 5. Advanced Neural Biosciences, Aschwin de Wolf &ndash; an independent cryonics research center created by ex-researchers from Suspended Animation<br /> 6. Brain observatory &ndash; brain scanning<br /> 7. University Hospital Careggi in Florence, Paolo Macchiarini &ndash; growing organs (not an American medical school, because this amount of money won&rsquo;t make any difference to the leading American centers)<br /> 8. Immortality institute &ndash; advocating for immortalism, selected experiments<br /> 9. IEET &ndash; institute of ethics and emerging technologies &ndash; promotion of transhumanist ideas<br /> 10. Small research grants of $50-300 thousand</p>\n<p>Now, if the task is to most effectively invest $30 million dollars, what projects would be chosen? (By effectiveness here I mean increasing the chances of radical life extension)</p>\n<p>Well, off the top of my head:</p>\n<p>1. The project: &ldquo;Creation of technologies to grow a human liver&rdquo; &ndash; $7 million. The project itself costs approximately $30-50 million, but $7 million is enough to achieve some significant intermediate results and will definitely attract more funds from potential investors.<br /> 2. Break the world record in sustaining viability of a mammalian head separate from the body - $0.7 million<br /> 3. Creation of an information system, which characterizes data on changes during aging in humans, integrates biomarkers of aging, and evaluates the role of pharmacological and other interventions in aging processes &ndash; $3 million<br /> 4. Research in increasing cryoprotectors efficacy - $3 million<br /> 5. Creation and realization of a program &ldquo;Regulation of epigenome&rdquo; - $5 million<br /> 6. Creation, promotion and lobbying of the program on research and fighting aging - $2 million<br /> 7. Educational programs in the fields of biogerontology, neuromodelling, regenerative medicine, engineered organs - $1.5 million<br /> 8. &ldquo;Artificial blood&rdquo; project - $2 million<br /> 9. Grants for authors, script writers, and art representatives for creation of pieces promoting transhumanism - $0.5 million<br /> 10. SENS Foundation project of removing senescent cells - $2 million<br /> 11. Creation of a US-based non-profit, which would protect and lobby the right to live and scientific research in life extension - $2 million<br /> 11. Participation of&nbsp; &ldquo;H+ managers&rdquo; in conferences, forums&nbsp; and social events - $1 million<br /> 12. Advocacy and creating content in social media - $0.3 million</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2Ki5pCjh383w8KSRe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 0, "extendedScore": null, "score": 6.465676342965107e-07, "legacy": true, "legacyId": "4031", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-17T19:12:41.564Z", "modifiedAt": null, "url": null, "title": "Imperfect Levers", "slug": "imperfect-levers", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:48.695Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blogospheroid", "createdAt": "2009-03-17T08:11:01.816Z", "isAdmin": false, "displayName": "blogospheroid"}, "userId": "dgscyYwrDh3u7dE7h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/izrW8nA65yHvQhmzy/imperfect-levers", "pageUrlRelative": "/posts/izrW8nA65yHvQhmzy/imperfect-levers", "linkUrl": "https://www.lesswrong.com/posts/izrW8nA65yHvQhmzy/imperfect-levers", "postedAtFormatted": "Wednesday, November 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Imperfect%20Levers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImperfect%20Levers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FizrW8nA65yHvQhmzy%2Fimperfect-levers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Imperfect%20Levers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FizrW8nA65yHvQhmzy%2Fimperfect-levers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FizrW8nA65yHvQhmzy%2Fimperfect-levers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1159, "htmlBody": "<p>Related to : <a href=\"/lw/le/lost_purposes/\">Lost Purposes</a>, <a href=\"/lw/1ws/the_importance_of_goodharts_law\">The importance of Goodhart's Law</a>, <a href=\"http://www.overcomingbias.com/2010/04/homo-hypocritus-signals.html\">Homo Hypocritus</a>, S<a href=\"/lw/2zg/ben_goertzel_the_singularity_institutes_scary\">IAI's scary idea</a>, <a href=\"/lesswrong.com/lw/2zj/value_deathism\">Value Deathism</a></p>\n<p>Summary : Whenever human beings seek to achieve goals far beyond their individual ability, they use leverage of some kind of another. Creating organizations to achieve goals is a very powerful source of leverage. However due to their nature, organizations are imperfect levers and the primary purpose is often lost. The inertia of present forms and processes dominates beyond its useful period. The present system of the world has many such imperfect organizations in power and any of them developing near-general intelligence without significant redesign of their utility function can be a source of existential risk/values risk.</p>\n<p><a id=\"more\"></a></p>\n<p>When human beings seek to achieve large ambitious goals, it is natural to use some kind of leverage, some kind of ability to multiply one's power. Financial leverage, taking debt, is one of the most common means of leverage as it turns a small profit or spread into a large one. An even more powerful means of leverage is to bring people together and creating organizations to achieve the purpose that one had set out to achieve.&nbsp;</p>\n<p>However, unlike the cleanliness of financial leverage(though not without problems of its own), using organizational leverage is messy, especially if the organization is created to achieve goals that are subtle and complex. There is an entire body of management literature that tries to align the interest of principals and agents. But most agents do not avoid Goodhart's law. As organizations grow, the mechanization of their incentive structures increases. Agents will do what they have been incentivized to do.</p>\n<p><strong>An example</strong></p>\n<p>Recently, we saw an awesome demonstration of great futility, Quantitative Easing II. The purchase of a large number of US government bonds by the US Fed in the hope of creating more prosperity. People feeling poor due to the bursting of the housing bubble are definitely not the recipients of this money. And these are the people who eventually have to start producing and consuming again for this recession to end. Where would they attain the money from? The expected causal chain (econ experts can correct me if I'm wrong here) went like this</p>\n<p>Buying US bonds - Creating the expectation of inflation in the market - Leading to banks wanting to lend out their reserves to people - leading to the people getting credit from banks - leading to them spending again - leading to improved profits in firms - leading to those firms hiring - leading to more jobs and so on.</p>\n<p>The extremely long causal chain is not the main failure feature here. Nor is the fact that there are direct contradictory policies acting against step3 (paying interest on reserves maintained at the Fed) . My point is that even if this entire chain were to happen and the nominal end result, GDP growth, were to be achieved, the resulting prosperity would not be long lasting because it is not one that is based on a sustainable pattern of production and trade.</p>\n<p>Maintaining equitable prosperity in a society that is facing competition from younger and poorer countries is a tough problem. Instead of tackling this problem head-on, the various governmental sources continued on their inertial paths and chose to adapt the patterns of home and asset ownership to create an illusion of prosperity. After all, the counter (GDP) was still running.</p>\n<p><strong>The Pattern</strong></p>\n<p>Many smart people have voiced opinions against such blind following of metrics in government, but almost all organizations, once beyond the grip of the founders fall into some such pattern. Compassionate mystic movements become rigid churches. Political parties for eg. pay more attention to lip service to issues, pomp, show and mind killing than to actual issues. Companies seek to make money at the expense of creating actual value, forgetting that money is only a symbol of value. A lot of people have bemoaned the brainpower that is moving into finance. And something even more repugnant, there is an entire economy thriving around the war on drugs, with everyone in on the cut.</p>\n<p>In the short run, all these organizations/formations/coalitions are winning. So, voices against their behaviour that do not threaten them are being ignored.</p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \"><strong>\"It is difficult to get a man to understand something, when his salary depends upon his not understanding it!</strong></span>\" - Upton Sinclair</p>\n<p>There is a great deal of intelligence being applied today in these areas by people far more smarter than you or me.&nbsp;But the overall systems are still not as intelligent as a human yet.&nbsp;In the long run, they are probably undermining their own foundations.</p>\n<p>The scary part really is these corporations and governments, which while being sub-humanly intelligent right now, are probably going to be at the forefront of creating GAI. I expect that near human intelligence will emerge in organizations and will most probably be a well knit human+computer team, probably with Brain Computer Interfaces. This team may or may not share all the values of the organization, but the incentives that this intelligence is being rewarded for, it will seek to achieve. And if these incentives are as narrowly phrased as</p>\n<p>&nbsp;</p>\n<ul>\n<li>maintain power over these certain set of people (As we might imagine the actual value system of government seems to be)&nbsp;</li>\n<li>make money without getting into legal trouble (As we might imagine the value system of a corporation would be)</li>\n</ul>\n<p>&nbsp;</p>\n<p>then there will be a continuation of today's sheer insane optimization, but on a monstrous scale. The altruists amongst us have shown the inability to curb the sub-humanly intelligent larvae, what will we do the near human or super human butterfly? Competition between such entities would very quickly eliminate most compassionate values and a lot of what humanity holds dear. (My personal belief is that corporate AIs might be a little safer as they would probably crash the money system, but would not be lobbing nuclear weapons onto rivals).&nbsp;</p>\n<p>In the end, I don't want to undermine the very idea of organizations because they have brought us unprecedented prosperity. I could not be transmitting my opinions to you without the support of many such organizations.&nbsp;</p>\n<p>So, my take is that, I don't find the scary idea of SIAI as a completely alien idea. The present sources of optimization power, whether they be People in governments, LLCs in the present mixed economy system or political parties in the present plurality system, do not show any inclination towards understanding or moving towards true human morality. They do not \"search their souls\", they respond to incentives. They act like a system with a utility function, a function indifferent to morality. Their AIs will inherit these characteristics&nbsp; of these imperfect levers and there is no reason to expect, from increased intelligence alone, that the AI will move towards friendliness/morality.</p>\n<p>EDIT : Edited to make clear the conclusion and set right the Goodhart's law link. Apologies to Vaniver, Nornagest, atucker, b1shop, Will_Sawin, AdShea, Jack, mwaser and magfrump who posted before the edit. Thanks to xamdam who pointed out the wrong link.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "izrW8nA65yHvQhmzy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 5, "extendedScore": null, "score": 6.4663672748996e-07, "legacy": true, "legacyId": "4033", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Related to : <a href=\"/lw/le/lost_purposes/\">Lost Purposes</a>, <a href=\"/lw/1ws/the_importance_of_goodharts_law\">The importance of Goodhart's Law</a>, <a href=\"http://www.overcomingbias.com/2010/04/homo-hypocritus-signals.html\">Homo Hypocritus</a>, S<a href=\"/lw/2zg/ben_goertzel_the_singularity_institutes_scary\">IAI's scary idea</a>, <a href=\"/lesswrong.com/lw/2zj/value_deathism\">Value Deathism</a></p>\n<p>Summary : Whenever human beings seek to achieve goals far beyond their individual ability, they use leverage of some kind of another. Creating organizations to achieve goals is a very powerful source of leverage. However due to their nature, organizations are imperfect levers and the primary purpose is often lost. The inertia of present forms and processes dominates beyond its useful period. The present system of the world has many such imperfect organizations in power and any of them developing near-general intelligence without significant redesign of their utility function can be a source of existential risk/values risk.</p>\n<p><a id=\"more\"></a></p>\n<p>When human beings seek to achieve large ambitious goals, it is natural to use some kind of leverage, some kind of ability to multiply one's power. Financial leverage, taking debt, is one of the most common means of leverage as it turns a small profit or spread into a large one. An even more powerful means of leverage is to bring people together and creating organizations to achieve the purpose that one had set out to achieve.&nbsp;</p>\n<p>However, unlike the cleanliness of financial leverage(though not without problems of its own), using organizational leverage is messy, especially if the organization is created to achieve goals that are subtle and complex. There is an entire body of management literature that tries to align the interest of principals and agents. But most agents do not avoid Goodhart's law. As organizations grow, the mechanization of their incentive structures increases. Agents will do what they have been incentivized to do.</p>\n<p><strong id=\"An_example\">An example</strong></p>\n<p>Recently, we saw an awesome demonstration of great futility, Quantitative Easing II. The purchase of a large number of US government bonds by the US Fed in the hope of creating more prosperity. People feeling poor due to the bursting of the housing bubble are definitely not the recipients of this money. And these are the people who eventually have to start producing and consuming again for this recession to end. Where would they attain the money from? The expected causal chain (econ experts can correct me if I'm wrong here) went like this</p>\n<p>Buying US bonds - Creating the expectation of inflation in the market - Leading to banks wanting to lend out their reserves to people - leading to the people getting credit from banks - leading to them spending again - leading to improved profits in firms - leading to those firms hiring - leading to more jobs and so on.</p>\n<p>The extremely long causal chain is not the main failure feature here. Nor is the fact that there are direct contradictory policies acting against step3 (paying interest on reserves maintained at the Fed) . My point is that even if this entire chain were to happen and the nominal end result, GDP growth, were to be achieved, the resulting prosperity would not be long lasting because it is not one that is based on a sustainable pattern of production and trade.</p>\n<p>Maintaining equitable prosperity in a society that is facing competition from younger and poorer countries is a tough problem. Instead of tackling this problem head-on, the various governmental sources continued on their inertial paths and chose to adapt the patterns of home and asset ownership to create an illusion of prosperity. After all, the counter (GDP) was still running.</p>\n<p><strong id=\"The_Pattern\">The Pattern</strong></p>\n<p>Many smart people have voiced opinions against such blind following of metrics in government, but almost all organizations, once beyond the grip of the founders fall into some such pattern. Compassionate mystic movements become rigid churches. Political parties for eg. pay more attention to lip service to issues, pomp, show and mind killing than to actual issues. Companies seek to make money at the expense of creating actual value, forgetting that money is only a symbol of value. A lot of people have bemoaned the brainpower that is moving into finance. And something even more repugnant, there is an entire economy thriving around the war on drugs, with everyone in on the cut.</p>\n<p>In the short run, all these organizations/formations/coalitions are winning. So, voices against their behaviour that do not threaten them are being ignored.</p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \"><strong>\"It is difficult to get a man to understand something, when his salary depends upon his not understanding it!</strong></span>\" - Upton Sinclair</p>\n<p>There is a great deal of intelligence being applied today in these areas by people far more smarter than you or me.&nbsp;But the overall systems are still not as intelligent as a human yet.&nbsp;In the long run, they are probably undermining their own foundations.</p>\n<p>The scary part really is these corporations and governments, which while being sub-humanly intelligent right now, are probably going to be at the forefront of creating GAI. I expect that near human intelligence will emerge in organizations and will most probably be a well knit human+computer team, probably with Brain Computer Interfaces. This team may or may not share all the values of the organization, but the incentives that this intelligence is being rewarded for, it will seek to achieve. And if these incentives are as narrowly phrased as</p>\n<p>&nbsp;</p>\n<ul>\n<li>maintain power over these certain set of people (As we might imagine the actual value system of government seems to be)&nbsp;</li>\n<li>make money without getting into legal trouble (As we might imagine the value system of a corporation would be)</li>\n</ul>\n<p>&nbsp;</p>\n<p>then there will be a continuation of today's sheer insane optimization, but on a monstrous scale. The altruists amongst us have shown the inability to curb the sub-humanly intelligent larvae, what will we do the near human or super human butterfly? Competition between such entities would very quickly eliminate most compassionate values and a lot of what humanity holds dear. (My personal belief is that corporate AIs might be a little safer as they would probably crash the money system, but would not be lobbing nuclear weapons onto rivals).&nbsp;</p>\n<p>In the end, I don't want to undermine the very idea of organizations because they have brought us unprecedented prosperity. I could not be transmitting my opinions to you without the support of many such organizations.&nbsp;</p>\n<p>So, my take is that, I don't find the scary idea of SIAI as a completely alien idea. The present sources of optimization power, whether they be People in governments, LLCs in the present mixed economy system or political parties in the present plurality system, do not show any inclination towards understanding or moving towards true human morality. They do not \"search their souls\", they respond to incentives. They act like a system with a utility function, a function indifferent to morality. Their AIs will inherit these characteristics&nbsp; of these imperfect levers and there is no reason to expect, from increased intelligence alone, that the AI will move towards friendliness/morality.</p>\n<p>EDIT : Edited to make clear the conclusion and set right the Goodhart's law link. Apologies to Vaniver, Nornagest, atucker, b1shop, Will_Sawin, AdShea, Jack, mwaser and magfrump who posted before the edit. Thanks to xamdam who pointed out the wrong link.&nbsp;</p>", "sections": [{"title": "An example", "anchor": "An_example", "level": 1}, {"title": "The Pattern", "anchor": "The_Pattern", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "36 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sP2Hg6uPwpfp3jZJN", "YtvZxRpZjcFNwJecS", "97TCYaiMe4ceRYoXs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-18T00:20:03.481Z", "modifiedAt": null, "url": null, "title": "Suspended Animation Inc. accused of incompetence", "slug": "suspended-animation-inc-accused-of-incompetence", "viewCount": null, "lastCommentedAt": "2015-08-07T13:18:55.657Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JrdS5DHjNKaTcGxpf/suspended-animation-inc-accused-of-incompetence", "pageUrlRelative": "/posts/JrdS5DHjNKaTcGxpf/suspended-animation-inc-accused-of-incompetence", "linkUrl": "https://www.lesswrong.com/posts/JrdS5DHjNKaTcGxpf/suspended-animation-inc-accused-of-incompetence", "postedAtFormatted": "Thursday, November 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suspended%20Animation%20Inc.%20accused%20of%20incompetence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuspended%20Animation%20Inc.%20accused%20of%20incompetence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJrdS5DHjNKaTcGxpf%2Fsuspended-animation-inc-accused-of-incompetence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suspended%20Animation%20Inc.%20accused%20of%20incompetence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJrdS5DHjNKaTcGxpf%2Fsuspended-animation-inc-accused-of-incompetence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJrdS5DHjNKaTcGxpf%2Fsuspended-animation-inc-accused-of-incompetence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 427, "htmlBody": "<p>I recently found something that may be of concern to some of the readers here.</p>\n<p>On <a href=\"http://cryomedical.blogspot.com/\">her blog</a>, Melody Maxim, former employee of Suspended Animation, provider of \"standby services\" for Cryonics Institute customers, describes several examples of gross incompetence in providing those services. Specifically,<a href=\"http://cryomedical.blogspot.com/2007/07/sas-failure-to-deliver-promised.html\"> spending large amounts of money on designing and manufacturing novel perfusion equipment</a> when cheaper, more effective devices that could be adapted to serve their purposes <a href=\"http://cryomedical.blogspot.com/2007/07/computerized-perfusion-circuits.html\">already existed</a>, <a href=\"http://cryomedical.blogspot.com/2010/06/truth-about-suspended-animations.html\">hiring laymen to perform difficult medical procedures</a> who then <a href=\"http://cryomedical.blogspot.com/2010/09/partial-review-of-suspended-animations.html\">botched them</a>, and <a href=\"http://cryomedical.blogspot.com/2010/09/cryonics-event-in-uk-no-brainer.html\">even finding themselves unable to get their equipment loaded onto a plane because it exceeded the weight limit</a>.</p>\n<p>An excerpt from one of her posts, \"<a href=\"http://cryomedical.blogspot.com/2010/10/why-i-believe-cryonics-should-be.html\">Why I Believe Cryonics Should Be Regulated</a>\":</p>\n<blockquote>\n<p><span style=\"font-family: Arial;\">It is no longer possible for me to believe what I witnessed was an isolated bit of corruption, and the picture gets bigger, by the year...<br /></span></p>\n<p><span style=\"font-family: Arial;\">For forty years, cryonics \"research\" has primarily consisted of laymen attempting to build equipment that already exists, and laymen trying to train other laymen how to perform the tasks of paramedics, perfusionists, and vascular surgeons...much of this time with the benefactors having ample funding to provide the real thing, in regard to both equipment and personnel. Organizations such as Alcor and Suspended Animation, which want to charge $60,000 to $150,000, (not to mention other extra charges, or years worth of membership dues), are not capable of preserving brains and/or bodies in a condition likely to be viable in the future. People associated with these companies, have been known to encourage people, not only to leave hefty life insurance policies with their organizations listed as the beneficiaries, to pay for these amateur surgical procedures, but to leave their estates and irrevocable trusts to cryonics organizations.</span></p>\n<p><span style=\"font-family: Arial;\">...</span></p>\n<p><span style=\"font-family: Arial;\">Again, I have no problem with people receiving their last wishes. If people want to be cryopreserved, I think they should have that right. BUT...companies should not be allowed to deceive people who wish to be cryopreserved. They should not be allowed to publish photos of what looks like medical professionals performing surgery, but in actuality, is a group of laymen playing doctor with a dead body...people whose incompetency will result in their clients being left warm (and decaying), for many hours while they struggle to perform a vascular cannulation, or people whose brains will be underperfused or turned to mush, by laymen who have no idea how to properly and safely operate a perfusion circuit. Cryonics companies should not be allowed to refer to laymen as \"Chief Surgeon,\" \"Surgeon,\" \"Perfusionist,\" when these people hold no medical credentials. <br /></span></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1, "xHjy88N2uJvGdgzfw": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JrdS5DHjNKaTcGxpf", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 49, "extendedScore": null, "score": 8.8e-05, "legacy": true, "legacyId": "4035", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 49, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 139, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2010-11-18T00:20:03.481Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-18T03:19:48.087Z", "modifiedAt": null, "url": null, "title": "Rationalist Diplomacy, Game 2, Game Over", "slug": "rationalist-diplomacy-game-2-game-over", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:38.750Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Randaly", "createdAt": "2010-04-20T23:31:03.738Z", "isAdmin": false, "displayName": "Randaly"}, "userId": "KdhDyNCDgA945WayD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RZuwnE7htZTgaruBJ/rationalist-diplomacy-game-2-game-over", "pageUrlRelative": "/posts/RZuwnE7htZTgaruBJ/rationalist-diplomacy-game-2-game-over", "linkUrl": "https://www.lesswrong.com/posts/RZuwnE7htZTgaruBJ/rationalist-diplomacy-game-2-game-over", "postedAtFormatted": "Thursday, November 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20Diplomacy%2C%20Game%202%2C%20Game%20Over&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20Diplomacy%2C%20Game%202%2C%20Game%20Over%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRZuwnE7htZTgaruBJ%2Frationalist-diplomacy-game-2-game-over%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20Diplomacy%2C%20Game%202%2C%20Game%20Over%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRZuwnE7htZTgaruBJ%2Frationalist-diplomacy-game-2-game-over", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRZuwnE7htZTgaruBJ%2Frationalist-diplomacy-game-2-game-over", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 302, "htmlBody": "<p>Note: The title refers to the upcoming turn.</p>\n<p>OK, here's the promised second game of diplomacy. The game name is 'Rationalist Diplomacy Game 2.'</p>\n<p>Kevin was Prime Minister of Great Britain<br />AlexMennen is President of France<br />tenshiko is Kaiser of Germany<br />Alexandros was King of Italy until his retirement<br />WrongBot is Emperor of Austria<br />Thausler is Czar <del></del>of Russia<br />Hugh Ristik is Sultan<del></del> of Turkey</p>\n<p>Randaly is the GM, and can be reached at <a href=\"mailto:nojustnoperson@gmail.com\">nojustnoperson@gmail.com</a></p>\n<p>&nbsp;</p>\n<h2>Peace For Our Time!</h2>\n<p>The leaders of the three surviving nations, France, Russia, and Turkey, agreed to a peace treaty in late August, bringing an end to this destructive conflict. Crowds across Europe broke out into spontaneous celebration, as national leaders began to account for the vast costs- human and monetary- of the wars.</p>\n<p>&nbsp;</p>\n<h4><img src=\"http://i1237.photobucket.com/albums/ff471/Randaly/F1909.png\" alt=\"\" width=\"600\" height=\"444\" /></h4>\n<blockquote>\n<p>All orders should be sent to nojustnoperson@gmail.com with an easy-to-read title like \"Rationalist Diplomacy Game 2: Russian Orders Spring 1901\". Only the LAST set of orders sent will be counted, so feel free to change your mind or to do something sneaky like sending in a fake set of orders cc your ally, and then sending in your real orders later. I'm not going to be too picky on exactly how you phrase your orders, but I prefer standard Diplomacy terminology like \"F kie -&gt; hel\". New players - remember that if you send two units to the same space, you MUST specify which is attacking and which is supporting. If you make a mistake there or anywhere else, I will probably email you and ask you which you meant, but if I don't have time you'll just be out of luck.</p>\n<p>ETA: HughRistik would like to underscore that, under the standard house rules, all draws are unranked.</p>\n</blockquote>\n<p>Past maps can be viewed&nbsp; <a href=\"http://s1237.photobucket.com/albums/ff471/Randaly/\">here</a>; the game history can be viewed&nbsp; <a href=\"http://wiki.lesswrong.com/wiki/Rationalist_Diplomacy,_Game_2\">here.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RZuwnE7htZTgaruBJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "4037", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-18T03:34:12.148Z", "modifiedAt": null, "url": null, "title": "IQ Scores Fail to Predict Academic Performance in Children With Autism", "slug": "iq-scores-fail-to-predict-academic-performance-in-children", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:49.074Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Asw2uEDBrkHPpCDvH/iq-scores-fail-to-predict-academic-performance-in-children", "pageUrlRelative": "/posts/Asw2uEDBrkHPpCDvH/iq-scores-fail-to-predict-academic-performance-in-children", "linkUrl": "https://www.lesswrong.com/posts/Asw2uEDBrkHPpCDvH/iq-scores-fail-to-predict-academic-performance-in-children", "postedAtFormatted": "Thursday, November 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20IQ%20Scores%20Fail%20to%20Predict%20Academic%20Performance%20in%20Children%20With%20Autism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIQ%20Scores%20Fail%20to%20Predict%20Academic%20Performance%20in%20Children%20With%20Autism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAsw2uEDBrkHPpCDvH%2Fiq-scores-fail-to-predict-academic-performance-in-children%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=IQ%20Scores%20Fail%20to%20Predict%20Academic%20Performance%20in%20Children%20With%20Autism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAsw2uEDBrkHPpCDvH%2Fiq-scores-fail-to-predict-academic-performance-in-children", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAsw2uEDBrkHPpCDvH%2Fiq-scores-fail-to-predict-academic-performance-in-children", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 571, "htmlBody": "<p>http://www.sciencedaily.com/releases/2010/11/101117141514.htm</p>\n<p>I find this study extremely interesting. Before anyone starts to spout anti-IQ rhetoric - let me say that I realize that the predictive effects of IQ (which are extensively documented in research journals) are only as good as they are simply because most people aren't that psychologically different from each other (owing both to genetic factors and socialization), which will natural reduce the variance in performance due to other factors (and increase the associated variance in performance attributed to IQ). In fact, one of the major findings in the IQ literature is that in the general population, the primary sources of intelligence are highly correlated with each other. Verbal intelligence is correlated with mathematical intelligence, and both of those are are even highly correlated with reaction times. But among people who are autistic, this finding may be less accurate than it is among neurotypicals. Some of us Aspies are exceptionally talented at certain things while simultaneously being incredibly incompetent at other things.</p>\n<p class=\"UIIntentionalStory_Message\"><span class=\"UIStory_Message\">Of course, findings only apply to the population at large and may not apply to specific subsets of people. Most research shows that people who sleep more get higher grades - but this is definitely not true for certain subgroups of people - there are plenty of intelligent people at MIT/Caltech who sleep far less than the average student in a state school. The same logic applies to skipping classes and lower grades (Caltech's classes have notoriously high absence rates since the students there are independent studiers). Similarly, IQ tests (and other assessments normalized to the general population) may not necessarily predict performance among certain subgroups of people, especially those who are non-neurotypical. This logic could apply to GPAs and SAT scores too (I know a mathematical genius with asperger's at </span><span class=\"UIStory_Message\">UChicago, </span><span class=\"UIStory_Message\">who only got 600s on his SATs for example). And I also suspect that it may apply to subgroups of people with Attention Deficit Disorder. </span><span class=\"UIStory_Message\">There's s a good chance that other factors may interfere with IQ, which may increase variance in performance due to other environmental factors (and decrease variance in performance due to autism).And simply increasing the variation in environment will naturally reduce the variation in IQ due to comparatively immalleable factors (early childhood influence is just as immalleable as genetics once you start measuring IQs of older children and teenagers, for example).<br /></span></p>\n<p class=\"UIIntentionalStory_Message\"><span class=\"UIStory_Message\">I'm still pretty sure that IQ tests will measure *something* in Aspies (as the article says, it's that Aspies tend to have uneven performance that tends to make them stick out). Scores on the individual scales will still say something about the Aspie's range of strengths and weaknesses. </span></p>\n<p class=\"UIIntentionalStory_Message\"><br />Anyways, I think this topic would be very interesting to this group, since it has a high population of non-neurotypical thinkers. In particular, I'd like to encourage the discussion of other metrics that may predict performance among neurotypicals, but not necessarily among non-neurotypicals. In particular, it has a lot of significant relation to signalling theory. If someone's a high school dropout and didn't continue onto college, for example, chances are that he probably isn't the type of person who would get along with people on lesswrong. But if you learn that he has Asperger's and other traits of an unusual background, then you might figure that the \"high school dropout\" signal has significantly less validity in that case, and he might have a much higher chance of getting along with people on lesswrong.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4cKQgA4S7xfNeeWXg": 2, "ksdiAMKfgSyEeKMo6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Asw2uEDBrkHPpCDvH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 6.467585629276591e-07, "legacy": true, "legacyId": "4038", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-18T08:27:20.737Z", "modifiedAt": null, "url": null, "title": "Weird characters in the Sequences", "slug": "weird-characters-in-the-sequences", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:33.609Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pmi8YDfYqtx8CMrQh/weird-characters-in-the-sequences", "pageUrlRelative": "/posts/Pmi8YDfYqtx8CMrQh/weird-characters-in-the-sequences", "linkUrl": "https://www.lesswrong.com/posts/Pmi8YDfYqtx8CMrQh/weird-characters-in-the-sequences", "postedAtFormatted": "Thursday, November 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weird%20characters%20in%20the%20Sequences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeird%20characters%20in%20the%20Sequences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPmi8YDfYqtx8CMrQh%2Fweird-characters-in-the-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weird%20characters%20in%20the%20Sequences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPmi8YDfYqtx8CMrQh%2Fweird-characters-in-the-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPmi8YDfYqtx8CMrQh%2Fweird-characters-in-the-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 246, "htmlBody": "<p>When the sequences were copied from Overcoming Bias to Less Wrong, it looks like something went very wrong with the character encoding. &nbsp;I found the following sequences of HTML entities in words in the sequences:</p>\n<p>&nbsp;</p>\n<p>&amp;#xE2;&amp;#x80;&amp;#x99;&amp;#x102;&amp;#x15E; d?tre</p>\n<p>&amp;#xC5;&amp;#xAB; M?lamadhyamaka</p>\n<p>&amp;#x102;&amp;#x15A; Ph?drus</p>\n<p>&amp;#x102;&amp;#x2D8;&amp;#xC2;&amp;#x80;&amp;#xC2;&amp;#x94; arbitrator?i window?and</p>\n<p>&amp;#x102;&amp;#x15E; b?te m?me</p>\n<p>&amp;#xE2;&amp;#x80;&amp;#xA6; over?and</p>\n<p>&amp;#xE23;&amp;#xE01; H?jek</p>\n<p>&amp;#x102;&amp;#x83;&amp;#xC2;&amp;#x17A; G?nther</p>\n<p>&amp;#x102;&amp;#x160; fianc?e proteg?s d?formation d?colletage am?ricaine d?sir</p>\n<p>&amp;#x102;&amp;#x83;&amp;#xC2;&amp;#x17B; na?ve na?vely</p>\n<p>&amp;#x139;&amp;#x8D; sh?nen</p>\n<p>&amp;#xC3;&amp;#xB6; Schr?dinger L?b</p>\n<p>&amp;#xE22;&amp;#xE07; ?ion</p>\n<p>&amp;#x102;&amp;#x83;&amp;#xC2;&amp;#x15B; Schr?dinger H?lldobler</p>\n<p>&amp;#x102;&amp;#x17A; D?sseldorf G?nther</p>\n<p>&amp;#xE2;&amp;#x80;&amp;#x93; ? Church? miracles?in Church?Turing</p>\n<p>&amp;#xE2;&amp;#x80;&amp;#x99; doesn?t he?s what?s let?s twin?s aren?t I?ll they?d ?s you?ve else?s EY?s Whate?er punish?d There?s Caledonian?s isn?t harm?s attack?d I?m that?s Google?s arguer?s Pascal?s don?t shouldn?t can?t form?d controll?d Schiller?s object?s They?re whatever?s everybody?s That?s Tetlock?s S?il it?s one?s didn?t Don?t Aslan?s we?ve We?ve Superman?s clamour?d America?s Everybody?s people?s you?d It?s state?s Harvey?s Let?s there?s Einstein?s won?t</p>\n<p>&amp;#x102;&amp;#x104; Alm?si Zolt?n</p>\n<p>&amp;#x102;&amp;#x164; pre?mpting re?valuate</p>\n<p>&amp;#x102;&amp;#x2D8;&amp;#xC2;&amp;#x89;&amp;#xC2;&amp;#xA0; ?</p>\n<p>&amp;#x102;&amp;#xA8; l?se m?ne accurs?d</p>\n<p>&amp;#xE23;&amp;#xE10; Ver?andi</p>\n<p>&amp;#xE2;&amp;#x86;&amp;#x92; high?low low?high</p>\n<p>&amp;#x102;&amp;#x2D8;&amp;#xC2;&amp;#x80;&amp;#xC2;&amp;#x99; doesn?t</p>\n<p>&amp;#xC4;&amp;#x81; k?rik Siddh?rtha</p>\n<p>&amp;#xE23;&amp;#xE16; Sj?berg G?delian L?b Schr?dinger G?gel G?del co?rdinate W?hler K?nigsberg P?lzl</p>\n<p>&amp;#x102;&amp;#x17B; na?vet</p>\n<p>&amp;#xC2;&amp;#xA0; I?understood ? I?was</p>\n<p>&amp;#x102;&amp;#x15B; Schr?dinger</p>\n<p>&amp;#x102;&amp;#x17D; pla?t</p>\n<p>&amp;#xFA;&amp;#xF1; N?ez</p>\n<p>&amp;#x139;&amp;#x82; Ceg?owski</p>\n<p>&amp;#xE2;&amp;#x80;&amp;#x94; PEOPLE?and smarter?supporting to?at problem?and probability?then valid?to opportunity?of time?in true?I view?wishing Kyi?and ones?such crudely?model stupid?which that?larger aside?from Ironically?but intelligence?such flower?but medicine?as</p>\n<p>&amp;#xE2;&amp;#x80;&amp;#x90; side?effect galactic?scale</p>\n<p>&amp;#xC2;&amp;#xB4; can?t Biko?s aren?t you?de didn?t don?t it?s</p>\n<p>&amp;#xE2;&amp;#x89;&amp;#xA0; P?NP</p>\n<p>&amp;#x7AB6;&amp;#x99AC; basically?ot</p>\n<p>&amp;#x139;&amp;#x91; Erd?s</p>\n<div>Now, an example like \"&amp;#xC3;&amp;#xB6; Schr?dinger L?b\" I can decode: \"C3 B6\" is the byte sequence for the UTF-8 encoding of \"<span style=\"font-family: 'Times New Roman'; font-size: medium; border-collapse: collapse; -webkit-border-horizontal-spacing: 2px; -webkit-border-vertical-spacing: 2px;\">U+00F6&nbsp;</span><span style=\"font-family: 'Times New Roman'; font-size: medium; border-collapse: collapse; -webkit-border-horizontal-spacing: 2px; -webkit-border-vertical-spacing: 2px;\">&ouml;</span>&nbsp;<span style=\"font-family: 'Times New Roman'; font-size: medium; border-collapse: collapse; -webkit-border-horizontal-spacing: 2px; -webkit-border-vertical-spacing: 2px;\">LATIN SMALL LETTER O WITH DIAERESIS\". &nbsp;But \"</span>&amp;#xFA;&amp;#xF1;\" is not a valid UTF-8 sequence - and those that contain entities larger than 255 are very mysterious. &nbsp;Anyone able to make any guesses?</div>\n<div>EDIT:&nbsp;&amp;#xE23;&amp;#xE16; translated into <a href=\"http://msdn.microsoft.com/en-us/goglobal/cc305142.aspx\">Windows codepage 874</a> is C3 B6!</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JMD7LTXTisBzGAfhX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pmi8YDfYqtx8CMrQh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 6.468297971608964e-07, "legacy": true, "legacyId": "4040", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-18T12:21:09.504Z", "modifiedAt": null, "url": null, "title": "\"Target audience\" size for the Less Wrong sequences", "slug": "target-audience-size-for-the-less-wrong-sequences", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:09.639Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HdWY4jB2NpXmWEHWz/target-audience-size-for-the-less-wrong-sequences", "pageUrlRelative": "/posts/HdWY4jB2NpXmWEHWz/target-audience-size-for-the-less-wrong-sequences", "linkUrl": "https://www.lesswrong.com/posts/HdWY4jB2NpXmWEHWz/target-audience-size-for-the-less-wrong-sequences", "postedAtFormatted": "Thursday, November 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Target%20audience%22%20size%20for%20the%20Less%20Wrong%20sequences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Target%20audience%22%20size%20for%20the%20Less%20Wrong%20sequences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHdWY4jB2NpXmWEHWz%2Ftarget-audience-size-for-the-less-wrong-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Target%20audience%22%20size%20for%20the%20Less%20Wrong%20sequences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHdWY4jB2NpXmWEHWz%2Ftarget-audience-size-for-the-less-wrong-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHdWY4jB2NpXmWEHWz%2Ftarget-audience-size-for-the-less-wrong-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1879, "htmlBody": "<p>[Note: <a href=\"/r/discussion/lw/33u/theoretical_target_audience_size_of_less_wrong/\">My last thread</a> was poorly worded in places and gave people the wrong impression that I was interested in talking about growing and shaping the Less Wrong community.&nbsp; I was really hoping to talk about something a bit different.&nbsp; Here's my revision with a completely redone methodology.]</p>\n<p><strong>How many people would invest their time to read the LW sequences if they were introduced to them?</strong><br /><br />So in other words, I&rsquo;m trying to estimate the theoretical upper-bound on the number of individuals world-wide who have the ability, desire, and time to read intellectual material online and who also have at least some pre-disposition to wanting to think rationally.<br /><br />I&rsquo;m not trying to evangelize to unprepared, &ldquo;reach&rdquo; candidates who <em>maybe</em>, <em>possibly</em> would like to read parts of the sequences.&nbsp; I&rsquo;m just looking for likely size of the core audience who already has the ability, the time, and doesn&rsquo;t need to jump through any major hoops to stomach the sequences (like deconverting from religion or radically changing their habits -- like suddenly devoting more of their time to using computers or reading.)<br /><br />The reason I&rsquo;m investigating this is because <a href=\"/r/discussion/lw/2zc/currently_buying_adwords_for_lesswrong/\">I want to build more rationalists</a>.&nbsp; I know some smart people whose opinions I respect (like <a href=\"http://intelligence.org/aboutus/team\">Michael Vassar</a>) who contend we shouldn&rsquo;t spend much time trying to reach more people with the sequences.&nbsp; They think the majority of people smart enough to follow the sequences and who do weird, eccentric things like &ldquo;read in their spare time&rdquo;, are already here.&nbsp; This is m<a href=\"/r/discussion/lw/33u/theoretical_target_audience_size_of_less_wrong/\">y second attempt to figure this out</a> in the last couple days, and unlike my rough 2M person figure I got with my previous, hasty analysis, this more detailed analysis leaves me with a much lower world-wide target audience of only 17,000.</p>\n<p>&nbsp;</p>\n<pre style=\"padding-left: 30px;\"><table border=\"1\"><tbody><tr><td><pre><strong>Filter</strong></pre>\n</td>\n<td>\n<pre><strong>Total Population</strong></pre>\n</td>\n<td>\n<pre><strong>Filters Away (%)</strong></pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre>Everyone</pre>\n</td>\n<td>\n<pre>6,880,000,000</pre>\n</td>\n<td>&nbsp;</td>\n</tr>\n<tr>\n<td>\n<pre>Speaks English + Internet Access</pre>\n</td>\n<td>\n<pre>536,000,000</pre>\n</td>\n<td>\n<pre>92.2%</pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre>Atheist/Agnostic</pre>\n</td>\n<td>\n<pre>40,000,000</pre>\n</td>\n<td>\n<pre>92.55%</pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre>Believes in evolution | Atheist/Agnostic</pre>\n</td>\n<td>\n<pre>30,400,000</pre>\n</td>\n<td>\n<pre>24%</pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre>&ldquo;NT&rdquo; (Rational) MBTI</pre>\n</td>\n<td>\n<pre>3,952,000</pre>\n</td>\n<td>\n<pre>87%</pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre>IQ 130+ (SD 15; US/UK-Atheist-NT 108 IQ)</pre>\n</td>\n<td>\n<pre>284,544</pre>\n</td>\n<td>\n<pre>92.8%</pre>\n</td>\n</tr>\n<tr>\n<td>\n<pre>30 min/day reading or on computers</pre>\n</td>\n<td>\n<pre> 16,930</pre>\n</td>\n<td>\n<pre>94.05%</pre>\n</td>\n</tr>\n</tbody>\n</table>\n</pre>\n<p><br /><br /> Yep, that&rsquo;s right.&nbsp; There are basically only a few thousand relatively bright people in the world who think reason makes sense and devote at least 2% of their day to arcane activities like &ldquo;reading&rdquo; and \"using computers\".<br /><br />Considering we have 6,438 Less Wrong logins created and a daily readership of around 5,500 people between logged in and anonymous readers, I now actually find it believable that we may have already reached a very large fraction of all the people in the world who we could theoretically convince to read the sequences.<br /><br />This actually matters because it makes me update in favor of different, more realistic growth strategies than <a href=\"/r/discussion/lw/2zc/currently_buying_adwords_for_lesswrong/\">buying AdWords</a> or <a href=\"/lw/2ag/on_less_wrong_traffic_and_new_users_and_how_you/\">doing SEO</a> to try and reach the small number of people left in our current target audience.&nbsp; Like translating the sequences into Chinese.&nbsp; Or creating an economic disaster that leaves most of the Westerner world unemployed (kidding!).&nbsp; Or waiting until Eliezer publishes his rationality book so that we can reach the vast majority of our <em>potential</em>, future audience who currently still reads but doesn&rsquo;t have time to do <em>anti-social</em>, <em>low-prestige</em> things like &ldquo;reading blogs&rdquo;.<br /><br /><br />For those of you who want to consider my methodology, here&rsquo;s the rationale for each step that I used to disqualify potential sequence readers:</p>\n<p><br /><br /><a href=\"http://www.internetworldstats.com/stats7.htm\">Doesn&rsquo;t Speak English or have Internet Access</a>:&nbsp; The sequences are English-only (right now) and online-only (right now).&nbsp; Don&rsquo;t think there&rsquo;s any contention here.&nbsp; This figure is the largest of the 3 figures I've found but all were around 500,000,000.<br /><br /><a href=\"http://www.americanreligionsurvey-aris.org/reports/NONES_08.pdf\">Not Atheist/Agnostic</a>: Not being an Atheist or Agnostic is a huge warning sign.&nbsp; 93% of LW is atheist/agnostic for a reason.&nbsp; It&rsquo;s probably a combo of&nbsp; 1) it&rsquo;s hard to stomach reading the sequences if you&rsquo;re a theist, and 2) you probably don&rsquo;t use thinking to guide the formation of your beliefs anyway so lessons in rationality are a complete waste of time for you.&nbsp; These people really needs to have the healing power of Dawkins come into their hearts before we can help them.&nbsp; Also, note that even though it wasn't mentioned in Yvain's top-level survey post, <a href=\"http://www.raikoth.net/Stuff/lwsurvey.xls\">the raw data showed</a> that around 1/3rd of LW users who gave a reason for participating on LW cite \"Atheism\".<br /><br /><a href=\"http://www.americanreligionsurvey-aris.org/reports/NONES_08.pdf\">Evolution denialist</a>: If you can&rsquo;t be bothered to be moved to correct beliefs about <em>the second most obvious conclusion in the world</em> by the mountains of evidence in favor of it, you&rsquo;re effectively saying you don&rsquo;t think induction or science can work at all.&nbsp; These people also need to go through Dawkins before we can help them.<br /><br /><a href=\"http://www.personalitypage.com/html/demographics.html\">Not &ldquo;NT&rdquo; on the Myers-Briggs typology</a>: Lots of people complain about the MBTI.&nbsp; But in this case, I don&rsquo;t think it matters that the MBTI isn&rsquo;t cleaving reality perfectly at the joints or that these types aren&rsquo;t natural categories.&nbsp; I realize Jung types aren&rsquo;t made of quarks and aren&rsquo;t fundamental.&nbsp; But I&rsquo;ve also met lots of people at the Less Wrong meet-ups.&nbsp; There&rsquo;s an even split of E/I and P/J in our community.&nbsp; But there is a uniform, overwhelmingly strong disposition towards N and T.&nbsp; And we shouldn&rsquo;t be surprised by this at all.&nbsp; People who are S instead of N take things at face value and resist using induction or intuition to extend their reasoning.&nbsp; These people can <a href=\"http://www.google.com.au/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBgQFjAA&amp;url=http%3A%2F%2Flesswrong.com%2Flw%2Fiq%2Fguessing_the_teachers_password%2F&amp;rct=j&amp;q=guess%20teachers%20password&amp;ei=7BblTLzSMIqmuAPc08DNDA&amp;usg=AFQjCNGO91sD_07kTOgz3ZnRuveGZO_Gjg&amp;sig2=V9eTIvppJwmS6Ta8vnvHlA&amp;cad=rja\">guess the teacher&rsquo;s password</a>, but they're not doing the same thing that you call \"thinking\".&nbsp; And if you&rsquo;re not a T (Thinking), then that means you&rsquo;re F (Feeling).&nbsp; And if you&rsquo;re using feelings to chose beliefs in lieu of thinking, there&rsquo;s nothing we can do for you -- you&rsquo;re permanently disqualified from enjoying the blessings of rationality.&nbsp; Note:&nbsp; I looked hard to see if I could find data suggesting that being NT and being Atheist correlated because I didn&rsquo;t want to &ldquo;double subtract&rdquo; out the same people twice.&nbsp; It turns out several studies have looked for this correlation with thousands of participants... and <a href=\"http://books.google.com.au/books?hl=en&amp;lr=&amp;id=BlobwmrY-dUC&amp;oi=fnd&amp;pg=PA15&amp;ots=XAn856TzF7&amp;sig=A97fer64RMGfHNDOQobpT_B-AfY#v=onepage&amp;q&amp;f=false\">it doesn&rsquo;t exist</a>.<br /><br /><a href=\"/lw/fk/survey_results/\">Lower than IQ 130</a>: Another non-natural category that people like to argue about.&nbsp; Plus, this feels super elitist, right?&nbsp; Excluding people just because they're \"not smart enough\". But it&rsquo;s really not asking that much when you consider that IQ 100 means you&rsquo;re <a href=\"http://www.npr.org/templates/story/story.php?storyId=129401497\">buying lottery tickets</a>, <a href=\"http://www.kaspersky.com/news?id=207576012\">installing malware on your computer</a>, and <a href=\"http://www.bls.gov/tus/\">spending most of your free time watching TV</a>.&nbsp; Those aren&rsquo;t the &ldquo;stupid people&rdquo; who are way down on the other side of the Gaussian -- that&rsquo;s what a normal 90 - 110 IQ looks like.&nbsp; Real stupid is so non-functional that you never even see it... probably because you don&rsquo;t hang out in prisons, asylums and homeless shelters.&nbsp; Really.&nbsp; And 130 isn&rsquo;t all that &ldquo;special&ldquo; once you find yourself being a white (<a href=\"http://anepigone.blogspot.com/2009/03/iq-estimates-by-intended-college-major.html\">+6IQ</a>) college graduate (<a href=\"http://inductivist.blogspot.com/2008/06/mean-iq-of-college-grads-dropped-9.html\">+5IQ</a>) atheist (<a href=\"http://dx.doi.org/10.1016%2Fj.intell.2008.08.003\">+4IQ</a>) who's &rdquo;NT&rdquo; on Myers-Briggs (<a href=\"http://www.opragroup.com/images/opra/pdf/community/Articles/intelligence%20in%20relation%20to%20jti.pdf\">+5IQ</a>).&nbsp; In <a href=\"/lw/fk/survey_results/\">Yvain&rsquo;s survey</a>, the average IQ on LW was 145.88.&nbsp; And only 4 out of 68 LWers reported IQs below 130... the lowest being 120.&nbsp; I find it inconceivable that EVERYONE lied on this survey.&nbsp; I also find it highly unlikely that only the top 1/2 reported.&nbsp; But even if everyone who didn&rsquo;t report was as low as the lowest IQ reported by anyone on Less Wrong, the average IQ would still be over 130.&nbsp; Note:&nbsp;&nbsp; I took the IQ boost from being atheist and being MBTI-&ldquo;N&rdquo; into account when figuring out the proportion of 130+ IQ conditional on the other traits already being factored in.<br /><br /><a href=\"http://www.bls.gov/tus/ \">Having no free time</a>: So you speak English, you don&rsquo;t hate science, you don&rsquo;t hate reason, and you&rsquo;re somewhat bright.&nbsp; Seem like you&rsquo;re a natural part of our target audience, right?&nbsp; Nope... wrong!&nbsp; There&rsquo;s at least one more big hurdle: Having some free time.&nbsp; Most people who are already awesome enough to have passed through all these filters are <strong>winning so hard at life </strong>(by American standards of success) that they are wayyy too busy to do <em>boring</em>, <em>anti-social</em> &amp; <em>low-prestige</em> tasks like reading online forums in their spare time (which they don&rsquo;t have much of).&nbsp; In fact, it&rsquo;s kind of like how <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">knowing a bit about biases can hurt you</a> and make you even more biased.&nbsp; Being a bit rational can skyrocket you to such a high level of narrowly-defined American-style \"success\" that you become a constantly-busy, middle-class wage-slave who zaps away all your free time in exchange for a mortgage and a car payment. Nice job buddy. Thanks for increasing my GDP epsilon%... now you are left with whatever rationality you started out with minus the effects of your bias dragging you back down to average over the ensuing years.&nbsp; The only ways I see out of this dilemma are 1) being in a relatively unstructured period of your life (ie, unemployed, college student, semi-retired, etc) or 2) having a completely broken motivation system which keeps you in a perpetually unstructured life against your will (akrasia) or perhaps 3) being a full-time computer professional who can multi-task and pass off reading online during your work day as actually working.&nbsp; That said, if you're unlucky enough to have a full-time job or you&rsquo;re married with children, <a href=\"http://www.bls.gov/tus/ \">you&rsquo;ve already fallen out of the population of people who read or use computers at least 30 minutes / day</a>.&nbsp; This is because having a spouse cuts your time spent reading and using computers in half.&nbsp; Having children cuts reading in half and reduces computer usage by 1/3rd.&nbsp; And having a job similarly cuts both reading and computer usage in half.&nbsp; Unfortunately, most people suffer from several of these afflictions.&nbsp; I can&rsquo;t find data that&rsquo;s conditional on being an IQ 130+ Atheist but my educated guess is employment is probably much better than average due to being so much more capable and I&rsquo;d speculate that relationships and children are about the same or perhaps a touch lower.&nbsp; All things equal, I think applying statistics from the general US civilian population and extrapolating is an acceptable approximation in this situation even if it likely <em>overestimates</em> the number of people who truly have 30 minutes of free time / day (the average amount of time needed <em>just to read LW</em> according to <a href=\"/lw/fk/survey_results/\">Yvain&rsquo;s survey</a>).&nbsp; 83% of people are employed full-time so they&rsquo;re gone.&nbsp; Of the remaining 17% who are unemployed, 10% of the men and 50% of the women are married and have children so that&rsquo;s another 5.1% off the top level leaving only 11.9% of people.&nbsp; Of that 11.9% left, the AVERAGE person has 1 hour they spend reading and &rdquo;Playing games and computer use for leisure&ldquo;.&nbsp; Let&rsquo;s be optimistic and assume they somehow devote half of their entire leisure budget to reading Less Wrong, that still only leaves 5.95%.&nbsp; Note: These numbers are a bit rough.&nbsp; If someone wants to go through the micro-data files of the US Time Use Survey for me and count the exact number of people who do more than 1 hour of \"reading\" and \"Playing games and computer use for leisure\", I welcome this help.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Anyone have thoughtful feedback on refinements or additional filters I could add to this?&nbsp; Do you know of better sources of statistics for any of the things I cite?&nbsp; <strong>And most importantly, do you have new, creative outreach strategies we could use now that we know this?</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HdWY4jB2NpXmWEHWz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 19, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "4041", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NbwbHQmgRoBuqtWrQ", "KjDuPTgfhYkR9LWHu", "bbf4ZWwcPQkRijEpt", "ZWC3n9c6v4s35rrZ3", "AdYdLP2sRqPMoe8fb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-18T20:25:06.045Z", "modifiedAt": null, "url": null, "title": "Another rationalist protagonist takes on \"magic\"", "slug": "another-rationalist-protagonist-takes-on-magic", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:37.127Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "PbyH3bFJCZjPX5DTp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7Mr8DcwjtXEHbNsKD/another-rationalist-protagonist-takes-on-magic", "pageUrlRelative": "/posts/7Mr8DcwjtXEHbNsKD/another-rationalist-protagonist-takes-on-magic", "linkUrl": "https://www.lesswrong.com/posts/7Mr8DcwjtXEHbNsKD/another-rationalist-protagonist-takes-on-magic", "postedAtFormatted": "Thursday, November 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20rationalist%20protagonist%20takes%20on%20%22magic%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20rationalist%20protagonist%20takes%20on%20%22magic%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Mr8DcwjtXEHbNsKD%2Fanother-rationalist-protagonist-takes-on-magic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20rationalist%20protagonist%20takes%20on%20%22magic%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Mr8DcwjtXEHbNsKD%2Fanother-rationalist-protagonist-takes-on-magic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Mr8DcwjtXEHbNsKD%2Fanother-rationalist-protagonist-takes-on-magic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<p>Along with the rest of the lesswrong community, I've been enjoying \"Harry Potter and the Methods of Rationality.\"&nbsp;</p>\n<p>I would like to heartily recommend another series with similar themes:</p>\n<p><a href=\"http://www.amazon.com/Steerswomans-Road-Rosemary-Kirstein/dp/0345461053/ref=cm_lmf_tit_5\">The Steerswoman's Road, by Rosemary Kirstein</a></p>\n<p>Like HP:MOR, we have a protagonist applying scientific methods to understand \"magic\". &nbsp;Unlike HP:MOR, this protagonist doesn't have the benefit of an accumulated body of knowledge on science and rationality (the setting is medieval era technology, more or less). Refreshingly and realistically, though, her pursuits are part of a larger collaboration, the \"steerswomen\", a group devoted to furthering and freely disseminating human knowledge. &nbsp;Unlike HP, the protagonist has to be confused (about things we as readers already know) not just for minutes, but for years. We are painstakingly led through every step and misstep of her reasoning.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7Mr8DcwjtXEHbNsKD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 17, "extendedScore": null, "score": 6.470042713774813e-07, "legacy": true, "legacyId": "4043", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-18T21:42:50.139Z", "modifiedAt": null, "url": null, "title": "The Benefits of Two Religious Educations", "slug": "the-benefits-of-two-religious-educations", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:35.046Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HonoreDB", "createdAt": "2010-11-18T19:42:02.810Z", "isAdmin": false, "displayName": "HonoreDB"}, "userId": "7eyYSfGvgCur6pXmk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dStthPQfjCSARXjk2/the-benefits-of-two-religious-educations", "pageUrlRelative": "/posts/dStthPQfjCSARXjk2/the-benefits-of-two-religious-educations", "linkUrl": "https://www.lesswrong.com/posts/dStthPQfjCSARXjk2/the-benefits-of-two-religious-educations", "postedAtFormatted": "Thursday, November 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Benefits%20of%20Two%20Religious%20Educations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Benefits%20of%20Two%20Religious%20Educations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdStthPQfjCSARXjk2%2Fthe-benefits-of-two-religious-educations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Benefits%20of%20Two%20Religious%20Educations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdStthPQfjCSARXjk2%2Fthe-benefits-of-two-religious-educations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdStthPQfjCSARXjk2%2Fthe-benefits-of-two-religious-educations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 452, "htmlBody": "<p><span style=\"font-family: Helvetica; font-size: 12px;\"><span style=\"white-space: pre;\"><strong><span style=\"white-space: pre;\"> </span></strong></span>It seems fitting that my first post here be an origin story, of sorts.&nbsp; Like any origin story, it is overly reductionistic and attributes a single cause to an overdetermined phenomenon.&nbsp; There's an old Spider-Man comic that claims that even if he hadn't been bitten by a radioactive spider, and even if he hadn't caused his uncle's death through inaction, Peter Parker would still have become a superhero thanks to his engineering talent and strong moral fiber.&nbsp; Nevertheless, I find it compelling to say that I became a skeptic (and from there a rationalist and consequentialist) because from an early age I attended two different religious schools at the same time.</span></p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica;\"><span style=\"white-space: pre;\"> <span style=\"white-space: pre;\"> </span></span>From age six, I spent my weekdays at a Christian independent school.&nbsp; From around the same time, I went to a Jewish \"Sunday school\" (and to Jewish religious services some Saturday evenings).&nbsp; I imagine this is a rare, bizarre-sounding way to grow up.&nbsp; In Jewish communities in rural Pennsylvania it's quite common.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica;\"><span style=\"white-space: pre;\"> </span>This led to a predictable phenomenon.&nbsp; Adults, teachers in similar positions of respect and authority, were (confidently and earnestly!) making different, contradictory assertions about extremely important subjects.&nbsp; People whom I respected equally had vastly different concepts of how the universe worked, and I was constantly reminded of this.&nbsp; The inference was inescapable: teachers were often wrong and I would have to use my own judgement.&nbsp; I remember briefly theorizing that there were simply two different gods, the Old Testament one and the New Testament one (who was also Gaia), which would certainly help reconcile everything.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica;\"><span style=\"white-space: pre;\"> </span>By the time I was ten, I questioned everything a teacher said, in any subject, to a fault (e.g., I refused to learn the backhand in tennis because I couldn't see the point). &nbsp; By the time I was twelve, I confidently identified as an atheist.&nbsp; My parents were still religious Jews, but they didn't really care as long as they could bully me into performing the rituals.&nbsp; We spent more time arguing about AI, as it happened, than the existence of God (my parents were both <a href=\"http://en.wikipedia.org/wiki/Chinese_room\" target=\"_blank\">Searle-ists</a>).&nbsp; By the time I was fifteen, I had decided to drop out of school and educate myself, etc.</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica;\">&nbsp;</p>\n<p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px Helvetica;\"><span style=\"white-space: pre;\"> <span style=\"white-space: pre;\"> </span></span>I think I would have gotten there anyway.&nbsp; But I find it appealing to speculate that I got there much faster than I would have if I'd received a secular education.&nbsp; I'm curious whether anyone here had a similar upbringing.&nbsp; Might this be a good way for atheists to deliberately inoculate their children?&nbsp; Might it be a good way, in general, to ensure that children grow up instinctively distrustful of authority?&nbsp; I realize that may be a negative trait in an ideal world, but in this corrupt one I think it's essential.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dStthPQfjCSARXjk2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 6.470231724122978e-07, "legacy": true, "legacyId": "4044", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-18T22:37:36.984Z", "modifiedAt": null, "url": null, "title": "Goals for which Less Wrong does (and doesn't) help", "slug": "goals-for-which-less-wrong-does-and-doesn-t-help", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:34.167Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7dRGYDqA2z6Zt7Q4h/goals-for-which-less-wrong-does-and-doesn-t-help", "pageUrlRelative": "/posts/7dRGYDqA2z6Zt7Q4h/goals-for-which-less-wrong-does-and-doesn-t-help", "linkUrl": "https://www.lesswrong.com/posts/7dRGYDqA2z6Zt7Q4h/goals-for-which-less-wrong-does-and-doesn-t-help", "postedAtFormatted": "Thursday, November 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Goals%20for%20which%20Less%20Wrong%20does%20(and%20doesn't)%20help&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGoals%20for%20which%20Less%20Wrong%20does%20(and%20doesn't)%20help%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7dRGYDqA2z6Zt7Q4h%2Fgoals-for-which-less-wrong-does-and-doesn-t-help%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Goals%20for%20which%20Less%20Wrong%20does%20(and%20doesn't)%20help%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7dRGYDqA2z6Zt7Q4h%2Fgoals-for-which-less-wrong-does-and-doesn-t-help", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7dRGYDqA2z6Zt7Q4h%2Fgoals-for-which-less-wrong-does-and-doesn-t-help", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1063, "htmlBody": "<p>Related to: <a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\">Self-Improvement or Shiny Distraction: Why Less Wrong is anti-Instrumental Rationality</a></p>\n<p>We&rsquo;ve had a lot of good criticism of Less Wrong lately (including Patri&rsquo;s post above, which contains a number of useful points). But to prevent those posts from confusing newcomers, this may be a good time to review what Less Wrong <em>is</em> useful for.</p>\n<p>In particular: I had a conversation last Sunday with a fellow, I&rsquo;ll call him Jim, who was trying to choose a career that would let him &ldquo;help shape the singularity (or simply the future of humanity) in a positive way&rdquo;. &nbsp;He was trying to sort out what was efficient, and he aimed to be careful to have <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">goals</a> and not <a href=\"http://vimeo.com/7397629\">roles</a>. &nbsp;</p>\n<p>So far, excellent news, right? &nbsp;A thoughtful, capable person is trying to sort out how, exactly, to have the best impact on humanity&rsquo;s future. &nbsp;Whatever your views on the <a href=\"http://wiki.lesswrong.com/wiki/Existential_risk\">existential risks</a> landscape, it&rsquo;s clear humanity could use more people like that.</p>\n<p>The part that concerned me was that Jim had put a site-blocker on LW (as well as all of his blogs) after reading Patri&rsquo;s post, which, he said, had &ldquo;hit him like a load of bricks&rdquo;. &nbsp;Jim wanted to get his act together and really help the world, not diddle around reading shiny-fun blog comments. &nbsp;But his discussion of <em>how</em> to &ldquo;really help the world&rdquo; seemed to me to contain a number of errors[1] -- errors enough that, if he cannot sort them out somehow, his total impact won&rsquo;t be nearly what it could be. &nbsp;And they were the sort of errors LW could have helped with. &nbsp;And there was no obvious force in his off-line, focused, productive life of a sort that could similarly help.</p>\n<p>So, in case it&rsquo;s useful to others, a review of what LW <em>is</em> useful for.</p>\n<h2><span style=\"font-weight: normal;\"><a id=\"more\"></a>When you do (and don&rsquo;t) need epistemic rationality</span></h2>\n<p><strong></strong>For some tasks, the world provides rich, inexpensive empirical feedback. &nbsp;In these tasks you hardly need reasoning. &nbsp;Just try the task many ways, steal from the best role-models you can find, and take care to notice what is and isn&rsquo;t giving you results.</p>\n<p>Thus, if you want to learn to sculpt, reading Less Wrong is a bad way to go about it. &nbsp;Better to find some clay and a hands-on sculpting course. &nbsp;The situation is similar for small talk, cooking, selling, programming, and many other useful skills.</p>\n<p>Unfortunately, most of us also have goals for which we can obtain no such ready success/failure data. For example, if you want to know whether <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">cryonics</a> is a good buy, you can&rsquo;t just try buying it and not-buying it and see which works better. &nbsp;If you miss your first bet, you&rsquo;re out for good.</p>\n<p>There is similarly no easy way to use the &ldquo;try it and see&rdquo; method to sort out what ethics and meta-ethics to endorse, or what long-term human outcomes are likely, how you can have a positive impact on the distant poor, or which retirement investments *really will* be <a href=\"http://en.wikipedia.org/wiki/Equity_premium_puzzle#Denial_of_equity_premium\">safe</a> <a href=\"http://en.wikipedia.org/wiki/United_States_housing_bubble\">bets</a> for the next forty years. &nbsp;For these goals we are forced to use reasoning, as failure-prone as human reasoning is. &nbsp;If the issue is tricky enough, we&rsquo;re forced to additionally develop our skill at reasoning -- to develop &ldquo;epistemic rationality&rdquo;.</p>\n<p>The traditional alternative is to deem subjects on which one cannot gather empirical data \"unscientific\" subjects on which respectable people should not speak, or else to focus one's discussion on the most similar-seeming subject for which it *is* easy to gather empirical data (and so to, for example, rate charities as \"good\" when they have a <a href=\"http://www.philanthropyaction.com/nc/the_worst_and_best_way_to_pick_a_charity_this_year/\">low percentage of overhead</a>, instead of a high impact). Insofar as we are stuck caring about such goals and betting our actions on various routes for their achievement, this is not much help.[2]</p>\n<p><span style=\"font-size: 16px;\">How to develop epistemic rationality</span></p>\n<p>If you want to develop epistemic rationality, it helps to spend time with the best epistemic rationalists you can find. &nbsp;For many, although not all, this will mean Less Wrong. &nbsp;Read the <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">sequences</a>. &nbsp;Read the top current conversations. &nbsp;Put your own thinking out there (in the <a href=\"/r/discussion/\">discussion section</a>, for starters) so that others can help you find mistakes in your thinking, and so that you can get used to holding your own thinking to high standards. &nbsp;Find or build an in-person community of aspiring rationalists if you can.</p>\n<p>Is it useful to try to read every single comment? &nbsp;Probably not, on the margin; <a href=\"http://www.overcomingbias.com/2010/06/read-a-classic.html\">better</a> <a href=\"http://www.amazon.com/Influence-Psychology-Persuasion-Robert-Cialdini/dp/0688128165\">to</a> <a href=\"http://www.amazon.com/Probability-Theory-Logic-Science-Vol/dp/0521592712/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1290108761&amp;sr=1-1\">read</a> <a href=\"/lw/jv/recommended_rationalist_reading/\">textbooks</a> or to do <a href=\"/lw/2ps/september_2010_southern_california_meetup/2mbq?c=1\">rationality</a>&nbsp;<a href=\"http://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html\">exercises</a> yourself. &nbsp;But reading the Sequences helped many of us quite a bit; and epistemic rationality is the sort of thing for which sitting around reading (even reading things that are shiny-fun) can actually help.</p>\n<p>&nbsp;</p>\n<hr />\n<p>[1] &nbsp;To be specific: Jim was considering personally \"raising awareness\" about the virtues of the free market, in the hopes that this would (indirectly) boost economic growth in the third world, which would enable more people to be educated, which would enable more people to help aim for a positive human future and an eventual positive singularity.</p>\n<p>There are several difficulties with this plan. &nbsp;For one thing, it's complicated; in order to work, his awareness raising would need to indeed boost free market enthusiasm AND US citizens' free market enthusiasm would need to indeed increase the use of free markets in the third world AND this result would need to indeed boost welfare and education in those countries AND a world in which more people could think about humanity's future would need to indeed result in a better future.&nbsp;Conjunctions are <a href=\"/lw/ji/conjunction_fallacy/\">unlikely</a>, and this route didn't sound like the most direct path to Jim's stated goal.</p>\n<p>For another thing, there are good general <a href=\"http://www.utilitarian-essays.com/make-money.html\">arguments</a> suggesting that it is often better to donate than to work directly in a given field, and that, given the many <em><a href=\"http://blog.givewell.org/2010/01/28/can-choosing-the-right-charity-double-your-impact/\">orders of magnitude</a></em> differences in efficacy between different sorts of philanthropy, it's worth doing considerable <a href=\"http://web.archive.org/web/20070627155750/www.felicifia.com/showDiary.do?diaryId=37\">research</a> into how best to give. &nbsp;(Although to be fair, Jim's emailing me was such research, and he may well have appreciated that point.)&nbsp;</p>\n<p>The biggest reason it seemed Jim would benefit from LW was just manner; Jim seemed smart and well-meaning, but more verbally jumbled, and less good at factoring complex questions into distinct, analyzable pieces, than I would expect if he spent longer around LW.</p>\n<div>[2] The traditional rationalist reply would be that if human reasoning is completely and permanently hopeless when divorced from the simple empirical tests of Popperian science, then avoiding such \"unscientific\" subjects is all we can do.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "Ng8Gice9KNkncxqcj": 1, "9YFoDPFwMoWthzgkY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7dRGYDqA2z6Zt7Q4h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 67, "baseScore": 85, "extendedScore": null, "score": 0.000153, "legacy": true, "legacyId": "4042", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 85, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 104, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uFYQaGCRwt3wKtyZP", "PBRWb2Em5SNeWYwwB", "RiQYixgCdvd8eWsjg", "QAK43nNCTQQycAcYe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-19T01:53:26.991Z", "modifiedAt": null, "url": null, "title": "Yes, a blog.", "slug": "yes-a-blog", "viewCount": null, "lastCommentedAt": "2018-06-22T00:44:31.581Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DXcezGmnBcAYL2Y2u/yes-a-blog", "pageUrlRelative": "/posts/DXcezGmnBcAYL2Y2u/yes-a-blog", "linkUrl": "https://www.lesswrong.com/posts/DXcezGmnBcAYL2Y2u/yes-a-blog", "postedAtFormatted": "Friday, November 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Yes%2C%20a%20blog.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYes%2C%20a%20blog.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXcezGmnBcAYL2Y2u%2Fyes-a-blog%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Yes%2C%20a%20blog.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXcezGmnBcAYL2Y2u%2Fyes-a-blog", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXcezGmnBcAYL2Y2u%2Fyes-a-blog", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1053, "htmlBody": "<!-- Yes, a blog. -->\n<p>When I recommend LessWrong to people, their gut reaction is usually \"What?  You think the best existing philosophical treatise on rationality is a <em>blog?</em>\"</p>\n<p>Well, yes, at the moment I do.</p>\n<p>\"But why is it not an ancient philosophical manuscript written by a single Very Special Person with no access to the massive knowledge the human race has accumulated over the last 100 years?\"</p>\n<p>Besides the obvious?  Three reasons: <em>idea selection, critical mass, and <a href=\"http://wiki.lesswrong.com/wiki/A_Human's_Guide_to_Words\">helpful</a> <a href=\"/lw/1p1/logical_rudeness/\">standards</a> for collaboration and debate.</em></p>\n<p><em>Idea selection.</em></p>\n<p>Ancient people came up with some amazing ideas, like how to make fire, tools, and languages.  Those ideas have stuck around, and become integrated in our daily lives to the point where they barely seem like knowledge anymore.  The great thing is that we don't have to read ancient cave writings to be reminded that fire can keep us warm; we simply haven't forgotten.  That's why more people agree that fire can heat your home than on how the universe began.</p>\n<p>Classical philosophers like Hume came up with some great ideas, too, especially considering that they had no access to modern scientific knowledge.  But you don't have to spend thousands of hours reading through their flawed or now-uninteresting writings to find their few truly inspiring ideas, because their best ideas have <em>become</em> modern scientific knowledge.  You don't <em>need</em> to read Hume to know about empiricism, because we simply <em>haven't forgotten it</em>... that's what science <em>is</em> now.  You don't <em>have</em> to read Kant to think abstractly about Time; thinking about \"timelines\" is practically built into our language nowadays.</p>\n<p>See, society works like a great sieve that remembers good ideas, and forgets <em>some</em> of the bad ones.  Plenty of bad ideas stick around because they're viral (self-propagating for reasons other than <a href=\"http://wiki.lesswrong.com/wiki/Rationality#Instrumental_rationality\">helpfulness</a>/<a href=\"http://wiki.lesswrong.com/wiki/Rationality#Epistemic_rationality\">verifiability</a>), so you can't always trust an idea just because it's old.  But that's how any sieve works: it narrows your search. It keeps the stuff you want, and throws away <em>some</em> of the bad stuff so you don't have to look at it.</p>\n<p>LessWrong itself <a href=\"/lw/1/about_less_wrong/\">is an update patch</a> for philosophy to fix compatibility issues with science and render it more useful.  That it would exist now rather than much earlier is no coincidence: right now, it's the gold at the bottom of the pan, because it's taking the idea filtering process to a whole new level.  Here's a rough timeline of how LessWrong happened:  <a id=\"more\"></a></p>\n<p><em>Critical mass.</em></p>\n<p>To get off the ground, a critical mass of very good ideas was needed: the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">LessWrong Sequences</a>.  Eliezer Yudkowsky spent several years posting a lot of extremely sane writing on OvercomingBias.com, and then founded LessWrong.com, attracting the attention of other people who were annoyed at the lower density of good ideas in older literature.</p>\n<p>Part of what made them successful is that the sequences are written in a widely learned, widely applicable language: the language of basic science and mathematics.  A lot of the serious effort in classical philosophy was spent trying to develop precise and appropriate terminology in which to communicate, and so joining the conversation always required a serious exclusive study of the accumulated lingo and concepts.  But nowadays we can study rationality by <a href=\"http://en.wikipedia.org/wiki/Transfer_of_learning\">transfer of learning</a> from tried-and-true technical disciplines like probability theory, computer science, biology, and even physics.  So the Sequences were written.</p>\n<p>Then, using an explicit upvote system, LessWrong and its readers began accelerating the historically slow process of idea selection: if you wanted to be sure to see something inspiring, you just had to click <a href=\"/top/\">\"TOP\"</a> to see a list of top voted posts.<a href=\"#top\"><sup>1</sup></a></p>\n<p><em>Collaboration and debate.</em></p>\n<p>Finally, with a firm foundation taking hold, there is now a context, a language, and a community that will understand <em>your</em> good ideas.  Reading LessWrong makes it <em>vastly</em> easier to collaborate effectively on resolving abstract practical issues<a href=\"#ap\"><sup>2</sup></a>.  <strong>And if you disagree with LessWrong, reading LessWrong will help you communicate your disagreement better.</strong> There was a time when you couldn't have a productive abstract conversation with someone unless you spent a few days establishing a context with that person; now you have LessWrong sequences to do that for you.</p>\n<p>The sequences also refer to plenty of historical mistakes made by old-school philosophers, so you don't <em>necessarily</em> have to spend thousands of hours reading very old books to learn what not to do.  This leaves you with more time to develop basic or advanced skills in math and science<a href=\"#abridged\"><sup>3</sup></a>, which, aside from the obvious career benefits, gets you closer to understanding subjects like cognitive and neuropsychology, probability and statistics, information and coding theory, formal logic, complexity theory, decision theory, quantum physics, relativity... Any philosophical discussion predating these subjects is simply <em>out of the loop</em>.  A lot of their mistakes aren't even <em>about</em> the things we need to be analysing now.</p>\n<p>So yes, if you want good ideas about rationality, and particularly its applications to understanding the nature of reality and life, you can restrict a lot of your attention to what people are talking about <em>right now</em>, and you'll be at a comparatively low risk of missing out on something important. Of course, you have to use your judgement to finish to search.  Luckily, LessWrong tries to teach that, too.  It's really a very good deal.  Plus, if you upvote your favorite posts, you start contributing right away by helping the idea selection process.</p>\n<p>Don't forget: <em>Wikipedia happened</em>.  It didn't sell out.  It didn't fall to vandals.  Encyclopedic knowledge is now free, accessible, collaborative, and even addictive.  Now, LessWrong is happening to rationality.</p>\n<p>&nbsp;</p>\n<hr />\n<p><a name=\"top\"><sup>1</sup></a> In my experience, the Top Posts section works like an anti-sieve: pretty much everything on there is clever, but in any one reader's opinion there is probably a lot of great material that didn't make it to the top.</p>\n<p><a name=\"ap\"><sup>2</sup></a> I sometimes describe the LessWrong dialogue as about \"abstract practicality\", because to most people the word \"philosophy\" communicates a sense of explicit uselessness, which LessWrong defies. The discussions here are all aimed at resolving real-life decisions of some kind or another, be it whether to start meditating or whether to freeze yourself when you die.</p>\n<p><a name=\"abridged\"><sup>3</sup></a> I compiled <a href=\"http://wiki.lesswrong.com/wiki/User:Academian#Abridged_entry_to_the_LessWrong_community\">this abridged list</a> of sequence posts for people who already have a strong background in math and science, to accomodate a faster exposure to the LessWrong \"introductory\" material.</p>\n<p><a name=\"other\"><sup>4</sup></a> This post is about how LessWrong happened as a blog.  For recent general discussion of LessWrong's good and bad effects, consider <a href=\"/lw/34a/when_you_need_less_wrong/\">When you need Less Wrong</a> and <a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\"> Self-Improvement or Shiny Distraction?</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 2, "MfpEPj6kJneT9gWT6": 7, "a3W2TSzPuxKr3Hm9j": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DXcezGmnBcAYL2Y2u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 118, "baseScore": 133, "extendedScore": null, "score": 0.000256, "legacy": true, "legacyId": "4015", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 133, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 108, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["srge9MCLHSiwzaX6r", "2om7AHEHtbogJmT5s", "7dRGYDqA2z6Zt7Q4h", "uFYQaGCRwt3wKtyZP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-19T03:10:45.715Z", "modifiedAt": null, "url": null, "title": "Advice for a Budding Rationalist", "slug": "advice-for-a-budding-rationalist", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:51.316Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dWhA58LswXm3yoQz4/advice-for-a-budding-rationalist", "pageUrlRelative": "/posts/dWhA58LswXm3yoQz4/advice-for-a-budding-rationalist", "linkUrl": "https://www.lesswrong.com/posts/dWhA58LswXm3yoQz4/advice-for-a-budding-rationalist", "postedAtFormatted": "Friday, November 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advice%20for%20a%20Budding%20Rationalist&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvice%20for%20a%20Budding%20Rationalist%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdWhA58LswXm3yoQz4%2Fadvice-for-a-budding-rationalist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advice%20for%20a%20Budding%20Rationalist%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdWhA58LswXm3yoQz4%2Fadvice-for-a-budding-rationalist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdWhA58LswXm3yoQz4%2Fadvice-for-a-budding-rationalist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 241, "htmlBody": "<p>Most people in the US with internet connections who are reading this site will at some point in their lives graduate high school. I haven't yet, and it seems like what I do afterwards will have a pretty big effect on the rest of my life.*&nbsp;<br /><br />Given that, I think I should ask for some advice.</p>\n<p>Generally,<br />Any advice? Anything you wish you knew? Disagreement with the premise? (If you disagree, please explain what to do anyway.)</p>\n<p>More specific to the site,<br />Any advice for high schoolers with a rationalist and singularitarian bent? Who are probably looking at going to college?<br />Anything particularly effective for working against existential risk?<br />Any fields particularly useful for rationalists to know?<br />Any fields in which rationalists would be particularly helpful?</p>\n<p>This is intended to be a pretty general reference for life advice for the young ones among us. With a college selection bent, probably. If you're in high school and have a specific situation that you want help with/advice for, please reply to this post with that. I think that a most people have specific skills/background they could leverage, so a one-size-fits all approach seems to be somewhat simplistic.</p>\n<p>*I understand that I can always change plans later, but there are many many things that seem to require some level of commitment, like college.<br /><br />Edit:<br />As Unnamed pointed out, also look at&nbsp;<a href=\"/lw/2du/a_rational_education/\">this article about undergraduate course selection</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dWhA58LswXm3yoQz4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 6.471029176076693e-07, "legacy": true, "legacyId": "4045", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gztAhEueePQi3RNHs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-19T11:13:45.490Z", "modifiedAt": null, "url": null, "title": "Fiction: \"Chicken Little\" by Cory Doctorow", "slug": "fiction-chicken-little-by-cory-doctorow", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:35.696Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/he8fCLY47QZQ3kgey/fiction-chicken-little-by-cory-doctorow", "pageUrlRelative": "/posts/he8fCLY47QZQ3kgey/fiction-chicken-little-by-cory-doctorow", "linkUrl": "https://www.lesswrong.com/posts/he8fCLY47QZQ3kgey/fiction-chicken-little-by-cory-doctorow", "postedAtFormatted": "Friday, November 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fiction%3A%20%22Chicken%20Little%22%20by%20Cory%20Doctorow&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFiction%3A%20%22Chicken%20Little%22%20by%20Cory%20Doctorow%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhe8fCLY47QZQ3kgey%2Ffiction-chicken-little-by-cory-doctorow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fiction%3A%20%22Chicken%20Little%22%20by%20Cory%20Doctorow%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhe8fCLY47QZQ3kgey%2Ffiction-chicken-little-by-cory-doctorow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhe8fCLY47QZQ3kgey%2Ffiction-chicken-little-by-cory-doctorow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<p>What would happen if people could feel what they knew about probability? The story's in <a href=\"http://www.amazon.com/exec/obidos/ASIN/0765326620/downandoutint-20\">Gateways</a>, a tribute collection for Frederik Pohl.  Unfortunately, the story is more about whether a drug with that effect is a good idea rather than extended exploration of the effects, but it's still interesting.  One of the points is that, while people on the drug are more sensible (laugh at lotteries, don't eat food that makes them feel bad), they don't have children.  That last might be less of an obvious outcome than it seems-- rationality can improve parenthood.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "he8fCLY47QZQ3kgey", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 6.472204044359898e-07, "legacy": true, "legacyId": "4048", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-19T11:26:27.972Z", "modifiedAt": null, "url": null, "title": "Help with (pseudo-)rational film characters", "slug": "help-with-pseudo-rational-film-characters", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:52.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "funkycoder", "createdAt": "2010-11-19T10:56:25.900Z", "isAdmin": false, "displayName": "funkycoder"}, "userId": "5HhxJy5fTzSuuR6rC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ASmoRyd49DMY8PsYs/help-with-pseudo-rational-film-characters", "pageUrlRelative": "/posts/ASmoRyd49DMY8PsYs/help-with-pseudo-rational-film-characters", "linkUrl": "https://www.lesswrong.com/posts/ASmoRyd49DMY8PsYs/help-with-pseudo-rational-film-characters", "postedAtFormatted": "Friday, November 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20with%20(pseudo-)rational%20film%20characters&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20with%20(pseudo-)rational%20film%20characters%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FASmoRyd49DMY8PsYs%2Fhelp-with-pseudo-rational-film-characters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20with%20(pseudo-)rational%20film%20characters%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FASmoRyd49DMY8PsYs%2Fhelp-with-pseudo-rational-film-characters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FASmoRyd49DMY8PsYs%2Fhelp-with-pseudo-rational-film-characters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 318, "htmlBody": "<p>Hi,</p>\n<p>I'm currently developing a web-series based around two people who call themselves rationalists, planning to assassinate people in order to bring down the catholic church.</p>\n<p>So my fixpoints are that they need sound, or at least compelling, rational argument to justify killing people for \"the greater good\" (because they are sure the church works very strongly against that). Also, since it'll be the style of the series, they document themselves while planning their actions. They post encrypted versions of their videos and all data used online and have a mechanism for releasing the keys in case they are caught. Being rational, they know that they will be caught eventually and want to get \"their method\" out there, hoping to set off some kind of \"non-ideological revolution\". Although they know it will probably end badly for them, they are as careful as possible, while trying to not be overly paranoid since it my affect their performance.</p>\n<p>Of course they are biased, since they first had the idea of killing the pope, and then they thought of doing so \"rationally\". They do see their bias here, though, and try to work around it a bit, but they must come out on the side of pursuing their plans (or the series ends).</p>\n<p>There won't be a real \"message\" to the series, although they need to make some quite obvious false assumptions (and some personal bias, too, no fridge-stuffing, but rather they are more ideological than they know, in that born-again christian-become-atheist kind of way, occasionally not understanding the difference between what the basic axioms are, and what dogmas are. Also some bipolar disorder and depression might come in handy), so nobody will get stupid ideas. Also I don't want to be persecuted for providing terror-manuals.</p>\n<p>&nbsp;</p>\n<p><em>Let me rephrase that: Starting out as a (sloppy) rationalist, what logical fallacies do you have to trip in order to end up on a position that would endorse terrorism?</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ASmoRyd49DMY8PsYs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -11, "extendedScore": null, "score": 6.472234961221821e-07, "legacy": true, "legacyId": "4049", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-19T12:55:00.130Z", "modifiedAt": null, "url": null, "title": "Common Sense Atheism summarizing the Sequences", "slug": "common-sense-atheism-summarizing-the-sequences", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:34.543Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexandros", "createdAt": "2009-04-21T11:07:48.256Z", "isAdmin": false, "displayName": "Alexandros"}, "userId": "GQ6FJrTSW7qWeuQDD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wzAWcXdbpn94qJv42/common-sense-atheism-summarizing-the-sequences", "pageUrlRelative": "/posts/wzAWcXdbpn94qJv42/common-sense-atheism-summarizing-the-sequences", "linkUrl": "https://www.lesswrong.com/posts/wzAWcXdbpn94qJv42/common-sense-atheism-summarizing-the-sequences", "postedAtFormatted": "Friday, November 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Common%20Sense%20Atheism%20summarizing%20the%20Sequences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACommon%20Sense%20Atheism%20summarizing%20the%20Sequences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwzAWcXdbpn94qJv42%2Fcommon-sense-atheism-summarizing-the-sequences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Common%20Sense%20Atheism%20summarizing%20the%20Sequences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwzAWcXdbpn94qJv42%2Fcommon-sense-atheism-summarizing-the-sequences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwzAWcXdbpn94qJv42%2Fcommon-sense-atheism-summarizing-the-sequences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<p>Since popularising the sequences seems to be a pursuit that's been in the spotlight recently, I thought I'd point out that blogger Luke Muehlhauser of Common Sense Atheism has started blogging through the sequences. The first installment is here:</p>\n<p><a href=\"http://commonsenseatheism.com/?p=12774\">Reading Yudkowsky, Part 1</a></p>\n<p>Perhaps someone you know will benefit from the sequences but can/will not invest the time to go though the whole thing can be directed to Luke's metasequence.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JMD7LTXTisBzGAfhX": 2, "8SfkJYYMe75MwjHzN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wzAWcXdbpn94qJv42", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 6.47244914657751e-07, "legacy": true, "legacyId": "4050", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-20T02:12:03.269Z", "modifiedAt": null, "url": null, "title": "Rationality and being child-free", "slug": "rationality-and-being-child-free", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:28.214Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y5zje2RRitGxftbGa/rationality-and-being-child-free", "pageUrlRelative": "/posts/y5zje2RRitGxftbGa/rationality-and-being-child-free", "linkUrl": "https://www.lesswrong.com/posts/y5zje2RRitGxftbGa/rationality-and-being-child-free", "postedAtFormatted": "Saturday, November 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20and%20being%20child-free&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20and%20being%20child-free%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy5zje2RRitGxftbGa%2Frationality-and-being-child-free%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20and%20being%20child-free%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy5zje2RRitGxftbGa%2Frationality-and-being-child-free", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy5zje2RRitGxftbGa%2Frationality-and-being-child-free", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p>So I found this post quite interesting:</p>\n<p>http://www.gnxp.com/blog/2009/03/gnxp-readers-do-not-breed.php</p>\n<p>(I'm quite sure that the demographics of this site closely parallel the demographics on Gene Expression).</p>\n<p>Research seems to indicate that people are happiest when they're married, but that each child imposes a net decrease in happiness (parents in fact, enjoy a boost in happiness once their children leave the house). It's possible, of course, that adult children may be pleasurable to interact with, but it seems that in many cases, the parents want to interact with the children more than the children want to interact with the parent (although daughters generally seem more interactive with their parents).</p>\n<p>So how do you think being child-free relates to rationality/happiness? Of course, Bryan Caplan (who is pro-natalist) cites research (from Judith Rich Harris) saying that parents really have less influence over their children than they think they have (so it's a good idea for parents to spend less effort in trying to \"mold\" their children, since their efforts will inevitably result in much frustration). And in fact, if parents did this, it's possible that they may beat the average.</p>\n<p>(This doesn't convince me in my specific case, however, and I'm still committed to not having children).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b7ZSAGimsbzrLR5CR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y5zje2RRitGxftbGa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 15, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "4051", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-20T04:41:39.635Z", "modifiedAt": null, "url": null, "title": "Games People Play", "slug": "games-people-play", "viewCount": null, "lastCommentedAt": "2010-11-23T22:52:49.091Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Knight", "createdAt": "2009-03-15T05:34:58.304Z", "isAdmin": false, "displayName": "Douglas_Knight"}, "userId": "BfkiKScoERGRt8MWC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T7nx89Rb3WWugghZi/games-people-play", "pageUrlRelative": "/posts/T7nx89Rb3WWugghZi/games-people-play", "linkUrl": "https://www.lesswrong.com/posts/T7nx89Rb3WWugghZi/games-people-play", "postedAtFormatted": "Saturday, November 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Games%20People%20Play&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGames%20People%20Play%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT7nx89Rb3WWugghZi%2Fgames-people-play%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Games%20People%20Play%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT7nx89Rb3WWugghZi%2Fgames-people-play", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT7nx89Rb3WWugghZi%2Fgames-people-play", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 541, "htmlBody": "<p>Game theory is great if you know what game you're playing. All this talk of Diplomacy reminds me of this memory of <a href=\"http://adamcadre.ac/calendar/11646.html\">Adam Cadre</a>:</p>\n<blockquote>\n<p>I remember that in my ninth grade history class, the teacher had us play a game that was supposed to demonstrate how shifting alliances work. He divided the class into seven groups &mdash; dubbed Britain, France, Germany, Belgium, Italy, Austria and Russia &mdash; and, every few minutes, declared a \"battle\" between two of the countries. Then there was a negotiation period, during which we all were supposed to walk around the room making deals. Whichever warring country collected the most allies would win the battle and a certain number of points to divvy up with its allies. The idea, I think, was that countries in a battle would try to win over the wavering countries by promising them extra points to jump aboard.</p>\n<p>That's not how it worked in practice. Three or four guys &mdash; the same ones who had gotten themselves elected to ASB, the student government &mdash; decided among themselves during the first negotiation period what the outcome would be, and told people whom to vote for. <em>And the others just shrugged and did as they were told.</em> The ASB guys had decided that Germany would win, followed by France, Britain, Belgium, Austria, Italy and Russia. The first battle was France vs. Russia. Germany and Britain both signed up on the French side. Austria and Italy, realizing that if they just went along with the ASB plan they'd come in 5th and 6th, joined up with Russia. That left it up to Belgium. I was on team Belgium. I voted to give our vote to the Russian side, because that way at least we weren't doomed to come in 4th. And no one else on my team went along. They meekly gave their points to the French side. (As I recall, Josh Lorton was particularly adamant about this. I guess he thought it would make the ASB guys like him.) After that, there was no contest. Britain vs. Austria? 6-1, Britain. Germany vs. Belgium? 6-1, Germany. (And we could have beaten them if we'd just formed a bloc with the other three losers!) The teacher noticed that Germany and France were always on the same side and declared Germany vs. France. Outcome: 6-1, Germany.</p>\n<p>The ASB guys were able to just impose their will on a class of 40 students. No carrots, no sticks, just \"here's what will happen\" and everyone else nodding. I have no idea how that works. I do recall that because they were in student government, for fourth period they had to take a class called Leadership. From what I could tell they just spent the class playing volleyball out in the quad. But I guess they were learning something!</p>\n</blockquote>\n<p>What happened? Why did Italy and Russia fall into line and abandon Austria in the second battle?</p>\n<p>This utterly failed to demonstrate the \"shifting alliances\" that Adam thought the teacher wanted. Does this happen every year?</p>\n<p>Yes, the students were coerced into \"playing\" this game, but <a href=\"http://adamcadre.ac/calendar/12986.html\">elsewhere</a> he describes the same thing happen in games that people choose to play. Moreover, he tells the first story to illustrate his perception of politics.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T7nx89Rb3WWugghZi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "4052", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-20T12:47:42.727Z", "modifiedAt": null, "url": null, "title": "What I've learned from Less Wrong", "slug": "what-i-ve-learned-from-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:38.275Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qGEqpy7J78bZh3awf/what-i-ve-learned-from-less-wrong", "pageUrlRelative": "/posts/qGEqpy7J78bZh3awf/what-i-ve-learned-from-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/qGEqpy7J78bZh3awf/what-i-ve-learned-from-less-wrong", "postedAtFormatted": "Saturday, November 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20I've%20learned%20from%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20I've%20learned%20from%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqGEqpy7J78bZh3awf%2Fwhat-i-ve-learned-from-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20I've%20learned%20from%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqGEqpy7J78bZh3awf%2Fwhat-i-ve-learned-from-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqGEqpy7J78bZh3awf%2Fwhat-i-ve-learned-from-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 957, "htmlBody": "<p>Related to: <a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\">Goals for which Less Wrong does (and doesn&rsquo;t) help</a></p>\n<p>I've been compiling a list of the top things I&rsquo;ve learned from Less Wrong in the past few months. If you&rsquo;re new here or haven&rsquo;t been here since the beginning of this blog, perhaps my personal experience from reading <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">the back-log of articles known as the sequences</a> can introduce you to some of the more useful insights you might get from reading and using Less Wrong.<br /><br /><strong>1. <a href=\"http://yudkowsky.net/rational/the-simple-truth\">Things can be correct</a></strong> - Seriously, I forgot. For the past ten years or so, I politely agreed with the <a href=\"/lw/k8/how_to_seem_and_be_deep/\">&ldquo;deeply wise&rdquo;</a> convention that truth could never really be determined or that it might not really exist or that if it existed anywhere at all, it was <a href=\"http://intelligence.org/ourresearch/publications/GISAI/GISAI.html#mind_consensus\">only in the consensus of human opinion</a>. I think I went this route because being sloppy here helped me &ldquo;fit in&rdquo; better with society. It&rsquo;s much easier to be egalitarian and respect everyone when you can always say &ldquo;Well, I suppose that might be right -- you never know!&rdquo;<br /><br /><strong>2. <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">Beliefs are for controlling anticipation</a></strong> (Not for being interesting) - I think in the past, I looked to believe <a href=\"/lw/v7/expected_creative_surprises/\">surprising</a>, <a href=\"/lw/ir/science_as_attire/\">interesting</a> things whenever I could get away with <a href=\"/lw/i7/belief_as_attire/\">the results not mattering</a> too much. Also, in a desire to be exceptional, I <a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">na&iuml;vely reasoned</a> that believing similar things to other smart people would probably get me the same boring life outcomes that many of them seemed to be getting... so I mostly tried to have <a href=\"/lw/vp/worse_than_random/\">extra random</a> beliefs in order to give myself a better shot at being the most amazingly successful and awesome person I could be.<a id=\"more\"></a></p>\n<p><strong>3. <a href=\"/lw/i4/belief_in_belief/\">Most peoples' beliefs aren&rsquo;t worth considering</a></strong> - Since I&rsquo;m no longer interested in collecting interesting &ldquo;beliefs&rdquo; to show off how fascinating I am or give myself better odds of out-doing others, it no longer makes sense to be a meme collecting, universal egalitarian the same way I was before. This includes dropping the habit of seriously considering all others&rsquo; improper beliefs that don&rsquo;t tell me what to anticipate and are only there for sounding interesting or smart.<br /><br /><strong>4. <a href=\"/lw/jn/how_much_evidence_does_it_take/\">Most of science is actually done by induction</a></strong> - Real scientists don&rsquo;t get their hypotheses by sitting in bathtubs and screaming &ldquo;Eureka!&rdquo;. To come up with something worth testing, <a href=\"/lw/jo/einsteins_arrogance/\">a scientist needs to do lots of sound induction first</a> or borrow an idea from someone who already used induction. This is because induction is the only way to reliably find candidate hypotheses which deserve attention. Examples of bad ways to find hypotheses include finding something <em>interesting</em> or <em>surprising</em> to believe in and then pinning all your hopes on that thing turning out to be true.<br /><br /><strong>5. <a href=\"/lw/r0/thou_art_physics/\">I have free will</a></strong> - Not only is the <a href=\"http://wiki.lesswrong.com/wiki/Free_will_%28solution%29\">free will problem solved</a>, but it turns out it was easy. I have the kind of free will worth caring about and that&rsquo;s actually comforting since <a href=\"/lw/2yp/making_your_explicit_reasoning_trustworthy/\">I had been unconsciously ignoring this</a> out of fear that the evidence appeared to be going against what I wanted to believe. Looking back, I think this was actually kind of depressing me and probably contributing to my attitude that having interesting rather than correct beliefs was fine since it looked like it might not matter what I did or believed anyway. Also, philosophers failing to uniformly mark this as &ldquo;settled&rdquo; and move on is not because this is a questionable result... they&rsquo;re just in a world where most philosophers are still having trouble figuring out if god exists or not. So it&rsquo;s not really easy to make progress on anything when there is more noise than signal in the &ldquo;philosophical community&rdquo;. Come to think of it, the AI community and most other scientific communities have this same problem... which is why I no longer read breaking science news anymore -- it's almost all noise.<br /><br /><strong>6. <a href=\"/lw/oj/probability_is_in_the_mind/\">Probability / Uncertainty isn&rsquo;t in objects or events</a></strong> - It&rsquo;s only in minds. Sounds simple after you understand it, but I feel like this one insight often allows me to have longer trains of thought now without going completely wrong.<br /><br /><strong>7. <a href=\"/lw/wq/you_only_live_twice/\">Cryonics is reasonable</a></strong> - Due to reading and understanding the <a href=\"/lw/r5/the_quantum_physics_sequence/\">quantum physics sequence</a>, I ended up contacting <a href=\"http://www.rudihoffman.com/\">Rudi Hoffman</a> for a life insurance quote to fund cryonics. It&rsquo;s only a few hundred dollars a year for me. It&rsquo;s well within my budget for caring about myself and others... such as my future selves in forward branching multi-verses.</p>\n<p><br />There are countless other important things that I've learned but haven't documented yet. I find it pretty amazing what this site has taught me in only 8 months of sporadic reading. Although, to be fair, it didn't happen by accident or by reading the recent comments and promoted posts but almost exclusively by <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">reading all the core sequences</a> and <em>then participating more</em> after that.</p>\n<p>And as a personal aside (<em>possibly</em> some others can relate): I still love-hate Less Wrong and find reading and participating on this blog to be one of the most frustrating and challenging things I do. And <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">many of the people in this community rub me the wrong way</a>. But in the final analysis, the astounding benefits gained make the annoying bits more than worth it.<br /><br />So if you've been thinking about <a href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">reading the sequences</a> but haven't been making the time do it, I second <a href=\"/lw/34a/goals_for_which_less_wrong_does_and_doesnt_help/\">Anna&rsquo;s suggestion</a> that you get around to that. And the <a href=\"http://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html\">rationality exercise she linked to</a> was easily the single most effective hour of personal growth I had this year so I highly recommend that as well if you're game.</p>\n<p>&nbsp;</p>\n<p>So, <strong>what have you learned from Less Wrong?</strong> I'm interested in hearing others' experiences too.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "3RnEKrsNgNEDxuNnw": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qGEqpy7J78bZh3awf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 93, "baseScore": 104, "extendedScore": null, "score": 0.000184, "legacy": true, "legacyId": "4054", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 104, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 235, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7dRGYDqA2z6Zt7Q4h", "aSQy7yHj6nPD44RNo", "a7n8GdKiAZRX86T5A", "rEDpaTTEzhPLz4fHh", "4Bwr6s9dofvqPWakn", "nYkMLFpx77Rz3uo9c", "qNZM3EGoE5ZeMdCRt", "GYuKqAL95eaWTDje5", "CqyJzDZWvGhhFJ7dY", "nj8JKFoLSMEmD3RGp", "MwQRucYo6BZZwjKE7", "NEeW7eSXThPz7o4Ne", "m5AH78nscsGjMbBwv", "f6ZLxEWaankRZ2Crv", "yKXKcyoBzWtECzXrE", "hc9Eg6erp6hk9bWhn", "7FzD7pNm9X68Gp5ZC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-20T20:06:32.222Z", "modifiedAt": null, "url": null, "title": "Pseudolikelihood as a source of cognitive bias", "slug": "pseudolikelihood-as-a-source-of-cognitive-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:36.575Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Peter_de_Blanc", "createdAt": "2009-02-27T14:15:28.882Z", "isAdmin": false, "displayName": "Peter_de_Blanc"}, "userId": "vRvaAqR5tcjGEWaoC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sEWpaLZkJRohWPF2b/pseudolikelihood-as-a-source-of-cognitive-bias", "pageUrlRelative": "/posts/sEWpaLZkJRohWPF2b/pseudolikelihood-as-a-source-of-cognitive-bias", "linkUrl": "https://www.lesswrong.com/posts/sEWpaLZkJRohWPF2b/pseudolikelihood-as-a-source-of-cognitive-bias", "postedAtFormatted": "Saturday, November 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pseudolikelihood%20as%20a%20source%20of%20cognitive%20bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APseudolikelihood%20as%20a%20source%20of%20cognitive%20bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEWpaLZkJRohWPF2b%2Fpseudolikelihood-as-a-source-of-cognitive-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pseudolikelihood%20as%20a%20source%20of%20cognitive%20bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEWpaLZkJRohWPF2b%2Fpseudolikelihood-as-a-source-of-cognitive-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsEWpaLZkJRohWPF2b%2Fpseudolikelihood-as-a-source-of-cognitive-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 350, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Pseudolikelihood\">Pseudolikelihood</a> is method for approximating joint probability distributions. I'm bringing this up because I think something like this might be used in human cognition. If so, it would tend to produce overconfident estimates.</p>\n<p>Say we have some joint distribution over X, Y, and Z, and we want to know about the probability of some particular vector (x, y, z). The pseudolikelihood estimate involves asking yourself how likely each piece of information is, given all of the other pieces of information. Then you multiply these together. So the pseudolikelihood of (x, y, z) is P(x|yz) P(y|xz) P(z|xy).</p>\n<p>Not only is this wrong, but it gets more wrong as your system is bigger. By that I mean that a ratio of two pseudolikelihoods will tend towards 0 or infinity for big problems, even if the likelihoods are close to the same.</p>\n<p>So how can we avoid this? A correct way to calculate a joint probability P(x,y,z) looks like P(x) P(y|x) P(z|xy). At each step we only condition on information \"prior\" to the thing we are asking about. My guess about how to do do this involves making your beliefs look more like a <a href=\"http://en.wikipedia.org/wiki/Directed_acyclic_graph\">directed acyclic graph</a>. Given two adjacent beliefs, you need to be clear on which is the \"cause\" and which is the \"effect.\" The cause talks to the effect in terms of prior probabilities and the effect talks to the cause in terms of likelihoods.</p>\n<p>Failure to do this could take the form of an undirected relationship (two beliefs are \"related\" without either belief being the cause or the effect), or loops in a directed graph. I don't actually think we want to get rid of undirected relationships entirely -- people <a href=\"http://en.wikipedia.org/wiki/Markov_network\">do use them</a> in machine learning -- but I can't see any good reason for keeping the latter.</p>\n<p>An example of a causal loop would be if you thought of math as an abstraction from everyday reality, and then turned around and calculated prior probabilities of fundamental physical theories in terms of mathematical elegance. One way out is to declare yourself a mathematical Platonist. I'm not sure what the other way would look like.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sEWpaLZkJRohWPF2b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 6.477006667968016e-07, "legacy": true, "legacyId": "4056", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-20T20:37:14.926Z", "modifiedAt": null, "url": null, "title": "The true prisoner's dilemma with skewed payoff matrix", "slug": "the-true-prisoner-s-dilemma-with-skewed-payoff-matrix", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:45.273Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonii", "createdAt": "2009-07-15T05:24:30.383Z", "isAdmin": false, "displayName": "Jonii"}, "userId": "xa8EysPtEYKcEDeNg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YTXW3L5cme8sRa5c6/the-true-prisoner-s-dilemma-with-skewed-payoff-matrix", "pageUrlRelative": "/posts/YTXW3L5cme8sRa5c6/the-true-prisoner-s-dilemma-with-skewed-payoff-matrix", "linkUrl": "https://www.lesswrong.com/posts/YTXW3L5cme8sRa5c6/the-true-prisoner-s-dilemma-with-skewed-payoff-matrix", "postedAtFormatted": "Saturday, November 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20true%20prisoner's%20dilemma%20with%20skewed%20payoff%20matrix&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20true%20prisoner's%20dilemma%20with%20skewed%20payoff%20matrix%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTXW3L5cme8sRa5c6%2Fthe-true-prisoner-s-dilemma-with-skewed-payoff-matrix%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20true%20prisoner's%20dilemma%20with%20skewed%20payoff%20matrix%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTXW3L5cme8sRa5c6%2Fthe-true-prisoner-s-dilemma-with-skewed-payoff-matrix", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTXW3L5cme8sRa5c6%2Fthe-true-prisoner-s-dilemma-with-skewed-payoff-matrix", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 812, "htmlBody": "<p><strong>Related to </strong><a href=\"/lw/tn/the_true_prisoners_dilemma/\">The True Prisoner's Dilemma</a>, <a href=\"/lw/2x8/lets_split_the_cake_lengthwise_upwise_and/\">Let's split the cake, lengthwise, upwise and slantwise</a>, <a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">If you don't know the name of the game, just tell me what I mean to you</a></p>\n<p><em>tl;dr: Playing the true PD, it might be that you should co-operate when expecting the other one to defect, or vice versa, in some situations, against agents that are capable of superrationality. This is because relative weight of outcomes for both parties might vary. This could lead this sort of agents to outperform even superrational ones.<br /></em></p>\n<p>So, it happens that our benevolent Omega has actually an evil twin, that is as trustworthy as his sibling, but abducts people into a lot worse hypothetical scenarios. Here we have one:</p>\n<p>You wake up in a strange dimension, and this Evil-Omega is smiling at you, and explains that you're about to play a game with unknown paperclip maximizer from another dimension that you haven't interacted with before and won't interact with ever after. The alien is like GLUT when it comes to consciousness, it runs a simple approximation of rational decision algorithm but nothing that you could think of as \"personality\" or \"soul\". Also, since it doesn't have a soul, you have absolutely no reason to feel bad for it's losses. This is true PD.</p>\n<p>You are also told some specifics about the algorithm that the alien uses to reach its decision, and likewise told that alien is told about as much about you. At this point I don't want to nail the algorithm the opposing alien uses down to one specific. We're looking for a method that wins when summing up all these possibilities. Next, especially, we're looking at the group of AI's that are capable of superrationality, since against other's the game is trivial.</p>\n<p>The payoff matrix is like this:</p>\n<p>DD=(lose 3 billion lives and be tortured, lose 4 paperclips), CC=(2 billion lives and be made miserable, lose 2 paperclips), CD=(lose 5 billion lives and be tortured a lot, nothing), DC=(nothing, lose 8 paperclips)</p>\n<p>So, what do you do? Opponent is capable of superrationality. In the post \"The True Prisoner's Dilemma\", it was(kinda, vaguely, implicitly) assumed for simplicity's sake that this information is enough to decide whether to defect or not. Answer, based on this information, could be to co-operate. However, I argue that information given is not enough.</p>\n<p>Back to the hypothetical: In-hypothetical you is still wondering about his/her decision, but we zoom out and observe that, unbeknownst to you, Omega has abducted your fellow LW reader and another paperclip maximizer from that same dimension, and is making them play PD. But this time their payoff matrix is like this:</p>\n<p>DD=(lose $0.04, make 2 random, small changes to alien's utility function and 200 paperclips lost), CC=(lose $0.02, 1 change, 100 paperclips), CD=(lose $0.08, nothing), DC=(nothing, 4 changes, 400 paperclips)</p>\n<p>Now, if it's not \"rational\" to take the relative loss into account, we're bound to find ourselves in a situation where billions of humans die. You could be regretting your rationality, even. It should become obvious now that you'd wish you could somehow negotiate both of these PD's so that you would defect and your opponent co-operate. You'd be totally willing to take a $0.08 hit for that, maybe paying it in its entirety for your friend. And so it happens, paperclip maximizers would also have an incentive to do this.</p>\n<p>But, of course, players don't know about this entire situation, so they might not be able to operate in optimal way in this specific scenario. However, if they take into account how much the other cares about those results, using some unknown method, they just might be able to systematically perform better(if we made more of this sorts of problems, or if we selected payoffs at random for the one-shot game), than \"naive\" PD-players playing against each other. Naivity here would imply that they simply and blindly co-operate against equally rational opponents. How to achieve that is the open question.</p>\n<p>-</p>\n<p>Stuart Armstrong, for example, has an actual idea of how to co-operate when the payoffs are skewed, while I'm just pointing out that there's a problem to be solved, so this is not really news or anything. Anyways, I still think that this topic has not been explored as much as it should be.</p>\n<p><strong>Edit.</strong> Added this bit: <em>You are also told some specifics about the algorithm that the alien uses to reach its decision, and likewise told that alien is told about as much about you. At this point I don't want to nail the algorithm the opposing alien uses down to one specific. We're looking for a method that wins when summing up all these possibilities. Next, especially, we'</em><em>re looking at the group of AI's that are capable of superrationality, since against other sort of agents the game is trivial</em><em>.</em></p>\n<p><strong>Edit. </strong>Corrected some huge errors here and there, like, mixing hypothetical you and hypothetical LW-friend.</p>\n<p><strong>Edit. </strong>Transfer Discussion -&gt; Real LW complete!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YTXW3L5cme8sRa5c6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 3, "extendedScore": null, "score": 6.477081483833197e-07, "legacy": true, "legacyId": "4017", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HFyWNBnDNEDsDNLrZ", "hCwFxBai3oNnxrM9v", "W2ufY8ihDDWWqJA7h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-21T16:41:10.603Z", "modifiedAt": null, "url": null, "title": "Agents of No Moral Value: Constrained Cognition?", "slug": "agents-of-no-moral-value-constrained-cognition", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:35.332Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9k3YcLyPQ4v5386o9/agents-of-no-moral-value-constrained-cognition", "pageUrlRelative": "/posts/9k3YcLyPQ4v5386o9/agents-of-no-moral-value-constrained-cognition", "linkUrl": "https://www.lesswrong.com/posts/9k3YcLyPQ4v5386o9/agents-of-no-moral-value-constrained-cognition", "postedAtFormatted": "Sunday, November 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Agents%20of%20No%20Moral%20Value%3A%20Constrained%20Cognition%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAgents%20of%20No%20Moral%20Value%3A%20Constrained%20Cognition%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9k3YcLyPQ4v5386o9%2Fagents-of-no-moral-value-constrained-cognition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Agents%20of%20No%20Moral%20Value%3A%20Constrained%20Cognition%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9k3YcLyPQ4v5386o9%2Fagents-of-no-moral-value-constrained-cognition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9k3YcLyPQ4v5386o9%2Fagents-of-no-moral-value-constrained-cognition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 254, "htmlBody": "<p>Thought experiments involving multiple agents usually <a href=\"/lw/tn/the_true_prisoners_dilemma/\">postulate</a> that the agents have no moral value, so that the explicitly specified payoff from the choice of actions can be considered in isolation, as both the sole <em>reason</em> and <em>evaluation criterion</em> for agents' decisions. But is that really possible to require from an opposing agent to have no moral value, without constraining what it's allowed to think about?</p>\n<p>If agent B is not a person, how do we know it can't decide to become a person for the sole reason of gaming the problem, manipulating agent A (since B doesn't care about personhood, so it costs B nothing, but A does)? If it's stipulated as part of the problem statement, it seems that B's cognition is restricted, and the most rational course of action is prohibited from being considered for no within-thought-experiment reason accessible to B.</p>\n<p>It's not enough to require that the other agent is inhuman in the sense of not being a person and not holding human values, as our agent must also not care about the other agent. And once both agents don't care about each other's cognition, the requirement for them not being persons or valuable becomes extraneous.</p>\n<p>Thus,&nbsp;<em>instead of</em> requiring that the other agent is not a person, the correct way of setting up the problem is to require that our agent is indifferent to whether the other agent is a person (and conversely).</p>\n<p>(It's not a very substantive observation I would've posted with less polish in an open thread if not for the discussion section.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9k3YcLyPQ4v5386o9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 6.480015540614769e-07, "legacy": true, "legacyId": "4058", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HFyWNBnDNEDsDNLrZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-21T16:57:35.003Z", "modifiedAt": null, "url": null, "title": "Competition to write the best stand-alone article on efficient charity", "slug": "competition-to-write-the-best-stand-alone-article-on", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:35.526Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kxZD8ycCqK9bccGkf/competition-to-write-the-best-stand-alone-article-on", "pageUrlRelative": "/posts/kxZD8ycCqK9bccGkf/competition-to-write-the-best-stand-alone-article-on", "linkUrl": "https://www.lesswrong.com/posts/kxZD8ycCqK9bccGkf/competition-to-write-the-best-stand-alone-article-on", "postedAtFormatted": "Sunday, November 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Competition%20to%20write%20the%20best%20stand-alone%20article%20on%20efficient%20charity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACompetition%20to%20write%20the%20best%20stand-alone%20article%20on%20efficient%20charity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkxZD8ycCqK9bccGkf%2Fcompetition-to-write-the-best-stand-alone-article-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Competition%20to%20write%20the%20best%20stand-alone%20article%20on%20efficient%20charity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkxZD8ycCqK9bccGkf%2Fcompetition-to-write-the-best-stand-alone-article-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkxZD8ycCqK9bccGkf%2Fcompetition-to-write-the-best-stand-alone-article-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 502, "htmlBody": "<p>I have a friend who is currently in a lucrative management consultancy career, but is considering getting a job in eco-tourism because he \"wants to&nbsp;make the world a better place\" and we got into a debate about Efficient Charity, Roles&nbsp;vs. Goals, and <a href=\"/lw/6z/purchase_fuzzies_and_utilons_separately/\">Optimizing versus Acquiring Warm Fuzzies</a>.&nbsp;</p>\n<p>I thought that there would be a good article here that I could send him to, but there&nbsp;isn't. So I've decided to ask people to write such an article.&nbsp;What I am looking for is an article that is less than 1800 words long, and explains the&nbsp;following ideas:&nbsp;</p>\n<ol>\n<li>Charity should be about actually trying to do as much expected good as possible for a&nbsp;given amount of resource (time, $), in a quantified sense. I.e. \"5000 lives saved in&nbsp;expectation\", not \"we made a big difference\".&nbsp;</li>\n<li>The norms and framing of our society regarding charity currently get it wrong, i.e.&nbsp;people send lots of $ to charities that do a lot less good than other charities. The&nbsp;\"inefficiency\" here is very large, i.e. GWWC estimates by a factor of 10,000 at least. Therefore most money donated to charity is almostly entirely&nbsp;wasted.</li>\n<li>It is usually better to work a highly-paid job and donate because if you work for a&nbsp;charity you replace the person who would have been hired had you not applied</li>\n<li>Our instincts will tend to tempt us to optimize for signalling, this is to be&nbsp;resisted unless (or to the extent that)&nbsp;it is what you actually want to do</li>\n<li>Our motivational centre will tend to want to optimize for \"Warm Fuzzies\". These&nbsp;should be purchased separately from utilons.&nbsp;</li>\n</ol>\n<p>but without using any unexplained LW Jargon. (Utilons, Warm Fuzzies, optimizing).&nbsp;Linking to posts explaining jargon is <strong>NOT</strong> OK.&nbsp;I will judge the winner based upon these criteria and the score that the article gets&nbsp;on LW. I may present a small prize to the winner, if (s)he desires it!&nbsp;</p>\n<p>Happy Writing</p>\n<p>Roko</p>\n<p><strong>EDIT</strong>: As well as saying that he <em>will </em>pay $100 to the winner, Jsalvatier makes two additional points that I feel should be included in the specification of the article:</p>\n<p style=\"padding-left: 30px;\">6. &nbsp;Your intuition about what counts as a cause worth giving money to is extremely bad. This is completely natural:&nbsp;everyone's intuition about this is bad. Why? Because your brain was not optimized by evolution to be good at thinking clearly about large problems involving millions of people and how to allocate resources.&nbsp;</p>\n<p style=\"padding-left: 30px;\">7. Not only is your intuition about this naturally very bad (as well as cultural memes surrounding how to donate to charity being utterly awful),<em> you don't realize that your intuition is bad.</em> This is a deceptively hard problem.&nbsp;</p>\n<p>And I would also like to add:</p>\n<p style=\"padding-left: 30px;\">8. Explicitly make the point that our current norm of ranking charities based upon how much (or little) they spend on overheads is utterly insane. Yes, the entire world of charities is stupid with respect to the problem of how to prioritize their own efforts.&nbsp;</p>\n<p style=\"padding-left: 30px;\">9. Mention the point that other groups are slowly edging their way towards the same conclusion, e.g. Giving What We Can (GWWC), Copenhagen Consensus, GiveWell.&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kxZD8ycCqK9bccGkf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 16, "extendedScore": null, "score": 6.480055540972541e-07, "legacy": true, "legacyId": "4059", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3p3CYauiX8oLjmwRF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-21T17:46:05.731Z", "modifiedAt": null, "url": null, "title": "Can cryoprotectant toxicity be crowd-sourced?", "slug": "can-cryoprotectant-toxicity-be-crowd-sourced", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:35.033Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZWPX5gWpyD6EtTgSi/can-cryoprotectant-toxicity-be-crowd-sourced", "pageUrlRelative": "/posts/ZWPX5gWpyD6EtTgSi/can-cryoprotectant-toxicity-be-crowd-sourced", "linkUrl": "https://www.lesswrong.com/posts/ZWPX5gWpyD6EtTgSi/can-cryoprotectant-toxicity-be-crowd-sourced", "postedAtFormatted": "Sunday, November 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can%20cryoprotectant%20toxicity%20be%20crowd-sourced%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan%20cryoprotectant%20toxicity%20be%20crowd-sourced%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZWPX5gWpyD6EtTgSi%2Fcan-cryoprotectant-toxicity-be-crowd-sourced%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can%20cryoprotectant%20toxicity%20be%20crowd-sourced%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZWPX5gWpyD6EtTgSi%2Fcan-cryoprotectant-toxicity-be-crowd-sourced", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZWPX5gWpyD6EtTgSi%2Fcan-cryoprotectant-toxicity-be-crowd-sourced", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 503, "htmlBody": "<p>From the article <a href=\"http://www.depressedmetabolism.com/2009/01/15/the-red-blood-cell-as-a-model-for-cryoprotectant-toxicity/\">The red blood cell as a model for cryoprotectant toxicity</a> by Aschwin de Wolf</p>\n<blockquote>\n<p>One simple model that allows for &ldquo;high throughput&rdquo; investigations of cryoprotectant toxicity are red blood cells (erythrocytes). Although the toxic effects of various cryoprotective agents may differ between red blood cells, other cells, and organized tissues, positive results in a red blood cell model can be considered the first experimental hurdle that needs to be cleared before the agent is considered for testing in other models.&nbsp; Because red blood cells are widely available for research, this model eliminates the need for animal experiments for initial studies. It also allows researchers to investigate <em>human</em> cells. Other advantages include the reduced complexity of the model&nbsp; (packed red blood cells can be obtained as an off-the-shelf product) and lower costs.</p>\n</blockquote>\n<p>It sounds to me like this is a very cheap assay for viability. You don't need much equipment. High toxicity compounds can be screened on visual appearance. More detailed analysis can be done by a light microscope or a spectrophotometer.</p>\n<p>The biggest issue facing cryonics (and the holy grail of suspended animation with true&nbsp; biostasis) is the existence of cryoprotectant toxicity. Less toxic solutions can be perfused for a longer period of time, and thus penetrate the entire organism without triggering additional loss of viability. Vitrification already eliminates <em>all</em> ice formation -- we know enough to know that without toxicity, it <em>should work</em> for trivially reversible forms of long-term suspended animation.</p>\n<p>Thus if we want to ask what can be done cheaply <a href=\"http://en.wikipedia.org/wiki/Crowdsourcing\">by a lot of people</a> to help cryonics move forward, one possibility is that they could perform empirical tests on the compounds most likely to prove effective for cryoprotection.</p>\n<p>We can speculate about the brain being reparable at all kinds of levels of damage -- but that is speculation. Sure we <em>do</em> have to make a decision to sign up or not based on that speculation. But the more hard evidence we can obtain, the more of a chance that we aren't being distracted from the reality of the situation by wishful thinking -- and the more likely we are to persuade our fellow self-identifying rational skeptics to take our side. Furthermore (and I know this sounds obvious, but it still needs to be said) in taking a more empirical approach to actually resolving the issues as quickly as possible, we are <em>more likely to survive</em> than otherwise.</p>\n<p>There are still a lot of questions that are raised in my mind by this crowdsourcing idea. What kinds of mechanisms would be best for collaboration and publication of results? Are there many <em>other</em> dirt-cheap empirical testing methods that small unfunded groups of nonspecialists could employ for useful research? How many people and groups could/should get involved in such a project? Aschwin mentions \"theoretical work in organic chemistry\" as the first step -- how much of that has already been done, or needs to be done? What kind of a learning curve is there on learning enough organic chemistry to propose a useful test?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZWPX5gWpyD6EtTgSi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 1, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "4060", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-22T05:31:59.316Z", "modifiedAt": null, "url": null, "title": "Risk is not empirically correlated with return", "slug": "risk-is-not-empirically-correlated-with-return", "viewCount": null, "lastCommentedAt": "2020-08-24T07:42:46.164Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o56JnkJ8YS6aTw46S/risk-is-not-empirically-correlated-with-return", "pageUrlRelative": "/posts/o56JnkJ8YS6aTw46S/risk-is-not-empirically-correlated-with-return", "linkUrl": "https://www.lesswrong.com/posts/o56JnkJ8YS6aTw46S/risk-is-not-empirically-correlated-with-return", "postedAtFormatted": "Monday, November 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Risk%20is%20not%20empirically%20correlated%20with%20return&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARisk%20is%20not%20empirically%20correlated%20with%20return%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo56JnkJ8YS6aTw46S%2Frisk-is-not-empirically-correlated-with-return%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Risk%20is%20not%20empirically%20correlated%20with%20return%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo56JnkJ8YS6aTw46S%2Frisk-is-not-empirically-correlated-with-return", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo56JnkJ8YS6aTw46S%2Frisk-is-not-empirically-correlated-with-return", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 525, "htmlBody": "<p>The most widely appreciated finance theory is the <a href=\"http://en.wikipedia.org/wiki/Capital_asset_pricing_model\">Capital Asset Pricing Model</a>. It basically says that diminishing marginal utility of absolute wealth implies that riskier financial assets should have higher expected returns than less risky assets and that only risk correlated with the market (beta risk) is a whole is important because other risk can be diversified out.</p>\n<p>Eric Falkenstein argues that the evidence does not support this theory; that the riskiness of assets (by any reasonable definition) is not positively correlated with return (some caveats apply). He has a <a href=\"http://www.efalken.com/RiskReturn.html\">paper</a> (long but many parts are skimmable; not peer reviewed; also on <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1420356\">SSRN</a>) as well as a <a href=\"http://www.amazon.com/Finding-Alpha-Search-Return-Finance/dp/0470445904\">book</a> on the topic. I recommend reading parts of the paper.</p>\n<p>The gist of his competing theory is that people care mostly about relative gains rather than absolute gains. This implies that riskier financial assets will not have higher expected returns than less risky assets. People will not require a higher return to hold assets with higher undiversifiable variance because everyone is exposed to the same variance and people only care about their relative wealth.</p>\n<p>Falkenstein has a substantial quantity of evidence to back up his claim. I am not sure if his competing theory is correct, but I find the evidence against the standard theory quite convincing.</p>\n<p>If risk is not correlated with returns, then anyone who is mostly concerned with absolute wealth can profit from this by choosing a low beta risk portfolio.</p>\n<p>This topic seems more appropriate for the discussion section, but I am not completely sure, so if people think it belongs in the main area, let me know.</p>\n<p><strong>Added some (hopefully) clarifying material:</strong></p>\n<p>All this assumes that you eliminate idiosyncratic risk through diversification. Technically impossible, but you can get it reasonably low. The R's are all *instantaneous* returns; though since these are linear models they apply to geometrically accumulated returns as well. The idea that E(R_asset) are independent of past returns is a background assumption for both models and most of finance.</p>\n<p>Beta_portfolio = Cov(R_portfolio, R_market)/variance(R_market)</p>\n<p>In CAPM your expected and variance are:<br /><br />E(R_portfolio) = R_rfree + Beta_portfolio * (E(R_market) - R_rfree)<br />Var(R_portfolio) = Beta_portfolio * Var(R_market)<br /><br />in Falkenstein's model your expected return are:<br /><br />E(R_portfolio) = R_market&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # you could also say = R_rfree; the point is that its a constant<br />Var(R_portfolio) = Beta_portfolio * Var(R_market)<br /><br />The major caveat being that it doesn't apply very close to Beta_portfolio = 0; Falkenstein attributes this to liquidity benefits. And it doesn't apply to very high Beta_portfolio; he attributes this to \"buying hope\". See the paper for more.<br /><br />Falkenstein argues that his model fits the facts more closely than CAPM. Assuming Falkenstein's model describes reality, if your utility declines with rising Var(R_portfolio) (the standard assumption), then you'll want to hold a portfolio with a beta of zero; or taking into account the caveats, a low Beta_portfolio. If your utility is declining with Var(R_portfolio - R_market), then you'll want to hold the market portfolio. Both of these results are unambiguous since there's no trade off between either measure of risk and return.</p>\n<p><strong>Some additional evidence from another source, and discussion:</strong>&nbsp;http://falkenblog.blogspot.com/2010/12/frazzini-and-pedersen-simulate-beta.html</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xYLtnJ6keSHGfrLpe": 1, "jgcAJnksReZRuvgzp": 1, "3uE2pXvbcnS9nnZRE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o56JnkJ8YS6aTw46S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "4062", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-22T09:01:06.419Z", "modifiedAt": null, "url": null, "title": "The Self-Reinforcing Binary", "slug": "the-self-reinforcing-binary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:39.125Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lucidfox", "createdAt": "2010-11-22T06:58:06.993Z", "isAdmin": false, "displayName": "lucidfox"}, "userId": "hNnKSqvajCMRj9eKK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ihzDAw6shkSKqM4fj/the-self-reinforcing-binary", "pageUrlRelative": "/posts/ihzDAw6shkSKqM4fj/the-self-reinforcing-binary", "linkUrl": "https://www.lesswrong.com/posts/ihzDAw6shkSKqM4fj/the-self-reinforcing-binary", "postedAtFormatted": "Monday, November 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Self-Reinforcing%20Binary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Self-Reinforcing%20Binary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FihzDAw6shkSKqM4fj%2Fthe-self-reinforcing-binary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Self-Reinforcing%20Binary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FihzDAw6shkSKqM4fj%2Fthe-self-reinforcing-binary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FihzDAw6shkSKqM4fj%2Fthe-self-reinforcing-binary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1443, "htmlBody": "<p><em>I originally wrote this post for </em><a href=\"http://lucidfox.dreamwidth.org/644.html\"><em>my own blog</em></a><em>, but after discovering Less Wrong, I've thought that it might make sense to submit it here.</em></p>\n<p>The late 20th - early 21st century have been rich with various concepts beginning with \"post-\". Postindustrial society, postmodernism, post-theism, postgenderism, posthumanism... The opinions on these, as well as the larger trends behind them all, are of course divided, but if anything, this only illustrates the point I'm trying to make.</p>\n<p>I think that what happened is that as the barriers of communication fell down, as we learned more about different cultures and lifestyles, so did we realize that many social concepts formerly thought of as absolute and rigid actually weren't. It will take another generation, or perhaps more than one, just to process this very idea to its fullest. We have come to realize that concepts and ideas, real or fictional, live in the historical and cultural context of their creators, and can only be fully understood in a relative rather than absolute way. No matter how many times literary critics say \"death of the author\", you can't abstract away from the fact that George Orwell had the political trends of early-to-mid-20th century in mind when he wrote <em>1984</em>, or that J.R.R. Tolkien's Catholic beliefs influenced the cosmology and tone of <em>The Silmarillion</em> and <em>The Lord of the Rings</em>.</p>\n<p>Social ideas and norms are much the same way. Appeal to tradition, \"it has always been that way\", is just about the worst argument you can make when defending an existing social custom, right next to \"God decrees so\". Even if the God you believe in tells you that someone will go to Hell for the terrible, terrible moral crime of enjoying sex without the intent of procreation, it's not your business to try and \"save\" them. Just act yourself the way your beliefs dictate. Hence the \"post-\": not in the sense of rejection, but in the sense of outgrowing. A post-theistic society is not an atheistic society, but merely one that got over theism, a society where religion is a matter of personal choice rather than a shaping force in politics.</p>\n<p>And yes, I realize that my own writing is influenced by my atheist bias, conscious and unconscious. While I cannot fully abstract from them, I can be made aware of them; let the unconscious become conscious.</p>\n<p>So how does it all relate to the gender binary? Well, the way I see it, gender roles and religious dogmas have a lot in common &mdash; they are self-propagating memes. A good example to illustrate the problem is the origin of the Russian word for bear, \"medved'\". It literally meant \"honey eater\" in Old Slavic and was originally created as a euphemism, because the real name of the animal was taboo. However, over time, this fact was forgotten and \"medved'\" became the only known name, and thus itself considered something to be avoided by superstitious hunters.  Religious fundamentalists take the words of their prophets and saints dropped here and there throughout their lives, often out of context, and declare them absolute, immutable truth. Proponents of the gender binary take emergent prejudices that shaped themselves due to a combination of circumstances, sometimes mind-bogglingly arbitrary, and declare them gospel. In any case, we are faced with codification, with social expectations and taboos shaped by minutae.  It's like if a fictional character had their complexity stripped away and become defined by a single trait based on something they vaguely did in that one episode. <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/Flanderization\">Oh wait.</a></p>\n<p><a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/Flanderization\"></a>What originally prompted this post was a paragraph I saw while reading <a href=\"http://andrewrilstone.com\">Andrew Rilstone</a> commentary on some common themes and tropes in fiction, namely, the points made by Joseph Campbell's <em>The Hero with a Thousand Faces</em> (itself subjected to gospelization: while Campbell himself was only writing about common themes in a distinct kind of stories, some of his followers went so far as to claim that the structure he pointed out was inherent in every story ever written). After a series of posts making logical arguments, the latest of which contrasted stories where the hero returned home with a boon from the travels with stories where the hero reached their destination and stayed there, when I kept going \"Yes, yes, that's exactly it!\", I suddenly stumbled upon <a href=\"http://www.andrewrilstone.com/2010/09/10-road-back.html\">this non sequitur</a>.</p>\n<blockquote>When I did literary theory at college, it was a truism that stories in which someone set forth to achieve something &ndash; stories which rushed headlong to a dramatic conclusion &ndash; were Male (and therefore bad). Stories which reached no final conclusion, which described a state of being, which cycled back to the beginning and achieved multiple climaxes were Female (and therefore good). The cleverer students, the ones with berets, went so far as to claim that the whole idea of stories &ndash; in fact the whole idea of writing in sentences -- was dangerously \"phallocentric\". But one does take the point that boys' stories like Moby Dick have beginnings, middles and ends in a way that girls' stories like Middlemarch really don't. The soap opera, which is all middle, is the female narrative form par excellence. You would search in vein for a monomyth in Coronation Street.</blockquote>\n<p>For a minute, I just blinked at the text in silence, trying to make any sense out of it. Wikipedia defines a <a href=\"http://en.wikipedia.org/wiki/Truism\">truism</a> as \"a claim that is so obvious or self-evident as to be hardly worth mentioning, except as a reminder or as a rhetorical or literary device\". In other words, the author took this piece of essentialist drivel for granted so much that he assumed everyone else shared it.</p>\n<p>Which made me think: what, exactly, causes people to assign concepts to genders in such an utterly arbitrary fashion? The answer, I believe, lies in the pervasive, all-encompassing nature of the gender binary. The human society, we are taught from infancy, consists of men and women. We know - some of us, anyway - that it's merely an approximation in the same sense that Newtonian physics are an approximation of relativistic physics and the real world, one that is valid for most everyday uses but fails when we broaden the horizons of our knowledge. But the idea is tempting.  After all, ideas, as <a href=\"http://en.wikipedia.org/wiki/Inception_(film)\">Christopher Nolan helpfully points out</a>, are the most persistent kind of infection known to humanity.</p>\n<p>And as such, when we encounter a new kind of idea (in this case, a binary), it is tempting to explain it in the concept of another binary we know, even if the analogy makes no sense. The actual mapping is often hard to explain rationally. Ancient paganists knew about the day/night binary and their corresponding celestial bodies. As such, in many mythologies over the world, the gods or personifications of the Sun and the Moon are of different genders, but it varies which is which. On one hand, we have Helios and Selene, Apollo and Artemis; on the other, S&oacute;l and M&aacute;ni, who no doubt inflienced Tolkien's Arien and Tilion.</p>\n<p>Sometimes, it's not random. The earliest known examples of gender roles in prehistoric tribes, and such basic dichotomies as hard/soft, strong/weak, big/small, outward/inward, are probably influenced by real physical differences. From there, it kept fracturing, expanding since then. Perhaps many concepts declared \"masculine\" or \"feminine\" were not assigned randomly, but based on associations with existing concepts already sorted into the binary. The gender binary was not static, but, as geekfeminism.org pointed out, a <a href=\"http://geekfeminism.org/2010/01/09/quick-hit-the-gender-binary-fractal-in-geekdom/\">fractal with internalized sexism</a> (for example, while science itself is considered a \"masculine\" career, there are individual sciences perceived as predominantly masculine or feminine, etc.; even feminism itself could have contributed to such perceptions, if the \"hairy-legged man-hater\" stereotype is any indication). And not just a static fractal, but an ever-expanding, path-dependent chain of associations that solidified over time; what might first have been a helpful rhetorical device became unquestionable taboo.</p>\n<p>What can be done to break this pattern? Feminism contributes to the reverse process of conflation, of removing gender association stigma from logically unrelated concepts. But a true breakdown of the binary, I believe, will only happen when people en masse change their fundamental patterns of thought, and cast off or at least become aware of implicit assumptions underlying their arguments and actions. It is in the nature of the human mind to think in opposites, but the process of exposing the context can move the mental opposites from socially harmful areas and place more focus on, say, personal beliefs, ethics, and political ideologies - ideas that people <em>choose</em> to accept instead of being assigned to them by virtue of birth. And then, perhaps, we can outgrow the labeling of just about everything as masculine or feminine; in other words, walk into a post-binary world.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ihzDAw6shkSKqM4fj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -5, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "4063", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-11-22T09:52:44.215Z", "modifiedAt": null, "url": null, "title": "Does cognitive therapy encourage bias?", "slug": "does-cognitive-therapy-encourage-bias-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:55.137Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fortyeridania", "createdAt": "2010-07-21T15:35:12.558Z", "isAdmin": false, "displayName": "fortyeridania"}, "userId": "roBPqtzsvG6dC3YFT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xnoHu4qZavy9WxNY2/does-cognitive-therapy-encourage-bias-0", "pageUrlRelative": "/posts/xnoHu4qZavy9WxNY2/does-cognitive-therapy-encourage-bias-0", "linkUrl": "https://www.lesswrong.com/posts/xnoHu4qZavy9WxNY2/does-cognitive-therapy-encourage-bias-0", "postedAtFormatted": "Monday, November 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20cognitive%20therapy%20encourage%20bias%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20cognitive%20therapy%20encourage%20bias%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxnoHu4qZavy9WxNY2%2Fdoes-cognitive-therapy-encourage-bias-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20cognitive%20therapy%20encourage%20bias%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxnoHu4qZavy9WxNY2%2Fdoes-cognitive-therapy-encourage-bias-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxnoHu4qZavy9WxNY2%2Fdoes-cognitive-therapy-encourage-bias-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 600, "htmlBody": "<p>Summary: Cognitive therapy may encourage [motivated cognition](<a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\">http://lesswrong.com/lw/km/motivated_stopping_and_motivated_continuation/</a>). My main source for this post is Judith Beck's [Cognitive Therapy: Basics and Beyond](<a href=\"http://www.amazon.com/Cognitive-Therapy-Judith-Beck-Phd/dp/0898628474/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1290418167&amp;sr=1-1\">http://www.amazon.com/Cognitive-Therapy-Judith-Beck-Phd/dp/0898628474/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1290418167&amp;sr=1-1</a>)</p>\r\n<p>\"[Cognitive behavioral therapy](<a href=\"http://en.wikipedia.org/wiki/Cognitive_behavioral_therapy\">http://en.wikipedia.org/wiki/Cognitive_behavioral_therapy</a>)\" (CBT) is a catch-all term for a variety of therapeutic practices and theories. Among other things, it aims to teach patients to modify their own beliefs. The rationale seems to be this:</p>\r\n<p>(1) Affect, behavior, and cognition are interrelated such that changes in one of the three will lead to changes in the other two.&nbsp;</p>\r\n<p>(2) Affective problems, such as depression, can thus be addressed in a roundabout fashion: modifying the beliefs from which the undesired feelings stem.</p>\r\n<p>So far, so good.&nbsp;And how does one modify destructive beliefs? CBT offers many techniques.</p>\r\n<p>Alas, included among them seems to be motivated skepticism. For example, consider a depressed college student. She and her therapist decide that one of her bad beliefs is \"I'm inadequate.\" They want to replace that bad one with a more positive one, namely, \"I'm adequate in most ways (but I'm only human, too).\" Their method is to do a worksheet comparing evidence for and against the old, negative belief. Listen to their dialog:</p>\r\n<p style=\"padding-left: 30px;\">[Therapist]: What evidence do you have that you're inadequate?</p>\r\n<p style=\"padding-left: 30px;\">[Patient]: Well, I didn't understand a concept my economics professor presented in class today.</p>\r\n<p style=\"padding-left: 30px;\">T: Okay, write that down on the right side, then put a big \"BUT\" next to it...Now, let's see if there could be another explanation for why you might not have understood the concept <em>other</em> than that you're inadequate.</p>\r\n<p style=\"padding-left: 30px;\">P: Well, it was the first time she talked about it. And it wasn't in the readings.</p>\r\n<p>Thus the bad belief is treated with suspicion.&nbsp;What's wrong with that? Well, see what they do about evidence <em>against</em> her inadequacy:</p>\r\n<p style=\"padding-left: 30px;\">&nbsp;T: Okay, let's try the left side now. What evidence do you have from <em>today</em> that you <em>are</em> adequate at many things? I'll warn you, this can be hard if your screen is operating.</p>\r\n<p style=\"padding-left: 30px;\">P: Well, I worked on my literature paper.</p>\r\n<p style=\"padding-left: 30px;\">T: Good. Write that down. What else?</p>\r\n<p style=\"padding-left: 30px;\">(pp. 179-180; ellipsis and emphasis both in the original)</p>\r\n<p>When they encounter evidence for the patient's bad belief, they investigate further, looking for ways to avoid inferring that she is inadequate. However, when they find evidence against the bad belief, they just chalk it up.</p>\r\n<p>This is not how one should approach evidence...assuming one wants correct beliefs.</p>\r\n<p>So why does Beck advocate this approach? Here are some possible reasons.</p>\r\n<p>A. If beliefs are keeping you depressed, maybe you should fight them even at the cost of a little correctness (and of the increased habituation to motivated cognition).</p>\r\n<p>B. Depressed patients are already predisposed to find the downside of any given event. They don't need help doubting themselves. Therefore, therapists' encouraging them to seek alternative explanations for negative events doesn't skew their beliefs. On the contrary, it helps to bring the depressed patients' beliefs back into correspondence with reality.</p>\r\n<p>C. Strictly speaking, this motivated cognition does not lead to false beliefs because beliefs of the form \"I'm inadequate,\" along with its more helpful replacement, are not truth-apt. They can't be true or false. After all, what experiences do they induce believers to [anticipate] (<a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/</a>)? (If this were the rationale, then what&nbsp;would the sense of the term \"evidence\" be in this context?)</p>\r\n<p>What do you guys think? Is this common to other CBT authors as well?&nbsp;I've only read two other books in this vein (Albert Ellis and Robert A. Harper's [A Guide to Rational Living](<a href=\"http://www.amazon.com/Guide-Rational-Living-Albert-Ellis/dp/0879800429/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1290418180&amp;sr=1-1\">http://www.amazon.com/Guide-Rational-Living-Albert-Ellis/dp/0879800429/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1290418180&amp;sr=1-1</a>) and Jacqueline Persons' [Cognitive Therapy in Practice: A Case Formulation Approach](<a href=\"http://www.amazon.com/Cognitive-Therapy-Practice-Formulation-Approach/dp/0393700771/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1290420954&amp;sr=1-2\">http://www.amazon.com/Cognitive-Therapy-Practice-Formulation-Approach/dp/0393700771/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1290420954&amp;sr=1-2</a>)) and I can't recall either one explicitly doing this, but I may have missed it. I do remember that Ellis and Harper seemed to conflate instrumental and epistemic rationality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xnoHu4qZavy9WxNY2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "4064", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["L32LHWzy9FzSDazEg", "a7n8GdKiAZRX86T5A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}